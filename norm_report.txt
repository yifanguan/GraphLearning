Experiment run
dataset: cora
num_mp_layers: 3
mp_hidden_dim: 4000
optimizer_lr: 0.001
loss_func: CrossEntropyLoss
total_epoch: 500
energy_lambda: 0
Norm before input: 0.2552422881126404
Norm after input: 0.9947524070739746
Norm after each mp layer: 0.6566787362098694
Norm after each mp layer: 0.6222918033599854
Norm after each mp layer: 0.6184736490249634
Norm before input: 0.2552422881126404
Norm after input: 1.0018221139907837
Norm after each mp layer: 1.8246673345565796
Norm after each mp layer: 1.6395460367202759
Norm after each mp layer: 1.6299455165863037
Norm before input: 0.2552422881126404
Norm after input: 1.0018221139907837
Norm after each mp layer: 1.8246673345565796
Norm after each mp layer: 1.6395460367202759
Norm after each mp layer: 1.6299455165863037
Norm before input: 0.2552422881126404
Norm after input: 1.0543195009231567
Norm after each mp layer: 2.7995738983154297
Norm after each mp layer: 5.275699615478516
Norm after each mp layer: 14.316189765930176
Norm before input: 0.2552422881126404
Norm after input: 1.0543195009231567
Norm after each mp layer: 2.7995738983154297
Norm after each mp layer: 5.275699615478516
Norm after each mp layer: 14.316189765930176
Norm before input: 0.2552422881126404
Norm after input: 1.1246622800827026
Norm after each mp layer: 4.174091815948486
Norm after each mp layer: 12.078545570373535
Norm after each mp layer: 44.793678283691406
Norm before input: 0.2552422881126404
Norm after input: 1.1246622800827026
Norm after each mp layer: 4.174091815948486
Norm after each mp layer: 12.078545570373535
Norm after each mp layer: 44.793678283691406
Norm before input: 0.2552422881126404
Norm after input: 1.1186320781707764
Norm after each mp layer: 4.360568523406982
Norm after each mp layer: 13.666961669921875
Norm after each mp layer: 52.45283508300781
Norm before input: 0.2552422881126404
Norm after input: 1.1186320781707764
Norm after each mp layer: 4.360568523406982
Norm after each mp layer: 13.666961669921875
Norm after each mp layer: 52.45283508300781
Norm before input: 0.2552422881126404
Norm after input: 1.0755409002304077
Norm after each mp layer: 4.004382610321045
Norm after each mp layer: 10.638557434082031
Norm after each mp layer: 38.634864807128906
Epoch: 05, Loss: 3.9547, Energy: 418061.6875, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 1.0755409002304077
Norm after each mp layer: 4.004382610321045
Norm after each mp layer: 10.638557434082031
Norm after each mp layer: 38.634864807128906
Norm before input: 0.2552422881126404
Norm after input: 1.015497088432312
Norm after each mp layer: 3.5913965702056885
Norm after each mp layer: 7.946715831756592
Norm after each mp layer: 17.750818252563477
Norm before input: 0.2552422881126404
Norm after input: 1.015497088432312
Norm after each mp layer: 3.5913965702056885
Norm after each mp layer: 7.946715831756592
Norm after each mp layer: 17.750818252563477
Norm before input: 0.2552422881126404
Norm after input: 0.977205753326416
Norm after each mp layer: 3.4075663089752197
Norm after each mp layer: 8.611185073852539
Norm after each mp layer: 23.443235397338867
Norm before input: 0.2552422881126404
Norm after input: 0.977205753326416
Norm after each mp layer: 3.4075663089752197
Norm after each mp layer: 8.611185073852539
Norm after each mp layer: 23.443235397338867
Norm before input: 0.2552422881126404
Norm after input: 0.9453364610671997
Norm after each mp layer: 3.3119471073150635
Norm after each mp layer: 9.659045219421387
Norm after each mp layer: 26.465518951416016
Norm before input: 0.2552422881126404
Norm after input: 0.9453364610671997
Norm after each mp layer: 3.3119471073150635
Norm after each mp layer: 9.659045219421387
Norm after each mp layer: 26.465518951416016
Norm before input: 0.2552422881126404
Norm after input: 0.90815269947052
Norm after each mp layer: 3.1361987590789795
Norm after each mp layer: 9.609097480773926
Norm after each mp layer: 31.754072189331055
Norm before input: 0.2552422881126404
Norm after input: 0.90815269947052
Norm after each mp layer: 3.1361987590789795
Norm after each mp layer: 9.609097480773926
Norm after each mp layer: 31.754072189331055
Norm before input: 0.2552422881126404
Norm after input: 0.8714064955711365
Norm after each mp layer: 2.947901487350464
Norm after each mp layer: 9.505642890930176
Norm after each mp layer: 33.57133483886719
Epoch: 10, Loss: 2.3898, Energy: 140779.7344, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.8714064955711365
Norm after each mp layer: 2.947901487350464
Norm after each mp layer: 9.505642890930176
Norm after each mp layer: 33.57133483886719
Norm before input: 0.2552422881126404
Norm after input: 0.8286895155906677
Norm after each mp layer: 2.677910566329956
Norm after each mp layer: 8.763093948364258
Norm after each mp layer: 31.630191802978516
Norm before input: 0.2552422881126404
Norm after input: 0.8286895155906677
Norm after each mp layer: 2.677910566329956
Norm after each mp layer: 8.763093948364258
Norm after each mp layer: 31.630191802978516
Norm before input: 0.2552422881126404
Norm after input: 0.7887349724769592
Norm after each mp layer: 2.4397547245025635
Norm after each mp layer: 8.152522087097168
Norm after each mp layer: 28.959630966186523
Norm before input: 0.2552422881126404
Norm after input: 0.7887349724769592
Norm after each mp layer: 2.4397547245025635
Norm after each mp layer: 8.152522087097168
Norm after each mp layer: 28.959630966186523
Norm before input: 0.2552422881126404
Norm after input: 0.7432724833488464
Norm after each mp layer: 2.1467669010162354
Norm after each mp layer: 7.0584917068481445
Norm after each mp layer: 25.921077728271484
Norm before input: 0.2552422881126404
Norm after input: 0.7432724833488464
Norm after each mp layer: 2.1467669010162354
Norm after each mp layer: 7.0584917068481445
Norm after each mp layer: 25.921077728271484
Norm before input: 0.2552422881126404
Norm after input: 0.7018876075744629
Norm after each mp layer: 1.8808116912841797
Norm after each mp layer: 6.063938140869141
Norm after each mp layer: 23.318145751953125
Norm before input: 0.2552422881126404
Norm after input: 0.7018876075744629
Norm after each mp layer: 1.8808116912841797
Norm after each mp layer: 6.063938140869141
Norm after each mp layer: 23.318143844604492
Norm before input: 0.2552422881126404
Norm after input: 0.6696938872337341
Norm after each mp layer: 1.6983615159988403
Norm after each mp layer: 5.417511463165283
Norm after each mp layer: 21.19574737548828
Epoch: 15, Loss: 1.9592, Energy: 71810.8125, Train: 25.50%, Valid: 23.60%, Test: 22.00%, Best Valid: 35.20%, Best Test: 35.10%
Norm before input: 0.2552422881126404
Norm after input: 0.6696938872337341
Norm after each mp layer: 1.6983615159988403
Norm after each mp layer: 5.417511463165283
Norm after each mp layer: 21.19574737548828
Norm before input: 0.2552422881126404
Norm after input: 0.6419596672058105
Norm after each mp layer: 1.5502268075942993
Norm after each mp layer: 4.892458438873291
Norm after each mp layer: 19.49121856689453
Norm before input: 0.2552422881126404
Norm after input: 0.6419596672058105
Norm after each mp layer: 1.5502268075942993
Norm after each mp layer: 4.892458438873291
Norm after each mp layer: 19.49121856689453
Norm before input: 0.2552422881126404
Norm after input: 0.614952027797699
Norm after each mp layer: 1.3987207412719727
Norm after each mp layer: 4.353885173797607
Norm after each mp layer: 17.811279296875
Norm before input: 0.2552422881126404
Norm after input: 0.614952027797699
Norm after each mp layer: 1.3987207412719727
Norm after each mp layer: 4.353885173797607
Norm after each mp layer: 17.811279296875
Norm before input: 0.2552422881126404
Norm after input: 0.5922397971153259
Norm after each mp layer: 1.275671362876892
Norm after each mp layer: 3.9579567909240723
Norm after each mp layer: 16.589859008789062
Norm before input: 0.2552422881126404
Norm after input: 0.5922397971153259
Norm after each mp layer: 1.275671362876892
Norm after each mp layer: 3.9579567909240723
Norm after each mp layer: 16.589859008789062
Norm before input: 0.2552422881126404
Norm after input: 0.577430784702301
Norm after each mp layer: 1.223608374595642
Norm after each mp layer: 3.847686767578125
Norm after each mp layer: 16.447355270385742
Norm before input: 0.2552422881126404
Norm after input: 0.577430784702301
Norm after each mp layer: 1.223608374595642
Norm after each mp layer: 3.847686767578125
Norm after each mp layer: 16.447355270385742
Norm before input: 0.2552422881126404
Norm after input: 0.5688997507095337
Norm after each mp layer: 1.2256981134414673
Norm after each mp layer: 3.955305337905884
Norm after each mp layer: 17.27399253845215
Epoch: 20, Loss: 1.6382, Energy: 32848.3047, Train: 39.16%, Valid: 40.80%, Test: 40.90%, Best Valid: 40.80%, Best Test: 40.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5688997507095337
Norm after each mp layer: 1.2256981134414673
Norm after each mp layer: 3.955305337905884
Norm after each mp layer: 17.27399253845215
Norm before input: 0.2552422881126404
Norm after input: 0.5627244114875793
Norm after each mp layer: 1.242649793624878
Norm after each mp layer: 4.144487380981445
Norm after each mp layer: 18.511680603027344
Norm before input: 0.2552422881126404
Norm after input: 0.5627244114875793
Norm after each mp layer: 1.242649793624878
Norm after each mp layer: 4.144487380981445
Norm after each mp layer: 18.511680603027344
Norm before input: 0.2552422881126404
Norm after input: 0.5560600757598877
Norm after each mp layer: 1.2478116750717163
Norm after each mp layer: 4.291601181030273
Norm after each mp layer: 19.50949478149414
Norm before input: 0.2552422881126404
Norm after input: 0.5560600757598877
Norm after each mp layer: 1.2478116750717163
Norm after each mp layer: 4.291601181030273
Norm after each mp layer: 19.50949478149414
Norm before input: 0.2552422881126404
Norm after input: 0.5499969124794006
Norm after each mp layer: 1.2546464204788208
Norm after each mp layer: 4.438795566558838
Norm after each mp layer: 20.47503089904785
Norm before input: 0.2552422881126404
Norm after input: 0.5499969124794006
Norm after each mp layer: 1.2546464204788208
Norm after each mp layer: 4.438795566558838
Norm after each mp layer: 20.47503089904785
Norm before input: 0.2552422881126404
Norm after input: 0.5457466840744019
Norm after each mp layer: 1.2752034664154053
Norm after each mp layer: 4.630591869354248
Norm after each mp layer: 21.76247215270996
Norm before input: 0.2552422881126404
Norm after input: 0.5457466840744019
Norm after each mp layer: 1.2752034664154053
Norm after each mp layer: 4.630591869354248
Norm after each mp layer: 21.76247215270996
Norm before input: 0.2552422881126404
Norm after input: 0.5428598523139954
Norm after each mp layer: 1.3049265146255493
Norm after each mp layer: 4.854841709136963
Norm after each mp layer: 23.398347854614258
Epoch: 25, Loss: 1.3724, Energy: 56458.2695, Train: 43.63%, Valid: 39.00%, Test: 41.60%, Best Valid: 45.80%, Best Test: 46.20%
Norm before input: 0.2552422881126404
Norm after input: 0.5428598523139954
Norm after each mp layer: 1.3049265146255493
Norm after each mp layer: 4.854841709136963
Norm after each mp layer: 23.398347854614258
Norm before input: 0.2552422881126404
Norm after input: 0.5403755903244019
Norm after each mp layer: 1.3371195793151855
Norm after each mp layer: 5.095489025115967
Norm after each mp layer: 25.250883102416992
Norm before input: 0.2552422881126404
Norm after input: 0.5403755903244019
Norm after each mp layer: 1.3371195793151855
Norm after each mp layer: 5.095489025115967
Norm after each mp layer: 25.250883102416992
Norm before input: 0.2552422881126404
Norm after input: 0.5362920165061951
Norm after each mp layer: 1.3543158769607544
Norm after each mp layer: 5.264960289001465
Norm after each mp layer: 26.63152503967285
Norm before input: 0.2552422881126404
Norm after input: 0.5362920165061951
Norm after each mp layer: 1.3543158769607544
Norm after each mp layer: 5.264960289001465
Norm after each mp layer: 26.63152503967285
Norm before input: 0.2552422881126404
Norm after input: 0.5305492877960205
Norm after each mp layer: 1.354354977607727
Norm after each mp layer: 5.336799144744873
Norm after each mp layer: 27.262805938720703
Norm before input: 0.2552422881126404
Norm after input: 0.5305492877960205
Norm after each mp layer: 1.354354977607727
Norm after each mp layer: 5.336799144744873
Norm after each mp layer: 27.262805938720703
Norm before input: 0.2552422881126404
Norm after input: 0.5264998078346252
Norm after each mp layer: 1.3628722429275513
Norm after each mp layer: 5.440557479858398
Norm after each mp layer: 28.015186309814453
Norm before input: 0.2552422881126404
Norm after input: 0.5264998078346252
Norm after each mp layer: 1.3628722429275513
Norm after each mp layer: 5.440557479858398
Norm after each mp layer: 28.015186309814453
Norm before input: 0.2552422881126404
Norm after input: 0.5258817672729492
Norm after each mp layer: 1.391257643699646
Norm after each mp layer: 5.644186496734619
Norm after each mp layer: 29.456369400024414
Epoch: 30, Loss: 1.2198, Energy: 88690.1172, Train: 47.52%, Valid: 47.60%, Test: 47.00%, Best Valid: 47.60%, Best Test: 47.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5258817672729492
Norm after each mp layer: 1.391257643699646
Norm after each mp layer: 5.644186496734619
Norm after each mp layer: 29.456369400024414
Norm before input: 0.2552422881126404
Norm after input: 0.5266929268836975
Norm after each mp layer: 1.4250520467758179
Norm after each mp layer: 5.87478494644165
Norm after each mp layer: 31.030372619628906
Norm before input: 0.2552422881126404
Norm after input: 0.5266929268836975
Norm after each mp layer: 1.4250520467758179
Norm after each mp layer: 5.87478494644165
Norm after each mp layer: 31.030372619628906
Norm before input: 0.2552422881126404
Norm after input: 0.5260931253433228
Norm after each mp layer: 1.443974494934082
Norm after each mp layer: 6.0119948387146
Norm after each mp layer: 31.857097625732422
Norm before input: 0.2552422881126404
Norm after input: 0.5260931253433228
Norm after each mp layer: 1.443974494934082
Norm after each mp layer: 6.0119948387146
Norm after each mp layer: 31.857101440429688
Norm before input: 0.2552422881126404
Norm after input: 0.5246114134788513
Norm after each mp layer: 1.457724690437317
Norm after each mp layer: 6.10805082321167
Norm after each mp layer: 32.39729309082031
Norm before input: 0.2552422881126404
Norm after input: 0.5246114134788513
Norm after each mp layer: 1.457724690437317
Norm after each mp layer: 6.10805082321167
Norm after each mp layer: 32.39729309082031
Norm before input: 0.2552422881126404
Norm after input: 0.5239335894584656
Norm after each mp layer: 1.479476809501648
Norm after each mp layer: 6.2526044845581055
Norm after each mp layer: 33.41175079345703
Norm before input: 0.2552422881126404
Norm after input: 0.5239335894584656
Norm after each mp layer: 1.479476809501648
Norm after each mp layer: 6.2526044845581055
Norm after each mp layer: 33.41175079345703
Norm before input: 0.2552422881126404
Norm after input: 0.5239233374595642
Norm after each mp layer: 1.5059044361114502
Norm after each mp layer: 6.436402320861816
Norm after each mp layer: 34.81055450439453
Epoch: 35, Loss: 1.0423, Energy: 124669.3047, Train: 52.57%, Valid: 50.40%, Test: 51.40%, Best Valid: 52.60%, Best Test: 51.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5239233374595642
Norm after each mp layer: 1.5059044361114502
Norm after each mp layer: 6.436402320861816
Norm after each mp layer: 34.81055450439453
Norm before input: 0.2552422881126404
Norm after input: 0.521998941898346
Norm after each mp layer: 1.517602562904358
Norm after each mp layer: 6.527888298034668
Norm after each mp layer: 35.42649459838867
Norm before input: 0.2552422881126404
Norm after input: 0.521998941898346
Norm after each mp layer: 1.517602562904358
Norm after each mp layer: 6.527888298034668
Norm after each mp layer: 35.42649459838867
Norm before input: 0.2552422881126404
Norm after input: 0.5196884870529175
Norm after each mp layer: 1.524641752243042
Norm after each mp layer: 6.587050437927246
Norm after each mp layer: 35.70793533325195
Norm before input: 0.2552422881126404
Norm after input: 0.5196884870529175
Norm after each mp layer: 1.524641752243042
Norm after each mp layer: 6.587050437927246
Norm after each mp layer: 35.70793533325195
Norm before input: 0.2552422881126404
Norm after input: 0.5199768543243408
Norm after each mp layer: 1.5446380376815796
Norm after each mp layer: 6.741466999053955
Norm after each mp layer: 36.84312438964844
Norm before input: 0.2552422881126404
Norm after input: 0.5199768543243408
Norm after each mp layer: 1.5446380376815796
Norm after each mp layer: 6.741466999053955
Norm after each mp layer: 36.84312438964844
Norm before input: 0.2552422881126404
Norm after input: 0.5204029679298401
Norm after each mp layer: 1.563073754310608
Norm after each mp layer: 6.8810133934021
Norm after each mp layer: 37.770668029785156
Norm before input: 0.2552422881126404
Norm after input: 0.5204029679298401
Norm after each mp layer: 1.563073754310608
Norm after each mp layer: 6.8810133934021
Norm after each mp layer: 37.770668029785156
Norm before input: 0.2552422881126404
Norm after input: 0.5193663239479065
Norm after each mp layer: 1.571457028388977
Norm after each mp layer: 6.936548709869385
Norm after each mp layer: 37.87112045288086
Epoch: 40, Loss: 0.9425, Energy: 146130.2969, Train: 64.49%, Valid: 58.20%, Test: 57.40%, Best Valid: 58.20%, Best Test: 57.80%
Norm before input: 0.2552422881126404
Norm after input: 0.5193663239479065
Norm after each mp layer: 1.571457028388977
Norm after each mp layer: 6.936548709869385
Norm after each mp layer: 37.87112045288086
Norm before input: 0.2552422881126404
Norm after input: 0.5196282267570496
Norm after each mp layer: 1.5884599685668945
Norm after each mp layer: 7.051360130310059
Norm after each mp layer: 38.52505874633789
Norm before input: 0.2552422881126404
Norm after input: 0.5196282267570496
Norm after each mp layer: 1.5884599685668945
Norm after each mp layer: 7.051360130310059
Norm after each mp layer: 38.52505874633789
Norm before input: 0.2552422881126404
Norm after input: 0.520583987236023
Norm after each mp layer: 1.61078679561615
Norm after each mp layer: 7.209710597991943
Norm after each mp layer: 39.66489791870117
Norm before input: 0.2552422881126404
Norm after input: 0.520583987236023
Norm after each mp layer: 1.61078679561615
Norm after each mp layer: 7.209710597991943
Norm after each mp layer: 39.66489791870117
Norm before input: 0.2552422881126404
Norm after input: 0.5194492936134338
Norm after each mp layer: 1.6233465671539307
Norm after each mp layer: 7.2881999015808105
Norm after each mp layer: 40.10078811645508
Norm before input: 0.2552422881126404
Norm after input: 0.5194492936134338
Norm after each mp layer: 1.6233465671539307
Norm after each mp layer: 7.2881999015808105
Norm after each mp layer: 40.10078811645508
Norm before input: 0.2552422881126404
Norm after input: 0.5183505415916443
Norm after each mp layer: 1.6365960836410522
Norm after each mp layer: 7.381740570068359
Norm after each mp layer: 40.71555709838867
Norm before input: 0.2552422881126404
Norm after input: 0.5183505415916443
Norm after each mp layer: 1.6365960836410522
Norm after each mp layer: 7.381740570068359
Norm after each mp layer: 40.71555709838867
Norm before input: 0.2552422881126404
Norm after input: 0.5181170701980591
Norm after each mp layer: 1.6531262397766113
Norm after each mp layer: 7.513121604919434
Norm after each mp layer: 41.74298858642578
Epoch: 45, Loss: 0.8823, Energy: 156820.3594, Train: 62.17%, Valid: 56.00%, Test: 57.50%, Best Valid: 58.40%, Best Test: 60.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5181170701980591
Norm after each mp layer: 1.6531262397766113
Norm after each mp layer: 7.513121604919434
Norm after each mp layer: 41.74298858642578
Norm before input: 0.2552422881126404
Norm after input: 0.5165719389915466
Norm after each mp layer: 1.6607972383499146
Norm after each mp layer: 7.569331645965576
Norm after each mp layer: 41.998451232910156
Norm before input: 0.2552422881126404
Norm after input: 0.5165719389915466
Norm after each mp layer: 1.6607972383499146
Norm after each mp layer: 7.569331645965576
Norm after each mp layer: 41.998451232910156
Norm before input: 0.2552422881126404
Norm after input: 0.5161795616149902
Norm after each mp layer: 1.6721458435058594
Norm after each mp layer: 7.652246952056885
Norm after each mp layer: 42.478816986083984
Norm before input: 0.2552422881126404
Norm after input: 0.5161795616149902
Norm after each mp layer: 1.6721458435058594
Norm after each mp layer: 7.652246952056885
Norm after each mp layer: 42.478816986083984
Norm before input: 0.2552422881126404
Norm after input: 0.5170651078224182
Norm after each mp layer: 1.6881898641586304
Norm after each mp layer: 7.770386695861816
Norm after each mp layer: 43.2708625793457
Norm before input: 0.2552422881126404
Norm after input: 0.5170651078224182
Norm after each mp layer: 1.6881898641586304
Norm after each mp layer: 7.770386695861816
Norm after each mp layer: 43.2708625793457
Norm before input: 0.2552422881126404
Norm after input: 0.5167161822319031
Norm after each mp layer: 1.6983298063278198
Norm after each mp layer: 7.823276519775391
Norm after each mp layer: 43.35997009277344
Norm before input: 0.2552422881126404
Norm after input: 0.5167161822319031
Norm after each mp layer: 1.6983298063278198
Norm after each mp layer: 7.823276519775391
Norm after each mp layer: 43.35997009277344
Norm before input: 0.2552422881126404
Norm after input: 0.5182921886444092
Norm after each mp layer: 1.7176107168197632
Norm after each mp layer: 7.969951629638672
Norm after each mp layer: 44.45576477050781
Epoch: 50, Loss: 0.8191, Energy: 180414.1562, Train: 69.62%, Valid: 61.20%, Test: 59.30%, Best Valid: 61.20%, Best Test: 60.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5182921886444092
Norm after each mp layer: 1.7176107168197632
Norm after each mp layer: 7.969951629638672
Norm after each mp layer: 44.45576095581055
Norm before input: 0.2552422881126404
Norm after input: 0.5181190967559814
Norm after each mp layer: 1.730233907699585
Norm after each mp layer: 8.069661140441895
Norm after each mp layer: 45.09639358520508
Norm before input: 0.2552422881126404
Norm after input: 0.5181190967559814
Norm after each mp layer: 1.730233907699585
Norm after each mp layer: 8.069661140441895
Norm after each mp layer: 45.09639358520508
Norm before input: 0.2552422881126404
Norm after input: 0.5165599584579468
Norm after each mp layer: 1.7377954721450806
Norm after each mp layer: 8.138643264770508
Norm after each mp layer: 45.44160079956055
Norm before input: 0.2552422881126404
Norm after input: 0.5165599584579468
Norm after each mp layer: 1.7377954721450806
Norm after each mp layer: 8.138643264770508
Norm after each mp layer: 45.44160079956055
Norm before input: 0.2552422881126404
Norm after input: 0.5171261429786682
Norm after each mp layer: 1.7557876110076904
Norm after each mp layer: 8.31562328338623
Norm after each mp layer: 46.86003875732422
Norm before input: 0.2552422881126404
Norm after input: 0.5171261429786682
Norm after each mp layer: 1.7557876110076904
Norm after each mp layer: 8.31562328338623
Norm after each mp layer: 46.86003875732422
Norm before input: 0.2552422881126404
Norm after input: 0.5146973729133606
Norm after each mp layer: 1.7596454620361328
Norm after each mp layer: 8.357887268066406
Norm after each mp layer: 46.878116607666016
Norm before input: 0.2552422881126404
Norm after input: 0.5146973729133606
Norm after each mp layer: 1.7596454620361328
Norm after each mp layer: 8.357887268066406
Norm after each mp layer: 46.87812042236328
Norm before input: 0.2552422881126404
Norm after input: 0.5158645510673523
Norm after each mp layer: 1.780155897140503
Norm after each mp layer: 8.542930603027344
Norm after each mp layer: 48.23551559448242
Epoch: 55, Loss: 0.7501, Energy: 202210.0469, Train: 75.17%, Valid: 62.20%, Test: 62.50%, Best Valid: 63.60%, Best Test: 62.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5158645510673523
Norm after each mp layer: 1.780155897140503
Norm after each mp layer: 8.542930603027344
Norm after each mp layer: 48.23551559448242
Norm before input: 0.2552422881126404
Norm after input: 0.5147154927253723
Norm after each mp layer: 1.7890558242797852
Norm after each mp layer: 8.61065673828125
Norm after each mp layer: 48.42313003540039
Norm before input: 0.2552422881126404
Norm after input: 0.5147154927253723
Norm after each mp layer: 1.7890558242797852
Norm after each mp layer: 8.61065673828125
Norm after each mp layer: 48.42313766479492
Norm before input: 0.2552422881126404
Norm after input: 0.515591561794281
Norm after each mp layer: 1.8078962564468384
Norm after each mp layer: 8.774008750915527
Norm after each mp layer: 49.53372573852539
Norm before input: 0.2552422881126404
Norm after input: 0.515591561794281
Norm after each mp layer: 1.8078962564468384
Norm after each mp layer: 8.774008750915527
Norm after each mp layer: 49.53372573852539
Norm before input: 0.2552422881126404
Norm after input: 0.5149547457695007
Norm after each mp layer: 1.819991111755371
Norm after each mp layer: 8.88368034362793
Norm after each mp layer: 50.16459274291992
Norm before input: 0.2552422881126404
Norm after input: 0.5149547457695007
Norm after each mp layer: 1.819991111755371
Norm after each mp layer: 8.88368034362793
Norm after each mp layer: 50.16459274291992
Norm before input: 0.2552422881126404
Norm after input: 0.5144806504249573
Norm after each mp layer: 1.8332324028015137
Norm after each mp layer: 9.017967224121094
Norm after each mp layer: 51.05712890625
Norm before input: 0.2552422881126404
Norm after input: 0.5144806504249573
Norm after each mp layer: 1.8332324028015137
Norm after each mp layer: 9.017967224121094
Norm after each mp layer: 51.05712890625
Norm before input: 0.2552422881126404
Norm after input: 0.5139490962028503
Norm after each mp layer: 1.8460979461669922
Norm after each mp layer: 9.158228874206543
Norm after each mp layer: 52.003482818603516
Epoch: 60, Loss: 0.6748, Energy: 243058.9062, Train: 78.81%, Valid: 64.80%, Test: 64.60%, Best Valid: 64.80%, Best Test: 64.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5139490962028503
Norm after each mp layer: 1.8460979461669922
Norm after each mp layer: 9.158228874206543
Norm after each mp layer: 52.003482818603516
Norm before input: 0.2552422881126404
Norm after input: 0.5127056837081909
Norm after each mp layer: 1.8548341989517212
Norm after each mp layer: 9.263531684875488
Norm after each mp layer: 52.60110092163086
Norm before input: 0.2552422881126404
Norm after input: 0.5127056837081909
Norm after each mp layer: 1.8548344373703003
Norm after each mp layer: 9.263531684875488
Norm after each mp layer: 52.60110092163086
Norm before input: 0.2552422881126404
Norm after input: 0.5132821202278137
Norm after each mp layer: 1.8729238510131836
Norm after each mp layer: 9.456347465515137
Norm after each mp layer: 53.9426383972168
Norm before input: 0.2552422881126404
Norm after input: 0.5132821202278137
Norm after each mp layer: 1.8729238510131836
Norm after each mp layer: 9.456347465515137
Norm after each mp layer: 53.9426383972168
Norm before input: 0.2552422881126404
Norm after input: 0.5106503367424011
Norm after each mp layer: 1.8744237422943115
Norm after each mp layer: 9.489669799804688
Norm after each mp layer: 53.828826904296875
Norm before input: 0.2552422881126404
Norm after input: 0.5106503367424011
Norm after each mp layer: 1.8744237422943115
Norm after each mp layer: 9.489669799804688
Norm after each mp layer: 53.828826904296875
Norm before input: 0.2552422881126404
Norm after input: 0.5145197510719299
Norm after each mp layer: 1.91250479221344
Norm after each mp layer: 9.85703182220459
Norm after each mp layer: 56.605220794677734
Norm before input: 0.2552422881126404
Norm after input: 0.5145197510719299
Norm after each mp layer: 1.91250479221344
Norm after each mp layer: 9.85703182220459
Norm after each mp layer: 56.605220794677734
Norm before input: 0.2552422881126404
Norm after input: 0.507114589214325
Norm after each mp layer: 1.8927541971206665
Norm after each mp layer: 9.696432113647461
Norm after each mp layer: 54.82939147949219
Epoch: 65, Loss: 0.6655, Energy: 321914.2188, Train: 76.74%, Valid: 65.60%, Test: 61.60%, Best Valid: 66.00%, Best Test: 64.70%
Norm before input: 0.2552422881126404
Norm after input: 0.507114589214325
Norm after each mp layer: 1.8927541971206665
Norm after each mp layer: 9.696432113647461
Norm after each mp layer: 54.82939147949219
Norm before input: 0.2552422881126404
Norm after input: 0.5131967067718506
Norm after each mp layer: 1.9394348859786987
Norm after each mp layer: 10.125770568847656
Norm after each mp layer: 58.18442916870117
Norm before input: 0.2552422881126404
Norm after input: 0.5131967067718506
Norm after each mp layer: 1.9394348859786987
Norm after each mp layer: 10.125770568847656
Norm after each mp layer: 58.18442916870117
Norm before input: 0.2552422881126404
Norm after input: 0.508892834186554
Norm after each mp layer: 1.9325454235076904
Norm after each mp layer: 10.113304138183594
Norm after each mp layer: 57.81198501586914
Norm before input: 0.2552422881126404
Norm after input: 0.508892834186554
Norm after each mp layer: 1.9325454235076904
Norm after each mp layer: 10.113304138183594
Norm after each mp layer: 57.81198501586914
Norm before input: 0.2552422881126404
Norm after input: 0.5034743547439575
Norm after each mp layer: 1.9208558797836304
Norm after each mp layer: 10.080500602722168
Norm after each mp layer: 57.226112365722656
Norm before input: 0.2552422881126404
Norm after input: 0.5034743547439575
Norm after each mp layer: 1.9208558797836304
Norm after each mp layer: 10.080500602722168
Norm after each mp layer: 57.226112365722656
Norm before input: 0.2552422881126404
Norm after input: 0.5080147981643677
Norm after each mp layer: 1.9587764739990234
Norm after each mp layer: 10.450102806091309
Norm after each mp layer: 60.00320816040039
Norm before input: 0.2552422881126404
Norm after input: 0.5080147981643677
Norm after each mp layer: 1.9587764739990234
Norm after each mp layer: 10.450102806091309
Norm after each mp layer: 60.00320816040039
Norm before input: 0.2552422881126404
Norm after input: 0.5050951838493347
Norm after each mp layer: 1.9562151432037354
Norm after each mp layer: 10.489235877990723
Norm after each mp layer: 60.09501647949219
Epoch: 70, Loss: 0.5754, Energy: 375485.1562, Train: 83.69%, Valid: 68.20%, Test: 65.80%, Best Valid: 68.80%, Best Test: 65.80%
Norm before input: 0.2552422881126404
Norm after input: 0.5050951838493347
Norm after each mp layer: 1.9562151432037354
Norm after each mp layer: 10.489235877990723
Norm after each mp layer: 60.09501647949219
Norm before input: 0.2552422881126404
Norm after input: 0.4991534352302551
Norm after each mp layer: 1.937527060508728
Norm after each mp layer: 10.398824691772461
Norm after each mp layer: 59.1448860168457
Norm before input: 0.2552422881126404
Norm after input: 0.4991534352302551
Norm after each mp layer: 1.937527060508728
Norm after each mp layer: 10.398824691772461
Norm after each mp layer: 59.1448860168457
Norm before input: 0.2552422881126404
Norm after input: 0.5019691586494446
Norm after each mp layer: 1.9674397706985474
Norm after each mp layer: 10.670443534851074
Norm after each mp layer: 61.08299255371094
Norm before input: 0.2552422881126404
Norm after input: 0.5019691586494446
Norm after each mp layer: 1.9674397706985474
Norm after each mp layer: 10.670443534851074
Norm after each mp layer: 61.08299255371094
Norm before input: 0.2552422881126404
Norm after input: 0.502399742603302
Norm after each mp layer: 1.9846279621124268
Norm after each mp layer: 10.853357315063477
Norm after each mp layer: 62.27725601196289
Norm before input: 0.2552422881126404
Norm after input: 0.502399742603302
Norm after each mp layer: 1.9846279621124268
Norm after each mp layer: 10.853357315063477
Norm after each mp layer: 62.27725601196289
Norm before input: 0.2552422881126404
Norm after input: 0.49712926149368286
Norm after each mp layer: 1.9658598899841309
Norm after each mp layer: 10.758489608764648
Norm after each mp layer: 61.315223693847656
Norm before input: 0.2552422881126404
Norm after input: 0.49712926149368286
Norm after each mp layer: 1.9658598899841309
Norm after each mp layer: 10.758488655090332
Norm after each mp layer: 61.315223693847656
Norm before input: 0.2552422881126404
Norm after input: 0.4967333972454071
Norm after each mp layer: 1.9753810167312622
Norm after each mp layer: 10.889495849609375
Norm after each mp layer: 62.12893295288086
Epoch: 75, Loss: 0.4955, Energy: 388920.1562, Train: 85.93%, Valid: 69.00%, Test: 66.30%, Best Valid: 69.20%, Best Test: 67.30%
Norm before input: 0.2552422881126404
Norm after input: 0.4967333972454071
Norm after each mp layer: 1.9753810167312622
Norm after each mp layer: 10.889495849609375
Norm after each mp layer: 62.12893295288086
Norm before input: 0.2552422881126404
Norm after input: 0.499749094247818
Norm after each mp layer: 2.006830930709839
Norm after each mp layer: 11.19744873046875
Norm after each mp layer: 64.28297424316406
Norm before input: 0.2552422881126404
Norm after input: 0.499749094247818
Norm after each mp layer: 2.006830930709839
Norm after each mp layer: 11.19744873046875
Norm after each mp layer: 64.28297424316406
Norm before input: 0.2552422881126404
Norm after input: 0.497010201215744
Norm after each mp layer: 2.0036754608154297
Norm after each mp layer: 11.221686363220215
Norm after each mp layer: 64.2297134399414
Norm before input: 0.2552422881126404
Norm after input: 0.497010201215744
Norm after each mp layer: 2.0036754608154297
Norm after each mp layer: 11.221686363220215
Norm after each mp layer: 64.2297134399414
Norm before input: 0.2552422881126404
Norm after input: 0.4952722489833832
Norm after each mp layer: 2.0100698471069336
Norm after each mp layer: 11.308393478393555
Norm after each mp layer: 64.59178161621094
Norm before input: 0.2552422881126404
Norm after input: 0.4952722489833832
Norm after each mp layer: 2.0100698471069336
Norm after each mp layer: 11.308393478393555
Norm after each mp layer: 64.59178161621094
Norm before input: 0.2552422881126404
Norm after input: 0.4989378750324249
Norm after each mp layer: 2.0513575077056885
Norm after each mp layer: 11.66092300415039
Norm after each mp layer: 66.95707702636719
Norm before input: 0.2552422881126404
Norm after input: 0.4989378750324249
Norm after each mp layer: 2.0513575077056885
Norm after each mp layer: 11.66092300415039
Norm after each mp layer: 66.95707702636719
Norm before input: 0.2552422881126404
Norm after input: 0.49863699078559875
Norm after each mp layer: 2.065098524093628
Norm after each mp layer: 11.808995246887207
Norm after each mp layer: 67.81523132324219
Epoch: 80, Loss: 0.4238, Energy: 539532.6250, Train: 86.84%, Valid: 71.00%, Test: 68.90%, Best Valid: 71.00%, Best Test: 69.00%
Norm before input: 0.2552422881126404
Norm after input: 0.49863699078559875
Norm after each mp layer: 2.065098524093628
Norm after each mp layer: 11.808995246887207
Norm after each mp layer: 67.81523132324219
Norm before input: 0.2552422881126404
Norm after input: 0.4957534074783325
Norm after each mp layer: 2.059156894683838
Norm after each mp layer: 11.817208290100098
Norm after each mp layer: 67.66658782958984
Norm before input: 0.2552422881126404
Norm after input: 0.4957534074783325
Norm after each mp layer: 2.059156894683838
Norm after each mp layer: 11.817208290100098
Norm after each mp layer: 67.66658782958984
Norm before input: 0.2552422881126404
Norm after input: 0.49846580624580383
Norm after each mp layer: 2.090635299682617
Norm after each mp layer: 12.109882354736328
Norm after each mp layer: 69.61788177490234
Norm before input: 0.2552422881126404
Norm after input: 0.49846580624580383
Norm after each mp layer: 2.090635299682617
Norm after each mp layer: 12.109882354736328
Norm after each mp layer: 69.61788177490234
Norm before input: 0.2552422881126404
Norm after input: 0.49966883659362793
Norm after each mp layer: 2.1112430095672607
Norm after each mp layer: 12.318613052368164
Norm after each mp layer: 70.96699523925781
Norm before input: 0.2552422881126404
Norm after input: 0.49966883659362793
Norm after each mp layer: 2.1112430095672607
Norm after each mp layer: 12.318613052368164
Norm after each mp layer: 70.96699523925781
Norm before input: 0.2552422881126404
Norm after input: 0.4969445466995239
Norm after each mp layer: 2.1043214797973633
Norm after each mp layer: 12.294587135314941
Norm after each mp layer: 70.61097717285156
Norm before input: 0.2552422881126404
Norm after input: 0.4969445466995239
Norm after each mp layer: 2.1043214797973633
Norm after each mp layer: 12.294587135314941
Norm after each mp layer: 70.61097717285156
Norm before input: 0.2552422881126404
Norm after input: 0.4991278350353241
Norm after each mp layer: 2.131716251373291
Norm after each mp layer: 12.542850494384766
Norm after each mp layer: 72.26128387451172
Epoch: 85, Loss: 0.3688, Energy: 622663.1250, Train: 89.65%, Valid: 72.80%, Test: 70.30%, Best Valid: 72.80%, Best Test: 70.30%
Norm before input: 0.2552422881126404
Norm after input: 0.4991278350353241
Norm after each mp layer: 2.131716251373291
Norm after each mp layer: 12.542850494384766
Norm after each mp layer: 72.26128387451172
Norm before input: 0.2552422881126404
Norm after input: 0.5008101463317871
Norm after each mp layer: 2.1534152030944824
Norm after each mp layer: 12.764716148376465
Norm after each mp layer: 73.77857971191406
Norm before input: 0.2552422881126404
Norm after input: 0.5008101463317871
Norm after each mp layer: 2.1534152030944824
Norm after each mp layer: 12.764716148376465
Norm after each mp layer: 73.77857971191406
Norm before input: 0.2552422881126404
Norm after input: 0.4985042214393616
Norm after each mp layer: 2.1466622352600098
Norm after each mp layer: 12.737896919250488
Norm after each mp layer: 73.4601821899414
Norm before input: 0.2552422881126404
Norm after input: 0.4985042214393616
Norm after each mp layer: 2.1466622352600098
Norm after each mp layer: 12.737896919250488
Norm after each mp layer: 73.4601821899414
Norm before input: 0.2552422881126404
Norm after input: 0.5003618597984314
Norm after each mp layer: 2.170109748840332
Norm after each mp layer: 12.949109077453613
Norm after each mp layer: 74.89100646972656
Norm before input: 0.2552422881126404
Norm after input: 0.5003618597984314
Norm after each mp layer: 2.170109748840332
Norm after each mp layer: 12.949109077453613
Norm after each mp layer: 74.89100646972656
Norm before input: 0.2552422881126404
Norm after input: 0.5021429657936096
Norm after each mp layer: 2.191828966140747
Norm after each mp layer: 13.158785820007324
Norm after each mp layer: 76.37326049804688
Norm before input: 0.2552422881126404
Norm after input: 0.5021429657936096
Norm after each mp layer: 2.191828966140747
Norm after each mp layer: 13.158785820007324
Norm after each mp layer: 76.37326049804688
Norm before input: 0.2552422881126404
Norm after input: 0.5002818703651428
Norm after each mp layer: 2.187751531600952
Norm after each mp layer: 13.138717651367188
Norm after each mp layer: 76.12096405029297
Epoch: 90, Loss: 0.2955, Energy: 840048.7500, Train: 92.22%, Valid: 73.00%, Test: 72.10%, Best Valid: 74.20%, Best Test: 72.80%
Norm before input: 0.2552422881126404
Norm after input: 0.5002818703651428
Norm after each mp layer: 2.187751531600952
Norm after each mp layer: 13.138717651367188
Norm after each mp layer: 76.12096405029297
Norm before input: 0.2552422881126404
Norm after input: 0.5016741752624512
Norm after each mp layer: 2.207735776901245
Norm after each mp layer: 13.317608833312988
Norm after each mp layer: 77.3367919921875
Norm before input: 0.2552422881126404
Norm after input: 0.5016741752624512
Norm after each mp layer: 2.207735776901245
Norm after each mp layer: 13.317608833312988
Norm after each mp layer: 77.3367919921875
Norm before input: 0.2552422881126404
Norm after input: 0.5039044618606567
Norm after each mp layer: 2.2320992946624756
Norm after each mp layer: 13.549687385559082
Norm after each mp layer: 79.00945281982422
Norm before input: 0.2552422881126404
Norm after input: 0.5039044618606567
Norm after each mp layer: 2.2320992946624756
Norm after each mp layer: 13.549687385559082
Norm after each mp layer: 79.00945281982422
Norm before input: 0.2552422881126404
Norm after input: 0.502552330493927
Norm after each mp layer: 2.2308926582336426
Norm after each mp layer: 13.548666000366211
Norm after each mp layer: 78.87126922607422
Norm before input: 0.2552422881126404
Norm after input: 0.502552330493927
Norm after each mp layer: 2.2308926582336426
Norm after each mp layer: 13.548666000366211
Norm after each mp layer: 78.87126922607422
Norm before input: 0.2552422881126404
Norm after input: 0.5032911896705627
Norm after each mp layer: 2.245971441268921
Norm after each mp layer: 13.691198348999023
Norm after each mp layer: 79.88360595703125
Norm before input: 0.2552422881126404
Norm after input: 0.5032911896705627
Norm after each mp layer: 2.245971441268921
Norm after each mp layer: 13.691198348999023
Norm after each mp layer: 79.88360595703125
Norm before input: 0.2552422881126404
Norm after input: 0.5051915049552917
Norm after each mp layer: 2.268367052078247
Norm after each mp layer: 13.91085433959961
Norm after each mp layer: 81.6357192993164
Epoch: 95, Loss: 0.2240, Energy: 985489.0000, Train: 94.12%, Valid: 75.40%, Test: 75.50%, Best Valid: 75.40%, Best Test: 75.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5051915049552917
Norm after each mp layer: 2.268367052078247
Norm after each mp layer: 13.91085433959961
Norm after each mp layer: 81.63571166992188
Norm before input: 0.2552422881126404
Norm after input: 0.5043080449104309
Norm after each mp layer: 2.2699930667877197
Norm after each mp layer: 13.935032844543457
Norm after each mp layer: 81.85995483398438
Norm before input: 0.2552422881126404
Norm after input: 0.5043080449104309
Norm after each mp layer: 2.2699930667877197
Norm after each mp layer: 13.935032844543457
Norm after each mp layer: 81.85995483398438
Norm before input: 0.2552422881126404
Norm after input: 0.5039036273956299
Norm after each mp layer: 2.273519992828369
Norm after each mp layer: 13.978955268859863
Norm after each mp layer: 82.28897094726562
Norm before input: 0.2552422881126404
Norm after input: 0.5039036273956299
Norm after each mp layer: 2.273519992828369
Norm after each mp layer: 13.978955268859863
Norm after each mp layer: 82.28897094726562
Norm before input: 0.2552422881126404
Norm after input: 0.5065110325813293
Norm after each mp layer: 2.302828311920166
Norm after each mp layer: 14.228034019470215
Norm after each mp layer: 84.2518539428711
Norm before input: 0.2552422881126404
Norm after input: 0.5065110325813293
Norm after each mp layer: 2.302828311920166
Norm after each mp layer: 14.228034019470215
Norm after each mp layer: 84.2518539428711
Norm before input: 0.2552422881126404
Norm after input: 0.5058395266532898
Norm after each mp layer: 2.307018995285034
Norm after each mp layer: 14.251387596130371
Norm after each mp layer: 84.38893127441406
Norm before input: 0.2552422881126404
Norm after input: 0.5058395266532898
Norm after each mp layer: 2.307018995285034
Norm after each mp layer: 14.251387596130371
Norm after each mp layer: 84.38893127441406
Norm before input: 0.2552422881126404
Norm after input: 0.5047595500946045
Norm after each mp layer: 2.3038363456726074
Norm after each mp layer: 14.243871688842773
Norm after each mp layer: 84.49872589111328
Epoch: 100, Loss: 0.1709, Energy: 1204096.1250, Train: 95.03%, Valid: 75.80%, Test: 76.20%, Best Valid: 76.20%, Best Test: 76.20%
Norm before input: 0.2552422881126404
Norm after input: 0.5047595500946045
Norm after each mp layer: 2.3038363456726074
Norm after each mp layer: 14.243871688842773
Norm after each mp layer: 84.49872589111328
Norm before input: 0.2552422881126404
Norm after input: 0.5071761012077332
Norm after each mp layer: 2.3278632164001465
Norm after each mp layer: 14.467673301696777
Norm after each mp layer: 86.43871307373047
Norm before input: 0.2552422881126404
Norm after input: 0.5071761012077332
Norm after each mp layer: 2.3278632164001465
Norm after each mp layer: 14.467673301696777
Norm after each mp layer: 86.43871307373047
Norm before input: 0.2552422881126404
Norm after input: 0.5069485306739807
Norm after each mp layer: 2.335120916366577
Norm after each mp layer: 14.516273498535156
Norm after each mp layer: 86.70130920410156
Norm before input: 0.2552422881126404
Norm after input: 0.5069485306739807
Norm after each mp layer: 2.335120916366577
Norm after each mp layer: 14.516273498535156
Norm after each mp layer: 86.70130920410156
Norm before input: 0.2552422881126404
Norm after input: 0.5060425996780396
Norm after each mp layer: 2.3356173038482666
Norm after each mp layer: 14.520795822143555
Norm after each mp layer: 86.67489624023438
Norm before input: 0.2552422881126404
Norm after input: 0.5060425996780396
Norm after each mp layer: 2.3356173038482666
Norm after each mp layer: 14.520795822143555
Norm after each mp layer: 86.67489624023438
Norm before input: 0.2552422881126404
Norm after input: 0.5080957412719727
Norm after each mp layer: 2.3564929962158203
Norm after each mp layer: 14.71780014038086
Norm after each mp layer: 88.35417175292969
Norm before input: 0.2552422881126404
Norm after input: 0.5080957412719727
Norm after each mp layer: 2.3564929962158203
Norm after each mp layer: 14.71780014038086
Norm after each mp layer: 88.35417175292969
Norm before input: 0.2552422881126404
Norm after input: 0.5084955096244812
Norm after each mp layer: 2.3649373054504395
Norm after each mp layer: 14.806872367858887
Norm after each mp layer: 89.13092803955078
Epoch: 105, Loss: 0.1356, Energy: 1410461.2500, Train: 96.52%, Valid: 77.40%, Test: 76.60%, Best Valid: 77.40%, Best Test: 76.70%
Norm before input: 0.2552422881126404
Norm after input: 0.5084955096244812
Norm after each mp layer: 2.3649373054504395
Norm after each mp layer: 14.806872367858887
Norm after each mp layer: 89.13092803955078
Norm before input: 0.2552422881126404
Norm after input: 0.5071927905082703
Norm after each mp layer: 2.362732410430908
Norm after each mp layer: 14.783286094665527
Norm after each mp layer: 88.77074432373047
Norm before input: 0.2552422881126404
Norm after input: 0.5071927905082703
Norm after each mp layer: 2.362732410430908
Norm after each mp layer: 14.783286094665527
Norm after each mp layer: 88.77074432373047
Norm before input: 0.2552422881126404
Norm after input: 0.5095258951187134
Norm after each mp layer: 2.385714054107666
Norm after each mp layer: 14.996393203735352
Norm after each mp layer: 90.4415283203125
Norm before input: 0.2552422881126404
Norm after input: 0.5095258951187134
Norm after each mp layer: 2.385714054107666
Norm after each mp layer: 14.996393203735352
Norm after each mp layer: 90.4415283203125
Norm before input: 0.2552422881126404
Norm after input: 0.5099436640739441
Norm after each mp layer: 2.392077922821045
Norm after each mp layer: 15.077210426330566
Norm after each mp layer: 91.14556121826172
Norm before input: 0.2552422881126404
Norm after input: 0.5099436640739441
Norm after each mp layer: 2.392077922821045
Norm after each mp layer: 15.077210426330566
Norm after each mp layer: 91.14556121826172
Norm before input: 0.2552422881126404
Norm after input: 0.5080955624580383
Norm after each mp layer: 2.3814187049865723
Norm after each mp layer: 14.99906063079834
Norm after each mp layer: 90.44595336914062
Norm before input: 0.2552422881126404
Norm after input: 0.5080955624580383
Norm after each mp layer: 2.3814187049865723
Norm after each mp layer: 14.99906063079834
Norm after each mp layer: 90.44595336914062
Norm before input: 0.2552422881126404
Norm after input: 0.5096189975738525
Norm after each mp layer: 2.3976263999938965
Norm after each mp layer: 15.15287971496582
Norm after each mp layer: 91.56975555419922
Epoch: 110, Loss: 0.1156, Energy: 1530111.6250, Train: 97.43%, Valid: 77.20%, Test: 76.60%, Best Valid: 77.60%, Best Test: 76.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5096189975738525
Norm after each mp layer: 2.3976263999938965
Norm after each mp layer: 15.15287971496582
Norm after each mp layer: 91.56975555419922
Norm before input: 0.2552422881126404
Norm after input: 0.5105491280555725
Norm after each mp layer: 2.407614231109619
Norm after each mp layer: 15.2658052444458
Norm after each mp layer: 92.54637908935547
Norm before input: 0.2552422881126404
Norm after input: 0.5105491280555725
Norm after each mp layer: 2.407614231109619
Norm after each mp layer: 15.2658052444458
Norm after each mp layer: 92.54637908935547
Norm before input: 0.2552422881126404
Norm after input: 0.5093704462051392
Norm after each mp layer: 2.401341438293457
Norm after each mp layer: 15.229652404785156
Norm after each mp layer: 92.27296447753906
Norm before input: 0.2552422881126404
Norm after input: 0.5093704462051392
Norm after each mp layer: 2.401341438293457
Norm after each mp layer: 15.229652404785156
Norm after each mp layer: 92.27296447753906
Norm before input: 0.2552422881126404
Norm after input: 0.5090319514274597
Norm after each mp layer: 2.402740716934204
Norm after each mp layer: 15.259082794189453
Norm after each mp layer: 92.50153350830078
Norm before input: 0.2552422881126404
Norm after input: 0.5090319514274597
Norm after each mp layer: 2.402740716934204
Norm after each mp layer: 15.259082794189453
Norm after each mp layer: 92.50153350830078
Norm before input: 0.2552422881126404
Norm after input: 0.511391282081604
Norm after each mp layer: 2.428865671157837
Norm after each mp layer: 15.496223449707031
Norm after each mp layer: 94.2301025390625
Norm before input: 0.2552422881126404
Norm after input: 0.511391282081604
Norm after each mp layer: 2.428865671157837
Norm after each mp layer: 15.496223449707031
Norm after each mp layer: 94.2301025390625
Norm before input: 0.2552422881126404
Norm after input: 0.5112990736961365
Norm after each mp layer: 2.434863328933716
Norm after each mp layer: 15.555377006530762
Norm after each mp layer: 94.49958038330078
Epoch: 115, Loss: 0.0954, Energy: 1759831.8750, Train: 97.43%, Valid: 77.00%, Test: 76.90%, Best Valid: 77.80%, Best Test: 76.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5112990736961365
Norm after each mp layer: 2.434863328933716
Norm after each mp layer: 15.555375099182129
Norm after each mp layer: 94.49958038330078
Norm before input: 0.2552422881126404
Norm after input: 0.5100625157356262
Norm after each mp layer: 2.428447723388672
Norm after each mp layer: 15.525228500366211
Norm after each mp layer: 94.29320526123047
Norm before input: 0.2552422881126404
Norm after input: 0.5100625157356262
Norm after each mp layer: 2.428447723388672
Norm after each mp layer: 15.525228500366211
Norm after each mp layer: 94.29320526123047
Norm before input: 0.2552422881126404
Norm after input: 0.510963499546051
Norm after each mp layer: 2.43709397315979
Norm after each mp layer: 15.637955665588379
Norm after each mp layer: 95.17628479003906
Norm before input: 0.2552422881126404
Norm after input: 0.510963499546051
Norm after each mp layer: 2.43709397315979
Norm after each mp layer: 15.637955665588379
Norm after each mp layer: 95.1762924194336
Norm before input: 0.2552422881126404
Norm after input: 0.5121519565582275
Norm after each mp layer: 2.449240207672119
Norm after each mp layer: 15.77171802520752
Norm after each mp layer: 95.90355682373047
Norm before input: 0.2552422881126404
Norm after input: 0.5121519565582275
Norm after each mp layer: 2.449240207672119
Norm after each mp layer: 15.77171802520752
Norm after each mp layer: 95.90355682373047
Norm before input: 0.2552422881126404
Norm after input: 0.5115892887115479
Norm after each mp layer: 2.4503087997436523
Norm after each mp layer: 15.783026695251465
Norm after each mp layer: 95.36863708496094
Norm before input: 0.2552422881126404
Norm after input: 0.5115892887115479
Norm after each mp layer: 2.4503087997436523
Norm after each mp layer: 15.783026695251465
Norm after each mp layer: 95.36863708496094
Norm before input: 0.2552422881126404
Norm after input: 0.5115025639533997
Norm after each mp layer: 2.453610897064209
Norm after each mp layer: 15.832022666931152
Norm after each mp layer: 95.58598327636719
Epoch: 120, Loss: 0.0769, Energy: 1797219.6250, Train: 97.60%, Valid: 77.00%, Test: 77.10%, Best Valid: 77.80%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5115025639533997
Norm after each mp layer: 2.453610897064209
Norm after each mp layer: 15.832022666931152
Norm after each mp layer: 95.58598327636719
Norm before input: 0.2552422881126404
Norm after input: 0.5129910111427307
Norm after each mp layer: 2.468362331390381
Norm after each mp layer: 15.997876167297363
Norm after each mp layer: 97.10457611083984
Norm before input: 0.2552422881126404
Norm after input: 0.5129910111427307
Norm after each mp layer: 2.468362331390381
Norm after each mp layer: 15.997876167297363
Norm after each mp layer: 97.10457611083984
Norm before input: 0.2552422881126404
Norm after input: 0.5132554173469543
Norm after each mp layer: 2.473464012145996
Norm after each mp layer: 16.07087516784668
Norm after each mp layer: 97.80792999267578
Norm before input: 0.2552422881126404
Norm after input: 0.5132554173469543
Norm after each mp layer: 2.473464012145996
Norm after each mp layer: 16.07087516784668
Norm after each mp layer: 97.80792999267578
Norm before input: 0.2552422881126404
Norm after input: 0.5124618411064148
Norm after each mp layer: 2.471510410308838
Norm after each mp layer: 16.059329986572266
Norm after each mp layer: 97.41477966308594
Norm before input: 0.2552422881126404
Norm after input: 0.5124618411064148
Norm after each mp layer: 2.471510410308838
Norm after each mp layer: 16.059329986572266
Norm after each mp layer: 97.41477966308594
Norm before input: 0.2552422881126404
Norm after input: 0.5133218169212341
Norm after each mp layer: 2.483217477798462
Norm after each mp layer: 16.165128707885742
Norm after each mp layer: 97.6097640991211
Norm before input: 0.2552422881126404
Norm after input: 0.5133218169212341
Norm after each mp layer: 2.483217477798462
Norm after each mp layer: 16.165128707885742
Norm after each mp layer: 97.6097640991211
Norm before input: 0.2552422881126404
Norm after input: 0.5143025517463684
Norm after each mp layer: 2.4927048683166504
Norm after each mp layer: 16.275367736816406
Norm after each mp layer: 98.33110046386719
Epoch: 125, Loss: 0.0644, Energy: 1919569.6250, Train: 97.85%, Valid: 77.60%, Test: 76.60%, Best Valid: 77.80%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5143025517463684
Norm after each mp layer: 2.4927048683166504
Norm after each mp layer: 16.275367736816406
Norm after each mp layer: 98.33110046386719
Norm before input: 0.2552422881126404
Norm after input: 0.5139533877372742
Norm after each mp layer: 2.4899563789367676
Norm after each mp layer: 16.28622055053711
Norm after each mp layer: 98.68336486816406
Norm before input: 0.2552422881126404
Norm after input: 0.5139533877372742
Norm after each mp layer: 2.4899563789367676
Norm after each mp layer: 16.28622055053711
Norm after each mp layer: 98.68336486816406
Norm before input: 0.2552422881126404
Norm after input: 0.5134240388870239
Norm after each mp layer: 2.487898349761963
Norm after each mp layer: 16.290142059326172
Norm after each mp layer: 98.79345703125
Norm before input: 0.2552422881126404
Norm after input: 0.5134240388870239
Norm after each mp layer: 2.487898349761963
Norm after each mp layer: 16.290142059326172
Norm after each mp layer: 98.79345703125
Norm before input: 0.2552422881126404
Norm after input: 0.5153611898422241
Norm after each mp layer: 2.5107619762420654
Norm after each mp layer: 16.49605369567871
Norm after each mp layer: 99.80545043945312
Norm before input: 0.2552422881126404
Norm after input: 0.5153611898422241
Norm after each mp layer: 2.5107619762420654
Norm after each mp layer: 16.49605369567871
Norm after each mp layer: 99.80545043945312
Norm before input: 0.2552422881126404
Norm after input: 0.5158839821815491
Norm after each mp layer: 2.5212795734405518
Norm after each mp layer: 16.59351921081543
Norm after each mp layer: 100.22562408447266
Norm before input: 0.2552422881126404
Norm after input: 0.5158839821815491
Norm after each mp layer: 2.5212795734405518
Norm after each mp layer: 16.59351921081543
Norm after each mp layer: 100.22562408447266
Norm before input: 0.2552422881126404
Norm after input: 0.5151339769363403
Norm after each mp layer: 2.5178041458129883
Norm after each mp layer: 16.587465286254883
Norm after each mp layer: 100.49903106689453
Epoch: 130, Loss: 0.0558, Energy: 2101349.7500, Train: 98.43%, Valid: 77.60%, Test: 76.90%, Best Valid: 77.80%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5151339769363403
Norm after each mp layer: 2.5178041458129883
Norm after each mp layer: 16.587465286254883
Norm after each mp layer: 100.49903106689453
Norm before input: 0.2552422881126404
Norm after input: 0.5152401924133301
Norm after each mp layer: 2.5193653106689453
Norm after each mp layer: 16.63840675354004
Norm after each mp layer: 101.21956634521484
Norm before input: 0.2552422881126404
Norm after input: 0.5152401924133301
Norm after each mp layer: 2.5193653106689453
Norm after each mp layer: 16.63840675354004
Norm after each mp layer: 101.21956634521484
Norm before input: 0.2552422881126404
Norm after input: 0.516300618648529
Norm after each mp layer: 2.530670166015625
Norm after each mp layer: 16.76183319091797
Norm after each mp layer: 101.8593978881836
Norm before input: 0.2552422881126404
Norm after input: 0.516300618648529
Norm after each mp layer: 2.530670166015625
Norm after each mp layer: 16.76183319091797
Norm after each mp layer: 101.8593978881836
Norm before input: 0.2552422881126404
Norm after input: 0.5161346197128296
Norm after each mp layer: 2.5366547107696533
Norm after each mp layer: 16.800100326538086
Norm after each mp layer: 101.32760620117188
Norm before input: 0.2552422881126404
Norm after input: 0.5161346197128296
Norm after each mp layer: 2.5366547107696533
Norm after each mp layer: 16.800100326538086
Norm after each mp layer: 101.32760620117188
Norm before input: 0.2552422881126404
Norm after input: 0.5164284706115723
Norm after each mp layer: 2.5432419776916504
Norm after each mp layer: 16.876667022705078
Norm after each mp layer: 101.91429901123047
Norm before input: 0.2552422881126404
Norm after input: 0.5164284706115723
Norm after each mp layer: 2.5432419776916504
Norm after each mp layer: 16.876667022705078
Norm after each mp layer: 101.91429901123047
Norm before input: 0.2552422881126404
Norm after input: 0.5171481966972351
Norm after each mp layer: 2.550462484359741
Norm after each mp layer: 16.985570907592773
Norm after each mp layer: 103.38822174072266
Epoch: 135, Loss: 0.0479, Energy: 2196766.5000, Train: 98.59%, Valid: 77.60%, Test: 77.40%, Best Valid: 77.80%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5171481966972351
Norm after each mp layer: 2.550462484359741
Norm after each mp layer: 16.985570907592773
Norm after each mp layer: 103.38822174072266
Norm before input: 0.2552422881126404
Norm after input: 0.5172678828239441
Norm after each mp layer: 2.5543062686920166
Norm after each mp layer: 17.045238494873047
Norm after each mp layer: 104.13402557373047
Norm before input: 0.2552422881126404
Norm after input: 0.5172678828239441
Norm after each mp layer: 2.5543062686920166
Norm after each mp layer: 17.045238494873047
Norm after each mp layer: 104.13402557373047
Norm before input: 0.2552422881126404
Norm after input: 0.5173495411872864
Norm after each mp layer: 2.5609612464904785
Norm after each mp layer: 17.096967697143555
Norm after each mp layer: 103.94806671142578
Norm before input: 0.2552422881126404
Norm after input: 0.5173495411872864
Norm after each mp layer: 2.5609612464904785
Norm after each mp layer: 17.096967697143555
Norm after each mp layer: 103.94806671142578
Norm before input: 0.2552422881126404
Norm after input: 0.5179893970489502
Norm after each mp layer: 2.5721304416656494
Norm after each mp layer: 17.193248748779297
Norm after each mp layer: 104.05183410644531
Norm before input: 0.2552422881126404
Norm after input: 0.5179893970489502
Norm after each mp layer: 2.5721304416656494
Norm after each mp layer: 17.193248748779297
Norm after each mp layer: 104.05183410644531
Norm before input: 0.2552422881126404
Norm after input: 0.518549382686615
Norm after each mp layer: 2.578968048095703
Norm after each mp layer: 17.286672592163086
Norm after each mp layer: 105.1343765258789
Norm before input: 0.2552422881126404
Norm after input: 0.518549382686615
Norm after each mp layer: 2.578968048095703
Norm after each mp layer: 17.286672592163086
Norm after each mp layer: 105.1343765258789
Norm before input: 0.2552422881126404
Norm after input: 0.5183871984481812
Norm after each mp layer: 2.5792510509490967
Norm after each mp layer: 17.322980880737305
Norm after each mp layer: 105.97897338867188
Epoch: 140, Loss: 0.0404, Energy: 2436294.7500, Train: 99.17%, Valid: 78.00%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5183871984481812
Norm after each mp layer: 2.5792510509490967
Norm after each mp layer: 17.322980880737305
Norm after each mp layer: 105.97897338867188
Norm before input: 0.2552422881126404
Norm after input: 0.5183542370796204
Norm after each mp layer: 2.5828793048858643
Norm after each mp layer: 17.369327545166016
Norm after each mp layer: 106.26776123046875
Norm before input: 0.2552422881126404
Norm after input: 0.5183542370796204
Norm after each mp layer: 2.5828793048858643
Norm after each mp layer: 17.369327545166016
Norm after each mp layer: 106.26776123046875
Norm before input: 0.2552422881126404
Norm after input: 0.519544780254364
Norm after each mp layer: 2.59908127784729
Norm after each mp layer: 17.516386032104492
Norm after each mp layer: 106.59925079345703
Norm before input: 0.2552422881126404
Norm after input: 0.519544780254364
Norm after each mp layer: 2.59908127784729
Norm after each mp layer: 17.516386032104492
Norm after each mp layer: 106.59925079345703
Norm before input: 0.2552422881126404
Norm after input: 0.5204123258590698
Norm after each mp layer: 2.6112520694732666
Norm after each mp layer: 17.636444091796875
Norm after each mp layer: 107.00968170166016
Norm before input: 0.2552422881126404
Norm after input: 0.5204123258590698
Norm after each mp layer: 2.6112520694732666
Norm after each mp layer: 17.636444091796875
Norm after each mp layer: 107.00968170166016
Norm before input: 0.2552422881126404
Norm after input: 0.5203421115875244
Norm after each mp layer: 2.612849712371826
Norm after each mp layer: 17.679121017456055
Norm after each mp layer: 107.6024169921875
Norm before input: 0.2552422881126404
Norm after input: 0.5203421115875244
Norm after each mp layer: 2.612849712371826
Norm after each mp layer: 17.679121017456055
Norm after each mp layer: 107.6024169921875
Norm before input: 0.2552422881126404
Norm after input: 0.5201488137245178
Norm after each mp layer: 2.6128005981445312
Norm after each mp layer: 17.710735321044922
Norm after each mp layer: 108.25582885742188
Epoch: 145, Loss: 0.0343, Energy: 2624631.5000, Train: 99.09%, Valid: 77.80%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5201488137245178
Norm after each mp layer: 2.6128005981445312
Norm after each mp layer: 17.710735321044922
Norm after each mp layer: 108.2558364868164
Norm before input: 0.2552422881126404
Norm after input: 0.5209096074104309
Norm after each mp layer: 2.622986316680908
Norm after each mp layer: 17.820722579956055
Norm after each mp layer: 108.78752899169922
Norm before input: 0.2552422881126404
Norm after input: 0.5209096074104309
Norm after each mp layer: 2.622986316680908
Norm after each mp layer: 17.820722579956055
Norm after each mp layer: 108.78752899169922
Norm before input: 0.2552422881126404
Norm after input: 0.5215632915496826
Norm after each mp layer: 2.6335740089416504
Norm after each mp layer: 17.92243766784668
Norm after each mp layer: 108.95335388183594
Norm before input: 0.2552422881126404
Norm after input: 0.5215632915496826
Norm after each mp layer: 2.6335740089416504
Norm after each mp layer: 17.92243766784668
Norm after each mp layer: 108.95335388183594
Norm before input: 0.2552422881126404
Norm after input: 0.5217172503471375
Norm after each mp layer: 2.639124631881714
Norm after each mp layer: 17.98505210876465
Norm after each mp layer: 109.21544647216797
Norm before input: 0.2552422881126404
Norm after input: 0.5217172503471375
Norm after each mp layer: 2.639124631881714
Norm after each mp layer: 17.98505210876465
Norm after each mp layer: 109.21544647216797
Norm before input: 0.2552422881126404
Norm after input: 0.5219634771347046
Norm after each mp layer: 2.643829345703125
Norm after each mp layer: 18.05820655822754
Norm after each mp layer: 110.11286926269531
Norm before input: 0.2552422881126404
Norm after input: 0.5219634771347046
Norm after each mp layer: 2.643829345703125
Norm after each mp layer: 18.05820655822754
Norm after each mp layer: 110.11286926269531
Norm before input: 0.2552422881126404
Norm after input: 0.5226123929023743
Norm after each mp layer: 2.652249574661255
Norm after each mp layer: 18.165193557739258
Norm after each mp layer: 111.10823822021484
Epoch: 150, Loss: 0.0296, Energy: 2800466.7500, Train: 99.17%, Valid: 77.80%, Test: 77.30%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5226123929023743
Norm after each mp layer: 2.652249574661255
Norm after each mp layer: 18.165193557739258
Norm after each mp layer: 111.10823822021484
Norm before input: 0.2552422881126404
Norm after input: 0.5231752395629883
Norm after each mp layer: 2.661252021789551
Norm after each mp layer: 18.26039695739746
Norm after each mp layer: 111.46389770507812
Norm before input: 0.2552422881126404
Norm after input: 0.5231752395629883
Norm after each mp layer: 2.661252021789551
Norm after each mp layer: 18.260395050048828
Norm after each mp layer: 111.46389770507812
Norm before input: 0.2552422881126404
Norm after input: 0.5234444737434387
Norm after each mp layer: 2.667816162109375
Norm after each mp layer: 18.328248977661133
Norm after each mp layer: 111.50225067138672
Norm before input: 0.2552422881126404
Norm after input: 0.5234444737434387
Norm after each mp layer: 2.667816162109375
Norm after each mp layer: 18.328248977661133
Norm after each mp layer: 111.50225067138672
Norm before input: 0.2552422881126404
Norm after input: 0.5237658023834229
Norm after each mp layer: 2.6732208728790283
Norm after each mp layer: 18.40203094482422
Norm after each mp layer: 112.04998779296875
Norm before input: 0.2552422881126404
Norm after input: 0.5237658023834229
Norm after each mp layer: 2.6732208728790283
Norm after each mp layer: 18.40203094482422
Norm after each mp layer: 112.04998779296875
Norm before input: 0.2552422881126404
Norm after input: 0.5241929888725281
Norm after each mp layer: 2.678774118423462
Norm after each mp layer: 18.486345291137695
Norm after each mp layer: 112.93584442138672
Norm before input: 0.2552422881126404
Norm after input: 0.5241929888725281
Norm after each mp layer: 2.678774118423462
Norm after each mp layer: 18.486345291137695
Norm after each mp layer: 112.93584442138672
Norm before input: 0.2552422881126404
Norm after input: 0.5245287418365479
Norm after each mp layer: 2.684576988220215
Norm after each mp layer: 18.561634063720703
Norm after each mp layer: 113.47008514404297
Epoch: 155, Loss: 0.0260, Energy: 3023521.7500, Train: 99.25%, Valid: 77.40%, Test: 77.30%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5245287418365479
Norm after each mp layer: 2.684576988220215
Norm after each mp layer: 18.561634063720703
Norm after each mp layer: 113.47008514404297
Norm before input: 0.2552422881126404
Norm after input: 0.5249390006065369
Norm after each mp layer: 2.6922779083251953
Norm after each mp layer: 18.64276123046875
Norm after each mp layer: 113.67540740966797
Norm before input: 0.2552422881126404
Norm after input: 0.5249390006065369
Norm after each mp layer: 2.6922779083251953
Norm after each mp layer: 18.64276123046875
Norm after each mp layer: 113.67540740966797
Norm before input: 0.2552422881126404
Norm after input: 0.5255538821220398
Norm after each mp layer: 2.7015602588653564
Norm after each mp layer: 18.744403839111328
Norm after each mp layer: 114.13324737548828
Norm before input: 0.2552422881126404
Norm after input: 0.5255538821220398
Norm after each mp layer: 2.7015602588653564
Norm after each mp layer: 18.744403839111328
Norm after each mp layer: 114.13324737548828
Norm before input: 0.2552422881126404
Norm after input: 0.526033341884613
Norm after each mp layer: 2.708636999130249
Norm after each mp layer: 18.837085723876953
Norm after each mp layer: 114.92987823486328
Norm before input: 0.2552422881126404
Norm after input: 0.526033341884613
Norm after each mp layer: 2.708636999130249
Norm after each mp layer: 18.837085723876953
Norm after each mp layer: 114.92987823486328
Norm before input: 0.2552422881126404
Norm after input: 0.5261991024017334
Norm after each mp layer: 2.712860345840454
Norm after each mp layer: 18.900814056396484
Norm after each mp layer: 115.54926300048828
Norm before input: 0.2552422881126404
Norm after input: 0.5261991024017334
Norm after each mp layer: 2.712860345840454
Norm after each mp layer: 18.900814056396484
Norm after each mp layer: 115.54926300048828
Norm before input: 0.2552422881126404
Norm after input: 0.5264599323272705
Norm after each mp layer: 2.718468427658081
Norm after each mp layer: 18.96882438659668
Norm after each mp layer: 115.85690307617188
Epoch: 160, Loss: 0.0228, Energy: 3232656.7500, Train: 99.34%, Valid: 76.60%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5264599323272705
Norm after each mp layer: 2.718468427658081
Norm after each mp layer: 18.96882438659668
Norm after each mp layer: 115.85690307617188
Norm before input: 0.2552422881126404
Norm after input: 0.5270267128944397
Norm after each mp layer: 2.726750612258911
Norm after each mp layer: 19.062219619750977
Norm after each mp layer: 116.1944808959961
Norm before input: 0.2552422881126404
Norm after input: 0.5270267128944397
Norm after each mp layer: 2.726750612258911
Norm after each mp layer: 19.062219619750977
Norm after each mp layer: 116.1944808959961
Norm before input: 0.2552422881126404
Norm after input: 0.527479887008667
Norm after each mp layer: 2.7333261966705322
Norm after each mp layer: 19.1468563079834
Norm after each mp layer: 116.73478698730469
Norm before input: 0.2552422881126404
Norm after input: 0.527479887008667
Norm after each mp layer: 2.7333261966705322
Norm after each mp layer: 19.1468563079834
Norm after each mp layer: 116.73480224609375
Norm before input: 0.2552422881126404
Norm after input: 0.5276539921760559
Norm after each mp layer: 2.737259864807129
Norm after each mp layer: 19.207672119140625
Norm after each mp layer: 117.2890625
Norm before input: 0.2552422881126404
Norm after input: 0.5276539921760559
Norm after each mp layer: 2.737259864807129
Norm after each mp layer: 19.207672119140625
Norm after each mp layer: 117.2890625
Norm before input: 0.2552422881126404
Norm after input: 0.5279300808906555
Norm after each mp layer: 2.742595672607422
Norm after each mp layer: 19.276865005493164
Norm after each mp layer: 117.74790954589844
Norm before input: 0.2552422881126404
Norm after input: 0.5279300808906555
Norm after each mp layer: 2.742595672607422
Norm after each mp layer: 19.276865005493164
Norm after each mp layer: 117.74790954589844
Norm before input: 0.2552422881126404
Norm after input: 0.5285030007362366
Norm after each mp layer: 2.75091290473938
Norm after each mp layer: 19.371267318725586
Norm after each mp layer: 118.16642761230469
Epoch: 165, Loss: 0.0202, Energy: 3408977.5000, Train: 99.59%, Valid: 76.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5285030007362366
Norm after each mp layer: 2.75091290473938
Norm after each mp layer: 19.371267318725586
Norm after each mp layer: 118.16642761230469
Norm before input: 0.2552422881126404
Norm after input: 0.5289475917816162
Norm after each mp layer: 2.7577645778656006
Norm after each mp layer: 19.453218460083008
Norm after each mp layer: 118.577392578125
Norm before input: 0.2552422881126404
Norm after input: 0.5289475917816162
Norm after each mp layer: 2.7577645778656006
Norm after each mp layer: 19.453218460083008
Norm after each mp layer: 118.57738494873047
Norm before input: 0.2552422881126404
Norm after input: 0.5290748476982117
Norm after each mp layer: 2.761198043823242
Norm after each mp layer: 19.505983352661133
Norm after each mp layer: 119.0274429321289
Norm before input: 0.2552422881126404
Norm after input: 0.5290748476982117
Norm after each mp layer: 2.761198043823242
Norm after each mp layer: 19.505983352661133
Norm after each mp layer: 119.0274429321289
Norm before input: 0.2552422881126404
Norm after input: 0.5292613506317139
Norm after each mp layer: 2.764932632446289
Norm after each mp layer: 19.56349754333496
Norm after each mp layer: 119.55046081542969
Norm before input: 0.2552422881126404
Norm after input: 0.5292613506317139
Norm after each mp layer: 2.764932632446289
Norm after each mp layer: 19.56349754333496
Norm after each mp layer: 119.55046081542969
Norm before input: 0.2552422881126404
Norm after input: 0.5297725796699524
Norm after each mp layer: 2.771963119506836
Norm after each mp layer: 19.64970588684082
Norm after each mp layer: 120.03288269042969
Norm before input: 0.2552422881126404
Norm after input: 0.5297725796699524
Norm after each mp layer: 2.771963119506836
Norm after each mp layer: 19.64970588684082
Norm after each mp layer: 120.03288269042969
Norm before input: 0.2552422881126404
Norm after input: 0.5302078127861023
Norm after each mp layer: 2.778744697570801
Norm after each mp layer: 19.72884750366211
Norm after each mp layer: 120.35204315185547
Epoch: 170, Loss: 0.0179, Energy: 3604278.0000, Train: 99.59%, Valid: 76.80%, Test: 76.30%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5302078127861023
Norm after each mp layer: 2.778744697570801
Norm after each mp layer: 19.72884750366211
Norm after each mp layer: 120.35204315185547
Norm before input: 0.2552422881126404
Norm after input: 0.5304133296012878
Norm after each mp layer: 2.783381700515747
Norm after each mp layer: 19.788206100463867
Norm after each mp layer: 120.67884063720703
Norm before input: 0.2552422881126404
Norm after input: 0.5304133296012878
Norm after each mp layer: 2.783381700515747
Norm after each mp layer: 19.788206100463867
Norm after each mp layer: 120.67884063720703
Norm before input: 0.2552422881126404
Norm after input: 0.530655026435852
Norm after each mp layer: 2.7879819869995117
Norm after each mp layer: 19.8519287109375
Norm after each mp layer: 121.1937484741211
Norm before input: 0.2552422881126404
Norm after input: 0.530655026435852
Norm after each mp layer: 2.7879819869995117
Norm after each mp layer: 19.8519287109375
Norm after each mp layer: 121.1937484741211
Norm before input: 0.2552422881126404
Norm after input: 0.5310502052307129
Norm after each mp layer: 2.79400372505188
Norm after each mp layer: 19.92853546142578
Norm after each mp layer: 121.72024536132812
Norm before input: 0.2552422881126404
Norm after input: 0.5310502052307129
Norm after each mp layer: 2.79400372505188
Norm after each mp layer: 19.92853546142578
Norm after each mp layer: 121.72024536132812
Norm before input: 0.2552422881126404
Norm after input: 0.5314268469810486
Norm after each mp layer: 2.7999885082244873
Norm after each mp layer: 20.00170135498047
Norm after each mp layer: 122.1097412109375
Norm before input: 0.2552422881126404
Norm after input: 0.5314268469810486
Norm after each mp layer: 2.7999885082244873
Norm after each mp layer: 20.00170135498047
Norm after each mp layer: 122.1097412109375
Norm before input: 0.2552422881126404
Norm after input: 0.5316845178604126
Norm after each mp layer: 2.8047261238098145
Norm after each mp layer: 20.063800811767578
Norm after each mp layer: 122.47187805175781
Epoch: 175, Loss: 0.0160, Energy: 3782087.5000, Train: 99.67%, Valid: 76.80%, Test: 76.30%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5316845178604126
Norm after each mp layer: 2.8047261238098145
Norm after each mp layer: 20.063800811767578
Norm after each mp layer: 122.47187805175781
Norm before input: 0.2552422881126404
Norm after input: 0.5319083333015442
Norm after each mp layer: 2.809044599533081
Norm after each mp layer: 20.12357521057129
Norm after each mp layer: 122.90679168701172
Norm before input: 0.2552422881126404
Norm after input: 0.5319083333015442
Norm after each mp layer: 2.809044599533081
Norm after each mp layer: 20.12357521057129
Norm after each mp layer: 122.90679168701172
Norm before input: 0.2552422881126404
Norm after input: 0.5322778224945068
Norm after each mp layer: 2.814715623855591
Norm after each mp layer: 20.19695281982422
Norm after each mp layer: 123.3938980102539
Norm before input: 0.2552422881126404
Norm after input: 0.5322778224945068
Norm after each mp layer: 2.814715623855591
Norm after each mp layer: 20.19695281982422
Norm after each mp layer: 123.3938980102539
Norm before input: 0.2552422881126404
Norm after input: 0.5326787829399109
Norm after each mp layer: 2.820744752883911
Norm after each mp layer: 20.272079467773438
Norm after each mp layer: 123.80135345458984
Norm before input: 0.2552422881126404
Norm after input: 0.5326787829399109
Norm after each mp layer: 2.820744752883911
Norm after each mp layer: 20.272079467773438
Norm after each mp layer: 123.80135345458984
Norm before input: 0.2552422881126404
Norm after input: 0.5329071879386902
Norm after each mp layer: 2.825242519378662
Norm after each mp layer: 20.32984733581543
Norm after each mp layer: 124.10671997070312
Norm before input: 0.2552422881126404
Norm after input: 0.5329071879386902
Norm after each mp layer: 2.825242519378662
Norm after each mp layer: 20.32984733581543
Norm after each mp layer: 124.10671997070312
Norm before input: 0.2552422881126404
Norm after input: 0.5331371426582336
Norm after each mp layer: 2.8294804096221924
Norm after each mp layer: 20.38827133178711
Norm after each mp layer: 124.53701782226562
Epoch: 180, Loss: 0.0144, Energy: 3952817.2500, Train: 99.67%, Valid: 77.00%, Test: 76.40%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5331371426582336
Norm after each mp layer: 2.8294804096221924
Norm after each mp layer: 20.38827133178711
Norm after each mp layer: 124.53701782226562
Norm before input: 0.2552422881126404
Norm after input: 0.5334743857383728
Norm after each mp layer: 2.8346662521362305
Norm after each mp layer: 20.457313537597656
Norm after each mp layer: 125.06104278564453
Norm before input: 0.2552422881126404
Norm after input: 0.5334743857383728
Norm after each mp layer: 2.8346662521362305
Norm after each mp layer: 20.457313537597656
Norm after each mp layer: 125.06104278564453
Norm before input: 0.2552422881126404
Norm after input: 0.5338048934936523
Norm after each mp layer: 2.840005874633789
Norm after each mp layer: 20.525196075439453
Norm after each mp layer: 125.49122619628906
Norm before input: 0.2552422881126404
Norm after input: 0.5338048934936523
Norm after each mp layer: 2.840005874633789
Norm after each mp layer: 20.525196075439453
Norm after each mp layer: 125.49122619628906
Norm before input: 0.2552422881126404
Norm after input: 0.5340666174888611
Norm after each mp layer: 2.844762086868286
Norm after each mp layer: 20.58585548400879
Norm after each mp layer: 125.81759643554688
Norm before input: 0.2552422881126404
Norm after input: 0.5340666174888611
Norm after each mp layer: 2.844762086868286
Norm after each mp layer: 20.58585548400879
Norm after each mp layer: 125.81759643554688
Norm before input: 0.2552422881126404
Norm after input: 0.5343045592308044
Norm after each mp layer: 2.849245071411133
Norm after each mp layer: 20.6438045501709
Norm after each mp layer: 126.14744567871094
Norm before input: 0.2552422881126404
Norm after input: 0.5343045592308044
Norm after each mp layer: 2.849245071411133
Norm after each mp layer: 20.6438045501709
Norm after each mp layer: 126.14744567871094
Norm before input: 0.2552422881126404
Norm after input: 0.534683108329773
Norm after each mp layer: 2.85465407371521
Norm after each mp layer: 20.715335845947266
Norm after each mp layer: 126.62628936767578
Epoch: 185, Loss: 0.0131, Energy: 4124023.7500, Train: 99.67%, Valid: 76.80%, Test: 76.40%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.534683108329773
Norm after each mp layer: 2.85465407371521
Norm after each mp layer: 20.715335845947266
Norm after each mp layer: 126.62628936767578
Norm before input: 0.2552422881126404
Norm after input: 0.5349801778793335
Norm after each mp layer: 2.85941219329834
Norm after each mp layer: 20.77825164794922
Norm after each mp layer: 127.04462432861328
Norm before input: 0.2552422881126404
Norm after input: 0.5349801778793335
Norm after each mp layer: 2.85941219329834
Norm after each mp layer: 20.77825164794922
Norm after each mp layer: 127.04462432861328
Norm before input: 0.2552422881126404
Norm after input: 0.5352478623390198
Norm after each mp layer: 2.863814115524292
Norm after each mp layer: 20.838008880615234
Norm after each mp layer: 127.41989135742188
Norm before input: 0.2552422881126404
Norm after input: 0.5352478623390198
Norm after each mp layer: 2.863814115524292
Norm after each mp layer: 20.838008880615234
Norm after each mp layer: 127.41989135742188
Norm before input: 0.2552422881126404
Norm after input: 0.5354258418083191
Norm after each mp layer: 2.8677217960357666
Norm after each mp layer: 20.88872718811035
Norm after each mp layer: 127.71143341064453
Norm before input: 0.2552422881126404
Norm after input: 0.5354258418083191
Norm after each mp layer: 2.8677217960357666
Norm after each mp layer: 20.88872718811035
Norm after each mp layer: 127.71143341064453
Norm before input: 0.2552422881126404
Norm after input: 0.5358072519302368
Norm after each mp layer: 2.8730905055999756
Norm after each mp layer: 20.959943771362305
Norm after each mp layer: 128.18539428710938
Norm before input: 0.2552422881126404
Norm after input: 0.5358072519302368
Norm after each mp layer: 2.8730905055999756
Norm after each mp layer: 20.959943771362305
Norm after each mp layer: 128.18539428710938
Norm before input: 0.2552422881126404
Norm after input: 0.5360342860221863
Norm after each mp layer: 2.8773698806762695
Norm after each mp layer: 21.015483856201172
Norm after each mp layer: 128.55625915527344
Epoch: 190, Loss: 0.0120, Energy: 4301611.0000, Train: 99.75%, Valid: 76.80%, Test: 76.70%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5360342860221863
Norm after each mp layer: 2.8773698806762695
Norm after each mp layer: 21.015483856201172
Norm after each mp layer: 128.55625915527344
Norm before input: 0.2552422881126404
Norm after input: 0.5363514423370361
Norm after each mp layer: 2.8821208477020264
Norm after each mp layer: 21.079164505004883
Norm after each mp layer: 128.95944213867188
Norm before input: 0.2552422881126404
Norm after input: 0.5363514423370361
Norm after each mp layer: 2.8821208477020264
Norm after each mp layer: 21.079164505004883
Norm after each mp layer: 128.95944213867188
Norm before input: 0.2552422881126404
Norm after input: 0.5365906953811646
Norm after each mp layer: 2.8864612579345703
Norm after each mp layer: 21.134376525878906
Norm after each mp layer: 129.25233459472656
Norm before input: 0.2552422881126404
Norm after input: 0.5365906953811646
Norm after each mp layer: 2.8864612579345703
Norm after each mp layer: 21.134376525878906
Norm after each mp layer: 129.25233459472656
Norm before input: 0.2552422881126404
Norm after input: 0.5368790626525879
Norm after each mp layer: 2.8911232948303223
Norm after each mp layer: 21.194704055786133
Norm after each mp layer: 129.62063598632812
Norm before input: 0.2552422881126404
Norm after input: 0.5368790626525879
Norm after each mp layer: 2.8911232948303223
Norm after each mp layer: 21.194704055786133
Norm after each mp layer: 129.62063598632812
Norm before input: 0.2552422881126404
Norm after input: 0.537169337272644
Norm after each mp layer: 2.8955864906311035
Norm after each mp layer: 21.255657196044922
Norm after each mp layer: 130.0663604736328
Norm before input: 0.2552422881126404
Norm after input: 0.537169337272644
Norm after each mp layer: 2.8955864906311035
Norm after each mp layer: 21.255657196044922
Norm after each mp layer: 130.06637573242188
Norm before input: 0.2552422881126404
Norm after input: 0.5373304486274719
Norm after each mp layer: 2.8991873264312744
Norm after each mp layer: 21.303144454956055
Norm after each mp layer: 130.3892822265625
Epoch: 195, Loss: 0.0109, Energy: 4463568.5000, Train: 99.75%, Valid: 76.60%, Test: 76.50%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5373304486274719
Norm after each mp layer: 2.8991873264312744
Norm after each mp layer: 21.303144454956055
Norm after each mp layer: 130.3892822265625
Norm before input: 0.2552422881126404
Norm after input: 0.5376694798469543
Norm after each mp layer: 2.9040987491607666
Norm after each mp layer: 21.367530822753906
Norm after each mp layer: 130.7713623046875
Norm before input: 0.2552422881126404
Norm after input: 0.5376694798469543
Norm after each mp layer: 2.9040987491607666
Norm after each mp layer: 21.367530822753906
Norm after each mp layer: 130.7713623046875
Norm before input: 0.2552422881126404
Norm after input: 0.5378841757774353
Norm after each mp layer: 2.9081881046295166
Norm after each mp layer: 21.41928482055664
Norm after each mp layer: 131.07766723632812
Norm before input: 0.2552422881126404
Norm after input: 0.5378841757774353
Norm after each mp layer: 2.9081881046295166
Norm after each mp layer: 21.41928482055664
Norm after each mp layer: 131.07766723632812
Norm before input: 0.2552422881126404
Norm after input: 0.5381869673728943
Norm after each mp layer: 2.9126408100128174
Norm after each mp layer: 21.480058670043945
Norm after each mp layer: 131.51498413085938
Norm before input: 0.2552422881126404
Norm after input: 0.5381869673728943
Norm after each mp layer: 2.9126408100128174
Norm after each mp layer: 21.480058670043945
Norm after each mp layer: 131.51498413085938
Norm before input: 0.2552422881126404
Norm after input: 0.5384358763694763
Norm after each mp layer: 2.9167630672454834
Norm after each mp layer: 21.534976959228516
Norm after each mp layer: 131.88153076171875
Norm before input: 0.2552422881126404
Norm after input: 0.5384358763694763
Norm after each mp layer: 2.9167630672454834
Norm after each mp layer: 21.534976959228516
Norm after each mp layer: 131.88153076171875
Norm before input: 0.2552422881126404
Norm after input: 0.5386683344841003
Norm after each mp layer: 2.920865297317505
Norm after each mp layer: 21.587417602539062
Norm after each mp layer: 132.1694793701172
Epoch: 200, Loss: 0.0100, Energy: 4620621.5000, Train: 99.75%, Valid: 76.40%, Test: 76.50%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5386683344841003
Norm after each mp layer: 2.920865297317505
Norm after each mp layer: 21.587417602539062
Norm after each mp layer: 132.1694793701172
Norm before input: 0.2552422881126404
Norm after input: 0.5389511585235596
Norm after each mp layer: 2.9251184463500977
Norm after each mp layer: 21.64464569091797
Norm after each mp layer: 132.52540588378906
Norm before input: 0.2552422881126404
Norm after input: 0.5389511585235596
Norm after each mp layer: 2.9251184463500977
Norm after each mp layer: 21.64464569091797
Norm after each mp layer: 132.52540588378906
Norm before input: 0.2552422881126404
Norm after input: 0.5391063690185547
Norm after each mp layer: 2.9284026622772217
Norm after each mp layer: 21.68899917602539
Norm after each mp layer: 132.84849548339844
Norm before input: 0.2552422881126404
Norm after input: 0.5391063690185547
Norm after each mp layer: 2.9284026622772217
Norm after each mp layer: 21.68899917602539
Norm after each mp layer: 132.84849548339844
Norm before input: 0.2552422881126404
Norm after input: 0.5394298434257507
Norm after each mp layer: 2.932842254638672
Norm after each mp layer: 21.749914169311523
Norm after each mp layer: 133.27175903320312
Norm before input: 0.2552422881126404
Norm after input: 0.5394298434257507
Norm after each mp layer: 2.932842254638672
Norm after each mp layer: 21.749914169311523
Norm after each mp layer: 133.27175903320312
Norm before input: 0.2552422881126404
Norm after input: 0.5396186709403992
Norm after each mp layer: 2.936509370803833
Norm after each mp layer: 21.796836853027344
Norm after each mp layer: 133.5523223876953
Norm before input: 0.2552422881126404
Norm after input: 0.5396186709403992
Norm after each mp layer: 2.936509370803833
Norm after each mp layer: 21.796836853027344
Norm after each mp layer: 133.5523223876953
Norm before input: 0.2552422881126404
Norm after input: 0.5398791432380676
Norm after each mp layer: 2.9405784606933594
Norm after each mp layer: 21.850862503051758
Norm after each mp layer: 133.88601684570312
Epoch: 205, Loss: 0.0092, Energy: 4765756.5000, Train: 99.75%, Valid: 76.40%, Test: 76.50%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5398791432380676
Norm after each mp layer: 2.9405784606933594
Norm after each mp layer: 21.850862503051758
Norm after each mp layer: 133.88601684570312
Norm before input: 0.2552422881126404
Norm after input: 0.5401138067245483
Norm after each mp layer: 2.9444565773010254
Norm after each mp layer: 21.902294158935547
Norm after each mp layer: 134.2267608642578
Norm before input: 0.2552422881126404
Norm after input: 0.5401138067245483
Norm after each mp layer: 2.9444565773010254
Norm after each mp layer: 21.902294158935547
Norm after each mp layer: 134.2267608642578
Norm before input: 0.2552422881126404
Norm after input: 0.5403425097465515
Norm after each mp layer: 2.9482641220092773
Norm after each mp layer: 21.952877044677734
Norm after each mp layer: 134.5735321044922
Norm before input: 0.2552422881126404
Norm after input: 0.5403425097465515
Norm after each mp layer: 2.9482641220092773
Norm after each mp layer: 21.952877044677734
Norm after each mp layer: 134.5735321044922
Norm before input: 0.2552422881126404
Norm after input: 0.5406082272529602
Norm after each mp layer: 2.9521961212158203
Norm after each mp layer: 22.006715774536133
Norm after each mp layer: 134.9388885498047
Norm before input: 0.2552422881126404
Norm after input: 0.5406082272529602
Norm after each mp layer: 2.9521961212158203
Norm after each mp layer: 22.006715774536133
Norm after each mp layer: 134.9388885498047
Norm before input: 0.2552422881126404
Norm after input: 0.540759801864624
Norm after each mp layer: 2.9553911685943604
Norm after each mp layer: 22.048601150512695
Norm after each mp layer: 135.20567321777344
Norm before input: 0.2552422881126404
Norm after input: 0.540759801864624
Norm after each mp layer: 2.9553911685943604
Norm after each mp layer: 22.048601150512695
Norm after each mp layer: 135.20567321777344
Norm before input: 0.2552422881126404
Norm after input: 0.5410580039024353
Norm after each mp layer: 2.959547519683838
Norm after each mp layer: 22.10520362854004
Norm after each mp layer: 135.5714569091797
Epoch: 210, Loss: 0.0086, Energy: 4910757.5000, Train: 99.83%, Valid: 76.20%, Test: 76.50%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5410580039024353
Norm after each mp layer: 2.959547519683838
Norm after each mp layer: 22.10520362854004
Norm after each mp layer: 135.5714569091797
Norm before input: 0.2552422881126404
Norm after input: 0.5412159562110901
Norm after each mp layer: 2.9627583026885986
Norm after each mp layer: 22.14771270751953
Norm after each mp layer: 135.87257385253906
Norm before input: 0.2552422881126404
Norm after input: 0.5412159562110901
Norm after each mp layer: 2.9627583026885986
Norm after each mp layer: 22.14771270751953
Norm after each mp layer: 135.87257385253906
Norm before input: 0.2552422881126404
Norm after input: 0.5414913892745972
Norm after each mp layer: 2.9667069911956787
Norm after each mp layer: 22.201866149902344
Norm after each mp layer: 136.24412536621094
Norm before input: 0.2552422881126404
Norm after input: 0.5414913892745972
Norm after each mp layer: 2.9667069911956787
Norm after each mp layer: 22.201866149902344
Norm after each mp layer: 136.24412536621094
Norm before input: 0.2552422881126404
Norm after input: 0.5416871905326843
Norm after each mp layer: 2.9702038764953613
Norm after each mp layer: 22.247488021850586
Norm after each mp layer: 136.52923583984375
Norm before input: 0.2552422881126404
Norm after input: 0.5416871905326843
Norm after each mp layer: 2.9702038764953613
Norm after each mp layer: 22.247488021850586
Norm after each mp layer: 136.52923583984375
Norm before input: 0.2552422881126404
Norm after input: 0.5419186353683472
Norm after each mp layer: 2.9738807678222656
Norm after each mp layer: 22.296463012695312
Norm after each mp layer: 136.84205627441406
Norm before input: 0.2552422881126404
Norm after input: 0.5419186353683472
Norm after each mp layer: 2.9738807678222656
Norm after each mp layer: 22.296463012695312
Norm after each mp layer: 136.84205627441406
Norm before input: 0.2552422881126404
Norm after input: 0.5421450138092041
Norm after each mp layer: 2.9774303436279297
Norm after each mp layer: 22.34488868713379
Norm after each mp layer: 137.1832275390625
Epoch: 215, Loss: 0.0079, Energy: 5056918.0000, Train: 99.83%, Valid: 76.20%, Test: 76.60%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5421450138092041
Norm after each mp layer: 2.9774303436279297
Norm after each mp layer: 22.34488868713379
Norm after each mp layer: 137.1832275390625
Norm before input: 0.2552422881126404
Norm after input: 0.5423316359519958
Norm after each mp layer: 2.9807353019714355
Norm after each mp layer: 22.38910484313965
Norm after each mp layer: 137.4961700439453
Norm before input: 0.2552422881126404
Norm after input: 0.5423316359519958
Norm after each mp layer: 2.9807353019714355
Norm after each mp layer: 22.38910484313965
Norm after each mp layer: 137.4961700439453
Norm before input: 0.2552422881126404
Norm after input: 0.5425896048545837
Norm after each mp layer: 2.9845001697540283
Norm after each mp layer: 22.440265655517578
Norm after each mp layer: 137.83245849609375
Norm before input: 0.2552422881126404
Norm after input: 0.5425896048545837
Norm after each mp layer: 2.9845001697540283
Norm after each mp layer: 22.440265655517578
Norm after each mp layer: 137.83245849609375
Norm before input: 0.2552422881126404
Norm after input: 0.5427309274673462
Norm after each mp layer: 2.987488269805908
Norm after each mp layer: 22.479291915893555
Norm after each mp layer: 138.09133911132812
Norm before input: 0.2552422881126404
Norm after input: 0.5427309274673462
Norm after each mp layer: 2.987488269805908
Norm after each mp layer: 22.479291915893555
Norm after each mp layer: 138.09132385253906
Norm before input: 0.2552422881126404
Norm after input: 0.5430115461349487
Norm after each mp layer: 2.9913716316223145
Norm after each mp layer: 22.532333374023438
Norm after each mp layer: 138.44082641601562
Norm before input: 0.2552422881126404
Norm after input: 0.5430115461349487
Norm after each mp layer: 2.9913716316223145
Norm after each mp layer: 22.532333374023438
Norm after each mp layer: 138.44082641601562
Norm before input: 0.2552422881126404
Norm after input: 0.5431604385375977
Norm after each mp layer: 2.994372844696045
Norm after each mp layer: 22.57186508178711
Norm after each mp layer: 138.7267303466797
Epoch: 220, Loss: 0.0074, Energy: 5198854.5000, Train: 99.83%, Valid: 76.20%, Test: 76.70%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5431604385375977
Norm after each mp layer: 2.994372844696045
Norm after each mp layer: 22.57186508178711
Norm after each mp layer: 138.7267303466797
Norm before input: 0.2552422881126404
Norm after input: 0.5434289574623108
Norm after each mp layer: 2.9981439113616943
Norm after each mp layer: 22.6234073638916
Norm after each mp layer: 139.07664489746094
Norm before input: 0.2552422881126404
Norm after input: 0.5434289574623108
Norm after each mp layer: 2.9981439113616943
Norm after each mp layer: 22.6234073638916
Norm after each mp layer: 139.07664489746094
Norm before input: 0.2552422881126404
Norm after input: 0.5435930490493774
Norm after each mp layer: 3.0012502670288086
Norm after each mp layer: 22.6640682220459
Norm after each mp layer: 139.34986877441406
Norm before input: 0.2552422881126404
Norm after input: 0.5435930490493774
Norm after each mp layer: 3.0012502670288086
Norm after each mp layer: 22.66407012939453
Norm after each mp layer: 139.34986877441406
Norm before input: 0.2552422881126404
Norm after input: 0.5438221096992493
Norm after each mp layer: 3.0047719478607178
Norm after each mp layer: 22.7111873626709
Norm after each mp layer: 139.6578826904297
Norm before input: 0.2552422881126404
Norm after input: 0.5438221096992493
Norm after each mp layer: 3.0047719478607178
Norm after each mp layer: 22.7111873626709
Norm after each mp layer: 139.6578826904297
Norm before input: 0.2552422881126404
Norm after input: 0.5440167784690857
Norm after each mp layer: 3.007991313934326
Norm after each mp layer: 22.754667282104492
Norm after each mp layer: 139.96749877929688
Norm before input: 0.2552422881126404
Norm after input: 0.5440167784690857
Norm after each mp layer: 3.007991313934326
Norm after each mp layer: 22.754667282104492
Norm after each mp layer: 139.96749877929688
Norm before input: 0.2552422881126404
Norm after input: 0.5442060232162476
Norm after each mp layer: 3.011157989501953
Norm after each mp layer: 22.797365188598633
Norm after each mp layer: 140.2736053466797
Epoch: 225, Loss: 0.0068, Energy: 5334634.0000, Train: 99.92%, Valid: 76.00%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5442060232162476
Norm after each mp layer: 3.011157989501953
Norm after each mp layer: 22.79736328125
Norm after each mp layer: 140.2736053466797
Norm before input: 0.2552422881126404
Norm after input: 0.5444334745407104
Norm after each mp layer: 3.014585018157959
Norm after each mp layer: 22.843713760375977
Norm after each mp layer: 140.5849609375
Norm before input: 0.2552422881126404
Norm after input: 0.5444334745407104
Norm after each mp layer: 3.014585018157959
Norm after each mp layer: 22.843713760375977
Norm after each mp layer: 140.5849609375
Norm before input: 0.2552422881126404
Norm after input: 0.5445865988731384
Norm after each mp layer: 3.017507314682007
Norm after each mp layer: 22.88225746154785
Norm after each mp layer: 140.85166931152344
Norm before input: 0.2552422881126404
Norm after input: 0.5445865988731384
Norm after each mp layer: 3.017507314682007
Norm after each mp layer: 22.88225746154785
Norm after each mp layer: 140.85166931152344
Norm before input: 0.2552422881126404
Norm after input: 0.5448378324508667
Norm after each mp layer: 3.0210936069488525
Norm after each mp layer: 22.930789947509766
Norm after each mp layer: 141.1705322265625
Norm before input: 0.2552422881126404
Norm after input: 0.5448378324508667
Norm after each mp layer: 3.0210936069488525
Norm after each mp layer: 22.930789947509766
Norm after each mp layer: 141.1705322265625
Norm before input: 0.2552422881126404
Norm after input: 0.5449778437614441
Norm after each mp layer: 3.0239062309265137
Norm after each mp layer: 22.967796325683594
Norm after each mp layer: 141.4436798095703
Norm before input: 0.2552422881126404
Norm after input: 0.5449778437614441
Norm after each mp layer: 3.0239062309265137
Norm after each mp layer: 22.967796325683594
Norm after each mp layer: 141.4436798095703
Norm before input: 0.2552422881126404
Norm after input: 0.5452371835708618
Norm after each mp layer: 3.027513265609741
Norm after each mp layer: 23.016836166381836
Norm after each mp layer: 141.77023315429688
Epoch: 230, Loss: 0.0064, Energy: 5465668.5000, Train: 99.92%, Valid: 76.20%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5452371835708618
Norm after each mp layer: 3.027513265609741
Norm after each mp layer: 23.016836166381836
Norm after each mp layer: 141.77023315429688
Norm before input: 0.2552422881126404
Norm after input: 0.5453701615333557
Norm after each mp layer: 3.030245304107666
Norm after each mp layer: 23.05260467529297
Norm after each mp layer: 142.02651977539062
Norm before input: 0.2552422881126404
Norm after input: 0.5453701615333557
Norm after each mp layer: 3.030245304107666
Norm after each mp layer: 23.05260467529297
Norm after each mp layer: 142.02651977539062
Norm before input: 0.2552422881126404
Norm after input: 0.5456159710884094
Norm after each mp layer: 3.033747673034668
Norm after each mp layer: 23.09972381591797
Norm after each mp layer: 142.32754516601562
Norm before input: 0.2552422881126404
Norm after input: 0.5456159710884094
Norm after each mp layer: 3.033747673034668
Norm after each mp layer: 23.09972381591797
Norm after each mp layer: 142.32754516601562
Norm before input: 0.2552422881126404
Norm after input: 0.5457612872123718
Norm after each mp layer: 3.0364842414855957
Norm after each mp layer: 23.136371612548828
Norm after each mp layer: 142.599853515625
Norm before input: 0.2552422881126404
Norm after input: 0.5457612872123718
Norm after each mp layer: 3.0364842414855957
Norm after each mp layer: 23.136371612548828
Norm after each mp layer: 142.599853515625
Norm before input: 0.2552422881126404
Norm after input: 0.5459797382354736
Norm after each mp layer: 3.0397469997406006
Norm after each mp layer: 23.180376052856445
Norm after each mp layer: 142.89845275878906
Norm before input: 0.2552422881126404
Norm after input: 0.5459797382354736
Norm after each mp layer: 3.0397469997406006
Norm after each mp layer: 23.180376052856445
Norm after each mp layer: 142.89845275878906
Norm before input: 0.2552422881126404
Norm after input: 0.5461522936820984
Norm after each mp layer: 3.0426549911499023
Norm after each mp layer: 23.219484329223633
Norm after each mp layer: 143.1774139404297
Epoch: 235, Loss: 0.0059, Energy: 5598064.0000, Train: 99.92%, Valid: 76.20%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5461522936820984
Norm after each mp layer: 3.0426549911499023
Norm after each mp layer: 23.219484329223633
Norm after each mp layer: 143.1774139404297
Norm before input: 0.2552422881126404
Norm after input: 0.5463330745697021
Norm after each mp layer: 3.045625686645508
Norm after each mp layer: 23.259231567382812
Norm after each mp layer: 143.45046997070312
Norm before input: 0.2552422881126404
Norm after input: 0.5463330745697021
Norm after each mp layer: 3.045625686645508
Norm after each mp layer: 23.259231567382812
Norm after each mp layer: 143.45046997070312
Norm before input: 0.2552422881126404
Norm after input: 0.5465347170829773
Norm after each mp layer: 3.048736810684204
Norm after each mp layer: 23.300954818725586
Norm after each mp layer: 143.7309112548828
Norm before input: 0.2552422881126404
Norm after input: 0.5465347170829773
Norm after each mp layer: 3.048736810684204
Norm after each mp layer: 23.300954818725586
Norm after each mp layer: 143.7309112548828
Norm before input: 0.2552422881126404
Norm after input: 0.5466890335083008
Norm after each mp layer: 3.0514605045318604
Norm after each mp layer: 23.337644577026367
Norm after each mp layer: 144.0045623779297
Norm before input: 0.2552422881126404
Norm after input: 0.5466890335083008
Norm after each mp layer: 3.0514605045318604
Norm after each mp layer: 23.337644577026367
Norm after each mp layer: 144.0045623779297
Norm before input: 0.2552422881126404
Norm after input: 0.5469107031822205
Norm after each mp layer: 3.0546810626983643
Norm after each mp layer: 23.381126403808594
Norm after each mp layer: 144.29554748535156
Norm before input: 0.2552422881126404
Norm after input: 0.5469107031822205
Norm after each mp layer: 3.0546810626983643
Norm after each mp layer: 23.381126403808594
Norm after each mp layer: 144.29554748535156
Norm before input: 0.2552422881126404
Norm after input: 0.5470449924468994
Norm after each mp layer: 3.0572445392608643
Norm after each mp layer: 23.415390014648438
Norm after each mp layer: 144.55247497558594
Epoch: 240, Loss: 0.0056, Energy: 5725293.0000, Train: 99.92%, Valid: 76.20%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5470449924468994
Norm after each mp layer: 3.0572445392608643
Norm after each mp layer: 23.415390014648438
Norm after each mp layer: 144.55247497558594
Norm before input: 0.2552422881126404
Norm after input: 0.5472724437713623
Norm after each mp layer: 3.060502529144287
Norm after each mp layer: 23.4591064453125
Norm after each mp layer: 144.83010864257812
Norm before input: 0.2552422881126404
Norm after input: 0.5472724437713623
Norm after each mp layer: 3.060502529144287
Norm after each mp layer: 23.4591064453125
Norm after each mp layer: 144.83010864257812
Norm before input: 0.2552422881126404
Norm after input: 0.5473954081535339
Norm after each mp layer: 3.0629446506500244
Norm after each mp layer: 23.491851806640625
Norm after each mp layer: 145.08285522460938
Norm before input: 0.2552422881126404
Norm after input: 0.5473954081535339
Norm after each mp layer: 3.0629446506500244
Norm after each mp layer: 23.491851806640625
Norm after each mp layer: 145.08285522460938
Norm before input: 0.2552422881126404
Norm after input: 0.5476251840591431
Norm after each mp layer: 3.0661959648132324
Norm after each mp layer: 23.535490036010742
Norm after each mp layer: 145.36325073242188
Norm before input: 0.2552422881126404
Norm after input: 0.5476251840591431
Norm after each mp layer: 3.0661959648132324
Norm after each mp layer: 23.535490036010742
Norm after each mp layer: 145.36325073242188
Norm before input: 0.2552422881126404
Norm after input: 0.5477469563484192
Norm after each mp layer: 3.068574905395508
Norm after each mp layer: 23.567794799804688
Norm after each mp layer: 145.6224822998047
Norm before input: 0.2552422881126404
Norm after input: 0.5477469563484192
Norm after each mp layer: 3.068574905395508
Norm after each mp layer: 23.567794799804688
Norm after each mp layer: 145.6224822998047
Norm before input: 0.2552422881126404
Norm after input: 0.5479696393013
Norm after each mp layer: 3.071788787841797
Norm after each mp layer: 23.610382080078125
Norm after each mp layer: 145.8865966796875
Epoch: 245, Loss: 0.0052, Energy: 5844348.5000, Train: 99.92%, Valid: 76.80%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5479696393013
Norm after each mp layer: 3.071788787841797
Norm after each mp layer: 23.610382080078125
Norm after each mp layer: 145.8865966796875
Norm before input: 0.2552422881126404
Norm after input: 0.5480980277061462
Norm after each mp layer: 3.0741989612579346
Norm after each mp layer: 23.6430606842041
Norm after each mp layer: 146.14117431640625
Norm before input: 0.2552422881126404
Norm after input: 0.5480980277061462
Norm after each mp layer: 3.0741989612579346
Norm after each mp layer: 23.6430606842041
Norm after each mp layer: 146.14117431640625
Norm before input: 0.2552422881126404
Norm after input: 0.5483072996139526
Norm after each mp layer: 3.0772883892059326
Norm after each mp layer: 23.683984756469727
Norm after each mp layer: 146.4013671875
Norm before input: 0.2552422881126404
Norm after input: 0.5483072996139526
Norm after each mp layer: 3.0772883892059326
Norm after each mp layer: 23.683984756469727
Norm after each mp layer: 146.40138244628906
Norm before input: 0.2552422881126404
Norm after input: 0.5484484434127808
Norm after each mp layer: 3.0797653198242188
Norm after each mp layer: 23.71769905090332
Norm after each mp layer: 146.6627960205078
Norm before input: 0.2552422881126404
Norm after input: 0.5484484434127808
Norm after each mp layer: 3.0797653198242188
Norm after each mp layer: 23.71769905090332
Norm after each mp layer: 146.6627960205078
Norm before input: 0.2552422881126404
Norm after input: 0.5486399531364441
Norm after each mp layer: 3.0826709270477295
Norm after each mp layer: 23.756458282470703
Norm after each mp layer: 146.92088317871094
Norm before input: 0.2552422881126404
Norm after input: 0.5486399531364441
Norm after each mp layer: 3.0826709270477295
Norm after each mp layer: 23.756458282470703
Norm after each mp layer: 146.92088317871094
Norm before input: 0.2552422881126404
Norm after input: 0.5487901568412781
Norm after each mp layer: 3.085211753845215
Norm after each mp layer: 23.79075050354004
Norm after each mp layer: 147.17279052734375
Epoch: 250, Loss: 0.0049, Energy: 5964917.0000, Train: 99.92%, Valid: 76.80%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5487901568412781
Norm after each mp layer: 3.085211753845215
Norm after each mp layer: 23.79075050354004
Norm after each mp layer: 147.17279052734375
Norm before input: 0.2552422881126404
Norm after input: 0.5489650964736938
Norm after each mp layer: 3.0879547595977783
Norm after each mp layer: 23.827463150024414
Norm after each mp layer: 147.42332458496094
Norm before input: 0.2552422881126404
Norm after input: 0.5489650964736938
Norm after each mp layer: 3.0879547595977783
Norm after each mp layer: 23.827463150024414
Norm after each mp layer: 147.42332458496094
Norm before input: 0.2552422881126404
Norm after input: 0.5491230487823486
Norm after each mp layer: 3.0905370712280273
Norm after each mp layer: 23.862281799316406
Norm after each mp layer: 147.6752471923828
Norm before input: 0.2552422881126404
Norm after input: 0.5491230487823486
Norm after each mp layer: 3.0905370712280273
Norm after each mp layer: 23.862281799316406
Norm after each mp layer: 147.6752471923828
Norm before input: 0.2552422881126404
Norm after input: 0.5492879152297974
Norm after each mp layer: 3.0931694507598877
Norm after each mp layer: 23.897705078125
Norm after each mp layer: 147.9272918701172
Norm before input: 0.2552422881126404
Norm after input: 0.5492879152297974
Norm after each mp layer: 3.0931694507598877
Norm after each mp layer: 23.897703170776367
Norm after each mp layer: 147.92727661132812
Norm before input: 0.2552422881126404
Norm after input: 0.5494516491889954
Norm after each mp layer: 3.0957863330841064
Norm after each mp layer: 23.932876586914062
Norm after each mp layer: 148.17596435546875
Norm before input: 0.2552422881126404
Norm after input: 0.5494516491889954
Norm after each mp layer: 3.0957863330841064
Norm after each mp layer: 23.932876586914062
Norm after each mp layer: 148.17596435546875
Norm before input: 0.2552422881126404
Norm after input: 0.5496077537536621
Norm after each mp layer: 3.0983383655548096
Norm after each mp layer: 23.967100143432617
Norm after each mp layer: 148.41880798339844
Epoch: 255, Loss: 0.0047, Energy: 6079707.5000, Train: 99.92%, Valid: 76.80%, Test: 76.70%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5496077537536621
Norm after each mp layer: 3.0983383655548096
Norm after each mp layer: 23.967100143432617
Norm after each mp layer: 148.41880798339844
Norm before input: 0.2552422881126404
Norm after input: 0.5497763752937317
Norm after each mp layer: 3.1009883880615234
Norm after each mp layer: 24.002492904663086
Norm after each mp layer: 148.6604766845703
Norm before input: 0.2552422881126404
Norm after input: 0.5497763752937317
Norm after each mp layer: 3.1009883880615234
Norm after each mp layer: 24.002492904663086
Norm after each mp layer: 148.6604766845703
Norm before input: 0.2552422881126404
Norm after input: 0.549923300743103
Norm after each mp layer: 3.1034321784973145
Norm after each mp layer: 24.035491943359375
Norm after each mp layer: 148.90451049804688
Norm before input: 0.2552422881126404
Norm after input: 0.549923300743103
Norm after each mp layer: 3.1034321784973145
Norm after each mp layer: 24.035491943359375
Norm after each mp layer: 148.90451049804688
Norm before input: 0.2552422881126404
Norm after input: 0.5500994920730591
Norm after each mp layer: 3.106135845184326
Norm after each mp layer: 24.07143211364746
Norm after each mp layer: 149.14492797851562
Norm before input: 0.2552422881126404
Norm after input: 0.5500994920730591
Norm after each mp layer: 3.106135845184326
Norm after each mp layer: 24.07143211364746
Norm after each mp layer: 149.14492797851562
Norm before input: 0.2552422881126404
Norm after input: 0.5502303242683411
Norm after each mp layer: 3.108409881591797
Norm after each mp layer: 24.102487564086914
Norm after each mp layer: 149.3891143798828
Norm before input: 0.2552422881126404
Norm after input: 0.5502303242683411
Norm after each mp layer: 3.108409881591797
Norm after each mp layer: 24.102487564086914
Norm after each mp layer: 149.3891143798828
Norm before input: 0.2552422881126404
Norm after input: 0.5504225492477417
Norm after each mp layer: 3.1112630367279053
Norm after each mp layer: 24.13983154296875
Norm after each mp layer: 149.61785888671875
Epoch: 260, Loss: 0.0044, Energy: 6190794.5000, Train: 99.92%, Valid: 76.60%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5504225492477417
Norm after each mp layer: 3.1112630367279053
Norm after each mp layer: 24.13983154296875
Norm after each mp layer: 149.61785888671875
Norm before input: 0.2552422881126404
Norm after input: 0.5505214333534241
Norm after each mp layer: 3.1132185459136963
Norm after each mp layer: 24.167264938354492
Norm after each mp layer: 149.86363220214844
Norm before input: 0.2552422881126404
Norm after input: 0.5505214333534241
Norm after each mp layer: 3.1132185459136963
Norm after each mp layer: 24.167264938354492
Norm after each mp layer: 149.86363220214844
Norm before input: 0.2552422881126404
Norm after input: 0.5507562756538391
Norm after each mp layer: 3.1164567470550537
Norm after each mp layer: 24.20879364013672
Norm after each mp layer: 150.08258056640625
Norm before input: 0.2552422881126404
Norm after input: 0.5507562756538391
Norm after each mp layer: 3.1164567470550537
Norm after each mp layer: 24.20879364013672
Norm after each mp layer: 150.08258056640625
Norm before input: 0.2552422881126404
Norm after input: 0.5507887005805969
Norm after each mp layer: 3.1177546977996826
Norm after each mp layer: 24.22894859313965
Norm after each mp layer: 150.34225463867188
Norm before input: 0.2552422881126404
Norm after input: 0.5507887005805969
Norm after each mp layer: 3.1177546977996826
Norm after each mp layer: 24.22894859313965
Norm after each mp layer: 150.34225463867188
Norm before input: 0.2552422881126404
Norm after input: 0.551111102104187
Norm after each mp layer: 3.121807336807251
Norm after each mp layer: 24.27939224243164
Norm after each mp layer: 150.5409698486328
Norm before input: 0.2552422881126404
Norm after input: 0.551111102104187
Norm after each mp layer: 3.121807336807251
Norm after each mp layer: 24.27939224243164
Norm after each mp layer: 150.5409698486328
Norm before input: 0.2552422881126404
Norm after input: 0.5510426163673401
Norm after each mp layer: 3.122164726257324
Norm after each mp layer: 24.28851890563965
Norm after each mp layer: 150.8089141845703
Epoch: 265, Loss: 0.0044, Energy: 6302948.0000, Train: 99.92%, Valid: 76.60%, Test: 76.60%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5510426163673401
Norm after each mp layer: 3.122164726257324
Norm after each mp layer: 24.28851890563965
Norm after each mp layer: 150.8089141845703
Norm before input: 0.2552422881126404
Norm after input: 0.5514321327209473
Norm after each mp layer: 3.126775026321411
Norm after each mp layer: 24.345693588256836
Norm after each mp layer: 151.00440979003906
Norm before input: 0.2552422881126404
Norm after input: 0.5514321327209473
Norm after each mp layer: 3.126775026321411
Norm after each mp layer: 24.345693588256836
Norm after each mp layer: 151.00440979003906
Norm before input: 0.2552422881126404
Norm after input: 0.5513631105422974
Norm after each mp layer: 3.127225637435913
Norm after each mp layer: 24.35431671142578
Norm after each mp layer: 151.24656677246094
Norm before input: 0.2552422881126404
Norm after input: 0.5513631105422974
Norm after each mp layer: 3.127225637435913
Norm after each mp layer: 24.35431671142578
Norm after each mp layer: 151.24656677246094
Norm before input: 0.2552422881126404
Norm after input: 0.5516663193702698
Norm after each mp layer: 3.1309242248535156
Norm after each mp layer: 24.40211296081543
Norm after each mp layer: 151.4733123779297
Norm before input: 0.2552422881126404
Norm after input: 0.5516663193702698
Norm after each mp layer: 3.1309242248535156
Norm after each mp layer: 24.40211296081543
Norm after each mp layer: 151.4733123779297
Norm before input: 0.2552422881126404
Norm after input: 0.5517280697822571
Norm after each mp layer: 3.1325366497039795
Norm after each mp layer: 24.424222946166992
Norm after each mp layer: 151.6963653564453
Norm before input: 0.2552422881126404
Norm after input: 0.5517280697822571
Norm after each mp layer: 3.1325366497039795
Norm after each mp layer: 24.424222946166992
Norm after each mp layer: 151.6963653564453
Norm before input: 0.2552422881126404
Norm after input: 0.5518813729286194
Norm after each mp layer: 3.134949207305908
Norm after each mp layer: 24.455842971801758
Norm after each mp layer: 151.90573120117188
Epoch: 270, Loss: 0.0040, Energy: 6406855.0000, Train: 99.92%, Valid: 76.80%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5518813729286194
Norm after each mp layer: 3.134949207305908
Norm after each mp layer: 24.455842971801758
Norm after each mp layer: 151.90573120117188
Norm before input: 0.2552422881126404
Norm after input: 0.5520765781402588
Norm after each mp layer: 3.137603282928467
Norm after each mp layer: 24.49179458618164
Norm after each mp layer: 152.13192749023438
Norm before input: 0.2552422881126404
Norm after input: 0.5520765781402588
Norm after each mp layer: 3.137603282928467
Norm after each mp layer: 24.49179458618164
Norm after each mp layer: 152.13192749023438
Norm before input: 0.2552422881126404
Norm after input: 0.5521076321601868
Norm after each mp layer: 3.1389050483703613
Norm after each mp layer: 24.510223388671875
Norm after each mp layer: 152.3430633544922
Norm before input: 0.2552422881126404
Norm after input: 0.5521076321601868
Norm after each mp layer: 3.1389050483703613
Norm after each mp layer: 24.510223388671875
Norm after each mp layer: 152.3430633544922
Norm before input: 0.2552422881126404
Norm after input: 0.5523951649665833
Norm after each mp layer: 3.1425065994262695
Norm after each mp layer: 24.555707931518555
Norm after each mp layer: 152.5277099609375
Norm before input: 0.2552422881126404
Norm after input: 0.5523951649665833
Norm after each mp layer: 3.1425065994262695
Norm after each mp layer: 24.555707931518555
Norm after each mp layer: 152.5277099609375
Norm before input: 0.2552422881126404
Norm after input: 0.5523643493652344
Norm after each mp layer: 3.143005847930908
Norm after each mp layer: 24.567312240600586
Norm after each mp layer: 152.79490661621094
Norm before input: 0.2552422881126404
Norm after input: 0.5523643493652344
Norm after each mp layer: 3.143005847930908
Norm after each mp layer: 24.567312240600586
Norm after each mp layer: 152.79490661621094
Norm before input: 0.2552422881126404
Norm after input: 0.5526608228683472
Norm after each mp layer: 3.1467857360839844
Norm after each mp layer: 24.613361358642578
Norm after each mp layer: 152.94920349121094
Epoch: 275, Loss: 0.0039, Energy: 6506955.0000, Train: 99.92%, Valid: 76.80%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5526608228683472
Norm after each mp layer: 3.1467857360839844
Norm after each mp layer: 24.613361358642578
Norm after each mp layer: 152.94920349121094
Norm before input: 0.2552422881126404
Norm after input: 0.5526700615882874
Norm after each mp layer: 3.1476643085479736
Norm after each mp layer: 24.628812789916992
Norm after each mp layer: 153.19737243652344
Norm before input: 0.2552422881126404
Norm after input: 0.5526700615882874
Norm after each mp layer: 3.1476643085479736
Norm after each mp layer: 24.628812789916992
Norm after each mp layer: 153.19737243652344
Norm before input: 0.2552422881126404
Norm after input: 0.552879810333252
Norm after each mp layer: 3.1504714488983154
Norm after each mp layer: 24.665430068969727
Norm after each mp layer: 153.39256286621094
Norm before input: 0.2552422881126404
Norm after input: 0.552879810333252
Norm after each mp layer: 3.1504714488983154
Norm after each mp layer: 24.665430068969727
Norm after each mp layer: 153.39256286621094
Norm before input: 0.2552422881126404
Norm after input: 0.5529882311820984
Norm after each mp layer: 3.1524157524108887
Norm after each mp layer: 24.691181182861328
Norm after each mp layer: 153.5870361328125
Norm before input: 0.2552422881126404
Norm after input: 0.5529882311820984
Norm after each mp layer: 3.1524157524108887
Norm after each mp layer: 24.691181182861328
Norm after each mp layer: 153.5870361328125
Norm before input: 0.2552422881126404
Norm after input: 0.553092896938324
Norm after each mp layer: 3.1541900634765625
Norm after each mp layer: 24.716516494750977
Norm after each mp layer: 153.81263732910156
Norm before input: 0.2552422881126404
Norm after input: 0.553092896938324
Norm after each mp layer: 3.1541900634765625
Norm after each mp layer: 24.716516494750977
Norm after each mp layer: 153.81263732910156
Norm before input: 0.2552422881126404
Norm after input: 0.553301215171814
Norm after each mp layer: 3.1570029258728027
Norm after each mp layer: 24.752826690673828
Norm after each mp layer: 154.00302124023438
Epoch: 280, Loss: 0.0036, Energy: 6605909.5000, Train: 99.92%, Valid: 76.80%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.553301215171814
Norm after each mp layer: 3.1570029258728027
Norm after each mp layer: 24.752826690673828
Norm after each mp layer: 154.00302124023438
Norm before input: 0.2552422881126404
Norm after input: 0.5533155202865601
Norm after each mp layer: 3.158015489578247
Norm after each mp layer: 24.768394470214844
Norm after each mp layer: 154.226318359375
Norm before input: 0.2552422881126404
Norm after input: 0.5533155202865601
Norm after each mp layer: 3.158015489578247
Norm after each mp layer: 24.768394470214844
Norm after each mp layer: 154.226318359375
Norm before input: 0.2552422881126404
Norm after input: 0.5535908937454224
Norm after each mp layer: 3.161466598510742
Norm after each mp layer: 24.811737060546875
Norm after each mp layer: 154.39756774902344
Norm before input: 0.2552422881126404
Norm after input: 0.5535908937454224
Norm after each mp layer: 3.161466598510742
Norm after each mp layer: 24.811737060546875
Norm after each mp layer: 154.39756774902344
Norm before input: 0.2552422881126404
Norm after input: 0.5535700917243958
Norm after each mp layer: 3.162076234817505
Norm after each mp layer: 24.82338523864746
Norm after each mp layer: 154.64161682128906
Norm before input: 0.2552422881126404
Norm after input: 0.5535700917243958
Norm after each mp layer: 3.162076234817505
Norm after each mp layer: 24.82338523864746
Norm after each mp layer: 154.64161682128906
Norm before input: 0.2552422881126404
Norm after input: 0.5538432002067566
Norm after each mp layer: 3.1655867099761963
Norm after each mp layer: 24.866270065307617
Norm after each mp layer: 154.79183959960938
Norm before input: 0.2552422881126404
Norm after input: 0.5538432002067566
Norm after each mp layer: 3.1655867099761963
Norm after each mp layer: 24.866270065307617
Norm after each mp layer: 154.79183959960938
Norm before input: 0.2552422881126404
Norm after input: 0.5538567900657654
Norm after each mp layer: 3.166465997695923
Norm after each mp layer: 24.88134002685547
Norm after each mp layer: 155.03604125976562
Epoch: 285, Loss: 0.0035, Energy: 6703734.5000, Train: 99.92%, Valid: 76.80%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5538567900657654
Norm after each mp layer: 3.166465997695923
Norm after each mp layer: 24.88134002685547
Norm after each mp layer: 155.03604125976562
Norm before input: 0.2552422881126404
Norm after input: 0.5540704727172852
Norm after each mp layer: 3.169344186782837
Norm after each mp layer: 24.917673110961914
Norm after each mp layer: 155.2064666748047
Norm before input: 0.2552422881126404
Norm after input: 0.5540704727172852
Norm after each mp layer: 3.169344186782837
Norm after each mp layer: 24.917673110961914
Norm after each mp layer: 155.2064666748047
Norm before input: 0.2552422881126404
Norm after input: 0.5541461110115051
Norm after each mp layer: 3.170893430709839
Norm after each mp layer: 24.939144134521484
Norm after each mp layer: 155.40841674804688
Norm before input: 0.2552422881126404
Norm after input: 0.5541461110115051
Norm after each mp layer: 3.170893430709839
Norm after each mp layer: 24.939144134521484
Norm after each mp layer: 155.40841674804688
Norm before input: 0.2552422881126404
Norm after input: 0.5542895197868347
Norm after each mp layer: 3.173038959503174
Norm after each mp layer: 24.967792510986328
Norm after each mp layer: 155.60142517089844
Norm before input: 0.2552422881126404
Norm after input: 0.5542895197868347
Norm after each mp layer: 3.173038959503174
Norm after each mp layer: 24.967792510986328
Norm after each mp layer: 155.60142517089844
Norm before input: 0.2552422881126404
Norm after input: 0.5544283986091614
Norm after each mp layer: 3.1751608848571777
Norm after each mp layer: 24.995868682861328
Norm after each mp layer: 155.78897094726562
Norm before input: 0.2552422881126404
Norm after input: 0.5544283986091614
Norm after each mp layer: 3.1751608848571777
Norm after each mp layer: 24.995868682861328
Norm after each mp layer: 155.78897094726562
Norm before input: 0.2552422881126404
Norm after input: 0.5545071959495544
Norm after each mp layer: 3.1767044067382812
Norm after each mp layer: 25.017406463623047
Norm after each mp layer: 155.99082946777344
Epoch: 290, Loss: 0.0033, Energy: 6796772.0000, Train: 99.92%, Valid: 76.60%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5545071959495544
Norm after each mp layer: 3.1767044067382812
Norm after each mp layer: 25.017406463623047
Norm after each mp layer: 155.99082946777344
Norm before input: 0.2552422881126404
Norm after input: 0.5547012090682983
Norm after each mp layer: 3.1793267726898193
Norm after each mp layer: 25.05121612548828
Norm after each mp layer: 156.1646270751953
Norm before input: 0.2552422881126404
Norm after input: 0.5547012090682983
Norm after each mp layer: 3.1793267726898193
Norm after each mp layer: 25.05121612548828
Norm after each mp layer: 156.1646270751953
Norm before input: 0.2552422881126404
Norm after input: 0.5547289848327637
Norm after each mp layer: 3.1803271770477295
Norm after each mp layer: 25.067081451416016
Norm after each mp layer: 156.3851776123047
Norm before input: 0.2552422881126404
Norm after input: 0.5547289848327637
Norm after each mp layer: 3.1803271770477295
Norm after each mp layer: 25.067081451416016
Norm after each mp layer: 156.3851776123047
Norm before input: 0.2552422881126404
Norm after input: 0.5549694299697876
Norm after each mp layer: 3.183457374572754
Norm after each mp layer: 25.105676651000977
Norm after each mp layer: 156.52503967285156
Norm before input: 0.2552422881126404
Norm after input: 0.5549694299697876
Norm after each mp layer: 3.183457374572754
Norm after each mp layer: 25.105676651000977
Norm after each mp layer: 156.52503967285156
Norm before input: 0.2552422881126404
Norm after input: 0.5549505352973938
Norm after each mp layer: 3.1839563846588135
Norm after each mp layer: 25.116281509399414
Norm after each mp layer: 156.76499938964844
Norm before input: 0.2552422881126404
Norm after input: 0.5549505352973938
Norm after each mp layer: 3.1839563846588135
Norm after each mp layer: 25.116281509399414
Norm after each mp layer: 156.76499938964844
Norm before input: 0.2552422881126404
Norm after input: 0.555228054523468
Norm after each mp layer: 3.1874098777770996
Norm after each mp layer: 25.158653259277344
Norm after each mp layer: 156.89743041992188
Epoch: 295, Loss: 0.0033, Energy: 6886170.0000, Train: 99.92%, Valid: 76.60%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.555228054523468
Norm after each mp layer: 3.1874098777770996
Norm after each mp layer: 25.158653259277344
Norm after each mp layer: 156.89743041992188
Norm before input: 0.2552422881126404
Norm after input: 0.5551870465278625
Norm after each mp layer: 3.187725782394409
Norm after each mp layer: 25.16659164428711
Norm after each mp layer: 157.13287353515625
Norm before input: 0.2552422881126404
Norm after input: 0.5551870465278625
Norm after each mp layer: 3.187725782394409
Norm after each mp layer: 25.16659164428711
Norm after each mp layer: 157.13287353515625
Norm before input: 0.2552422881126404
Norm after input: 0.5554545521736145
Norm after each mp layer: 3.191049098968506
Norm after each mp layer: 25.207653045654297
Norm after each mp layer: 157.26889038085938
Norm before input: 0.2552422881126404
Norm after input: 0.5554545521736145
Norm after each mp layer: 3.191049098968506
Norm after each mp layer: 25.207653045654297
Norm after each mp layer: 157.26890563964844
Norm before input: 0.2552422881126404
Norm after input: 0.5554438233375549
Norm after each mp layer: 3.1916282176971436
Norm after each mp layer: 25.21861457824707
Norm after each mp layer: 157.49461364746094
Norm before input: 0.2552422881126404
Norm after input: 0.5554438233375549
Norm after each mp layer: 3.1916282176971436
Norm after each mp layer: 25.21861457824707
Norm after each mp layer: 157.49461364746094
Norm before input: 0.2552422881126404
Norm after input: 0.5556612610816956
Norm after each mp layer: 3.1944944858551025
Norm after each mp layer: 25.254077911376953
Norm after each mp layer: 157.6307830810547
Norm before input: 0.2552422881126404
Norm after input: 0.5556612610816956
Norm after each mp layer: 3.1944944858551025
Norm after each mp layer: 25.254077911376953
Norm after each mp layer: 157.6307830810547
Norm before input: 0.2552422881126404
Norm after input: 0.5556961297988892
Norm after each mp layer: 3.195488691329956
Norm after each mp layer: 25.26976203918457
Norm after each mp layer: 157.8419647216797
Epoch: 300, Loss: 0.0031, Energy: 6976371.0000, Train: 99.92%, Valid: 76.60%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5556961297988892
Norm after each mp layer: 3.195488691329956
Norm after each mp layer: 25.26976203918457
Norm after each mp layer: 157.8419647216797
Norm before input: 0.2552422881126404
Norm after input: 0.5558738708496094
Norm after each mp layer: 3.1979219913482666
Norm after each mp layer: 25.300811767578125
Norm after each mp layer: 157.9947967529297
Norm before input: 0.2552422881126404
Norm after input: 0.5558738708496094
Norm after each mp layer: 3.1979219913482666
Norm after each mp layer: 25.300811767578125
Norm after each mp layer: 157.9947967529297
Norm before input: 0.2552422881126404
Norm after input: 0.5559362769126892
Norm after each mp layer: 3.199220895767212
Norm after each mp layer: 25.31934356689453
Norm after each mp layer: 158.18429565429688
Norm before input: 0.2552422881126404
Norm after input: 0.5559362769126892
Norm after each mp layer: 3.199220895767212
Norm after each mp layer: 25.31934356689453
Norm after each mp layer: 158.18429565429688
Norm before input: 0.2552422881126404
Norm after input: 0.5560908913612366
Norm after each mp layer: 3.2014083862304688
Norm after each mp layer: 25.34779167175293
Norm after each mp layer: 158.34402465820312
Norm before input: 0.2552422881126404
Norm after input: 0.5560908913612366
Norm after each mp layer: 3.2014083862304688
Norm after each mp layer: 25.34779167175293
Norm after each mp layer: 158.34402465820312
Norm before input: 0.2552422881126404
Norm after input: 0.5561690330505371
Norm after each mp layer: 3.2028298377990723
Norm after each mp layer: 25.367902755737305
Norm after each mp layer: 158.53115844726562
Norm before input: 0.2552422881126404
Norm after input: 0.5561690330505371
Norm after each mp layer: 3.2028298377990723
Norm after each mp layer: 25.367902755737305
Norm after each mp layer: 158.53115844726562
Norm before input: 0.2552422881126404
Norm after input: 0.55631422996521
Norm after each mp layer: 3.204946756362915
Norm after each mp layer: 25.395217895507812
Norm after each mp layer: 158.68585205078125
Epoch: 305, Loss: 0.0030, Energy: 7060168.5000, Train: 99.92%, Valid: 76.60%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.55631422996521
Norm after each mp layer: 3.204946756362915
Norm after each mp layer: 25.395217895507812
Norm after each mp layer: 158.68585205078125
Norm before input: 0.2552422881126404
Norm after input: 0.5563875436782837
Norm after each mp layer: 3.2063093185424805
Norm after each mp layer: 25.4146728515625
Norm after each mp layer: 158.87451171875
Norm before input: 0.2552422881126404
Norm after input: 0.5563875436782837
Norm after each mp layer: 3.2063093185424805
Norm after each mp layer: 25.4146728515625
Norm after each mp layer: 158.87451171875
Norm before input: 0.2552422881126404
Norm after input: 0.5565472841262817
Norm after each mp layer: 3.2085323333740234
Norm after each mp layer: 25.443431854248047
Norm after each mp layer: 159.0288543701172
Norm before input: 0.2552422881126404
Norm after input: 0.5565472841262817
Norm after each mp layer: 3.2085323333740234
Norm after each mp layer: 25.443431854248047
Norm after each mp layer: 159.0288543701172
Norm before input: 0.2552422881126404
Norm after input: 0.5565931797027588
Norm after each mp layer: 3.209629535675049
Norm after each mp layer: 25.459754943847656
Norm after each mp layer: 159.2221221923828
Norm before input: 0.2552422881126404
Norm after input: 0.5565931797027588
Norm after each mp layer: 3.209629535675049
Norm after each mp layer: 25.459754943847656
Norm after each mp layer: 159.2221221923828
Norm before input: 0.2552422881126404
Norm after input: 0.5567879676818848
Norm after each mp layer: 3.2122085094451904
Norm after each mp layer: 25.492172241210938
Norm after each mp layer: 159.35670471191406
Norm before input: 0.2552422881126404
Norm after input: 0.5567879676818848
Norm after each mp layer: 3.2122085094451904
Norm after each mp layer: 25.492172241210938
Norm after each mp layer: 159.35670471191406
Norm before input: 0.2552422881126404
Norm after input: 0.5567806363105774
Norm after each mp layer: 3.212733030319214
Norm after each mp layer: 25.502532958984375
Norm after each mp layer: 159.57542419433594
Epoch: 310, Loss: 0.0029, Energy: 7143651.5000, Train: 99.92%, Valid: 76.60%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5567806363105774
Norm after each mp layer: 3.212733030319214
Norm after each mp layer: 25.502532958984375
Norm after each mp layer: 159.57542419433594
Norm before input: 0.2552422881126404
Norm after input: 0.557046115398407
Norm after each mp layer: 3.2160229682922363
Norm after each mp layer: 25.542461395263672
Norm after each mp layer: 159.6768035888672
Norm before input: 0.2552422881126404
Norm after input: 0.557046115398407
Norm after each mp layer: 3.2160229682922363
Norm after each mp layer: 25.542461395263672
Norm after each mp layer: 159.6768035888672
Norm before input: 0.2552422881126404
Norm after input: 0.5569539666175842
Norm after each mp layer: 3.215702772140503
Norm after each mp layer: 25.543331146240234
Norm after each mp layer: 159.92425537109375
Norm before input: 0.2552422881126404
Norm after input: 0.5569539666175842
Norm after each mp layer: 3.215702772140503
Norm after each mp layer: 25.543331146240234
Norm after each mp layer: 159.92425537109375
Norm before input: 0.2552422881126404
Norm after input: 0.5572870373725891
Norm after each mp layer: 3.2196261882781982
Norm after each mp layer: 25.590356826782227
Norm after each mp layer: 160.00302124023438
Norm before input: 0.2552422881126404
Norm after input: 0.5572870373725891
Norm after each mp layer: 3.2196261882781982
Norm after each mp layer: 25.590356826782227
Norm after each mp layer: 160.00302124023438
Norm before input: 0.2552422881126404
Norm after input: 0.5571736693382263
Norm after each mp layer: 3.2190961837768555
Norm after each mp layer: 25.58856773376465
Norm after each mp layer: 160.2527313232422
Norm before input: 0.2552422881126404
Norm after input: 0.5571736693382263
Norm after each mp layer: 3.2190961837768555
Norm after each mp layer: 25.58856773376465
Norm after each mp layer: 160.2527313232422
Norm before input: 0.2552422881126404
Norm after input: 0.5574570894241333
Norm after each mp layer: 3.2225241661071777
Norm after each mp layer: 25.629924774169922
Norm after each mp layer: 160.34304809570312
Epoch: 315, Loss: 0.0029, Energy: 7223793.5000, Train: 99.92%, Valid: 76.80%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5574570894241333
Norm after each mp layer: 3.2225241661071777
Norm after each mp layer: 25.629924774169922
Norm after each mp layer: 160.34304809570312
Norm before input: 0.2552422881126404
Norm after input: 0.5574307441711426
Norm after each mp layer: 3.222825288772583
Norm after each mp layer: 25.637393951416016
Norm after each mp layer: 160.55909729003906
Norm before input: 0.2552422881126404
Norm after input: 0.5574307441711426
Norm after each mp layer: 3.222825288772583
Norm after each mp layer: 25.637393951416016
Norm after each mp layer: 160.55909729003906
Norm before input: 0.2552422881126404
Norm after input: 0.557617723941803
Norm after each mp layer: 3.2252919673919678
Norm after each mp layer: 25.668092727661133
Norm after each mp layer: 160.67849731445312
Norm before input: 0.2552422881126404
Norm after input: 0.557617723941803
Norm after each mp layer: 3.2252919673919678
Norm after each mp layer: 25.668092727661133
Norm after each mp layer: 160.67849731445312
Norm before input: 0.2552422881126404
Norm after input: 0.5576683282852173
Norm after each mp layer: 3.2263641357421875
Norm after each mp layer: 25.683855056762695
Norm after each mp layer: 160.85646057128906
Norm before input: 0.2552422881126404
Norm after input: 0.5576683282852173
Norm after each mp layer: 3.2263641357421875
Norm after each mp layer: 25.683855056762695
Norm after each mp layer: 160.85646057128906
Norm before input: 0.2552422881126404
Norm after input: 0.5577934384346008
Norm after each mp layer: 3.2281758785247803
Norm after each mp layer: 25.70772361755371
Norm after each mp layer: 161.00294494628906
Norm before input: 0.2552422881126404
Norm after input: 0.5577934384346008
Norm after each mp layer: 3.2281758785247803
Norm after each mp layer: 25.70772361755371
Norm after each mp layer: 161.00294494628906
Norm before input: 0.2552422881126404
Norm after input: 0.5578902363777161
Norm after each mp layer: 3.229714870452881
Norm after each mp layer: 25.728471755981445
Norm after each mp layer: 161.1565704345703
Epoch: 320, Loss: 0.0027, Energy: 7303317.5000, Train: 99.92%, Valid: 76.80%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5578902363777161
Norm after each mp layer: 3.229714870452881
Norm after each mp layer: 25.728471755981445
Norm after each mp layer: 161.1565704345703
Norm before input: 0.2552422881126404
Norm after input: 0.5579714775085449
Norm after each mp layer: 3.2310962677001953
Norm after each mp layer: 25.747495651245117
Norm after each mp layer: 161.31591796875
Norm before input: 0.2552422881126404
Norm after input: 0.5579714775085449
Norm after each mp layer: 3.2310962677001953
Norm after each mp layer: 25.747495651245117
Norm after each mp layer: 161.31591796875
Norm before input: 0.2552422881126404
Norm after input: 0.5581079721450806
Norm after each mp layer: 3.2330212593078613
Norm after each mp layer: 25.772533416748047
Norm after each mp layer: 161.45379638671875
Norm before input: 0.2552422881126404
Norm after input: 0.5581079721450806
Norm after each mp layer: 3.2330212593078613
Norm after each mp layer: 25.772533416748047
Norm after each mp layer: 161.45379638671875
Norm before input: 0.2552422881126404
Norm after input: 0.5581456422805786
Norm after each mp layer: 3.233950138092041
Norm after each mp layer: 25.786706924438477
Norm after each mp layer: 161.63372802734375
Norm before input: 0.2552422881126404
Norm after input: 0.5581456422805786
Norm after each mp layer: 3.233950138092041
Norm after each mp layer: 25.786706924438477
Norm after each mp layer: 161.63372802734375
Norm before input: 0.2552422881126404
Norm after input: 0.5583354234695435
Norm after each mp layer: 3.2364258766174316
Norm after each mp layer: 25.8175106048584
Norm after each mp layer: 161.74441528320312
Norm before input: 0.2552422881126404
Norm after input: 0.5583354234695435
Norm after each mp layer: 3.2364258766174316
Norm after each mp layer: 25.8175106048584
Norm after each mp layer: 161.74441528320312
Norm before input: 0.2552422881126404
Norm after input: 0.5583012700080872
Norm after each mp layer: 3.23661470413208
Norm after each mp layer: 25.82366943359375
Norm after each mp layer: 161.95785522460938
Epoch: 325, Loss: 0.0027, Energy: 7378196.5000, Train: 99.92%, Valid: 76.80%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5583012700080872
Norm after each mp layer: 3.23661470413208
Norm after each mp layer: 25.82366943359375
Norm after each mp layer: 161.95785522460938
Norm before input: 0.2552422881126404
Norm after input: 0.5585814118385315
Norm after each mp layer: 3.239982843399048
Norm after each mp layer: 25.864227294921875
Norm after each mp layer: 162.03102111816406
Norm before input: 0.2552422881126404
Norm after input: 0.5585814118385315
Norm after each mp layer: 3.239982843399048
Norm after each mp layer: 25.864227294921875
Norm after each mp layer: 162.03102111816406
Norm before input: 0.2552422881126404
Norm after input: 0.5584546327590942
Norm after each mp layer: 3.2392418384552
Norm after each mp layer: 25.859966278076172
Norm after each mp layer: 162.28152465820312
Norm before input: 0.2552422881126404
Norm after input: 0.5584546327590942
Norm after each mp layer: 3.2392418384552
Norm after each mp layer: 25.859966278076172
Norm after each mp layer: 162.28152465820312
Norm before input: 0.2552422881126404
Norm after input: 0.5587891936302185
Norm after each mp layer: 3.243143081665039
Norm after each mp layer: 25.906238555908203
Norm after each mp layer: 162.32984924316406
Norm before input: 0.2552422881126404
Norm after input: 0.5587891936302185
Norm after each mp layer: 3.243143081665039
Norm after each mp layer: 25.906238555908203
Norm after each mp layer: 162.32984924316406
Norm before input: 0.2552422881126404
Norm after input: 0.5586683750152588
Norm after each mp layer: 3.242448091506958
Norm after each mp layer: 25.902297973632812
Norm after each mp layer: 162.57603454589844
Norm before input: 0.2552422881126404
Norm after input: 0.5586683750152588
Norm after each mp layer: 3.242448091506958
Norm after each mp layer: 25.902297973632812
Norm after each mp layer: 162.57603454589844
Norm before input: 0.2552422881126404
Norm after input: 0.558926522731781
Norm after each mp layer: 3.245570421218872
Norm after each mp layer: 25.93992042541504
Norm after each mp layer: 162.65179443359375
Epoch: 330, Loss: 0.0027, Energy: 7453112.0000, Train: 99.92%, Valid: 76.80%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.558926522731781
Norm after each mp layer: 3.245570421218872
Norm after each mp layer: 25.93992042541504
Norm after each mp layer: 162.65179443359375
Norm before input: 0.2552422881126404
Norm after input: 0.5589048862457275
Norm after each mp layer: 3.2458548545837402
Norm after each mp layer: 25.94671630859375
Norm after each mp layer: 162.8511199951172
Norm before input: 0.2552422881126404
Norm after input: 0.5589048862457275
Norm after each mp layer: 3.2458548545837402
Norm after each mp layer: 25.94671630859375
Norm after each mp layer: 162.8511199951172
Norm before input: 0.2552422881126404
Norm after input: 0.5590687394142151
Norm after each mp layer: 3.248023748397827
Norm after each mp layer: 25.97388458251953
Norm after each mp layer: 162.9607391357422
Norm before input: 0.2552422881126404
Norm after input: 0.5590687394142151
Norm after each mp layer: 3.248023748397827
Norm after each mp layer: 25.97388458251953
Norm after each mp layer: 162.9607391357422
Norm before input: 0.2552422881126404
Norm after input: 0.5591176152229309
Norm after each mp layer: 3.2490153312683105
Norm after each mp layer: 25.988384246826172
Norm after each mp layer: 163.12257385253906
Norm before input: 0.2552422881126404
Norm after input: 0.5591176152229309
Norm after each mp layer: 3.2490153312683105
Norm after each mp layer: 25.988384246826172
Norm after each mp layer: 163.12257385253906
Norm before input: 0.2552422881126404
Norm after input: 0.5592288970947266
Norm after each mp layer: 3.2506353855133057
Norm after each mp layer: 26.00974464416504
Norm after each mp layer: 163.25450134277344
Norm before input: 0.2552422881126404
Norm after input: 0.5592288970947266
Norm after each mp layer: 3.2506353855133057
Norm after each mp layer: 26.00974464416504
Norm after each mp layer: 163.25450134277344
Norm before input: 0.2552422881126404
Norm after input: 0.5593162178993225
Norm after each mp layer: 3.25201678276062
Norm after each mp layer: 26.028465270996094
Norm after each mp layer: 163.39480590820312
Epoch: 335, Loss: 0.0025, Energy: 7525917.5000, Train: 99.92%, Valid: 76.80%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5593162178993225
Norm after each mp layer: 3.25201678276062
Norm after each mp layer: 26.028465270996094
Norm after each mp layer: 163.39480590820312
Norm before input: 0.2552422881126404
Norm after input: 0.5593938827514648
Norm after each mp layer: 3.2533011436462402
Norm after each mp layer: 26.046106338500977
Norm after each mp layer: 163.53836059570312
Norm before input: 0.2552422881126404
Norm after input: 0.5593938827514648
Norm after each mp layer: 3.2533011436462402
Norm after each mp layer: 26.046106338500977
Norm after each mp layer: 163.53836059570312
Norm before input: 0.2552422881126404
Norm after input: 0.5595119595527649
Norm after each mp layer: 3.254991292953491
Norm after each mp layer: 26.068206787109375
Norm after each mp layer: 163.6636962890625
Norm before input: 0.2552422881126404
Norm after input: 0.5595119595527649
Norm after each mp layer: 3.254991292953491
Norm after each mp layer: 26.068206787109375
Norm after each mp layer: 163.6636962890625
Norm before input: 0.2552422881126404
Norm after input: 0.5595548152923584
Norm after each mp layer: 3.2559149265289307
Norm after each mp layer: 26.081960678100586
Norm after each mp layer: 163.82492065429688
Norm before input: 0.2552422881126404
Norm after input: 0.5595548152923584
Norm after each mp layer: 3.2559149265289307
Norm after each mp layer: 26.081960678100586
Norm after each mp layer: 163.82492065429688
Norm before input: 0.2552422881126404
Norm after input: 0.5597174167633057
Norm after each mp layer: 3.2580618858337402
Norm after each mp layer: 26.10892677307129
Norm after each mp layer: 163.92800903320312
Norm before input: 0.2552422881126404
Norm after input: 0.5597174167633057
Norm after each mp layer: 3.2580618858337402
Norm after each mp layer: 26.10892677307129
Norm after each mp layer: 163.92800903320312
Norm before input: 0.2552422881126404
Norm after input: 0.5596964955329895
Norm after each mp layer: 3.2583324909210205
Norm after each mp layer: 26.115543365478516
Norm after each mp layer: 164.120849609375
Epoch: 340, Loss: 0.0024, Energy: 7594088.0000, Train: 99.92%, Valid: 77.00%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5596964955329895
Norm after each mp layer: 3.2583324909210205
Norm after each mp layer: 26.115543365478516
Norm after each mp layer: 164.120849609375
Norm before input: 0.2552422881126404
Norm after input: 0.5599454641342163
Norm after each mp layer: 3.2613446712493896
Norm after each mp layer: 26.151941299438477
Norm after each mp layer: 164.18460083007812
Norm before input: 0.2552422881126404
Norm after input: 0.5599454641342163
Norm after each mp layer: 3.2613446712493896
Norm after each mp layer: 26.151941299438477
Norm after each mp layer: 164.18460083007812
Norm before input: 0.2552422881126404
Norm after input: 0.5598204135894775
Norm after each mp layer: 3.260552167892456
Norm after each mp layer: 26.146827697753906
Norm after each mp layer: 164.42764282226562
Norm before input: 0.2552422881126404
Norm after input: 0.5598204135894775
Norm after each mp layer: 3.260552167892456
Norm after each mp layer: 26.146827697753906
Norm after each mp layer: 164.42764282226562
Norm before input: 0.2552422881126404
Norm after input: 0.5601608157157898
Norm after each mp layer: 3.2644803524017334
Norm after each mp layer: 26.193119049072266
Norm after each mp layer: 164.4475555419922
Norm before input: 0.2552422881126404
Norm after input: 0.5601608157157898
Norm after each mp layer: 3.2644803524017334
Norm after each mp layer: 26.193119049072266
Norm after each mp layer: 164.4475555419922
Norm before input: 0.2552422881126404
Norm after input: 0.5599968433380127
Norm after each mp layer: 3.2632884979248047
Norm after each mp layer: 26.183324813842773
Norm after each mp layer: 164.70619201660156
Norm before input: 0.2552422881126404
Norm after input: 0.5599968433380127
Norm after each mp layer: 3.2632884979248047
Norm after each mp layer: 26.183324813842773
Norm after each mp layer: 164.70619201660156
Norm before input: 0.2552422881126404
Norm after input: 0.5602858662605286
Norm after each mp layer: 3.266674280166626
Norm after each mp layer: 26.22364044189453
Norm after each mp layer: 164.74853515625
Epoch: 345, Loss: 0.0025, Energy: 7663535.5000, Train: 99.92%, Valid: 77.00%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5602858662605286
Norm after each mp layer: 3.266674280166626
Norm after each mp layer: 26.22364044189453
Norm after each mp layer: 164.74853515625
Norm before input: 0.2552422881126404
Norm after input: 0.560222327709198
Norm after each mp layer: 3.2664804458618164
Norm after each mp layer: 26.22478485107422
Norm after each mp layer: 164.956787109375
Norm before input: 0.2552422881126404
Norm after input: 0.560222327709198
Norm after each mp layer: 3.2664804458618164
Norm after each mp layer: 26.22478485107422
Norm after each mp layer: 164.956787109375
Norm before input: 0.2552422881126404
Norm after input: 0.5604038834571838
Norm after each mp layer: 3.268789052963257
Norm after each mp layer: 26.253080368041992
Norm after each mp layer: 165.03854370117188
Norm before input: 0.2552422881126404
Norm after input: 0.5604038834571838
Norm after each mp layer: 3.268789052963257
Norm after each mp layer: 26.253080368041992
Norm after each mp layer: 165.03854370117188
Norm before input: 0.2552422881126404
Norm after input: 0.5604218244552612
Norm after each mp layer: 3.2694125175476074
Norm after each mp layer: 26.263206481933594
Norm after each mp layer: 165.2018280029297
Norm before input: 0.2552422881126404
Norm after input: 0.5604218244552612
Norm after each mp layer: 3.2694125175476074
Norm after each mp layer: 26.263206481933594
Norm after each mp layer: 165.2018280029297
Norm before input: 0.2552422881126404
Norm after input: 0.5605478286743164
Norm after each mp layer: 3.2711269855499268
Norm after each mp layer: 26.285327911376953
Norm after each mp layer: 165.31101989746094
Norm before input: 0.2552422881126404
Norm after input: 0.5605478286743164
Norm after each mp layer: 3.2711269855499268
Norm after each mp layer: 26.285327911376953
Norm after each mp layer: 165.31101989746094
Norm before input: 0.2552422881126404
Norm after input: 0.5606000423431396
Norm after each mp layer: 3.2721006870269775
Norm after each mp layer: 26.299257278442383
Norm after each mp layer: 165.45242309570312
Epoch: 350, Loss: 0.0023, Energy: 7731782.0000, Train: 99.92%, Valid: 77.00%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5606000423431396
Norm after each mp layer: 3.2721006870269775
Norm after each mp layer: 26.299257278442383
Norm after each mp layer: 165.45242309570312
Norm before input: 0.2552422881126404
Norm after input: 0.5607026219367981
Norm after each mp layer: 3.2735908031463623
Norm after each mp layer: 26.318777084350586
Norm after each mp layer: 165.56600952148438
Norm before input: 0.2552422881126404
Norm after input: 0.5607026219367981
Norm after each mp layer: 3.2735908031463623
Norm after each mp layer: 26.318777084350586
Norm after each mp layer: 165.56600952148438
Norm before input: 0.2552422881126404
Norm after input: 0.5607644319534302
Norm after each mp layer: 3.274653911590576
Norm after each mp layer: 26.333765029907227
Norm after each mp layer: 165.701904296875
Norm before input: 0.2552422881126404
Norm after input: 0.5607644319534302
Norm after each mp layer: 3.274653911590576
Norm after each mp layer: 26.333765029907227
Norm after each mp layer: 165.701904296875
Norm before input: 0.2552422881126404
Norm after input: 0.5608676075935364
Norm after each mp layer: 3.2761406898498535
Norm after each mp layer: 26.353363037109375
Norm after each mp layer: 165.8162841796875
Norm before input: 0.2552422881126404
Norm after input: 0.5608676075935364
Norm after each mp layer: 3.2761406898498535
Norm after each mp layer: 26.353363037109375
Norm after each mp layer: 165.8162841796875
Norm before input: 0.2552422881126404
Norm after input: 0.5609192252159119
Norm after each mp layer: 3.2771108150482178
Norm after each mp layer: 26.367198944091797
Norm after each mp layer: 165.95498657226562
Norm before input: 0.2552422881126404
Norm after input: 0.5609192252159119
Norm after each mp layer: 3.2771108150482178
Norm after each mp layer: 26.367198944091797
Norm after each mp layer: 165.95498657226562
Norm before input: 0.2552422881126404
Norm after input: 0.5610402226448059
Norm after each mp layer: 3.278784990310669
Norm after each mp layer: 26.388744354248047
Norm after each mp layer: 166.05975341796875
Epoch: 355, Loss: 0.0022, Energy: 7793280.5000, Train: 99.92%, Valid: 77.00%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5610402226448059
Norm after each mp layer: 3.278784990310669
Norm after each mp layer: 26.388744354248047
Norm after each mp layer: 166.05975341796875
Norm before input: 0.2552422881126404
Norm after input: 0.5610604882240295
Norm after each mp layer: 3.279418706893921
Norm after each mp layer: 26.3990421295166
Norm after each mp layer: 166.21969604492188
Norm before input: 0.2552422881126404
Norm after input: 0.5610604882240295
Norm after each mp layer: 3.279418706893921
Norm after each mp layer: 26.3990421295166
Norm after each mp layer: 166.21969604492188
Norm before input: 0.2552422881126404
Norm after input: 0.5612350702285767
Norm after each mp layer: 3.2816319465637207
Norm after each mp layer: 26.4265079498291
Norm after each mp layer: 166.2994384765625
Norm before input: 0.2552422881126404
Norm after input: 0.5612350702285767
Norm after each mp layer: 3.2816319465637207
Norm after each mp layer: 26.4265079498291
Norm after each mp layer: 166.2994384765625
Norm before input: 0.2552422881126404
Norm after input: 0.5611730217933655
Norm after each mp layer: 3.2814323902130127
Norm after each mp layer: 26.427522659301758
Norm after each mp layer: 166.500244140625
Norm before input: 0.2552422881126404
Norm after input: 0.5611730217933655
Norm after each mp layer: 3.2814323902130127
Norm after each mp layer: 26.427522659301758
Norm after each mp layer: 166.500244140625
Norm before input: 0.2552422881126404
Norm after input: 0.561458945274353
Norm after each mp layer: 3.2847821712493896
Norm after each mp layer: 26.46731185913086
Norm after each mp layer: 166.5217742919922
Norm before input: 0.2552422881126404
Norm after input: 0.561458945274353
Norm after each mp layer: 3.2847821712493896
Norm after each mp layer: 26.46731185913086
Norm after each mp layer: 166.52178955078125
Norm before input: 0.2552422881126404
Norm after input: 0.5612756013870239
Norm after each mp layer: 3.2833306789398193
Norm after each mp layer: 26.45452117919922
Norm after each mp layer: 166.78733825683594
Epoch: 360, Loss: 0.0024, Energy: 7857321.0000, Train: 99.92%, Valid: 77.20%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5612756013870239
Norm after each mp layer: 3.2833306789398193
Norm after each mp layer: 26.45452117919922
Norm after each mp layer: 166.78733825683594
Norm before input: 0.2552422881126404
Norm after input: 0.5616371631622314
Norm after each mp layer: 3.2874224185943604
Norm after each mp layer: 26.50243377685547
Norm after each mp layer: 166.77261352539062
Norm before input: 0.2552422881126404
Norm after input: 0.5616371631622314
Norm after each mp layer: 3.2874224185943604
Norm after each mp layer: 26.50243377685547
Norm after each mp layer: 166.77261352539062
Norm before input: 0.2552422881126404
Norm after input: 0.5614657998085022
Norm after each mp layer: 3.286097288131714
Norm after each mp layer: 26.490617752075195
Norm after each mp layer: 167.0259552001953
Norm before input: 0.2552422881126404
Norm after input: 0.5614657998085022
Norm after each mp layer: 3.286097288131714
Norm after each mp layer: 26.490617752075195
Norm after each mp layer: 167.0259552001953
Norm before input: 0.2552422881126404
Norm after input: 0.5617197155952454
Norm after each mp layer: 3.2890894412994385
Norm after each mp layer: 26.526243209838867
Norm after each mp layer: 167.05810546875
Norm before input: 0.2552422881126404
Norm after input: 0.5617197155952454
Norm after each mp layer: 3.2890894412994385
Norm after each mp layer: 26.526243209838867
Norm after each mp layer: 167.05810546875
Norm before input: 0.2552422881126404
Norm after input: 0.5616769194602966
Norm after each mp layer: 3.2890372276306152
Norm after each mp layer: 26.528657913208008
Norm after each mp layer: 167.24415588378906
Norm before input: 0.2552422881126404
Norm after input: 0.5616769194602966
Norm after each mp layer: 3.2890372276306152
Norm after each mp layer: 26.528657913208008
Norm after each mp layer: 167.24415588378906
Norm before input: 0.2552422881126404
Norm after input: 0.5618276000022888
Norm after each mp layer: 3.290975570678711
Norm after each mp layer: 26.552711486816406
Norm after each mp layer: 167.32008361816406
Epoch: 365, Loss: 0.0022, Energy: 7922494.0000, Train: 99.92%, Valid: 77.20%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5618276000022888
Norm after each mp layer: 3.290975570678711
Norm after each mp layer: 26.552711486816406
Norm after each mp layer: 167.32008361816406
Norm before input: 0.2552422881126404
Norm after input: 0.5618483424186707
Norm after each mp layer: 3.2915868759155273
Norm after each mp layer: 26.562219619750977
Norm after each mp layer: 167.46136474609375
Norm before input: 0.2552422881126404
Norm after input: 0.5618483424186707
Norm after each mp layer: 3.2915868759155273
Norm after each mp layer: 26.562219619750977
Norm after each mp layer: 167.46136474609375
Norm before input: 0.2552422881126404
Norm after input: 0.561959981918335
Norm after each mp layer: 3.2931087017059326
Norm after each mp layer: 26.581924438476562
Norm after each mp layer: 167.55670166015625
Norm before input: 0.2552422881126404
Norm after input: 0.561959981918335
Norm after each mp layer: 3.2931087017059326
Norm after each mp layer: 26.581924438476562
Norm after each mp layer: 167.5567169189453
Norm before input: 0.2552422881126404
Norm after input: 0.5620006322860718
Norm after each mp layer: 3.293898344039917
Norm after each mp layer: 26.59368896484375
Norm after each mp layer: 167.69044494628906
Norm before input: 0.2552422881126404
Norm after input: 0.5620006322860718
Norm after each mp layer: 3.293898344039917
Norm after each mp layer: 26.59368896484375
Norm after each mp layer: 167.69044494628906
Norm before input: 0.2552422881126404
Norm after input: 0.5621090531349182
Norm after each mp layer: 3.2953991889953613
Norm after each mp layer: 26.613067626953125
Norm after each mp layer: 167.78297424316406
Norm before input: 0.2552422881126404
Norm after input: 0.5621090531349182
Norm after each mp layer: 3.2953991889953613
Norm after each mp layer: 26.613067626953125
Norm after each mp layer: 167.78297424316406
Norm before input: 0.2552422881126404
Norm after input: 0.5621328949928284
Norm after each mp layer: 3.296041965484619
Norm after each mp layer: 26.62298011779785
Norm after each mp layer: 167.91958618164062
Epoch: 370, Loss: 0.0021, Energy: 7979669.0000, Train: 99.92%, Valid: 77.20%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5621328949928284
Norm after each mp layer: 3.296041965484619
Norm after each mp layer: 26.62298011779785
Norm after each mp layer: 167.91958618164062
Norm before input: 0.2552422881126404
Norm after input: 0.5622742176055908
Norm after each mp layer: 3.2978761196136475
Norm after each mp layer: 26.64606475830078
Norm after each mp layer: 167.9956817626953
Norm before input: 0.2552422881126404
Norm after input: 0.5622742176055908
Norm after each mp layer: 3.2978761196136475
Norm after each mp layer: 26.64606475830078
Norm after each mp layer: 167.9956817626953
Norm before input: 0.2552422881126404
Norm after input: 0.5622455477714539
Norm after each mp layer: 3.2979655265808105
Norm after each mp layer: 26.650053024291992
Norm after each mp layer: 168.1661834716797
Norm before input: 0.2552422881126404
Norm after input: 0.5622455477714539
Norm after each mp layer: 3.2979655265808105
Norm after each mp layer: 26.650053024291992
Norm after each mp layer: 168.1661834716797
Norm before input: 0.2552422881126404
Norm after input: 0.5624709725379944
Norm after each mp layer: 3.3006646633148193
Norm after each mp layer: 26.68252944946289
Norm after each mp layer: 168.19696044921875
Norm before input: 0.2552422881126404
Norm after input: 0.5624709725379944
Norm after each mp layer: 3.3006646633148193
Norm after each mp layer: 26.68252944946289
Norm after each mp layer: 168.19696044921875
Norm before input: 0.2552422881126404
Norm after input: 0.5623271465301514
Norm after each mp layer: 3.299583911895752
Norm after each mp layer: 26.673450469970703
Norm after each mp layer: 168.4309844970703
Norm before input: 0.2552422881126404
Norm after input: 0.5623271465301514
Norm after each mp layer: 3.299583911895752
Norm after each mp layer: 26.673450469970703
Norm after each mp layer: 168.4309844970703
Norm before input: 0.2552422881126404
Norm after input: 0.5626738667488098
Norm after each mp layer: 3.3034965991973877
Norm after each mp layer: 26.7193546295166
Norm after each mp layer: 168.4027099609375
Epoch: 375, Loss: 0.0023, Energy: 8036373.0000, Train: 99.92%, Valid: 77.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5626738667488098
Norm after each mp layer: 3.3034965991973877
Norm after each mp layer: 26.71935272216797
Norm after each mp layer: 168.4027099609375
Norm before input: 0.2552422881126404
Norm after input: 0.5624614953994751
Norm after each mp layer: 3.3017001152038574
Norm after each mp layer: 26.70220184326172
Norm after each mp layer: 168.6764678955078
Norm before input: 0.2552422881126404
Norm after input: 0.5624614953994751
Norm after each mp layer: 3.3017001152038574
Norm after each mp layer: 26.70220184326172
Norm after each mp layer: 168.6764678955078
Norm before input: 0.2552422881126404
Norm after input: 0.5627686381340027
Norm after each mp layer: 3.3052010536193848
Norm after each mp layer: 26.743335723876953
Norm after each mp layer: 168.66595458984375
Norm before input: 0.2552422881126404
Norm after input: 0.5627686381340027
Norm after each mp layer: 3.3052010536193848
Norm after each mp layer: 26.743335723876953
Norm after each mp layer: 168.66595458984375
Norm before input: 0.2552422881126404
Norm after input: 0.5626609325408936
Norm after each mp layer: 3.304466724395752
Norm after each mp layer: 26.73773765563965
Norm after each mp layer: 168.87799072265625
Norm before input: 0.2552422881126404
Norm after input: 0.5626609325408936
Norm after each mp layer: 3.304466724395752
Norm after each mp layer: 26.73773765563965
Norm after each mp layer: 168.87799072265625
Norm before input: 0.2552422881126404
Norm after input: 0.5628509521484375
Norm after each mp layer: 3.306776762008667
Norm after each mp layer: 26.765615463256836
Norm after each mp layer: 168.92129516601562
Norm before input: 0.2552422881126404
Norm after input: 0.5628509521484375
Norm after each mp layer: 3.306776762008667
Norm after each mp layer: 26.765615463256836
Norm after each mp layer: 168.92127990722656
Norm before input: 0.2552422881126404
Norm after input: 0.5628300905227661
Norm after each mp layer: 3.306919574737549
Norm after each mp layer: 26.769763946533203
Norm after each mp layer: 169.0809326171875
Epoch: 380, Loss: 0.0021, Energy: 8097350.0000, Train: 99.92%, Valid: 77.20%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5628300905227661
Norm after each mp layer: 3.306919574737549
Norm after each mp layer: 26.769763946533203
Norm after each mp layer: 169.0809326171875
Norm before input: 0.2552422881126404
Norm after input: 0.5629682540893555
Norm after each mp layer: 3.3086838722229004
Norm after each mp layer: 26.791837692260742
Norm after each mp layer: 169.1510467529297
Norm before input: 0.2552422881126404
Norm after input: 0.5629682540893555
Norm after each mp layer: 3.3086838722229004
Norm after each mp layer: 26.791837692260742
Norm after each mp layer: 169.1510467529297
Norm before input: 0.2552422881126404
Norm after input: 0.5629668235778809
Norm after each mp layer: 3.3090176582336426
Norm after each mp layer: 26.798200607299805
Norm after each mp layer: 169.2979278564453
Norm before input: 0.2552422881126404
Norm after input: 0.5629668235778809
Norm after each mp layer: 3.3090176582336426
Norm after each mp layer: 26.798200607299805
Norm after each mp layer: 169.2979278564453
Norm before input: 0.2552422881126404
Norm after input: 0.5631113052368164
Norm after each mp layer: 3.3108468055725098
Norm after each mp layer: 26.821027755737305
Norm after each mp layer: 169.360595703125
Norm before input: 0.2552422881126404
Norm after input: 0.5631113052368164
Norm after each mp layer: 3.3108468055725098
Norm after each mp layer: 26.821027755737305
Norm after each mp layer: 169.360595703125
Norm before input: 0.2552422881126404
Norm after input: 0.5630757212638855
Norm after each mp layer: 3.3108317852020264
Norm after each mp layer: 26.823562622070312
Norm after each mp layer: 169.5236358642578
Norm before input: 0.2552422881126404
Norm after input: 0.5630757212638855
Norm after each mp layer: 3.3108317852020264
Norm after each mp layer: 26.823562622070312
Norm after each mp layer: 169.5236358642578
Norm before input: 0.2552422881126404
Norm after input: 0.5632830858230591
Norm after each mp layer: 3.3133063316345215
Norm after each mp layer: 26.85349464416504
Norm after each mp layer: 169.5472412109375
Epoch: 385, Loss: 0.0020, Energy: 8152041.0000, Train: 99.92%, Valid: 77.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5632830858230591
Norm after each mp layer: 3.3133063316345215
Norm after each mp layer: 26.85349464416504
Norm after each mp layer: 169.5472412109375
Norm before input: 0.2552422881126404
Norm after input: 0.5631577372550964
Norm after each mp layer: 3.3123795986175537
Norm after each mp layer: 26.845855712890625
Norm after each mp layer: 169.7584686279297
Norm before input: 0.2552422881126404
Norm after input: 0.5631577372550964
Norm after each mp layer: 3.3123795986175537
Norm after each mp layer: 26.845855712890625
Norm after each mp layer: 169.7584686279297
Norm before input: 0.2552422881126404
Norm after input: 0.5634676218032837
Norm after each mp layer: 3.3159005641937256
Norm after each mp layer: 26.887252807617188
Norm after each mp layer: 169.724365234375
Norm before input: 0.2552422881126404
Norm after input: 0.5634676218032837
Norm after each mp layer: 3.3159005641937256
Norm after each mp layer: 26.887252807617188
Norm after each mp layer: 169.72438049316406
Norm before input: 0.2552422881126404
Norm after input: 0.5632650256156921
Norm after each mp layer: 3.314176082611084
Norm after each mp layer: 26.870628356933594
Norm after each mp layer: 169.9826202392578
Norm before input: 0.2552422881126404
Norm after input: 0.5632650256156921
Norm after each mp layer: 3.314176082611084
Norm after each mp layer: 26.870628356933594
Norm after each mp layer: 169.9826202392578
Norm before input: 0.2552422881126404
Norm after input: 0.5635800361633301
Norm after each mp layer: 3.317727565765381
Norm after each mp layer: 26.912353515625
Norm after each mp layer: 169.95091247558594
Norm before input: 0.2552422881126404
Norm after input: 0.5635800361633301
Norm after each mp layer: 3.317727565765381
Norm after each mp layer: 26.912353515625
Norm after each mp layer: 169.95091247558594
Norm before input: 0.2552422881126404
Norm after input: 0.5634394884109497
Norm after each mp layer: 3.316624402999878
Norm after each mp layer: 26.902515411376953
Norm after each mp layer: 170.17613220214844
Epoch: 390, Loss: 0.0021, Energy: 8204961.0000, Train: 99.92%, Valid: 77.20%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5634394884109497
Norm after each mp layer: 3.316624402999878
Norm after each mp layer: 26.902515411376953
Norm after each mp layer: 170.17613220214844
Norm before input: 0.2552422881126404
Norm after input: 0.5636569261550903
Norm after each mp layer: 3.3191897869110107
Norm after each mp layer: 26.933109283447266
Norm after each mp layer: 170.19241333007812
Norm before input: 0.2552422881126404
Norm after input: 0.5636569261550903
Norm after each mp layer: 3.3191897869110107
Norm after each mp layer: 26.933109283447266
Norm after each mp layer: 170.1924285888672
Norm before input: 0.2552422881126404
Norm after input: 0.5636014342308044
Norm after each mp layer: 3.3189573287963867
Norm after each mp layer: 26.93284034729004
Norm after each mp layer: 170.3644561767578
Norm before input: 0.2552422881126404
Norm after input: 0.5636014342308044
Norm after each mp layer: 3.3189573287963867
Norm after each mp layer: 26.93284034729004
Norm after each mp layer: 170.3644561767578
Norm before input: 0.2552422881126404
Norm after input: 0.5637614130973816
Norm after each mp layer: 3.3209280967712402
Norm after each mp layer: 26.95696449279785
Norm after each mp layer: 170.41094970703125
Norm before input: 0.2552422881126404
Norm after input: 0.5637614130973816
Norm after each mp layer: 3.3209280967712402
Norm after each mp layer: 26.95696449279785
Norm after each mp layer: 170.41094970703125
Norm before input: 0.2552422881126404
Norm after input: 0.5637314915657043
Norm after each mp layer: 3.3209526538848877
Norm after each mp layer: 26.959609985351562
Norm after each mp layer: 170.56710815429688
Norm before input: 0.2552422881126404
Norm after input: 0.5637314915657043
Norm after each mp layer: 3.3209526538848877
Norm after each mp layer: 26.959609985351562
Norm after each mp layer: 170.56710815429688
Norm before input: 0.2552422881126404
Norm after input: 0.5638933181762695
Norm after each mp layer: 3.322942018508911
Norm after each mp layer: 26.98398208618164
Norm after each mp layer: 170.61065673828125
Epoch: 395, Loss: 0.0020, Energy: 8259699.5000, Train: 99.92%, Valid: 77.20%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5638933181762695
Norm after each mp layer: 3.322942018508911
Norm after each mp layer: 26.98398208618164
Norm after each mp layer: 170.61065673828125
Norm before input: 0.2552422881126404
Norm after input: 0.5638329386711121
Norm after each mp layer: 3.3226511478424072
Norm after each mp layer: 26.983198165893555
Norm after each mp layer: 170.78524780273438
Norm before input: 0.2552422881126404
Norm after input: 0.5638329386711121
Norm after each mp layer: 3.3226511478424072
Norm after each mp layer: 26.983198165893555
Norm after each mp layer: 170.78524780273438
Norm before input: 0.2552422881126404
Norm after input: 0.5640516877174377
Norm after each mp layer: 3.3252158164978027
Norm after each mp layer: 27.0140380859375
Norm after each mp layer: 170.79600524902344
Norm before input: 0.2552422881126404
Norm after input: 0.5640516877174377
Norm after each mp layer: 3.3252158164978027
Norm after each mp layer: 27.0140380859375
Norm after each mp layer: 170.79600524902344
Norm before input: 0.2552422881126404
Norm after input: 0.5639159679412842
Norm after each mp layer: 3.324152946472168
Norm after each mp layer: 27.00467300415039
Norm after each mp layer: 171.01341247558594
Norm before input: 0.2552422881126404
Norm after input: 0.5639159679412842
Norm after each mp layer: 3.324152946472168
Norm after each mp layer: 27.00467300415039
Norm after each mp layer: 171.01341247558594
Norm before input: 0.2552422881126404
Norm after input: 0.5642123818397522
Norm after each mp layer: 3.3275139331817627
Norm after each mp layer: 27.044208526611328
Norm after each mp layer: 170.97488403320312
Norm before input: 0.2552422881126404
Norm after input: 0.5642123818397522
Norm after each mp layer: 3.3275139331817627
Norm after each mp layer: 27.044208526611328
Norm after each mp layer: 170.97488403320312
Norm before input: 0.2552422881126404
Norm after input: 0.5640246272087097
Norm after each mp layer: 3.3259220123291016
Norm after each mp layer: 27.028757095336914
Norm after each mp layer: 171.2198944091797
Epoch: 400, Loss: 0.0021, Energy: 8310483.0000, Train: 99.92%, Valid: 77.20%, Test: 77.10%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5640246272087097
Norm after each mp layer: 3.3259220123291016
Norm after each mp layer: 27.028757095336914
Norm after each mp layer: 171.2198944091797
Norm before input: 0.2552422881126404
Norm after input: 0.5643157958984375
Norm after each mp layer: 3.3292150497436523
Norm after each mp layer: 27.067520141601562
Norm after each mp layer: 171.1840362548828
Norm before input: 0.2552422881126404
Norm after input: 0.5643157958984375
Norm after each mp layer: 3.3292150497436523
Norm after each mp layer: 27.067520141601562
Norm after each mp layer: 171.1840362548828
Norm before input: 0.2552422881126404
Norm after input: 0.5641804933547974
Norm after each mp layer: 3.328143358230591
Norm after each mp layer: 27.05783462524414
Norm after each mp layer: 171.39932250976562
Norm before input: 0.2552422881126404
Norm after input: 0.5641804933547974
Norm after each mp layer: 3.328143358230591
Norm after each mp layer: 27.05783462524414
Norm after each mp layer: 171.39932250976562
Norm before input: 0.2552422881126404
Norm after input: 0.5643963813781738
Norm after each mp layer: 3.3306679725646973
Norm after each mp layer: 27.087974548339844
Norm after each mp layer: 171.40333557128906
Norm before input: 0.2552422881126404
Norm after input: 0.5643963813781738
Norm after each mp layer: 3.3306679725646973
Norm after each mp layer: 27.087974548339844
Norm after each mp layer: 171.40333557128906
Norm before input: 0.2552422881126404
Norm after input: 0.564325749874115
Norm after each mp layer: 3.330256938934326
Norm after each mp layer: 27.085594177246094
Norm after each mp layer: 171.57740783691406
Norm before input: 0.2552422881126404
Norm after input: 0.564325749874115
Norm after each mp layer: 3.330256938934326
Norm after each mp layer: 27.085594177246094
Norm after each mp layer: 171.57740783691406
Norm before input: 0.2552422881126404
Norm after input: 0.5644979476928711
Norm after each mp layer: 3.3323283195495605
Norm after each mp layer: 27.110782623291016
Norm after each mp layer: 171.60577392578125
Epoch: 405, Loss: 0.0019, Energy: 8362740.0000, Train: 99.92%, Valid: 77.20%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5644979476928711
Norm after each mp layer: 3.3323283195495605
Norm after each mp layer: 27.110782623291016
Norm after each mp layer: 171.60577392578125
Norm before input: 0.2552422881126404
Norm after input: 0.564444363117218
Norm after each mp layer: 3.3320863246917725
Norm after each mp layer: 27.110355377197266
Norm after each mp layer: 171.7696533203125
Norm before input: 0.2552422881126404
Norm after input: 0.564444363117218
Norm after each mp layer: 3.3320863246917725
Norm after each mp layer: 27.110355377197266
Norm after each mp layer: 171.7696533203125
Norm before input: 0.2552422881126404
Norm after input: 0.5646239519119263
Norm after each mp layer: 3.3342370986938477
Norm after each mp layer: 27.136423110961914
Norm after each mp layer: 171.79135131835938
Norm before input: 0.2552422881126404
Norm after input: 0.5646239519119263
Norm after each mp layer: 3.3342370986938477
Norm after each mp layer: 27.136423110961914
Norm after each mp layer: 171.79135131835938
Norm before input: 0.2552422881126404
Norm after input: 0.5645378828048706
Norm after each mp layer: 3.3336691856384277
Norm after each mp layer: 27.13231086730957
Norm after each mp layer: 171.97369384765625
Norm before input: 0.2552422881126404
Norm after input: 0.5645378828048706
Norm after each mp layer: 3.3336691856384277
Norm after each mp layer: 27.13231086730957
Norm after each mp layer: 171.97369384765625
Norm before input: 0.2552422881126404
Norm after input: 0.56476891040802
Norm after each mp layer: 3.336345911026001
Norm after each mp layer: 27.164247512817383
Norm after each mp layer: 171.96441650390625
Norm before input: 0.2552422881126404
Norm after input: 0.56476891040802
Norm after each mp layer: 3.336345911026001
Norm after each mp layer: 27.164247512817383
Norm after each mp layer: 171.96441650390625
Norm before input: 0.2552422881126404
Norm after input: 0.5646227598190308
Norm after each mp layer: 3.3351588249206543
Norm after each mp layer: 27.153249740600586
Norm after each mp layer: 172.185302734375
Epoch: 410, Loss: 0.0020, Energy: 8410516.0000, Train: 99.92%, Valid: 77.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5646227598190308
Norm after each mp layer: 3.3351588249206543
Norm after each mp layer: 27.153249740600586
Norm after each mp layer: 172.185302734375
Norm before input: 0.2552422881126404
Norm after input: 0.5649070739746094
Norm after each mp layer: 3.3383753299713135
Norm after each mp layer: 27.191143035888672
Norm after each mp layer: 172.1441650390625
Norm before input: 0.2552422881126404
Norm after input: 0.5649070739746094
Norm after each mp layer: 3.3383753299713135
Norm after each mp layer: 27.191143035888672
Norm after each mp layer: 172.1441650390625
Norm before input: 0.2552422881126404
Norm after input: 0.5647352337837219
Norm after each mp layer: 3.336925506591797
Norm after each mp layer: 27.177061080932617
Norm after each mp layer: 172.3796844482422
Norm before input: 0.2552422881126404
Norm after input: 0.5647352337837219
Norm after each mp layer: 3.336925506591797
Norm after each mp layer: 27.177061080932617
Norm after each mp layer: 172.37969970703125
Norm before input: 0.2552422881126404
Norm after input: 0.5650026798248291
Norm after each mp layer: 3.3399648666381836
Norm after each mp layer: 27.212921142578125
Norm after each mp layer: 172.34654235839844
Norm before input: 0.2552422881126404
Norm after input: 0.5650026798248291
Norm after each mp layer: 3.3399648666381836
Norm after each mp layer: 27.212921142578125
Norm after each mp layer: 172.34654235839844
Norm before input: 0.2552422881126404
Norm after input: 0.5648765563964844
Norm after each mp layer: 3.3389716148376465
Norm after each mp layer: 27.203920364379883
Norm after each mp layer: 172.55374145507812
Norm before input: 0.2552422881126404
Norm after input: 0.5648765563964844
Norm after each mp layer: 3.3389716148376465
Norm after each mp layer: 27.203920364379883
Norm after each mp layer: 172.55374145507812
Norm before input: 0.2552422881126404
Norm after input: 0.5650877356529236
Norm after each mp layer: 3.34143328666687
Norm after each mp layer: 27.23332405090332
Norm after each mp layer: 172.55072021484375
Epoch: 415, Loss: 0.0019, Energy: 8461898.0000, Train: 99.92%, Valid: 77.20%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5650877356529236
Norm after each mp layer: 3.34143328666687
Norm after each mp layer: 27.23332405090332
Norm after each mp layer: 172.55072021484375
Norm before input: 0.2552422881126404
Norm after input: 0.5650070309638977
Norm after each mp layer: 3.3409061431884766
Norm after each mp layer: 27.229469299316406
Norm after each mp layer: 172.72628784179688
Norm before input: 0.2552422881126404
Norm after input: 0.5650070309638977
Norm after each mp layer: 3.3409061431884766
Norm after each mp layer: 27.229469299316406
Norm after each mp layer: 172.72628784179688
Norm before input: 0.2552422881126404
Norm after input: 0.5651887059211731
Norm after each mp layer: 3.3430633544921875
Norm after each mp layer: 27.255512237548828
Norm after each mp layer: 172.73829650878906
Norm before input: 0.2552422881126404
Norm after input: 0.5651887059211731
Norm after each mp layer: 3.3430633544921875
Norm after each mp layer: 27.255516052246094
Norm after each mp layer: 172.73829650878906
Norm before input: 0.2552422881126404
Norm after input: 0.5651148557662964
Norm after each mp layer: 3.3425989151000977
Norm after each mp layer: 27.252437591552734
Norm after each mp layer: 172.9099884033203
Norm before input: 0.2552422881126404
Norm after input: 0.5651148557662964
Norm after each mp layer: 3.3425989151000977
Norm after each mp layer: 27.252437591552734
Norm after each mp layer: 172.9099884033203
Norm before input: 0.2552422881126404
Norm after input: 0.565309464931488
Norm after each mp layer: 3.3448829650878906
Norm after each mp layer: 27.27997398376465
Norm after each mp layer: 172.9137725830078
Norm before input: 0.2552422881126404
Norm after input: 0.565309464931488
Norm after each mp layer: 3.3448829650878906
Norm after each mp layer: 27.27997398376465
Norm after each mp layer: 172.9137725830078
Norm before input: 0.2552422881126404
Norm after input: 0.5652039051055908
Norm after each mp layer: 3.34409499168396
Norm after each mp layer: 27.273269653320312
Norm after each mp layer: 173.10484313964844
Epoch: 420, Loss: 0.0019, Energy: 8507737.0000, Train: 99.92%, Valid: 77.20%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5652039051055908
Norm after each mp layer: 3.34409499168396
Norm after each mp layer: 27.273269653320312
Norm after each mp layer: 173.10484313964844
Norm before input: 0.2552422881126404
Norm after input: 0.5654408931732178
Norm after each mp layer: 3.3468165397644043
Norm after each mp layer: 27.305635452270508
Norm after each mp layer: 173.08050537109375
Norm before input: 0.2552422881126404
Norm after input: 0.5654408931732178
Norm after each mp layer: 3.3468165397644043
Norm after each mp layer: 27.305635452270508
Norm after each mp layer: 173.08050537109375
Norm before input: 0.2552422881126404
Norm after input: 0.5652907490730286
Norm after each mp layer: 3.345571517944336
Norm after each mp layer: 27.29376983642578
Norm after each mp layer: 173.29974365234375
Norm before input: 0.2552422881126404
Norm after input: 0.5652907490730286
Norm after each mp layer: 3.345571517944336
Norm after each mp layer: 27.29376983642578
Norm after each mp layer: 173.29974365234375
Norm before input: 0.2552422881126404
Norm after input: 0.5655599236488342
Norm after each mp layer: 3.348616123199463
Norm after each mp layer: 27.329729080200195
Norm after each mp layer: 173.2552032470703
Norm before input: 0.2552422881126404
Norm after input: 0.5655599236488342
Norm after each mp layer: 3.348616123199463
Norm after each mp layer: 27.329729080200195
Norm after each mp layer: 173.2552032470703
Norm before input: 0.2552422881126404
Norm after input: 0.565401554107666
Norm after each mp layer: 3.3472847938537598
Norm after each mp layer: 27.316802978515625
Norm after each mp layer: 173.47950744628906
Norm before input: 0.2552422881126404
Norm after input: 0.565401554107666
Norm after each mp layer: 3.3472847938537598
Norm after each mp layer: 27.316802978515625
Norm after each mp layer: 173.47950744628906
Norm before input: 0.2552422881126404
Norm after input: 0.5656505227088928
Norm after each mp layer: 3.3501217365264893
Norm after each mp layer: 27.350360870361328
Norm after each mp layer: 173.44532775878906
Epoch: 425, Loss: 0.0019, Energy: 8556177.0000, Train: 99.92%, Valid: 77.20%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5656505227088928
Norm after each mp layer: 3.3501217365264893
Norm after each mp layer: 27.350360870361328
Norm after each mp layer: 173.44532775878906
Norm before input: 0.2552422881126404
Norm after input: 0.5655285120010376
Norm after each mp layer: 3.3491578102111816
Norm after each mp layer: 27.341510772705078
Norm after each mp layer: 173.6460723876953
Norm before input: 0.2552422881126404
Norm after input: 0.5655285120010376
Norm after each mp layer: 3.3491578102111816
Norm after each mp layer: 27.341510772705078
Norm after each mp layer: 173.6460723876953
Norm before input: 0.2552422881126404
Norm after input: 0.5657374858856201
Norm after each mp layer: 3.3515801429748535
Norm after each mp layer: 27.370466232299805
Norm after each mp layer: 173.6352081298828
Norm before input: 0.2552422881126404
Norm after input: 0.5657374858856201
Norm after each mp layer: 3.3515801429748535
Norm after each mp layer: 27.370466232299805
Norm after each mp layer: 173.6352081298828
Norm before input: 0.2552422881126404
Norm after input: 0.5656465888023376
Norm after each mp layer: 3.3509302139282227
Norm after each mp layer: 27.36513900756836
Norm after each mp layer: 173.8151092529297
Norm before input: 0.2552422881126404
Norm after input: 0.5656465888023376
Norm after each mp layer: 3.3509302139282227
Norm after each mp layer: 27.36513900756836
Norm after each mp layer: 173.8151092529297
Norm before input: 0.2552422881126404
Norm after input: 0.5658377408981323
Norm after each mp layer: 3.353170156478882
Norm after each mp layer: 27.392061233520508
Norm after each mp layer: 173.81289672851562
Norm before input: 0.2552422881126404
Norm after input: 0.5658377408981323
Norm after each mp layer: 3.353170156478882
Norm after each mp layer: 27.392061233520508
Norm after each mp layer: 173.81288146972656
Norm before input: 0.2552422881126404
Norm after input: 0.5657463669776917
Norm after each mp layer: 3.35251522064209
Norm after each mp layer: 27.386672973632812
Norm after each mp layer: 173.991943359375
Epoch: 430, Loss: 0.0018, Energy: 8600717.0000, Train: 99.92%, Valid: 77.20%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5657463669776917
Norm after each mp layer: 3.35251522064209
Norm after each mp layer: 27.386672973632812
Norm after each mp layer: 173.991943359375
Norm before input: 0.2552422881126404
Norm after input: 0.5659518837928772
Norm after each mp layer: 3.3548996448516846
Norm after each mp layer: 27.415245056152344
Norm after each mp layer: 173.97938537597656
Norm before input: 0.2552422881126404
Norm after input: 0.5659518837928772
Norm after each mp layer: 3.3548996448516846
Norm after each mp layer: 27.415245056152344
Norm after each mp layer: 173.97938537597656
Norm before input: 0.2552422881126404
Norm after input: 0.5658330917358398
Norm after each mp layer: 3.353961944580078
Norm after each mp layer: 27.406700134277344
Norm after each mp layer: 174.17640686035156
Norm before input: 0.2552422881126404
Norm after input: 0.5658330917358398
Norm after each mp layer: 3.353961944580078
Norm after each mp layer: 27.406700134277344
Norm after each mp layer: 174.17640686035156
Norm before input: 0.2552422881126404
Norm after input: 0.5660709738731384
Norm after each mp layer: 3.3566761016845703
Norm after each mp layer: 27.43895149230957
Norm after each mp layer: 174.14205932617188
Norm before input: 0.2552422881126404
Norm after input: 0.5660709738731384
Norm after each mp layer: 3.3566761016845703
Norm after each mp layer: 27.43895149230957
Norm after each mp layer: 174.14205932617188
Norm before input: 0.2552422881126404
Norm after input: 0.5659217834472656
Norm after each mp layer: 3.3554275035858154
Norm after each mp layer: 27.426841735839844
Norm after each mp layer: 174.3585968017578
Norm before input: 0.2552422881126404
Norm after input: 0.5659217834472656
Norm after each mp layer: 3.3554275035858154
Norm after each mp layer: 27.42684555053711
Norm after each mp layer: 174.3585968017578
Norm before input: 0.2552422881126404
Norm after input: 0.5661765933036804
Norm after each mp layer: 3.358311176300049
Norm after each mp layer: 27.460962295532227
Norm after each mp layer: 174.31231689453125
Epoch: 435, Loss: 0.0019, Energy: 8647450.0000, Train: 99.92%, Valid: 77.20%, Test: 76.70%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5661765933036804
Norm after each mp layer: 3.358311176300049
Norm after each mp layer: 27.460962295532227
Norm after each mp layer: 174.31231689453125
Norm before input: 0.2552422881126404
Norm after input: 0.5660278797149658
Norm after each mp layer: 3.357063055038452
Norm after each mp layer: 27.448814392089844
Norm after each mp layer: 174.52865600585938
Norm before input: 0.2552422881126404
Norm after input: 0.5660278797149658
Norm after each mp layer: 3.357063055038452
Norm after each mp layer: 27.448814392089844
Norm after each mp layer: 174.52865600585938
Norm before input: 0.2552422881126404
Norm after input: 0.5662643909454346
Norm after each mp layer: 3.3597569465637207
Norm after each mp layer: 27.480783462524414
Norm after each mp layer: 174.49224853515625
Norm before input: 0.2552422881126404
Norm after input: 0.5662643909454346
Norm after each mp layer: 3.3597569465637207
Norm after each mp layer: 27.480783462524414
Norm after each mp layer: 174.49224853515625
Norm before input: 0.2552422881126404
Norm after input: 0.566142737865448
Norm after each mp layer: 3.3587827682495117
Norm after each mp layer: 27.471668243408203
Norm after each mp layer: 174.68954467773438
Norm before input: 0.2552422881126404
Norm after input: 0.566142737865448
Norm after each mp layer: 3.3587827682495117
Norm after each mp layer: 27.471668243408203
Norm after each mp layer: 174.68954467773438
Norm before input: 0.2552422881126404
Norm after input: 0.5663511753082275
Norm after each mp layer: 3.3611881732940674
Norm after each mp layer: 27.500396728515625
Norm after each mp layer: 174.66859436035156
Norm before input: 0.2552422881126404
Norm after input: 0.5663511753082275
Norm after each mp layer: 3.3611881732940674
Norm after each mp layer: 27.500396728515625
Norm after each mp layer: 174.66859436035156
Norm before input: 0.2552422881126404
Norm after input: 0.5662496089935303
Norm after each mp layer: 3.360417127609253
Norm after each mp layer: 27.493566513061523
Norm after each mp layer: 174.8517303466797
Epoch: 440, Loss: 0.0018, Energy: 8689857.0000, Train: 99.92%, Valid: 77.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5662496089935303
Norm after each mp layer: 3.360417127609253
Norm after each mp layer: 27.493566513061523
Norm after each mp layer: 174.8517303466797
Norm before input: 0.2552422881126404
Norm after input: 0.5664485096931458
Norm after each mp layer: 3.3627207279205322
Norm after each mp layer: 27.521188735961914
Norm after each mp layer: 174.8356170654297
Norm before input: 0.2552422881126404
Norm after input: 0.5664485096931458
Norm after each mp layer: 3.3627207279205322
Norm after each mp layer: 27.521188735961914
Norm after each mp layer: 174.8356170654297
Norm before input: 0.2552422881126404
Norm after input: 0.5663434267044067
Norm after each mp layer: 3.3619115352630615
Norm after each mp layer: 27.513933181762695
Norm after each mp layer: 175.02093505859375
Norm before input: 0.2552422881126404
Norm after input: 0.5663434267044067
Norm after each mp layer: 3.3619115352630615
Norm after each mp layer: 27.513933181762695
Norm after each mp layer: 175.02093505859375
Norm before input: 0.2552422881126404
Norm after input: 0.5665555000305176
Norm after each mp layer: 3.364349365234375
Norm after each mp layer: 27.54306983947754
Norm after each mp layer: 174.99485778808594
Norm before input: 0.2552422881126404
Norm after input: 0.5665555000305176
Norm after each mp layer: 3.364349365234375
Norm after each mp layer: 27.54306983947754
Norm after each mp layer: 174.99485778808594
Norm before input: 0.2552422881126404
Norm after input: 0.5664284229278564
Norm after each mp layer: 3.363313913345337
Norm after each mp layer: 27.53325080871582
Norm after each mp layer: 175.19488525390625
Norm before input: 0.2552422881126404
Norm after input: 0.5664284229278564
Norm after each mp layer: 3.363313913345337
Norm after each mp layer: 27.53325080871582
Norm after each mp layer: 175.19488525390625
Norm before input: 0.2552422881126404
Norm after input: 0.5666634440422058
Norm after each mp layer: 3.365985155105591
Norm after each mp layer: 27.565004348754883
Norm after each mp layer: 175.15280151367188
Epoch: 445, Loss: 0.0018, Energy: 8735193.0000, Train: 99.92%, Valid: 77.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5666634440422058
Norm after each mp layer: 3.365985155105591
Norm after each mp layer: 27.565004348754883
Norm after each mp layer: 175.15280151367188
Norm before input: 0.2552422881126404
Norm after input: 0.5665168762207031
Norm after each mp layer: 3.364748239517212
Norm after each mp layer: 27.552875518798828
Norm after each mp layer: 175.36605834960938
Norm before input: 0.2552422881126404
Norm after input: 0.5665168762207031
Norm after each mp layer: 3.364748239517212
Norm after each mp layer: 27.552875518798828
Norm after each mp layer: 175.36605834960938
Norm before input: 0.2552422881126404
Norm after input: 0.5667601823806763
Norm after each mp layer: 3.367501974105835
Norm after each mp layer: 27.58551025390625
Norm after each mp layer: 175.317138671875
Norm before input: 0.2552422881126404
Norm after input: 0.5667601823806763
Norm after each mp layer: 3.367501974105835
Norm after each mp layer: 27.58551025390625
Norm after each mp layer: 175.317138671875
Norm before input: 0.2552422881126404
Norm after input: 0.5666168928146362
Norm after each mp layer: 3.366297483444214
Norm after each mp layer: 27.57369041442871
Norm after each mp layer: 175.5276641845703
Norm before input: 0.2552422881126404
Norm after input: 0.5666168928146362
Norm after each mp layer: 3.366297483444214
Norm after each mp layer: 27.57369041442871
Norm after each mp layer: 175.5276641845703
Norm before input: 0.2552422881126404
Norm after input: 0.5668453574180603
Norm after each mp layer: 3.368896722793579
Norm after each mp layer: 27.60457420349121
Norm after each mp layer: 175.48680114746094
Norm before input: 0.2552422881126404
Norm after input: 0.5668453574180603
Norm after each mp layer: 3.368896722793579
Norm after each mp layer: 27.60457420349121
Norm after each mp layer: 175.48680114746094
Norm before input: 0.2552422881126404
Norm after input: 0.5667216181755066
Norm after each mp layer: 3.3678884506225586
Norm after each mp layer: 27.59494400024414
Norm after each mp layer: 175.68359375
Epoch: 450, Loss: 0.0018, Energy: 8775511.0000, Train: 99.92%, Valid: 77.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5667216181755066
Norm after each mp layer: 3.3678884506225586
Norm after each mp layer: 27.59494400024414
Norm after each mp layer: 175.68359375
Norm before input: 0.2552422881126404
Norm after input: 0.5669311285018921
Norm after each mp layer: 3.370291233062744
Norm after each mp layer: 27.623632431030273
Norm after each mp layer: 175.65347290039062
Norm before input: 0.2552422881126404
Norm after input: 0.5669311285018921
Norm after each mp layer: 3.370291233062744
Norm after each mp layer: 27.623632431030273
Norm after each mp layer: 175.65347290039062
Norm before input: 0.2552422881126404
Norm after input: 0.5668200254440308
Norm after each mp layer: 3.369410991668701
Norm after each mp layer: 27.615432739257812
Norm after each mp layer: 175.84083557128906
Norm before input: 0.2552422881126404
Norm after input: 0.5668200254440308
Norm after each mp layer: 3.369410991668701
Norm after each mp layer: 27.615432739257812
Norm after each mp layer: 175.84083557128906
Norm before input: 0.2552422881126404
Norm after input: 0.5670244693756104
Norm after each mp layer: 3.3717596530914307
Norm after each mp layer: 27.643529891967773
Norm after each mp layer: 175.81236267089844
Norm before input: 0.2552422881126404
Norm after input: 0.5670244693756104
Norm after each mp layer: 3.3717596530914307
Norm after each mp layer: 27.643529891967773
Norm after each mp layer: 175.81236267089844
Norm before input: 0.2552422881126404
Norm after input: 0.5669088363647461
Norm after each mp layer: 3.3708300590515137
Norm after each mp layer: 27.63477897644043
Norm after each mp layer: 176.0025634765625
Norm before input: 0.2552422881126404
Norm after input: 0.5669088363647461
Norm after each mp layer: 3.3708300590515137
Norm after each mp layer: 27.63477897644043
Norm after each mp layer: 176.0025634765625
Norm before input: 0.2552422881126404
Norm after input: 0.5671243667602539
Norm after each mp layer: 3.373290538787842
Norm after each mp layer: 27.664148330688477
Norm after each mp layer: 175.9653778076172
Epoch: 455, Loss: 0.0017, Energy: 8820045.0000, Train: 99.92%, Valid: 77.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5671243667602539
Norm after each mp layer: 3.373290538787842
Norm after each mp layer: 27.664148330688477
Norm after each mp layer: 175.9653778076172
Norm before input: 0.2552422881126404
Norm after input: 0.5669922232627869
Norm after each mp layer: 3.3721914291381836
Norm after each mp layer: 27.653459548950195
Norm after each mp layer: 176.16683959960938
Norm before input: 0.2552422881126404
Norm after input: 0.5669922232627869
Norm after each mp layer: 3.3721914291381836
Norm after each mp layer: 27.653459548950195
Norm after each mp layer: 176.16683959960938
Norm before input: 0.2552422881126404
Norm after input: 0.5672231316566467
Norm after each mp layer: 3.3748080730438232
Norm after each mp layer: 27.684568405151367
Norm after each mp layer: 176.11795043945312
Norm before input: 0.2552422881126404
Norm after input: 0.5672231316566467
Norm after each mp layer: 3.3748080730438232
Norm after each mp layer: 27.684568405151367
Norm after each mp layer: 176.11795043945312
Norm before input: 0.2552422881126404
Norm after input: 0.5670786499977112
Norm after each mp layer: 3.3735809326171875
Norm after each mp layer: 27.672399520874023
Norm after each mp layer: 176.32791137695312
Norm before input: 0.2552422881126404
Norm after input: 0.5670786499977112
Norm after each mp layer: 3.3735811710357666
Norm after each mp layer: 27.672399520874023
Norm after each mp layer: 176.32791137695312
Norm before input: 0.2552422881126404
Norm after input: 0.5673131942749023
Norm after each mp layer: 3.3762319087982178
Norm after each mp layer: 27.703876495361328
Norm after each mp layer: 176.27536010742188
Norm before input: 0.2552422881126404
Norm after input: 0.5673131942749023
Norm after each mp layer: 3.3762319087982178
Norm after each mp layer: 27.703876495361328
Norm after each mp layer: 176.27536010742188
Norm before input: 0.2552422881126404
Norm after input: 0.567172646522522
Norm after each mp layer: 3.3750429153442383
Norm after each mp layer: 27.69209861755371
Norm after each mp layer: 176.48243713378906
Epoch: 460, Loss: 0.0018, Energy: 8858133.0000, Train: 99.92%, Valid: 77.20%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.567172646522522
Norm after each mp layer: 3.3750429153442383
Norm after each mp layer: 27.69209861755371
Norm after each mp layer: 176.48243713378906
Norm before input: 0.2552422881126404
Norm after input: 0.5673958659172058
Norm after each mp layer: 3.3775765895843506
Norm after each mp layer: 27.722240447998047
Norm after each mp layer: 176.43585205078125
Norm before input: 0.2552422881126404
Norm after input: 0.5673958659172058
Norm after each mp layer: 3.3775765895843506
Norm after each mp layer: 27.722240447998047
Norm after each mp layer: 176.43585205078125
Norm before input: 0.2552422881126404
Norm after input: 0.5672687292098999
Norm after each mp layer: 3.3765225410461426
Norm after each mp layer: 27.711956024169922
Norm after each mp layer: 176.63291931152344
Norm before input: 0.2552422881126404
Norm after input: 0.5672687292098999
Norm after each mp layer: 3.3765225410461426
Norm after each mp layer: 27.711956024169922
Norm after each mp layer: 176.63291931152344
Norm before input: 0.2552422881126404
Norm after input: 0.5674793124198914
Norm after each mp layer: 3.3789238929748535
Norm after each mp layer: 27.740617752075195
Norm after each mp layer: 176.59323120117188
Norm before input: 0.2552422881126404
Norm after input: 0.5674793124198914
Norm after each mp layer: 3.3789238929748535
Norm after each mp layer: 27.740617752075195
Norm after each mp layer: 176.59323120117188
Norm before input: 0.2552422881126404
Norm after input: 0.567359983921051
Norm after each mp layer: 3.377948045730591
Norm after each mp layer: 27.731212615966797
Norm after each mp layer: 176.78427124023438
Norm before input: 0.2552422881126404
Norm after input: 0.567359983921051
Norm after each mp layer: 3.37794828414917
Norm after each mp layer: 27.731212615966797
Norm after each mp layer: 176.78427124023438
Norm before input: 0.2552422881126404
Norm after input: 0.5675682425498962
Norm after each mp layer: 3.3803234100341797
Norm after each mp layer: 27.759597778320312
Norm after each mp layer: 176.7447509765625
Epoch: 465, Loss: 0.0017, Energy: 8901855.0000, Train: 99.92%, Valid: 77.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5675682425498962
Norm after each mp layer: 3.3803234100341797
Norm after each mp layer: 27.759597778320312
Norm after each mp layer: 176.7447509765625
Norm before input: 0.2552422881126404
Norm after input: 0.5674445629119873
Norm after each mp layer: 3.3793013095855713
Norm after each mp layer: 27.749662399291992
Norm after each mp layer: 176.9386749267578
Norm before input: 0.2552422881126404
Norm after input: 0.5674445629119873
Norm after each mp layer: 3.3793013095855713
Norm after each mp layer: 27.749662399291992
Norm after each mp layer: 176.9386749267578
Norm before input: 0.2552422881126404
Norm after input: 0.5676612257957458
Norm after each mp layer: 3.3817615509033203
Norm after each mp layer: 27.77899932861328
Norm after each mp layer: 176.89208984375
Norm before input: 0.2552422881126404
Norm after input: 0.5676612257957458
Norm after each mp layer: 3.3817615509033203
Norm after each mp layer: 27.77899932861328
Norm after each mp layer: 176.89208984375
Norm before input: 0.2552422881126404
Norm after input: 0.5675258040428162
Norm after each mp layer: 3.380618095397949
Norm after each mp layer: 27.76767349243164
Norm after each mp layer: 177.09439086914062
Norm before input: 0.2552422881126404
Norm after input: 0.5675258040428162
Norm after each mp layer: 3.380618095397949
Norm after each mp layer: 27.76767349243164
Norm after each mp layer: 177.09439086914062
Norm before input: 0.2552422881126404
Norm after input: 0.5677525401115417
Norm after each mp layer: 3.3831803798675537
Norm after each mp layer: 27.798160552978516
Norm after each mp layer: 177.03964233398438
Norm before input: 0.2552422881126404
Norm after input: 0.5677525401115417
Norm after each mp layer: 3.3831803798675537
Norm after each mp layer: 27.798160552978516
Norm after each mp layer: 177.03964233398438
Norm before input: 0.2552422881126404
Norm after input: 0.567609429359436
Norm after each mp layer: 3.3819563388824463
Norm after each mp layer: 27.785888671875
Norm after each mp layer: 177.24745178222656
Epoch: 470, Loss: 0.0017, Energy: 8938090.0000, Train: 99.92%, Valid: 77.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.567609429359436
Norm after each mp layer: 3.3819563388824463
Norm after each mp layer: 27.785888671875
Norm after each mp layer: 177.24745178222656
Norm before input: 0.2552422881126404
Norm after input: 0.5678374767303467
Norm after each mp layer: 3.384530544281006
Norm after each mp layer: 27.816490173339844
Norm after each mp layer: 177.19039916992188
Norm before input: 0.2552422881126404
Norm after input: 0.5678374767303467
Norm after each mp layer: 3.384530544281006
Norm after each mp layer: 27.816490173339844
Norm after each mp layer: 177.19039916992188
Norm before input: 0.2552422881126404
Norm after input: 0.5676976442337036
Norm after each mp layer: 3.383338212966919
Norm after each mp layer: 27.804536819458008
Norm after each mp layer: 177.3955535888672
Norm before input: 0.2552422881126404
Norm after input: 0.5676976442337036
Norm after each mp layer: 3.383338212966919
Norm after each mp layer: 27.804536819458008
Norm after each mp layer: 177.3955535888672
Norm before input: 0.2552422881126404
Norm after input: 0.56791752576828
Norm after each mp layer: 3.3858258724212646
Norm after each mp layer: 27.834156036376953
Norm after each mp layer: 177.34266662597656
Norm before input: 0.2552422881126404
Norm after input: 0.56791752576828
Norm after each mp layer: 3.3858258724212646
Norm after each mp layer: 27.834156036376953
Norm after each mp layer: 177.34266662597656
Norm before input: 0.2552422881126404
Norm after input: 0.5677868127822876
Norm after each mp layer: 3.3847248554229736
Norm after each mp layer: 27.823223114013672
Norm after each mp layer: 177.5408477783203
Norm before input: 0.2552422881126404
Norm after input: 0.5677868127822876
Norm after each mp layer: 3.3847248554229736
Norm after each mp layer: 27.823223114013672
Norm after each mp layer: 177.54083251953125
Norm before input: 0.2552422881126404
Norm after input: 0.5679981112480164
Norm after each mp layer: 3.3871233463287354
Norm after each mp layer: 27.851839065551758
Norm after each mp layer: 177.49234008789062
Epoch: 475, Loss: 0.0017, Energy: 8980991.0000, Train: 99.92%, Valid: 77.20%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5679981112480164
Norm after each mp layer: 3.3871233463287354
Norm after each mp layer: 27.851839065551758
Norm after each mp layer: 177.49234008789062
Norm before input: 0.2552422881126404
Norm after input: 0.5678722262382507
Norm after each mp layer: 3.3860695362091064
Norm after each mp layer: 27.841432571411133
Norm after each mp layer: 177.6865692138672
Norm before input: 0.2552422881126404
Norm after input: 0.5678722262382507
Norm after each mp layer: 3.3860695362091064
Norm after each mp layer: 27.841432571411133
Norm after each mp layer: 177.6865692138672
Norm before input: 0.2552422881126404
Norm after input: 0.5680823922157288
Norm after each mp layer: 3.3884544372558594
Norm after each mp layer: 27.86990737915039
Norm after each mp layer: 177.63739013671875
Norm before input: 0.2552422881126404
Norm after input: 0.5680823922157288
Norm after each mp layer: 3.3884544372558594
Norm after each mp layer: 27.86990737915039
Norm after each mp layer: 177.63739013671875
Norm before input: 0.2552422881126404
Norm after input: 0.5679529309272766
Norm after each mp layer: 3.3873627185821533
Norm after each mp layer: 27.85905647277832
Norm after each mp layer: 177.83404541015625
Norm before input: 0.2552422881126404
Norm after input: 0.5679529309272766
Norm after each mp layer: 3.3873627185821533
Norm after each mp layer: 27.85905647277832
Norm after each mp layer: 177.83404541015625
Norm before input: 0.2552422881126404
Norm after input: 0.5681692361831665
Norm after each mp layer: 3.38980770111084
Norm after each mp layer: 27.88821792602539
Norm after each mp layer: 177.77931213378906
Norm before input: 0.2552422881126404
Norm after input: 0.5681692361831665
Norm after each mp layer: 3.38980770111084
Norm after each mp layer: 27.88821792602539
Norm after each mp layer: 177.77931213378906
Norm before input: 0.2552422881126404
Norm after input: 0.5680314898490906
Norm after each mp layer: 3.3886311054229736
Norm after each mp layer: 27.876384735107422
Norm after each mp layer: 177.98194885253906
Epoch: 480, Loss: 0.0017, Energy: 9015527.0000, Train: 99.92%, Valid: 77.20%, Test: 76.80%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5680314898490906
Norm after each mp layer: 3.3886311054229736
Norm after each mp layer: 27.876384735107422
Norm after each mp layer: 177.98194885253906
Norm before input: 0.2552422881126404
Norm after input: 0.5682543516159058
Norm after each mp layer: 3.3911423683166504
Norm after each mp layer: 27.90629005432129
Norm after each mp layer: 177.9212646484375
Norm before input: 0.2552422881126404
Norm after input: 0.5682543516159058
Norm after each mp layer: 3.3911423683166504
Norm after each mp layer: 27.90629005432129
Norm after each mp layer: 177.9212646484375
Norm before input: 0.2552422881126404
Norm after input: 0.5681117177009583
Norm after each mp layer: 3.3899130821228027
Norm after each mp layer: 27.893827438354492
Norm after each mp layer: 178.1275634765625
Norm before input: 0.2552422881126404
Norm after input: 0.5681117177009583
Norm after each mp layer: 3.3899130821228027
Norm after each mp layer: 27.893827438354492
Norm after each mp layer: 178.1275634765625
Norm before input: 0.2552422881126404
Norm after input: 0.5683349967002869
Norm after each mp layer: 3.3924272060394287
Norm after each mp layer: 27.923751831054688
Norm after each mp layer: 178.06532287597656
Norm before input: 0.2552422881126404
Norm after input: 0.5683349967002869
Norm after each mp layer: 3.3924272060394287
Norm after each mp layer: 27.923751831054688
Norm after each mp layer: 178.06532287597656
Norm before input: 0.2552422881126404
Norm after input: 0.5681946277618408
Norm after each mp layer: 3.3912205696105957
Norm after each mp layer: 27.911514282226562
Norm after each mp layer: 178.269775390625
Norm before input: 0.2552422881126404
Norm after input: 0.5681946277618408
Norm after each mp layer: 3.3912205696105957
Norm after each mp layer: 27.911514282226562
Norm after each mp layer: 178.269775390625
Norm before input: 0.2552422881126404
Norm after input: 0.5684123039245605
Norm after each mp layer: 3.393674373626709
Norm after each mp layer: 27.940753936767578
Norm after each mp layer: 178.21023559570312
Epoch: 485, Loss: 0.0017, Energy: 9057408.0000, Train: 99.92%, Valid: 77.20%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5684123039245605
Norm after each mp layer: 3.393674373626709
Norm after each mp layer: 27.940753936767578
Norm after each mp layer: 178.21023559570312
Norm before input: 0.2552422881126404
Norm after input: 0.568277895450592
Norm after each mp layer: 3.392526865005493
Norm after each mp layer: 27.929166793823242
Norm after each mp layer: 178.40994262695312
Norm before input: 0.2552422881126404
Norm after input: 0.568277895450592
Norm after each mp layer: 3.392526865005493
Norm after each mp layer: 27.929166793823242
Norm after each mp layer: 178.40994262695312
Norm before input: 0.2552422881126404
Norm after input: 0.5684897303581238
Norm after each mp layer: 3.3949203491210938
Norm after each mp layer: 27.95772361755371
Norm after each mp layer: 178.35299682617188
Norm before input: 0.2552422881126404
Norm after input: 0.5684897303581238
Norm after each mp layer: 3.3949203491210938
Norm after each mp layer: 27.95772361755371
Norm after each mp layer: 178.35299682617188
Norm before input: 0.2552422881126404
Norm after input: 0.5683583617210388
Norm after each mp layer: 3.3938019275665283
Norm after each mp layer: 27.946462631225586
Norm after each mp layer: 178.55015563964844
Norm before input: 0.2552422881126404
Norm after input: 0.5683583617210388
Norm after each mp layer: 3.3938019275665283
Norm after each mp layer: 27.946462631225586
Norm after each mp layer: 178.55015563964844
Norm before input: 0.2552422881126404
Norm after input: 0.568569540977478
Norm after each mp layer: 3.396186351776123
Norm after each mp layer: 27.974924087524414
Norm after each mp layer: 178.49234008789062
Norm before input: 0.2552422881126404
Norm after input: 0.568569540977478
Norm after each mp layer: 3.396186351776123
Norm after each mp layer: 27.974924087524414
Norm after each mp layer: 178.49234008789062
Norm before input: 0.2552422881126404
Norm after input: 0.5684356093406677
Norm after each mp layer: 3.3950395584106445
Norm after each mp layer: 27.963335037231445
Norm after each mp layer: 178.69125366210938
Epoch: 490, Loss: 0.0017, Energy: 9090455.0000, Train: 99.92%, Valid: 77.20%, Test: 76.90%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5684356093406677
Norm after each mp layer: 3.3950395584106445
Norm after each mp layer: 27.963335037231445
Norm after each mp layer: 178.69125366210938
Norm before input: 0.2552422881126404
Norm after input: 0.5686507225036621
Norm after each mp layer: 3.397463321685791
Norm after each mp layer: 27.992244720458984
Norm after each mp layer: 178.6291961669922
Norm before input: 0.2552422881126404
Norm after input: 0.5686507225036621
Norm after each mp layer: 3.397463321685791
Norm after each mp layer: 27.992244720458984
Norm after each mp layer: 178.6291961669922
Norm before input: 0.2552422881126404
Norm after input: 0.5685113072395325
Norm after each mp layer: 3.396259307861328
Norm after each mp layer: 27.979978561401367
Norm after each mp layer: 178.8322296142578
Norm before input: 0.2552422881126404
Norm after input: 0.5685113072395325
Norm after each mp layer: 3.396259307861328
Norm after each mp layer: 27.979978561401367
Norm after each mp layer: 178.8322296142578
Norm before input: 0.2552422881126404
Norm after input: 0.5687307119369507
Norm after each mp layer: 3.3987252712249756
Norm after each mp layer: 28.009363174438477
Norm after each mp layer: 178.76583862304688
Norm before input: 0.2552422881126404
Norm after input: 0.5687307119369507
Norm after each mp layer: 3.3987252712249756
Norm after each mp layer: 28.009363174438477
Norm after each mp layer: 178.76583862304688
Norm before input: 0.2552422881126404
Norm after input: 0.5685878396034241
Norm after each mp layer: 3.3974854946136475
Norm after each mp layer: 27.99666404724121
Norm after each mp layer: 178.97145080566406
Norm before input: 0.2552422881126404
Norm after input: 0.5685878396034241
Norm after each mp layer: 3.3974854946136475
Norm after each mp layer: 27.99666404724121
Norm after each mp layer: 178.97145080566406
Norm before input: 0.2552422881126404
Norm after input: 0.5688075423240662
Norm after each mp layer: 3.399951696395874
Norm after each mp layer: 28.026050567626953
Norm after each mp layer: 178.90367126464844
Epoch: 495, Loss: 0.0017, Energy: 9131354.0000, Train: 99.92%, Valid: 77.20%, Test: 77.00%, Best Valid: 78.00%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5688075423240662
Norm after each mp layer: 3.399951696395874
Norm after each mp layer: 28.026050567626953
Norm after each mp layer: 178.90367126464844
Norm before input: 0.2552422881126404
Norm after input: 0.5686661005020142
Norm after each mp layer: 3.3987247943878174
Norm after each mp layer: 28.013471603393555
Norm after each mp layer: 179.1081085205078
Norm before input: 0.2552422881126404
Norm after input: 0.5686661005020142
Norm after each mp layer: 3.3987247943878174
Norm after each mp layer: 28.013471603393555
Norm after each mp layer: 179.1081085205078
Norm before input: 0.2552422881126404
Norm after input: 0.5688818097114563
Norm after each mp layer: 3.4011499881744385
Norm after each mp layer: 28.042383193969727
Norm after each mp layer: 179.0418701171875
Norm before input: 0.2552422881126404
Norm after input: 0.5688818097114563
Norm after each mp layer: 3.4011499881744385
Norm after each mp layer: 28.042383193969727
Norm after each mp layer: 179.0418701171875
Norm before input: 0.2552422881126404
Norm after input: 0.5687443017959595
Norm after each mp layer: 3.399961471557617
Norm after each mp layer: 28.03022575378418
Norm after each mp layer: 179.24314880371094
Norm before input: 0.2552422881126404
Norm after input: 0.5687443017959595
Norm after each mp layer: 3.399961471557617
Norm after each mp layer: 28.03022575378418
Norm after each mp layer: 179.24314880371094
Norm before input: 0.2552422881126404
Norm after input: 0.5689559578895569
Norm after each mp layer: 3.40234375
Norm after each mp layer: 28.058656692504883
Norm after each mp layer: 179.178466796875
train_accuracy_list: [0.28228476821192056, 0.28228476821192056, 0.16225165562913907, 0.16225165562913907, 0.28228476821192056, 0.1630794701986755, 0.12748344370860928, 0.16225165562913907, 0.28228476821192056, 0.28228476821192056, 0.326158940397351, 0.16225165562913907, 0.2185430463576159, 0.28394039735099336, 0.25496688741721857, 0.22102649006622516, 0.37665562913907286, 0.3783112582781457, 0.37665562913907286, 0.39155629139072845, 0.43543046357615894, 0.4552980132450331, 0.41225165562913907, 0.3708609271523179, 0.43625827814569534, 0.4644039735099338, 0.4718543046357616, 0.47102649006622516, 0.4693708609271523, 0.47516556291390727, 0.4859271523178808, 0.5629139072847682, 0.5596026490066225, 0.5612582781456954, 0.5256622516556292, 0.5513245033112583, 0.570364238410596, 0.5935430463576159, 0.6200331125827815, 0.6448675496688742, 0.6357615894039735, 0.6432119205298014, 0.6423841059602649, 0.6059602649006622, 0.6216887417218543, 0.6713576158940397, 0.6945364238410596, 0.6705298013245033, 0.6945364238410596, 0.6961920529801324, 0.7119205298013245, 0.7259933774834437, 0.7408940397350994, 0.7533112582781457, 0.7516556291390728, 0.7541390728476821, 0.7574503311258278, 0.765728476821192, 0.7789735099337748, 0.7880794701986755, 0.7947019867549668, 0.8021523178807947, 0.793046357615894, 0.7831125827814569, 0.7673841059602649, 0.7947019867549668, 0.8261589403973509, 0.8079470198675497, 0.8153973509933775, 0.8369205298013245, 0.8170529801324503, 0.8493377483443708, 0.8352649006622517, 0.8509933774834437, 0.859271523178808, 0.8402317880794702, 0.8650662251655629, 0.8708609271523179, 0.8576158940397351, 0.8683774834437086, 0.8766556291390728, 0.8807947019867549, 0.8816225165562914, 0.8932119205298014, 0.8965231788079471, 0.8956953642384106, 0.9064569536423841, 0.918046357615894, 0.9172185430463576, 0.9221854304635762, 0.9337748344370861, 0.9279801324503312, 0.9379139072847682, 0.9445364238410596, 0.9412251655629139, 0.9486754966887417, 0.9445364238410596, 0.9478476821192053, 0.9544701986754967, 0.9503311258278145, 0.9602649006622517, 0.9619205298013245, 0.9552980132450332, 0.9668874172185431, 0.9652317880794702, 0.9602649006622517, 0.9701986754966887, 0.9710264900662252, 0.9619205298013245, 0.9743377483443708, 0.9735099337748344, 0.9701986754966887, 0.9693708609271523, 0.9743377483443708, 0.9743377483443708, 0.972682119205298, 0.9751655629139073, 0.9768211920529801, 0.9768211920529801, 0.9759933774834437, 0.9784768211920529, 0.9784768211920529, 0.9776490066225165, 0.9793046357615894, 0.9784768211920529, 0.9801324503311258, 0.9801324503311258, 0.9809602649006622, 0.9817880794701986, 0.984271523178808, 0.9834437086092715, 0.984271523178808, 0.9875827814569537, 0.9867549668874173, 0.9859271523178808, 0.9850993377483444, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9917218543046358, 0.9900662251655629, 0.9908940397350994, 0.9892384105960265, 0.9925496688741722, 0.9908940397350994, 0.9917218543046358, 0.9917218543046358, 0.9925496688741722, 0.9925496688741722, 0.9917218543046358, 0.9917218543046358, 0.9933774834437086, 0.9942052980132451, 0.9925496688741722, 0.9925496688741722, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9925496688741722, 0.9933774834437086, 0.9942052980132451, 0.9942052980132451, 0.9933774834437086, 0.9942052980132451, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9975165562913907, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9975165562913907, 0.9975165562913907, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9975165562913907, 0.9983443708609272, 0.9975165562913907, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9991721854304636, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636]
valid_accuracy_list: [0.316, 0.316, 0.156, 0.162, 0.316, 0.156, 0.142, 0.154, 0.316, 0.316, 0.352, 0.162, 0.206, 0.266, 0.236, 0.238, 0.392, 0.404, 0.404, 0.408, 0.442, 0.458, 0.394, 0.336, 0.39, 0.45, 0.464, 0.468, 0.464, 0.476, 0.486, 0.502, 0.486, 0.526, 0.504, 0.518, 0.532, 0.546, 0.56, 0.582, 0.558, 0.582, 0.584, 0.564, 0.56, 0.588, 0.604, 0.582, 0.604, 0.612, 0.626, 0.63, 0.614, 0.636, 0.622, 0.632, 0.628, 0.624, 0.64, 0.648, 0.656, 0.658, 0.66, 0.64, 0.656, 0.656, 0.688, 0.668, 0.668, 0.682, 0.682, 0.692, 0.688, 0.69, 0.69, 0.69, 0.708, 0.7, 0.706, 0.71, 0.708, 0.722, 0.72, 0.718, 0.728, 0.74, 0.724, 0.73, 0.742, 0.73, 0.738, 0.744, 0.744, 0.744, 0.754, 0.756, 0.754, 0.762, 0.76, 0.758, 0.772, 0.764, 0.758, 0.77, 0.774, 0.756, 0.774, 0.776, 0.76, 0.772, 0.776, 0.77, 0.768, 0.778, 0.77, 0.766, 0.778, 0.778, 0.772, 0.77, 0.778, 0.776, 0.776, 0.774, 0.776, 0.778, 0.778, 0.778, 0.776, 0.776, 0.778, 0.774, 0.77, 0.772, 0.776, 0.776, 0.774, 0.77, 0.774, 0.78, 0.78, 0.772, 0.77, 0.778, 0.778, 0.78, 0.774, 0.768, 0.77, 0.778, 0.774, 0.77, 0.77, 0.776, 0.774, 0.77, 0.768, 0.77, 0.774, 0.766, 0.764, 0.764, 0.766, 0.764, 0.762, 0.762, 0.764, 0.764, 0.768, 0.768, 0.766, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.77, 0.768, 0.766, 0.77, 0.77, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.766, 0.768, 0.762, 0.766, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.76, 0.76, 0.762, 0.762, 0.762, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.764, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.768, 0.766, 0.768, 0.766, 0.768, 0.766, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.772, 0.77, 0.772, 0.77, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772, 0.772]
test_accuracy_list: [0.319, 0.319, 0.144, 0.149, 0.319, 0.144, 0.121, 0.145, 0.319, 0.319, 0.351, 0.149, 0.187, 0.264, 0.22, 0.222, 0.397, 0.394, 0.391, 0.409, 0.454, 0.462, 0.399, 0.346, 0.416, 0.461, 0.467, 0.471, 0.463, 0.47, 0.479, 0.507, 0.493, 0.519, 0.514, 0.524, 0.545, 0.56, 0.578, 0.574, 0.571, 0.589, 0.605, 0.567, 0.575, 0.604, 0.604, 0.588, 0.593, 0.593, 0.606, 0.61, 0.624, 0.617, 0.625, 0.623, 0.625, 0.631, 0.637, 0.646, 0.647, 0.64, 0.636, 0.629, 0.616, 0.647, 0.654, 0.63, 0.656, 0.658, 0.635, 0.673, 0.673, 0.661, 0.663, 0.68, 0.674, 0.665, 0.69, 0.689, 0.689, 0.698, 0.701, 0.692, 0.703, 0.717, 0.702, 0.714, 0.728, 0.721, 0.73, 0.735, 0.734, 0.738, 0.755, 0.753, 0.754, 0.758, 0.759, 0.762, 0.764, 0.764, 0.767, 0.766, 0.766, 0.767, 0.766, 0.765, 0.769, 0.766, 0.763, 0.769, 0.767, 0.767, 0.769, 0.769, 0.769, 0.763, 0.769, 0.771, 0.765, 0.764, 0.771, 0.771, 0.766, 0.769, 0.769, 0.768, 0.769, 0.769, 0.772, 0.769, 0.766, 0.767, 0.774, 0.774, 0.771, 0.772, 0.771, 0.771, 0.77, 0.772, 0.772, 0.773, 0.771, 0.77, 0.772, 0.771, 0.77, 0.773, 0.771, 0.77, 0.771, 0.773, 0.773, 0.772, 0.771, 0.774, 0.772, 0.771, 0.771, 0.771, 0.77, 0.77, 0.769, 0.765, 0.762, 0.763, 0.762, 0.763, 0.762, 0.763, 0.764, 0.763, 0.763, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.765, 0.766, 0.767, 0.767, 0.766, 0.766, 0.766, 0.765, 0.766, 0.764, 0.767, 0.766, 0.765, 0.765, 0.766, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.768, 0.767, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.769, 0.768, 0.769, 0.769, 0.769, 0.768, 0.768, 0.768, 0.767, 0.767, 0.767, 0.767, 0.767, 0.767, 0.768, 0.768, 0.768, 0.768, 0.768, 0.767, 0.768, 0.766, 0.768, 0.766, 0.768, 0.768, 0.769, 0.768, 0.767, 0.768, 0.768, 0.768, 0.768, 0.769, 0.769, 0.768, 0.769, 0.769, 0.77, 0.77, 0.771, 0.77, 0.771, 0.77, 0.771, 0.771, 0.77, 0.772, 0.77, 0.772, 0.77, 0.771, 0.77, 0.771, 0.77, 0.771, 0.769, 0.771, 0.77, 0.771, 0.77, 0.771, 0.77, 0.771, 0.77, 0.771, 0.77, 0.771, 0.768, 0.772, 0.769, 0.771, 0.769, 0.77, 0.769, 0.769, 0.768, 0.768, 0.769, 0.768, 0.77, 0.768, 0.77, 0.768, 0.77, 0.768, 0.77, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.77, 0.771, 0.771, 0.771, 0.772, 0.77, 0.77, 0.77, 0.771, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.771, 0.77, 0.771, 0.771, 0.771, 0.771, 0.771, 0.771, 0.771, 0.771, 0.77, 0.77, 0.77, 0.769, 0.771, 0.769, 0.771, 0.77, 0.771, 0.77, 0.771, 0.77, 0.771, 0.77, 0.771, 0.769, 0.771, 0.769, 0.771, 0.769, 0.771, 0.769, 0.771, 0.768, 0.77, 0.768, 0.77, 0.768, 0.771, 0.769, 0.771, 0.768, 0.77, 0.768, 0.77, 0.768, 0.769, 0.767, 0.769, 0.767, 0.769, 0.768, 0.769, 0.768, 0.769, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.769, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.767, 0.768, 0.767, 0.769, 0.767, 0.769, 0.767, 0.769, 0.767, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.77, 0.769, 0.77, 0.769, 0.77, 0.769, 0.77, 0.769, 0.77, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.77, 0.769, 0.77, 0.769, 0.77, 0.769, 0.77, 0.768, 0.77, 0.768, 0.77, 0.768, 0.77, 0.768, 0.77, 0.768, 0.77, 0.768, 0.77, 0.769, 0.77, 0.769, 0.77, 0.769, 0.77, 0.769, 0.77, 0.769, 0.77, 0.769, 0.77]
best validation: 0.78
best test: 0.774
Experiment run
dataset: cora
num_mp_layers: 3
mp_hidden_dim: 4000
optimizer_lr: 0.001
loss_func: CrossEntropyLoss
total_epoch: 500
energy_lambda: 1e-09
Norm before input: 0.2552422881126404
Norm after input: 0.9947524070739746
Norm after each mp layer: 0.6566787362098694
Norm after each mp layer: 0.6222918033599854
Norm after each mp layer: 0.6184736490249634
Norm before input: 0.2552422881126404
Norm after input: 1.0018221139907837
Norm after each mp layer: 1.8246673345565796
Norm after each mp layer: 1.6395456790924072
Norm after each mp layer: 1.6299448013305664
Norm before input: 0.2552422881126404
Norm after input: 1.0018221139907837
Norm after each mp layer: 1.8246673345565796
Norm after each mp layer: 1.6395456790924072
Norm after each mp layer: 1.6299448013305664
Norm before input: 0.2552422881126404
Norm after input: 1.0543195009231567
Norm after each mp layer: 2.7995738983154297
Norm after each mp layer: 5.275698184967041
Norm after each mp layer: 14.316182136535645
Norm before input: 0.2552422881126404
Norm after input: 1.0543195009231567
Norm after each mp layer: 2.7995738983154297
Norm after each mp layer: 5.275698184967041
Norm after each mp layer: 14.316182136535645
Norm before input: 0.2552422881126404
Norm after input: 1.1246622800827026
Norm after each mp layer: 4.174095630645752
Norm after each mp layer: 12.078539848327637
Norm after each mp layer: 44.793548583984375
Norm before input: 0.2552422881126404
Norm after input: 1.1246622800827026
Norm after each mp layer: 4.174095630645752
Norm after each mp layer: 12.078539848327637
Norm after each mp layer: 44.793548583984375
Norm before input: 0.2552422881126404
Norm after input: 1.1186175346374512
Norm after each mp layer: 4.360331058502197
Norm after each mp layer: 13.665470123291016
Norm after each mp layer: 52.446250915527344
Norm before input: 0.2552422881126404
Norm after input: 1.1186175346374512
Norm after each mp layer: 4.360331058502197
Norm after each mp layer: 13.665470123291016
Norm after each mp layer: 52.446250915527344
Norm before input: 0.2552422881126404
Norm after input: 1.0755181312561035
Norm after each mp layer: 4.004016399383545
Norm after each mp layer: 10.636209487915039
Norm after each mp layer: 38.624267578125
Epoch: 05, Loss: 3.9548, Energy: 417949.0000, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 1.0755181312561035
Norm after each mp layer: 4.004016399383545
Norm after each mp layer: 10.636209487915039
Norm after each mp layer: 38.624267578125
Norm before input: 0.2552422881126404
Norm after input: 1.015473484992981
Norm after each mp layer: 3.5910913944244385
Norm after each mp layer: 7.945623397827148
Norm after each mp layer: 17.74387550354004
Norm before input: 0.2552422881126404
Norm after input: 1.015473484992981
Norm after each mp layer: 3.5910913944244385
Norm after each mp layer: 7.945623397827148
Norm after each mp layer: 17.74387550354004
Norm before input: 0.2552422881126404
Norm after input: 0.9771838188171387
Norm after each mp layer: 3.407231569290161
Norm after each mp layer: 8.608782768249512
Norm after each mp layer: 23.45598602294922
Norm before input: 0.2552422881126404
Norm after input: 0.9771838188171387
Norm after each mp layer: 3.407231569290161
Norm after each mp layer: 8.608782768249512
Norm after each mp layer: 23.45598602294922
Norm before input: 0.2552422881126404
Norm after input: 0.9452493786811829
Norm after each mp layer: 3.3108882904052734
Norm after each mp layer: 9.652267456054688
Norm after each mp layer: 26.446714401245117
Norm before input: 0.2552422881126404
Norm after input: 0.9452493786811829
Norm after each mp layer: 3.3108882904052734
Norm after each mp layer: 9.652267456054688
Norm after each mp layer: 26.446714401245117
Norm before input: 0.2552422881126404
Norm after input: 0.9080318808555603
Norm after each mp layer: 3.1349380016326904
Norm after each mp layer: 9.601465225219727
Norm after each mp layer: 31.706470489501953
Norm before input: 0.2552422881126404
Norm after input: 0.9080318808555603
Norm after each mp layer: 3.1349380016326904
Norm after each mp layer: 9.601465225219727
Norm after each mp layer: 31.706470489501953
Norm before input: 0.2552422881126404
Norm after input: 0.8712474703788757
Norm after each mp layer: 2.9461545944213867
Norm after each mp layer: 9.493681907653809
Norm after each mp layer: 33.52479553222656
Epoch: 10, Loss: 2.3849, Energy: 140338.1406, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.8712474703788757
Norm after each mp layer: 2.9461545944213867
Norm after each mp layer: 9.493681907653809
Norm after each mp layer: 33.52479553222656
Norm before input: 0.2552422881126404
Norm after input: 0.8286648392677307
Norm after each mp layer: 2.6777966022491455
Norm after each mp layer: 8.761536598205566
Norm after each mp layer: 31.614381790161133
Norm before input: 0.2552422881126404
Norm after input: 0.8286648392677307
Norm after each mp layer: 2.6777966022491455
Norm after each mp layer: 8.761536598205566
Norm after each mp layer: 31.6143798828125
Norm before input: 0.2552422881126404
Norm after input: 0.788832426071167
Norm after each mp layer: 2.440768003463745
Norm after each mp layer: 8.158180236816406
Norm after each mp layer: 28.986318588256836
Norm before input: 0.2552422881126404
Norm after input: 0.788832426071167
Norm after each mp layer: 2.440768003463745
Norm after each mp layer: 8.158180236816406
Norm after each mp layer: 28.986318588256836
Norm before input: 0.2552422881126404
Norm after input: 0.7434688806533813
Norm after each mp layer: 2.1485023498535156
Norm after each mp layer: 7.067230701446533
Norm after each mp layer: 25.96994400024414
Norm before input: 0.2552422881126404
Norm after input: 0.7434688806533813
Norm after each mp layer: 2.1485023498535156
Norm after each mp layer: 7.067230701446533
Norm after each mp layer: 25.96994400024414
Norm before input: 0.2552422881126404
Norm after input: 0.702235758304596
Norm after each mp layer: 1.8835439682006836
Norm after each mp layer: 6.076872825622559
Norm after each mp layer: 23.37902069091797
Norm before input: 0.2552422881126404
Norm after input: 0.702235758304596
Norm after each mp layer: 1.8835439682006836
Norm after each mp layer: 6.076872825622559
Norm after each mp layer: 23.37902069091797
Norm before input: 0.2552422881126404
Norm after input: 0.6701532006263733
Norm after each mp layer: 1.7018585205078125
Norm after each mp layer: 5.433500289916992
Norm after each mp layer: 21.26364517211914
Epoch: 15, Loss: 1.9592, Energy: 72202.1172, Train: 25.50%, Valid: 23.60%, Test: 21.90%, Best Valid: 35.20%, Best Test: 34.90%
Norm before input: 0.2552422881126404
Norm after input: 0.6701532006263733
Norm after each mp layer: 1.7018585205078125
Norm after each mp layer: 5.433500289916992
Norm after each mp layer: 21.26364517211914
Norm before input: 0.2552422881126404
Norm after input: 0.6424018740653992
Norm after each mp layer: 1.5532342195510864
Norm after each mp layer: 4.906569480895996
Norm after each mp layer: 19.555896759033203
Norm before input: 0.2552422881126404
Norm after input: 0.6424018740653992
Norm after each mp layer: 1.5532342195510864
Norm after each mp layer: 4.906569480895996
Norm after each mp layer: 19.555896759033203
Norm before input: 0.2552422881126404
Norm after input: 0.6153585314750671
Norm after each mp layer: 1.4009044170379639
Norm after each mp layer: 4.364556312561035
Norm after each mp layer: 17.86216926574707
Norm before input: 0.2552422881126404
Norm after input: 0.6153585314750671
Norm after each mp layer: 1.4009044170379639
Norm after each mp layer: 4.364556312561035
Norm after each mp layer: 17.86216926574707
Norm before input: 0.2552422881126404
Norm after input: 0.5926629900932312
Norm after each mp layer: 1.277531385421753
Norm after each mp layer: 3.9668190479278564
Norm after each mp layer: 16.629535675048828
Norm before input: 0.2552422881126404
Norm after input: 0.5926629900932312
Norm after each mp layer: 1.277531385421753
Norm after each mp layer: 3.9668190479278564
Norm after each mp layer: 16.62953758239746
Norm before input: 0.2552422881126404
Norm after input: 0.5779066681861877
Norm after each mp layer: 1.2256821393966675
Norm after each mp layer: 3.8562822341918945
Norm after each mp layer: 16.482072830200195
Norm before input: 0.2552422881126404
Norm after input: 0.5779066681861877
Norm after each mp layer: 1.2256821393966675
Norm after each mp layer: 3.8562822341918945
Norm after each mp layer: 16.482072830200195
Norm before input: 0.2552422881126404
Norm after input: 0.5694142580032349
Norm after each mp layer: 1.2278532981872559
Norm after each mp layer: 3.9642493724823
Norm after each mp layer: 17.308868408203125
Epoch: 20, Loss: 1.6350, Energy: 33010.0430, Train: 39.57%, Valid: 41.20%, Test: 41.50%, Best Valid: 41.20%, Best Test: 41.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5694142580032349
Norm after each mp layer: 1.2278532981872559
Norm after each mp layer: 3.9642493724823
Norm after each mp layer: 17.308868408203125
Norm before input: 0.2552422881126404
Norm after input: 0.5632151961326599
Norm after each mp layer: 1.2443253993988037
Norm after each mp layer: 4.152261257171631
Norm after each mp layer: 18.53976058959961
Norm before input: 0.2552422881126404
Norm after input: 0.5632151961326599
Norm after each mp layer: 1.2443253993988037
Norm after each mp layer: 4.152261257171631
Norm after each mp layer: 18.53976058959961
Norm before input: 0.2552422881126404
Norm after input: 0.5565205216407776
Norm after each mp layer: 1.249036431312561
Norm after each mp layer: 4.298089981079102
Norm after each mp layer: 19.53022575378418
Norm before input: 0.2552422881126404
Norm after input: 0.5565205216407776
Norm after each mp layer: 1.249036431312561
Norm after each mp layer: 4.298089981079102
Norm after each mp layer: 19.53022575378418
Norm before input: 0.2552422881126404
Norm after input: 0.5504767894744873
Norm after each mp layer: 1.2560105323791504
Norm after each mp layer: 4.446261405944824
Norm after each mp layer: 20.501705169677734
Norm before input: 0.2552422881126404
Norm after input: 0.5504767894744873
Norm after each mp layer: 1.2560105323791504
Norm after each mp layer: 4.446261405944824
Norm after each mp layer: 20.501705169677734
Norm before input: 0.2552422881126404
Norm after input: 0.5462607741355896
Norm after each mp layer: 1.2769277095794678
Norm after each mp layer: 4.639888286590576
Norm after each mp layer: 21.803068161010742
Norm before input: 0.2552422881126404
Norm after input: 0.5462607741355896
Norm after each mp layer: 1.2769277095794678
Norm after each mp layer: 4.639888286590576
Norm after each mp layer: 21.803068161010742
Norm before input: 0.2552422881126404
Norm after input: 0.5434458255767822
Norm after each mp layer: 1.3073222637176514
Norm after each mp layer: 4.868044853210449
Norm after each mp layer: 23.46598243713379
Epoch: 25, Loss: 1.3712, Energy: 56649.5352, Train: 44.21%, Valid: 39.00%, Test: 41.90%, Best Valid: 45.80%, Best Test: 46.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5434458255767822
Norm after each mp layer: 1.3073222637176514
Norm after each mp layer: 4.868044853210449
Norm after each mp layer: 23.46598243713379
Norm before input: 0.2552422881126404
Norm after input: 0.541029155254364
Norm after each mp layer: 1.3401252031326294
Norm after each mp layer: 5.112997531890869
Norm after each mp layer: 25.345134735107422
Norm before input: 0.2552422881126404
Norm after input: 0.541029155254364
Norm after each mp layer: 1.3401252031326294
Norm after each mp layer: 5.112997531890869
Norm after each mp layer: 25.345134735107422
Norm before input: 0.2552422881126404
Norm after input: 0.5368906259536743
Norm after each mp layer: 1.3567864894866943
Norm after each mp layer: 5.280355930328369
Norm after each mp layer: 26.709339141845703
Norm before input: 0.2552422881126404
Norm after input: 0.5368906259536743
Norm after each mp layer: 1.3567864894866943
Norm after each mp layer: 5.280355930328369
Norm after each mp layer: 26.709339141845703
Norm before input: 0.2552422881126404
Norm after input: 0.5310956835746765
Norm after each mp layer: 1.3562897443771362
Norm after each mp layer: 5.349088668823242
Norm after each mp layer: 27.315534591674805
Norm before input: 0.2552422881126404
Norm after input: 0.5310956835746765
Norm after each mp layer: 1.3562898635864258
Norm after each mp layer: 5.349088668823242
Norm after each mp layer: 27.315534591674805
Norm before input: 0.2552422881126404
Norm after input: 0.5271258354187012
Norm after each mp layer: 1.3652620315551758
Norm after each mp layer: 5.454919338226318
Norm after each mp layer: 28.080900192260742
Norm before input: 0.2552422881126404
Norm after input: 0.5271258354187012
Norm after each mp layer: 1.3652620315551758
Norm after each mp layer: 5.454919338226318
Norm after each mp layer: 28.080900192260742
Norm before input: 0.2552422881126404
Norm after input: 0.5266370177268982
Norm after each mp layer: 1.3944370746612549
Norm after each mp layer: 5.663105010986328
Norm after each mp layer: 29.553203582763672
Epoch: 30, Loss: 1.2195, Energy: 89178.1641, Train: 47.76%, Valid: 48.00%, Test: 47.20%, Best Valid: 48.00%, Best Test: 47.30%
Norm before input: 0.2552422881126404
Norm after input: 0.5266370177268982
Norm after each mp layer: 1.3944370746612549
Norm after each mp layer: 5.663105010986328
Norm after each mp layer: 29.553203582763672
Norm before input: 0.2552422881126404
Norm after input: 0.5274714827537537
Norm after each mp layer: 1.4283506870269775
Norm after each mp layer: 5.894866943359375
Norm after each mp layer: 31.132097244262695
Norm before input: 0.2552422881126404
Norm after input: 0.5274714827537537
Norm after each mp layer: 1.4283506870269775
Norm after each mp layer: 5.894866943359375
Norm after each mp layer: 31.132097244262695
Norm before input: 0.2552422881126404
Norm after input: 0.5267603397369385
Norm after each mp layer: 1.4465819597244263
Norm after each mp layer: 6.028192043304443
Norm after each mp layer: 31.93252944946289
Norm before input: 0.2552422881126404
Norm after input: 0.5267603397369385
Norm after each mp layer: 1.4465819597244263
Norm after each mp layer: 6.028192043304443
Norm after each mp layer: 31.93252944946289
Norm before input: 0.2552422881126404
Norm after input: 0.5251250863075256
Norm after each mp layer: 1.4595699310302734
Norm after each mp layer: 6.119500637054443
Norm after each mp layer: 32.44874572753906
Norm before input: 0.2552422881126404
Norm after input: 0.5251250863075256
Norm after each mp layer: 1.4595699310302734
Norm after each mp layer: 6.119500637054443
Norm after each mp layer: 32.44874572753906
Norm before input: 0.2552422881126404
Norm after input: 0.5244232416152954
Norm after each mp layer: 1.4812278747558594
Norm after each mp layer: 6.263790607452393
Norm after each mp layer: 33.46959686279297
Norm before input: 0.2552422881126404
Norm after input: 0.5244232416152954
Norm after each mp layer: 1.4812278747558594
Norm after each mp layer: 6.263790607452393
Norm after each mp layer: 33.46959686279297
Norm before input: 0.2552422881126404
Norm after input: 0.5244554281234741
Norm after each mp layer: 1.5077415704727173
Norm after each mp layer: 6.448853492736816
Norm after each mp layer: 34.88121795654297
Epoch: 35, Loss: 1.0415, Energy: 125017.8281, Train: 52.07%, Valid: 50.60%, Test: 51.20%, Best Valid: 51.20%, Best Test: 51.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5244554281234741
Norm after each mp layer: 1.5077415704727173
Norm after each mp layer: 6.448853492736816
Norm after each mp layer: 34.88121795654297
Norm before input: 0.2552422881126404
Norm after input: 0.5224475860595703
Norm after each mp layer: 1.5184130668640137
Norm after each mp layer: 6.533080577850342
Norm after each mp layer: 35.43244934082031
Norm before input: 0.2552422881126404
Norm after input: 0.5224475860595703
Norm after each mp layer: 1.5184130668640137
Norm after each mp layer: 6.5330810546875
Norm after each mp layer: 35.43244934082031
Norm before input: 0.2552422881126404
Norm after input: 0.5202355980873108
Norm after each mp layer: 1.5256481170654297
Norm after each mp layer: 6.593398094177246
Norm after each mp layer: 35.72468566894531
Norm before input: 0.2552422881126404
Norm after input: 0.5202355980873108
Norm after each mp layer: 1.5256481170654297
Norm after each mp layer: 6.593398094177246
Norm after each mp layer: 35.72468566894531
Norm before input: 0.2552422881126404
Norm after input: 0.5206427574157715
Norm after each mp layer: 1.5460290908813477
Norm after each mp layer: 6.751005172729492
Norm after each mp layer: 36.887939453125
Norm before input: 0.2552422881126404
Norm after input: 0.5206427574157715
Norm after each mp layer: 1.5460290908813477
Norm after each mp layer: 6.751005172729492
Norm after each mp layer: 36.887939453125
Norm before input: 0.2552422881126404
Norm after input: 0.5209531784057617
Norm after each mp layer: 1.5636813640594482
Norm after each mp layer: 6.885537147521973
Norm after each mp layer: 37.771026611328125
Norm before input: 0.2552422881126404
Norm after input: 0.5209531784057617
Norm after each mp layer: 1.5636813640594482
Norm after each mp layer: 6.885537147521973
Norm after each mp layer: 37.771026611328125
Norm before input: 0.2552422881126404
Norm after input: 0.5197861790657043
Norm after each mp layer: 1.5715469121932983
Norm after each mp layer: 6.938078880310059
Norm after each mp layer: 37.85328674316406
Epoch: 40, Loss: 0.9423, Energy: 146247.4219, Train: 64.16%, Valid: 57.20%, Test: 57.00%, Best Valid: 57.40%, Best Test: 57.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5197861790657043
Norm after each mp layer: 1.5715469121932983
Norm after each mp layer: 6.938078880310059
Norm after each mp layer: 37.85328674316406
Norm before input: 0.2552422881126404
Norm after input: 0.5199711322784424
Norm after each mp layer: 1.5884419679641724
Norm after each mp layer: 7.053345203399658
Norm after each mp layer: 38.52571105957031
Norm before input: 0.2552422881126404
Norm after input: 0.5199711322784424
Norm after each mp layer: 1.5884419679641724
Norm after each mp layer: 7.053345203399658
Norm after each mp layer: 38.52571105957031
Norm before input: 0.2552422881126404
Norm after input: 0.5208407640457153
Norm after each mp layer: 1.6106053590774536
Norm after each mp layer: 7.211005210876465
Norm after each mp layer: 39.669307708740234
Norm before input: 0.2552422881126404
Norm after input: 0.5208407640457153
Norm after each mp layer: 1.6106053590774536
Norm after each mp layer: 7.211005210876465
Norm after each mp layer: 39.669307708740234
Norm before input: 0.2552422881126404
Norm after input: 0.5196589827537537
Norm after each mp layer: 1.6229712963104248
Norm after each mp layer: 7.288106918334961
Norm after each mp layer: 40.09705352783203
Norm before input: 0.2552422881126404
Norm after input: 0.5196589827537537
Norm after each mp layer: 1.6229712963104248
Norm after each mp layer: 7.288106918334961
Norm after each mp layer: 40.09705352783203
Norm before input: 0.2552422881126404
Norm after input: 0.5185952186584473
Norm after each mp layer: 1.6362358331680298
Norm after each mp layer: 7.380938529968262
Norm after each mp layer: 40.702491760253906
Norm before input: 0.2552422881126404
Norm after input: 0.5185952186584473
Norm after each mp layer: 1.6362358331680298
Norm after each mp layer: 7.380938529968262
Norm after each mp layer: 40.702491760253906
Norm before input: 0.2552422881126404
Norm after input: 0.5184840559959412
Norm after each mp layer: 1.6530277729034424
Norm after each mp layer: 7.512969017028809
Norm after each mp layer: 41.73072052001953
Epoch: 45, Loss: 0.8837, Energy: 156426.5156, Train: 62.09%, Valid: 55.80%, Test: 57.40%, Best Valid: 58.40%, Best Test: 60.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5184840559959412
Norm after each mp layer: 1.6530277729034424
Norm after each mp layer: 7.512969017028809
Norm after each mp layer: 41.73072052001953
Norm before input: 0.2552422881126404
Norm after input: 0.517022967338562
Norm after each mp layer: 1.6606931686401367
Norm after each mp layer: 7.567717552185059
Norm after each mp layer: 41.96856689453125
Norm before input: 0.2552422881126404
Norm after input: 0.517022967338562
Norm after each mp layer: 1.6606931686401367
Norm after each mp layer: 7.567717552185059
Norm after each mp layer: 41.96856689453125
Norm before input: 0.2552422881126404
Norm after input: 0.5166682004928589
Norm after each mp layer: 1.671928882598877
Norm after each mp layer: 7.6488237380981445
Norm after each mp layer: 42.427913665771484
Norm before input: 0.2552422881126404
Norm after input: 0.5166682004928589
Norm after each mp layer: 1.671928882598877
Norm after each mp layer: 7.6488237380981445
Norm after each mp layer: 42.427913665771484
Norm before input: 0.2552422881126404
Norm after input: 0.5175877809524536
Norm after each mp layer: 1.6880290508270264
Norm after each mp layer: 7.767800331115723
Norm after each mp layer: 43.228389739990234
Norm before input: 0.2552422881126404
Norm after input: 0.5175877809524536
Norm after each mp layer: 1.6880290508270264
Norm after each mp layer: 7.767800331115723
Norm after each mp layer: 43.228389739990234
Norm before input: 0.2552422881126404
Norm after input: 0.5171801447868347
Norm after each mp layer: 1.6979637145996094
Norm after each mp layer: 7.8204145431518555
Norm after each mp layer: 43.323341369628906
Norm before input: 0.2552422881126404
Norm after input: 0.5171801447868347
Norm after each mp layer: 1.6979637145996094
Norm after each mp layer: 7.8204145431518555
Norm after each mp layer: 43.323341369628906
Norm before input: 0.2552422881126404
Norm after input: 0.518593430519104
Norm after each mp layer: 1.7166587114334106
Norm after each mp layer: 7.963155269622803
Norm after each mp layer: 44.384944915771484
Epoch: 50, Loss: 0.8198, Energy: 180302.7500, Train: 69.78%, Valid: 61.60%, Test: 59.60%, Best Valid: 61.60%, Best Test: 60.40%
Norm before input: 0.2552422881126404
Norm after input: 0.518593430519104
Norm after each mp layer: 1.7166587114334106
Norm after each mp layer: 7.963155269622803
Norm after each mp layer: 44.384944915771484
Norm before input: 0.2552422881126404
Norm after input: 0.5184622406959534
Norm after each mp layer: 1.7296061515808105
Norm after each mp layer: 8.067551612854004
Norm after each mp layer: 45.08148956298828
Norm before input: 0.2552422881126404
Norm after input: 0.5184622406959534
Norm after each mp layer: 1.7296061515808105
Norm after each mp layer: 8.067551612854004
Norm after each mp layer: 45.08148956298828
Norm before input: 0.2552422881126404
Norm after input: 0.5167841911315918
Norm after each mp layer: 1.7366443872451782
Norm after each mp layer: 8.132559776306152
Norm after each mp layer: 45.392120361328125
Norm before input: 0.2552422881126404
Norm after input: 0.5167841911315918
Norm after each mp layer: 1.7366443872451782
Norm after each mp layer: 8.132559776306152
Norm after each mp layer: 45.392120361328125
Norm before input: 0.2552422881126404
Norm after input: 0.5173787474632263
Norm after each mp layer: 1.7546908855438232
Norm after each mp layer: 8.308759689331055
Norm after each mp layer: 46.800838470458984
Norm before input: 0.2552422881126404
Norm after input: 0.5173787474632263
Norm after each mp layer: 1.7546908855438232
Norm after each mp layer: 8.308759689331055
Norm after each mp layer: 46.800838470458984
Norm before input: 0.2552422881126404
Norm after input: 0.5150643587112427
Norm after each mp layer: 1.7588024139404297
Norm after each mp layer: 8.35235595703125
Norm after each mp layer: 46.833892822265625
Norm before input: 0.2552422881126404
Norm after input: 0.5150643587112427
Norm after each mp layer: 1.7588024139404297
Norm after each mp layer: 8.35235595703125
Norm after each mp layer: 46.833892822265625
Norm before input: 0.2552422881126404
Norm after input: 0.5161104798316956
Norm after each mp layer: 1.7784450054168701
Norm after each mp layer: 8.52840518951416
Norm after each mp layer: 48.10386276245117
Epoch: 55, Loss: 0.7500, Energy: 201927.1406, Train: 75.08%, Valid: 62.60%, Test: 62.10%, Best Valid: 63.40%, Best Test: 62.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5161104798316956
Norm after each mp layer: 1.7784450054168701
Norm after each mp layer: 8.52840518951416
Norm after each mp layer: 48.10386276245117
Norm before input: 0.2552422881126404
Norm after input: 0.5152090191841125
Norm after each mp layer: 1.7883410453796387
Norm after each mp layer: 8.606010437011719
Norm after each mp layer: 48.39019775390625
Norm before input: 0.2552422881126404
Norm after input: 0.5152090191841125
Norm after each mp layer: 1.7883410453796387
Norm after each mp layer: 8.606010437011719
Norm after each mp layer: 48.39019775390625
Norm before input: 0.2552422881126404
Norm after input: 0.515738844871521
Norm after each mp layer: 1.8054678440093994
Norm after each mp layer: 8.755066871643066
Norm after each mp layer: 49.37065505981445
Norm before input: 0.2552422881126404
Norm after input: 0.515738844871521
Norm after each mp layer: 1.8054678440093994
Norm after each mp layer: 8.755066871643066
Norm after each mp layer: 49.37065505981445
Norm before input: 0.2552422881126404
Norm after input: 0.5153983235359192
Norm after each mp layer: 1.8189936876296997
Norm after each mp layer: 8.881113052368164
Norm after each mp layer: 50.16387176513672
Norm before input: 0.2552422881126404
Norm after input: 0.5153983235359192
Norm after each mp layer: 1.8189936876296997
Norm after each mp layer: 8.881113052368164
Norm after each mp layer: 50.163875579833984
Norm before input: 0.2552422881126404
Norm after input: 0.5145034790039062
Norm after each mp layer: 1.8301059007644653
Norm after each mp layer: 8.995905876159668
Norm after each mp layer: 50.87809753417969
Norm before input: 0.2552422881126404
Norm after input: 0.5145034790039062
Norm after each mp layer: 1.8301059007644653
Norm after each mp layer: 8.995905876159668
Norm after each mp layer: 50.87809753417969
Norm before input: 0.2552422881126404
Norm after input: 0.514495849609375
Norm after each mp layer: 1.8455451726913452
Norm after each mp layer: 9.160303115844727
Norm after each mp layer: 52.046112060546875
Epoch: 60, Loss: 0.6758, Energy: 239717.0938, Train: 79.14%, Valid: 64.40%, Test: 64.10%, Best Valid: 64.40%, Best Test: 64.10%
Norm before input: 0.2552422881126404
Norm after input: 0.514495849609375
Norm after each mp layer: 1.8455451726913452
Norm after each mp layer: 9.160303115844727
Norm after each mp layer: 52.046112060546875
Norm before input: 0.2552422881126404
Norm after input: 0.5126150250434875
Norm after each mp layer: 1.8508696556091309
Norm after each mp layer: 9.231097221374512
Norm after each mp layer: 52.32216262817383
Norm before input: 0.2552422881126404
Norm after input: 0.5126150250434875
Norm after each mp layer: 1.8508696556091309
Norm after each mp layer: 9.231097221374512
Norm after each mp layer: 52.32216262817383
Norm before input: 0.2552422881126404
Norm after input: 0.5144997835159302
Norm after each mp layer: 1.8761261701583862
Norm after each mp layer: 9.486159324645996
Norm after each mp layer: 54.200740814208984
Norm before input: 0.2552422881126404
Norm after input: 0.5144997835159302
Norm after each mp layer: 1.8761261701583862
Norm after each mp layer: 9.486159324645996
Norm after each mp layer: 54.200740814208984
Norm before input: 0.2552422881126404
Norm after input: 0.5100684762001038
Norm after each mp layer: 1.8692936897277832
Norm after each mp layer: 9.437471389770508
Norm after each mp layer: 53.3460807800293
Norm before input: 0.2552422881126404
Norm after input: 0.5100684762001038
Norm after each mp layer: 1.8692936897277832
Norm after each mp layer: 9.437471389770508
Norm after each mp layer: 53.3460807800293
Norm before input: 0.2552422881126404
Norm after input: 0.5158231258392334
Norm after each mp layer: 1.916224718093872
Norm after each mp layer: 9.872665405273438
Norm after each mp layer: 56.70387268066406
Norm before input: 0.2552422881126404
Norm after input: 0.5158231258392334
Norm after each mp layer: 1.916224718093872
Norm after each mp layer: 9.872665405273438
Norm after each mp layer: 56.70387268066406
Norm before input: 0.2552422881126404
Norm after input: 0.5084559321403503
Norm after each mp layer: 1.8954111337661743
Norm after each mp layer: 9.71342945098877
Norm after each mp layer: 55.04342269897461
Epoch: 65, Loss: 0.6918, Energy: 326938.9375, Train: 81.13%, Valid: 66.60%, Test: 64.10%, Best Valid: 66.60%, Best Test: 64.30%
Norm before input: 0.2552422881126404
Norm after input: 0.5084559321403503
Norm after each mp layer: 1.8954111337661743
Norm after each mp layer: 9.71342945098877
Norm after each mp layer: 55.04342269897461
Norm before input: 0.2552422881126404
Norm after input: 0.50883549451828
Norm after each mp layer: 1.9135406017303467
Norm after each mp layer: 9.907853126525879
Norm after each mp layer: 56.41640853881836
Norm before input: 0.2552422881126404
Norm after input: 0.50883549451828
Norm after each mp layer: 1.9135406017303467
Norm after each mp layer: 9.907853126525879
Norm after each mp layer: 56.41640853881836
Norm before input: 0.2552422881126404
Norm after input: 0.5106794834136963
Norm after each mp layer: 1.9397757053375244
Norm after each mp layer: 10.191957473754883
Norm after each mp layer: 58.49510192871094
Norm before input: 0.2552422881126404
Norm after input: 0.5106794834136963
Norm after each mp layer: 1.9397757053375244
Norm after each mp layer: 10.1919584274292
Norm after each mp layer: 58.49510192871094
Norm before input: 0.2552422881126404
Norm after input: 0.5045977830886841
Norm after each mp layer: 1.9209972620010376
Norm after each mp layer: 10.099483489990234
Norm after each mp layer: 57.46229934692383
Norm before input: 0.2552422881126404
Norm after input: 0.5045977830886841
Norm after each mp layer: 1.9209972620010376
Norm after each mp layer: 10.099483489990234
Norm after each mp layer: 57.46229934692383
Norm before input: 0.2552422881126404
Norm after input: 0.5048962235450745
Norm after each mp layer: 1.935749888420105
Norm after each mp layer: 10.274665832519531
Norm after each mp layer: 58.63289260864258
Norm before input: 0.2552422881126404
Norm after input: 0.5048962235450745
Norm after each mp layer: 1.935749888420105
Norm after each mp layer: 10.274665832519531
Norm after each mp layer: 58.63289260864258
Norm before input: 0.2552422881126404
Norm after input: 0.5062681436538696
Norm after each mp layer: 1.9574100971221924
Norm after each mp layer: 10.509163856506348
Norm after each mp layer: 60.26835250854492
Epoch: 70, Loss: 0.5393, Energy: 338026.6875, Train: 83.36%, Valid: 67.80%, Test: 66.10%, Best Valid: 68.00%, Best Test: 66.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5062681436538696
Norm after each mp layer: 1.9574100971221924
Norm after each mp layer: 10.509163856506348
Norm after each mp layer: 60.268348693847656
Norm before input: 0.2552422881126404
Norm after input: 0.5015525817871094
Norm after each mp layer: 1.943037509918213
Norm after each mp layer: 10.448949813842773
Norm after each mp layer: 59.58835983276367
Norm before input: 0.2552422881126404
Norm after input: 0.5015525817871094
Norm after each mp layer: 1.943037509918213
Norm after each mp layer: 10.448949813842773
Norm after each mp layer: 59.58835983276367
Norm before input: 0.2552422881126404
Norm after input: 0.5011262893676758
Norm after each mp layer: 1.953283667564392
Norm after each mp layer: 10.586194038391113
Norm after each mp layer: 60.473873138427734
Norm before input: 0.2552422881126404
Norm after input: 0.5011262893676758
Norm after each mp layer: 1.953283667564392
Norm after each mp layer: 10.586194038391113
Norm after each mp layer: 60.473873138427734
Norm before input: 0.2552422881126404
Norm after input: 0.5029118657112122
Norm after each mp layer: 1.9766520261764526
Norm after each mp layer: 10.831010818481445
Norm after each mp layer: 62.180152893066406
Norm before input: 0.2552422881126404
Norm after input: 0.5029118657112122
Norm after each mp layer: 1.9766520261764526
Norm after each mp layer: 10.831010818481445
Norm after each mp layer: 62.180152893066406
Norm before input: 0.2552422881126404
Norm after input: 0.49977532029151917
Norm after each mp layer: 1.9685956239700317
Norm after each mp layer: 10.823195457458496
Norm after each mp layer: 61.8891716003418
Norm before input: 0.2552422881126404
Norm after input: 0.49977532029151917
Norm after each mp layer: 1.9685956239700317
Norm after each mp layer: 10.823195457458496
Norm after each mp layer: 61.8891716003418
Norm before input: 0.2552422881126404
Norm after input: 0.4992811679840088
Norm after each mp layer: 1.9767415523529053
Norm after each mp layer: 10.947600364685059
Norm after each mp layer: 62.61463165283203
Epoch: 75, Loss: 0.4747, Energy: 401536.0312, Train: 86.51%, Valid: 69.20%, Test: 66.50%, Best Valid: 69.40%, Best Test: 66.90%
Norm before input: 0.2552422881126404
Norm after input: 0.4992811679840088
Norm after each mp layer: 1.9767415523529053
Norm after each mp layer: 10.947600364685059
Norm after each mp layer: 62.61463165283203
Norm before input: 0.2552422881126404
Norm after input: 0.5016924142837524
Norm after each mp layer: 2.004450559616089
Norm after each mp layer: 11.23019027709961
Norm after each mp layer: 64.53759002685547
Norm before input: 0.2552422881126404
Norm after input: 0.5016924142837524
Norm after each mp layer: 2.004450559616089
Norm after each mp layer: 11.23019027709961
Norm after each mp layer: 64.53759002685547
Norm before input: 0.2552422881126404
Norm after input: 0.49940478801727295
Norm after each mp layer: 2.0041708946228027
Norm after each mp layer: 11.273834228515625
Norm after each mp layer: 64.60397338867188
Norm before input: 0.2552422881126404
Norm after input: 0.49940478801727295
Norm after each mp layer: 2.0041708946228027
Norm after each mp layer: 11.273834228515625
Norm after each mp layer: 64.60397338867188
Norm before input: 0.2552422881126404
Norm after input: 0.5001478791236877
Norm after each mp layer: 2.025297164916992
Norm after each mp layer: 11.486613273620605
Norm after each mp layer: 65.938720703125
Norm before input: 0.2552422881126404
Norm after input: 0.5001478791236877
Norm after each mp layer: 2.025297164916992
Norm after each mp layer: 11.486613273620605
Norm after each mp layer: 65.938720703125
Norm before input: 0.2552422881126404
Norm after input: 0.5024864673614502
Norm after each mp layer: 2.0572078227996826
Norm after each mp layer: 11.790684700012207
Norm after each mp layer: 67.957763671875
Norm before input: 0.2552422881126404
Norm after input: 0.5024864673614502
Norm after each mp layer: 2.0572080612182617
Norm after each mp layer: 11.790684700012207
Norm after each mp layer: 67.957763671875
Norm before input: 0.2552422881126404
Norm after input: 0.50055992603302
Norm after each mp layer: 2.059182643890381
Norm after each mp layer: 11.847504615783691
Norm after each mp layer: 68.11044311523438
Epoch: 80, Loss: 0.4166, Energy: 550754.7500, Train: 88.00%, Valid: 71.20%, Test: 68.30%, Best Valid: 71.20%, Best Test: 68.90%
Norm before input: 0.2552422881126404
Norm after input: 0.50055992603302
Norm after each mp layer: 2.059182643890381
Norm after each mp layer: 11.847504615783691
Norm after each mp layer: 68.11044311523438
Norm before input: 0.2552422881126404
Norm after input: 0.5027950406074524
Norm after each mp layer: 2.0890305042266846
Norm after each mp layer: 12.127808570861816
Norm after each mp layer: 69.92249298095703
Norm before input: 0.2552422881126404
Norm after input: 0.5027950406074524
Norm after each mp layer: 2.0890305042266846
Norm after each mp layer: 12.127808570861816
Norm after each mp layer: 69.92249298095703
Norm before input: 0.2552422881126404
Norm after input: 0.5034637451171875
Norm after each mp layer: 2.10562801361084
Norm after each mp layer: 12.308205604553223
Norm after each mp layer: 71.02841186523438
Norm before input: 0.2552422881126404
Norm after input: 0.5034637451171875
Norm after each mp layer: 2.10562801361084
Norm after each mp layer: 12.308205604553223
Norm after each mp layer: 71.02841186523438
Norm before input: 0.2552422881126404
Norm after input: 0.5024768114089966
Norm after each mp layer: 2.108715534210205
Norm after each mp layer: 12.380566596984863
Norm after each mp layer: 71.37975311279297
Norm before input: 0.2552422881126404
Norm after input: 0.5024768114089966
Norm after each mp layer: 2.108715534210205
Norm after each mp layer: 12.380566596984863
Norm after each mp layer: 71.37975311279297
Norm before input: 0.2552422881126404
Norm after input: 0.5057051777839661
Norm after each mp layer: 2.1429433822631836
Norm after each mp layer: 12.703741073608398
Norm after each mp layer: 73.57591247558594
Norm before input: 0.2552422881126404
Norm after input: 0.5057051777839661
Norm after each mp layer: 2.1429433822631836
Norm after each mp layer: 12.703741073608398
Norm after each mp layer: 73.57591247558594
Norm before input: 0.2552422881126404
Norm after input: 0.5038557052612305
Norm after each mp layer: 2.1407551765441895
Norm after each mp layer: 12.708499908447266
Norm after each mp layer: 73.43984985351562
Epoch: 85, Loss: 0.3572, Energy: 703842.1875, Train: 90.31%, Valid: 72.00%, Test: 69.60%, Best Valid: 72.40%, Best Test: 70.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5038557052612305
Norm after each mp layer: 2.1407551765441895
Norm after each mp layer: 12.708499908447266
Norm after each mp layer: 73.43985748291016
Norm before input: 0.2552422881126404
Norm after input: 0.5060787796974182
Norm after each mp layer: 2.166306495666504
Norm after each mp layer: 12.955705642700195
Norm after each mp layer: 75.14207458496094
Norm before input: 0.2552422881126404
Norm after input: 0.5060787796974182
Norm after each mp layer: 2.166306495666504
Norm after each mp layer: 12.955705642700195
Norm after each mp layer: 75.14207458496094
Norm before input: 0.2552422881126404
Norm after input: 0.5065799951553345
Norm after each mp layer: 2.1773645877838135
Norm after each mp layer: 13.09173583984375
Norm after each mp layer: 76.07618713378906
Norm before input: 0.2552422881126404
Norm after input: 0.5065799951553345
Norm after each mp layer: 2.1773645877838135
Norm after each mp layer: 13.09173583984375
Norm after each mp layer: 76.07618713378906
Norm before input: 0.2552422881126404
Norm after input: 0.5060936808586121
Norm after each mp layer: 2.1812310218811035
Norm after each mp layer: 13.159464836120605
Norm after each mp layer: 76.49079895019531
Norm before input: 0.2552422881126404
Norm after input: 0.5060936808586121
Norm after each mp layer: 2.1812310218811035
Norm after each mp layer: 13.159463882446289
Norm after each mp layer: 76.49079895019531
Norm before input: 0.2552422881126404
Norm after input: 0.5094543695449829
Norm after each mp layer: 2.2180051803588867
Norm after each mp layer: 13.478665351867676
Norm after each mp layer: 78.64521789550781
Norm before input: 0.2552422881126404
Norm after input: 0.5094543695449829
Norm after each mp layer: 2.2180051803588867
Norm after each mp layer: 13.478665351867676
Norm after each mp layer: 78.64522552490234
Norm before input: 0.2552422881126404
Norm after input: 0.5083222985267639
Norm after each mp layer: 2.221837043762207
Norm after each mp layer: 13.497560501098633
Norm after each mp layer: 78.54923248291016
Epoch: 90, Loss: 0.2849, Energy: 885948.3750, Train: 92.63%, Valid: 72.60%, Test: 71.80%, Best Valid: 73.60%, Best Test: 72.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5083222985267639
Norm after each mp layer: 2.221837043762207
Norm after each mp layer: 13.497560501098633
Norm after each mp layer: 78.54923248291016
Norm before input: 0.2552422881126404
Norm after input: 0.5101856589317322
Norm after each mp layer: 2.2430777549743652
Norm after each mp layer: 13.708176612854004
Norm after each mp layer: 80.05661010742188
Norm before input: 0.2552422881126404
Norm after input: 0.5101856589317322
Norm after each mp layer: 2.2430777549743652
Norm after each mp layer: 13.708176612854004
Norm after each mp layer: 80.05661010742188
Norm before input: 0.2552422881126404
Norm after input: 0.5111620426177979
Norm after each mp layer: 2.25390625
Norm after each mp layer: 13.865418434143066
Norm after each mp layer: 81.33712768554688
Norm before input: 0.2552422881126404
Norm after input: 0.5111620426177979
Norm after each mp layer: 2.25390625
Norm after each mp layer: 13.865418434143066
Norm after each mp layer: 81.33712768554688
Norm before input: 0.2552422881126404
Norm after input: 0.5094448924064636
Norm after each mp layer: 2.2495274543762207
Norm after each mp layer: 13.832462310791016
Norm after each mp layer: 81.00101470947266
Norm before input: 0.2552422881126404
Norm after input: 0.5094448924064636
Norm after each mp layer: 2.2495274543762207
Norm after each mp layer: 13.832462310791016
Norm after each mp layer: 81.00101470947266
Norm before input: 0.2552422881126404
Norm after input: 0.5147534608840942
Norm after each mp layer: 2.3056817054748535
Norm after each mp layer: 14.282170295715332
Norm after each mp layer: 84.11698150634766
Norm before input: 0.2552422881126404
Norm after input: 0.5147534608840942
Norm after each mp layer: 2.3056817054748535
Norm after each mp layer: 14.282170295715332
Norm after each mp layer: 84.11698150634766
Norm before input: 0.2552422881126404
Norm after input: 0.5132483839988708
Norm after each mp layer: 2.308560609817505
Norm after each mp layer: 14.265862464904785
Norm after each mp layer: 83.78563690185547
Epoch: 95, Loss: 0.2386, Energy: 1123360.8750, Train: 94.04%, Valid: 74.20%, Test: 74.80%, Best Valid: 75.20%, Best Test: 74.80%
Norm before input: 0.2552422881126404
Norm after input: 0.5132483839988708
Norm after each mp layer: 2.308560609817505
Norm after each mp layer: 14.265862464904785
Norm after each mp layer: 83.78563690185547
Norm before input: 0.2552422881126404
Norm after input: 0.5114874839782715
Norm after each mp layer: 2.299699068069458
Norm after each mp layer: 14.241607666015625
Norm after each mp layer: 83.93209838867188
Norm before input: 0.2552422881126404
Norm after input: 0.5114874839782715
Norm after each mp layer: 2.299699068069458
Norm after each mp layer: 14.241607666015625
Norm after each mp layer: 83.93209838867188
Norm before input: 0.2552422881126404
Norm after input: 0.5140002369880676
Norm after each mp layer: 2.318546772003174
Norm after each mp layer: 14.479939460754395
Norm after each mp layer: 86.29117584228516
Norm before input: 0.2552422881126404
Norm after input: 0.5140002369880676
Norm after each mp layer: 2.318546772003174
Norm after each mp layer: 14.479939460754395
Norm after each mp layer: 86.29117584228516
Norm before input: 0.2552422881126404
Norm after input: 0.5121300220489502
Norm after each mp layer: 2.311861276626587
Norm after each mp layer: 14.397809982299805
Norm after each mp layer: 85.59761047363281
Norm before input: 0.2552422881126404
Norm after input: 0.5121300220489502
Norm after each mp layer: 2.311861276626587
Norm after each mp layer: 14.397809982299805
Norm after each mp layer: 85.59761047363281
Norm before input: 0.2552422881126404
Norm after input: 0.5114552974700928
Norm after each mp layer: 2.3180925846099854
Norm after each mp layer: 14.41102409362793
Norm after each mp layer: 85.4548568725586
Norm before input: 0.2552422881126404
Norm after input: 0.5114552974700928
Norm after each mp layer: 2.3180925846099854
Norm after each mp layer: 14.41102409362793
Norm after each mp layer: 85.4548568725586
Norm before input: 0.2552422881126404
Norm after input: 0.5166428685188293
Norm after each mp layer: 2.369234085083008
Norm after each mp layer: 14.825760841369629
Norm after each mp layer: 88.57598114013672
Epoch: 100, Loss: 0.1806, Energy: 1176789.3750, Train: 94.70%, Valid: 75.60%, Test: 75.20%, Best Valid: 76.20%, Best Test: 75.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5166428685188293
Norm after each mp layer: 2.369234085083008
Norm after each mp layer: 14.825760841369629
Norm after each mp layer: 88.57598114013672
Norm before input: 0.2552422881126404
Norm after input: 0.5154886841773987
Norm after each mp layer: 2.370732069015503
Norm after each mp layer: 14.826800346374512
Norm after each mp layer: 88.59874725341797
Norm before input: 0.2552422881126404
Norm after input: 0.5154886841773987
Norm after each mp layer: 2.370732069015503
Norm after each mp layer: 14.826800346374512
Norm after each mp layer: 88.59874725341797
Norm before input: 0.2552422881126404
Norm after input: 0.5125337243080139
Norm after each mp layer: 2.3560609817504883
Norm after each mp layer: 14.714864730834961
Norm after each mp layer: 87.93302154541016
Norm before input: 0.2552422881126404
Norm after input: 0.5125337243080139
Norm after each mp layer: 2.3560609817504883
Norm after each mp layer: 14.714864730834961
Norm after each mp layer: 87.93302154541016
Norm before input: 0.2552422881126404
Norm after input: 0.5150128602981567
Norm after each mp layer: 2.375725030899048
Norm after each mp layer: 14.930108070373535
Norm after each mp layer: 90.01981353759766
Norm before input: 0.2552422881126404
Norm after input: 0.5150128602981567
Norm after each mp layer: 2.375725030899048
Norm after each mp layer: 14.930108070373535
Norm after each mp layer: 90.01981353759766
Norm before input: 0.2552422881126404
Norm after input: 0.5148237347602844
Norm after each mp layer: 2.380810022354126
Norm after each mp layer: 14.95958137512207
Norm after each mp layer: 90.13427734375
Norm before input: 0.2552422881126404
Norm after input: 0.5148237347602844
Norm after each mp layer: 2.380810022354126
Norm after each mp layer: 14.95958137512207
Norm after each mp layer: 90.13427734375
Norm before input: 0.2552422881126404
Norm after input: 0.5129077434539795
Norm after each mp layer: 2.37339448928833
Norm after each mp layer: 14.882132530212402
Norm after each mp layer: 89.34561157226562
Epoch: 105, Loss: 0.1361, Energy: 1430177.0000, Train: 95.86%, Valid: 75.60%, Test: 77.00%, Best Valid: 77.00%, Best Test: 77.00%
Norm before input: 0.2552422881126404
Norm after input: 0.5129077434539795
Norm after each mp layer: 2.37339448928833
Norm after each mp layer: 14.882132530212402
Norm after each mp layer: 89.34561157226562
Norm before input: 0.2552422881126404
Norm after input: 0.515687108039856
Norm after each mp layer: 2.3996236324310303
Norm after each mp layer: 15.119489669799805
Norm after each mp layer: 91.20783233642578
Norm before input: 0.2552422881126404
Norm after input: 0.515687108039856
Norm after each mp layer: 2.3996236324310303
Norm after each mp layer: 15.119489669799805
Norm after each mp layer: 91.20783233642578
Norm before input: 0.2552422881126404
Norm after input: 0.516615629196167
Norm after each mp layer: 2.4119455814361572
Norm after each mp layer: 15.236990928649902
Norm after each mp layer: 92.1287841796875
Norm before input: 0.2552422881126404
Norm after input: 0.516615629196167
Norm after each mp layer: 2.4119455814361572
Norm after each mp layer: 15.236990928649902
Norm after each mp layer: 92.1287841796875
Norm before input: 0.2552422881126404
Norm after input: 0.5144335031509399
Norm after each mp layer: 2.40301513671875
Norm after each mp layer: 15.131996154785156
Norm after each mp layer: 90.98136901855469
Norm before input: 0.2552422881126404
Norm after input: 0.5144335031509399
Norm after each mp layer: 2.40301513671875
Norm after each mp layer: 15.131996154785156
Norm after each mp layer: 90.98136901855469
Norm before input: 0.2552422881126404
Norm after input: 0.5146970152854919
Norm after each mp layer: 2.407822370529175
Norm after each mp layer: 15.190298080444336
Norm after each mp layer: 91.62409973144531
Norm before input: 0.2552422881126404
Norm after input: 0.5146970152854919
Norm after each mp layer: 2.407822370529175
Norm after each mp layer: 15.190298080444336
Norm after each mp layer: 91.62409973144531
Norm before input: 0.2552422881126404
Norm after input: 0.5157977938652039
Norm after each mp layer: 2.417064666748047
Norm after each mp layer: 15.306526184082031
Norm after each mp layer: 92.92212677001953
Epoch: 110, Loss: 0.1088, Energy: 1552208.5000, Train: 97.19%, Valid: 77.40%, Test: 76.30%, Best Valid: 77.40%, Best Test: 77.00%
Norm before input: 0.2552422881126404
Norm after input: 0.5157977938652039
Norm after each mp layer: 2.417064666748047
Norm after each mp layer: 15.306526184082031
Norm after each mp layer: 92.92212677001953
Norm before input: 0.2552422881126404
Norm after input: 0.5139551162719727
Norm after each mp layer: 2.4062249660491943
Norm after each mp layer: 15.215415000915527
Norm after each mp layer: 92.19149780273438
Norm before input: 0.2552422881126404
Norm after input: 0.5139551162719727
Norm after each mp layer: 2.4062249660491943
Norm after each mp layer: 15.215415000915527
Norm after each mp layer: 92.19149780273438
Norm before input: 0.2552422881126404
Norm after input: 0.5129956007003784
Norm after each mp layer: 2.4033634662628174
Norm after each mp layer: 15.196907997131348
Norm after each mp layer: 91.91973876953125
Norm before input: 0.2552422881126404
Norm after input: 0.5129956007003784
Norm after each mp layer: 2.4033634662628174
Norm after each mp layer: 15.196907997131348
Norm after each mp layer: 91.91973876953125
Norm before input: 0.2552422881126404
Norm after input: 0.5169933438301086
Norm after each mp layer: 2.4416959285736084
Norm after each mp layer: 15.535331726074219
Norm after each mp layer: 94.16072082519531
Norm before input: 0.2552422881126404
Norm after input: 0.5169933438301086
Norm after each mp layer: 2.4416959285736084
Norm after each mp layer: 15.535331726074219
Norm after each mp layer: 94.16072082519531
Norm before input: 0.2552422881126404
Norm after input: 0.5169180631637573
Norm after each mp layer: 2.4483821392059326
Norm after each mp layer: 15.589557647705078
Norm after each mp layer: 94.14353942871094
Norm before input: 0.2552422881126404
Norm after input: 0.5169180631637573
Norm after each mp layer: 2.4483821392059326
Norm after each mp layer: 15.589557647705078
Norm after each mp layer: 94.14353942871094
Norm before input: 0.2552422881126404
Norm after input: 0.5145946741104126
Norm after each mp layer: 2.43554949760437
Norm after each mp layer: 15.490921020507812
Norm after each mp layer: 93.32171630859375
Epoch: 115, Loss: 0.0961, Energy: 1722586.0000, Train: 96.52%, Valid: 75.60%, Test: 77.10%, Best Valid: 77.80%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5145946741104126
Norm after each mp layer: 2.435549736022949
Norm after each mp layer: 15.490921020507812
Norm after each mp layer: 93.32171630859375
Norm before input: 0.2552422881126404
Norm after input: 0.5154305100440979
Norm after each mp layer: 2.442303419113159
Norm after each mp layer: 15.598440170288086
Norm after each mp layer: 94.46669006347656
Norm before input: 0.2552422881126404
Norm after input: 0.5154305100440979
Norm after each mp layer: 2.442303419113159
Norm after each mp layer: 15.598440170288086
Norm after each mp layer: 94.46668243408203
Norm before input: 0.2552422881126404
Norm after input: 0.5168333649635315
Norm after each mp layer: 2.4556102752685547
Norm after each mp layer: 15.743199348449707
Norm after each mp layer: 95.46456146240234
Norm before input: 0.2552422881126404
Norm after input: 0.5168333649635315
Norm after each mp layer: 2.4556102752685547
Norm after each mp layer: 15.743199348449707
Norm after each mp layer: 95.46456146240234
Norm before input: 0.2552422881126404
Norm after input: 0.5151823163032532
Norm after each mp layer: 2.4489614963531494
Norm after each mp layer: 15.662304878234863
Norm after each mp layer: 94.14659118652344
Norm before input: 0.2552422881126404
Norm after input: 0.5151823163032532
Norm after each mp layer: 2.4489614963531494
Norm after each mp layer: 15.662304878234863
Norm after each mp layer: 94.14659118652344
Norm before input: 0.2552422881126404
Norm after input: 0.514207661151886
Norm after each mp layer: 2.446146011352539
Norm after each mp layer: 15.641646385192871
Norm after each mp layer: 93.7352066040039
Norm before input: 0.2552422881126404
Norm after input: 0.514207661151886
Norm after each mp layer: 2.446146011352539
Norm after each mp layer: 15.641646385192871
Norm after each mp layer: 93.7352066040039
Norm before input: 0.2552422881126404
Norm after input: 0.5168099403381348
Norm after each mp layer: 2.4675028324127197
Norm after each mp layer: 15.871214866638184
Norm after each mp layer: 95.79434967041016
Epoch: 120, Loss: 0.0910, Energy: 1647628.5000, Train: 97.76%, Valid: 77.80%, Test: 76.00%, Best Valid: 78.20%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5168099403381348
Norm after each mp layer: 2.4675028324127197
Norm after each mp layer: 15.871214866638184
Norm after each mp layer: 95.79434967041016
Norm before input: 0.2552422881126404
Norm after input: 0.5171296000480652
Norm after each mp layer: 2.472066640853882
Norm after each mp layer: 15.946191787719727
Norm after each mp layer: 96.65206909179688
Norm before input: 0.2552422881126404
Norm after input: 0.5171296000480652
Norm after each mp layer: 2.472066640853882
Norm after each mp layer: 15.946191787719727
Norm after each mp layer: 96.65206909179688
Norm before input: 0.2552422881126404
Norm after input: 0.5151128768920898
Norm after each mp layer: 2.4606261253356934
Norm after each mp layer: 15.841470718383789
Norm after each mp layer: 95.59976959228516
Norm before input: 0.2552422881126404
Norm after input: 0.5151128768920898
Norm after each mp layer: 2.4606261253356934
Norm after each mp layer: 15.841470718383789
Norm after each mp layer: 95.59976959228516
Norm before input: 0.2552422881126404
Norm after input: 0.5154314637184143
Norm after each mp layer: 2.468616247177124
Norm after each mp layer: 15.90088939666748
Norm after each mp layer: 95.29130554199219
Norm before input: 0.2552422881126404
Norm after input: 0.5154314637184143
Norm after each mp layer: 2.468616247177124
Norm after each mp layer: 15.90088939666748
Norm after each mp layer: 95.29130554199219
Norm before input: 0.2552422881126404
Norm after input: 0.5178250670433044
Norm after each mp layer: 2.4905858039855957
Norm after each mp layer: 16.106935501098633
Norm after each mp layer: 96.2968521118164
Norm before input: 0.2552422881126404
Norm after input: 0.5178250670433044
Norm after each mp layer: 2.4905858039855957
Norm after each mp layer: 16.106935501098633
Norm after each mp layer: 96.2968521118164
Norm before input: 0.2552422881126404
Norm after input: 0.5178141593933105
Norm after each mp layer: 2.492199420928955
Norm after each mp layer: 16.142892837524414
Norm after each mp layer: 96.55799865722656
Epoch: 125, Loss: 0.0699, Energy: 1848136.6250, Train: 97.76%, Valid: 77.80%, Test: 76.30%, Best Valid: 78.60%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5178141593933105
Norm after each mp layer: 2.492199420928955
Norm after each mp layer: 16.142892837524414
Norm after each mp layer: 96.55799865722656
Norm before input: 0.2552422881126404
Norm after input: 0.5154052972793579
Norm after each mp layer: 2.4747061729431152
Norm after each mp layer: 16.005441665649414
Norm after each mp layer: 95.81543731689453
Norm before input: 0.2552422881126404
Norm after input: 0.5154052972793579
Norm after each mp layer: 2.4747061729431152
Norm after each mp layer: 16.005441665649414
Norm after each mp layer: 95.81543731689453
Norm before input: 0.2552422881126404
Norm after input: 0.5150470733642578
Norm after each mp layer: 2.4750046730041504
Norm after each mp layer: 16.02802085876465
Norm after each mp layer: 96.18367004394531
Norm before input: 0.2552422881126404
Norm after input: 0.5150470733642578
Norm after each mp layer: 2.4750046730041504
Norm after each mp layer: 16.02802085876465
Norm after each mp layer: 96.18367004394531
Norm before input: 0.2552422881126404
Norm after input: 0.5184794068336487
Norm after each mp layer: 2.509373426437378
Norm after each mp layer: 16.346046447753906
Norm after each mp layer: 98.2396240234375
Norm before input: 0.2552422881126404
Norm after input: 0.5184794068336487
Norm after each mp layer: 2.509373426437378
Norm after each mp layer: 16.346046447753906
Norm after each mp layer: 98.2396240234375
Norm before input: 0.2552422881126404
Norm after input: 0.5187052488327026
Norm after each mp layer: 2.5209274291992188
Norm after each mp layer: 16.419946670532227
Norm after each mp layer: 98.1343765258789
Norm before input: 0.2552422881126404
Norm after input: 0.5187052488327026
Norm after each mp layer: 2.5209274291992188
Norm after each mp layer: 16.419946670532227
Norm after each mp layer: 98.1343765258789
Norm before input: 0.2552422881126404
Norm after input: 0.5173046588897705
Norm after each mp layer: 2.516118288040161
Norm after each mp layer: 16.375469207763672
Norm after each mp layer: 97.86431121826172
Epoch: 130, Loss: 0.0604, Energy: 1951116.2500, Train: 97.76%, Valid: 76.60%, Test: 77.40%, Best Valid: 78.60%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5173046588897705
Norm after each mp layer: 2.516118288040161
Norm after each mp layer: 16.375469207763672
Norm after each mp layer: 97.86431121826172
Norm before input: 0.2552422881126404
Norm after input: 0.517460286617279
Norm after each mp layer: 2.5171332359313965
Norm after each mp layer: 16.434986114501953
Norm after each mp layer: 99.00894927978516
Norm before input: 0.2552422881126404
Norm after input: 0.517460286617279
Norm after each mp layer: 2.5171332359313965
Norm after each mp layer: 16.434986114501953
Norm after each mp layer: 99.00894927978516
Norm before input: 0.2552422881126404
Norm after input: 0.5184932947158813
Norm after each mp layer: 2.5256378650665283
Norm after each mp layer: 16.553056716918945
Norm after each mp layer: 100.00664520263672
Norm before input: 0.2552422881126404
Norm after input: 0.5184932947158813
Norm after each mp layer: 2.5256378650665283
Norm after each mp layer: 16.553056716918945
Norm after each mp layer: 100.00664520263672
Norm before input: 0.2552422881126404
Norm after input: 0.5178316235542297
Norm after each mp layer: 2.5241005420684814
Norm after each mp layer: 16.5330810546875
Norm after each mp layer: 99.19092559814453
Norm before input: 0.2552422881126404
Norm after input: 0.5178316235542297
Norm after each mp layer: 2.5241005420684814
Norm after each mp layer: 16.5330810546875
Norm after each mp layer: 99.19092559814453
Norm before input: 0.2552422881126404
Norm after input: 0.5169469714164734
Norm after each mp layer: 2.523041009902954
Norm after each mp layer: 16.50752067565918
Norm after each mp layer: 98.2757339477539
Norm before input: 0.2552422881126404
Norm after input: 0.5169469714164734
Norm after each mp layer: 2.523041009902954
Norm after each mp layer: 16.50752067565918
Norm after each mp layer: 98.2757339477539
Norm before input: 0.2552422881126404
Norm after input: 0.5181204676628113
Norm after each mp layer: 2.5372507572174072
Norm after each mp layer: 16.643260955810547
Norm after each mp layer: 99.0448226928711
Epoch: 135, Loss: 0.0555, Energy: 1890872.8750, Train: 98.68%, Valid: 77.40%, Test: 77.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5181204676628113
Norm after each mp layer: 2.5372507572174072
Norm after each mp layer: 16.643260955810547
Norm after each mp layer: 99.0448226928711
Norm before input: 0.2552422881126404
Norm after input: 0.5193331837654114
Norm after each mp layer: 2.55045747756958
Norm after each mp layer: 16.790578842163086
Norm after each mp layer: 100.5103988647461
Norm before input: 0.2552422881126404
Norm after input: 0.5193331837654114
Norm after each mp layer: 2.55045747756958
Norm after each mp layer: 16.790578842163086
Norm after each mp layer: 100.5103988647461
Norm before input: 0.2552422881126404
Norm after input: 0.5190098881721497
Norm after each mp layer: 2.5517077445983887
Norm after each mp layer: 16.818483352661133
Norm after each mp layer: 101.13328552246094
Norm before input: 0.2552422881126404
Norm after input: 0.5190098881721497
Norm after each mp layer: 2.5517077445983887
Norm after each mp layer: 16.818483352661133
Norm after each mp layer: 101.13328552246094
Norm before input: 0.2552422881126404
Norm after input: 0.5183464884757996
Norm after each mp layer: 2.5512304306030273
Norm after each mp layer: 16.810083389282227
Norm after each mp layer: 100.96302795410156
Norm before input: 0.2552422881126404
Norm after input: 0.5183464884757996
Norm after each mp layer: 2.5512304306030273
Norm after each mp layer: 16.810083389282227
Norm after each mp layer: 100.96302795410156
Norm before input: 0.2552422881126404
Norm after input: 0.5193947553634644
Norm after each mp layer: 2.5645837783813477
Norm after each mp layer: 16.920833587646484
Norm after each mp layer: 100.9893569946289
Norm before input: 0.2552422881126404
Norm after input: 0.5193947553634644
Norm after each mp layer: 2.5645837783813477
Norm after each mp layer: 16.920833587646484
Norm after each mp layer: 100.9893569946289
Norm before input: 0.2552422881126404
Norm after input: 0.5201496481895447
Norm after each mp layer: 2.573235034942627
Norm after each mp layer: 17.010282516479492
Norm after each mp layer: 101.38697814941406
Epoch: 140, Loss: 0.0438, Energy: 2097723.2500, Train: 98.68%, Valid: 77.60%, Test: 77.30%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5201496481895447
Norm after each mp layer: 2.573235034942627
Norm after each mp layer: 17.010282516479492
Norm after each mp layer: 101.38697814941406
Norm before input: 0.2552422881126404
Norm after input: 0.5194545984268188
Norm after each mp layer: 2.5684397220611572
Norm after each mp layer: 16.990999221801758
Norm after each mp layer: 101.65553283691406
Norm before input: 0.2552422881126404
Norm after input: 0.5194545984268188
Norm after each mp layer: 2.5684397220611572
Norm after each mp layer: 16.990999221801758
Norm after each mp layer: 101.65553283691406
Norm before input: 0.2552422881126404
Norm after input: 0.5183520913124084
Norm after each mp layer: 2.560619831085205
Norm after each mp layer: 16.942440032958984
Norm after each mp layer: 101.77886199951172
Norm before input: 0.2552422881126404
Norm after input: 0.5183520913124084
Norm after each mp layer: 2.560619831085205
Norm after each mp layer: 16.942440032958984
Norm after each mp layer: 101.77886199951172
Norm before input: 0.2552422881126404
Norm after input: 0.5188420414924622
Norm after each mp layer: 2.5678012371063232
Norm after each mp layer: 17.021587371826172
Norm after each mp layer: 102.29181671142578
Norm before input: 0.2552422881126404
Norm after input: 0.5188420414924622
Norm after each mp layer: 2.5678012371063232
Norm after each mp layer: 17.021587371826172
Norm after each mp layer: 102.29181671142578
Norm before input: 0.2552422881126404
Norm after input: 0.5201159715652466
Norm after each mp layer: 2.5834548473358154
Norm after each mp layer: 17.166353225708008
Norm after each mp layer: 102.776123046875
Norm before input: 0.2552422881126404
Norm after input: 0.5201159715652466
Norm after each mp layer: 2.5834548473358154
Norm after each mp layer: 17.166353225708008
Norm after each mp layer: 102.776123046875
Norm before input: 0.2552422881126404
Norm after input: 0.5206848382949829
Norm after each mp layer: 2.593924045562744
Norm after each mp layer: 17.258771896362305
Norm after each mp layer: 102.8868408203125
Epoch: 145, Loss: 0.0381, Energy: 2200203.5000, Train: 99.25%, Valid: 77.40%, Test: 77.40%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5206848382949829
Norm after each mp layer: 2.593924045562744
Norm after each mp layer: 17.258771896362305
Norm after each mp layer: 102.8868408203125
Norm before input: 0.2552422881126404
Norm after input: 0.5205163359642029
Norm after each mp layer: 2.5969905853271484
Norm after each mp layer: 17.296419143676758
Norm after each mp layer: 103.09785461425781
Norm before input: 0.2552422881126404
Norm after input: 0.5205163359642029
Norm after each mp layer: 2.5969905853271484
Norm after each mp layer: 17.296419143676758
Norm after each mp layer: 103.09785461425781
Norm before input: 0.2552422881126404
Norm after input: 0.5205456614494324
Norm after each mp layer: 2.5996508598327637
Norm after each mp layer: 17.349395751953125
Norm after each mp layer: 103.8183364868164
Norm before input: 0.2552422881126404
Norm after input: 0.5205456614494324
Norm after each mp layer: 2.5996508598327637
Norm after each mp layer: 17.349395751953125
Norm after each mp layer: 103.8183364868164
Norm before input: 0.2552422881126404
Norm after input: 0.5211808085441589
Norm after each mp layer: 2.607205867767334
Norm after each mp layer: 17.445758819580078
Norm after each mp layer: 104.53418731689453
Norm before input: 0.2552422881126404
Norm after input: 0.5211808085441589
Norm after each mp layer: 2.607205867767334
Norm after each mp layer: 17.445758819580078
Norm after each mp layer: 104.53418731689453
Norm before input: 0.2552422881126404
Norm after input: 0.5214153528213501
Norm after each mp layer: 2.6124846935272217
Norm after each mp layer: 17.504884719848633
Norm after each mp layer: 104.56941223144531
Norm before input: 0.2552422881126404
Norm after input: 0.5214153528213501
Norm after each mp layer: 2.6124846935272217
Norm after each mp layer: 17.504884719848633
Norm after each mp layer: 104.56941223144531
Norm before input: 0.2552422881126404
Norm after input: 0.5212268829345703
Norm after each mp layer: 2.614708423614502
Norm after each mp layer: 17.52972412109375
Norm after each mp layer: 104.31014251708984
Epoch: 150, Loss: 0.0335, Energy: 2291429.0000, Train: 99.34%, Valid: 77.00%, Test: 77.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5212268829345703
Norm after each mp layer: 2.614708423614502
Norm after each mp layer: 17.52972412109375
Norm after each mp layer: 104.31014251708984
Norm before input: 0.2552422881126404
Norm after input: 0.5214844346046448
Norm after each mp layer: 2.6196389198303223
Norm after each mp layer: 17.59332275390625
Norm after each mp layer: 104.68502807617188
Norm before input: 0.2552422881126404
Norm after input: 0.5214844346046448
Norm after each mp layer: 2.6196389198303223
Norm after each mp layer: 17.59332275390625
Norm after each mp layer: 104.68502807617188
Norm before input: 0.2552422881126404
Norm after input: 0.5221897959709167
Norm after each mp layer: 2.62770676612854
Norm after each mp layer: 17.696880340576172
Norm after each mp layer: 105.63583374023438
Norm before input: 0.2552422881126404
Norm after input: 0.5221897959709167
Norm after each mp layer: 2.62770676612854
Norm after each mp layer: 17.696880340576172
Norm after each mp layer: 105.63583374023438
Norm before input: 0.2552422881126404
Norm after input: 0.5225434899330139
Norm after each mp layer: 2.6338729858398438
Norm after each mp layer: 17.773176193237305
Norm after each mp layer: 106.2563247680664
Norm before input: 0.2552422881126404
Norm after input: 0.5225434899330139
Norm after each mp layer: 2.6338729858398438
Norm after each mp layer: 17.773176193237305
Norm after each mp layer: 106.2563247680664
Norm before input: 0.2552422881126404
Norm after input: 0.5226192474365234
Norm after each mp layer: 2.6388916969299316
Norm after each mp layer: 17.8245906829834
Norm after each mp layer: 106.349609375
Norm before input: 0.2552422881126404
Norm after input: 0.5226192474365234
Norm after each mp layer: 2.6388916969299316
Norm after each mp layer: 17.8245906829834
Norm after each mp layer: 106.34960174560547
Norm before input: 0.2552422881126404
Norm after input: 0.5230649709701538
Norm after each mp layer: 2.646780490875244
Norm after each mp layer: 17.90491485595703
Norm after each mp layer: 106.53154754638672
Epoch: 155, Loss: 0.0298, Energy: 2386725.2500, Train: 99.34%, Valid: 76.80%, Test: 77.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5230649709701538
Norm after each mp layer: 2.646780490875244
Norm after each mp layer: 17.90491485595703
Norm after each mp layer: 106.53154754638672
Norm before input: 0.2552422881126404
Norm after input: 0.5236566662788391
Norm after each mp layer: 2.654404640197754
Norm after each mp layer: 17.997255325317383
Norm after each mp layer: 107.10616302490234
Norm before input: 0.2552422881126404
Norm after input: 0.5236566662788391
Norm after each mp layer: 2.654404640197754
Norm after each mp layer: 17.997255325317383
Norm after each mp layer: 107.10616302490234
Norm before input: 0.2552422881126404
Norm after input: 0.5237820744514465
Norm after each mp layer: 2.657381296157837
Norm after each mp layer: 18.050399780273438
Norm after each mp layer: 107.61341094970703
Norm before input: 0.2552422881126404
Norm after input: 0.5237820744514465
Norm after each mp layer: 2.657381296157837
Norm after each mp layer: 18.050399780273438
Norm after each mp layer: 107.61341094970703
Norm before input: 0.2552422881126404
Norm after input: 0.5235609412193298
Norm after each mp layer: 2.6576714515686035
Norm after each mp layer: 18.072065353393555
Norm after each mp layer: 107.79447174072266
Norm before input: 0.2552422881126404
Norm after input: 0.5235609412193298
Norm after each mp layer: 2.6576714515686035
Norm after each mp layer: 18.072065353393555
Norm after each mp layer: 107.79446411132812
Norm before input: 0.2552422881126404
Norm after input: 0.5238289833068848
Norm after each mp layer: 2.662621021270752
Norm after each mp layer: 18.132198333740234
Norm after each mp layer: 107.9534683227539
Norm before input: 0.2552422881126404
Norm after input: 0.5238289833068848
Norm after each mp layer: 2.662621021270752
Norm after each mp layer: 18.132198333740234
Norm after each mp layer: 107.9534683227539
Norm before input: 0.2552422881126404
Norm after input: 0.5246162414550781
Norm after each mp layer: 2.6722586154937744
Norm after each mp layer: 18.236431121826172
Norm after each mp layer: 108.31934356689453
Epoch: 160, Loss: 0.0268, Energy: 2450468.7500, Train: 99.25%, Valid: 76.80%, Test: 77.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5246162414550781
Norm after each mp layer: 2.6722586154937744
Norm after each mp layer: 18.236431121826172
Norm after each mp layer: 108.31934356689453
Norm before input: 0.2552422881126404
Norm after input: 0.5250791311264038
Norm after each mp layer: 2.6792521476745605
Norm after each mp layer: 18.316722869873047
Norm after each mp layer: 108.73484802246094
Norm before input: 0.2552422881126404
Norm after input: 0.5250791311264038
Norm after each mp layer: 2.6792521476745605
Norm after each mp layer: 18.316722869873047
Norm after each mp layer: 108.73484802246094
Norm before input: 0.2552422881126404
Norm after input: 0.5250651240348816
Norm after each mp layer: 2.68219256401062
Norm after each mp layer: 18.35911750793457
Norm after each mp layer: 109.12022399902344
Norm before input: 0.2552422881126404
Norm after input: 0.5250651240348816
Norm after each mp layer: 2.68219256401062
Norm after each mp layer: 18.35911750793457
Norm after each mp layer: 109.12022399902344
Norm before input: 0.2552422881126404
Norm after input: 0.525173008441925
Norm after each mp layer: 2.686056137084961
Norm after each mp layer: 18.411033630371094
Norm after each mp layer: 109.53579711914062
Norm before input: 0.2552422881126404
Norm after input: 0.525173008441925
Norm after each mp layer: 2.686056137084961
Norm after each mp layer: 18.411033630371094
Norm after each mp layer: 109.53579711914062
Norm before input: 0.2552422881126404
Norm after input: 0.5257039666175842
Norm after each mp layer: 2.6934423446655273
Norm after each mp layer: 18.495405197143555
Norm after each mp layer: 109.90839385986328
Norm before input: 0.2552422881126404
Norm after input: 0.5257039666175842
Norm after each mp layer: 2.6934423446655273
Norm after each mp layer: 18.495405197143555
Norm after each mp layer: 109.90839385986328
Norm before input: 0.2552422881126404
Norm after input: 0.526068925857544
Norm after each mp layer: 2.699218273162842
Norm after each mp layer: 18.563396453857422
Norm after each mp layer: 110.10701751708984
Epoch: 165, Loss: 0.0241, Energy: 2560164.7500, Train: 99.50%, Valid: 76.40%, Test: 76.90%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.526068925857544
Norm after each mp layer: 2.699218273162842
Norm after each mp layer: 18.563396453857422
Norm after each mp layer: 110.10701751708984
Norm before input: 0.2552422881126404
Norm after input: 0.5260732173919678
Norm after each mp layer: 2.7014880180358887
Norm after each mp layer: 18.600372314453125
Norm after each mp layer: 110.2451171875
Norm before input: 0.2552422881126404
Norm after input: 0.5260732173919678
Norm after each mp layer: 2.7014880180358887
Norm after each mp layer: 18.600372314453125
Norm after each mp layer: 110.2451171875
Norm before input: 0.2552422881126404
Norm after input: 0.526119589805603
Norm after each mp layer: 2.703651189804077
Norm after each mp layer: 18.641599655151367
Norm after each mp layer: 110.54405975341797
Norm before input: 0.2552422881126404
Norm after input: 0.526119589805603
Norm after each mp layer: 2.703651189804077
Norm after each mp layer: 18.641599655151367
Norm after each mp layer: 110.54405975341797
Norm before input: 0.2552422881126404
Norm after input: 0.5264889597892761
Norm after each mp layer: 2.708761215209961
Norm after each mp layer: 18.710037231445312
Norm after each mp layer: 110.9070816040039
Norm before input: 0.2552422881126404
Norm after input: 0.5264889597892761
Norm after each mp layer: 2.708761215209961
Norm after each mp layer: 18.710037231445312
Norm after each mp layer: 110.9070816040039
Norm before input: 0.2552422881126404
Norm after input: 0.5268967747688293
Norm after each mp layer: 2.7148616313934326
Norm after each mp layer: 18.780689239501953
Norm after each mp layer: 111.0992431640625
Norm before input: 0.2552422881126404
Norm after input: 0.5268967747688293
Norm after each mp layer: 2.7148616313934326
Norm after each mp layer: 18.780689239501953
Norm after each mp layer: 111.0992431640625
Norm before input: 0.2552422881126404
Norm after input: 0.52713942527771
Norm after each mp layer: 2.719749927520752
Norm after each mp layer: 18.837064743041992
Norm after each mp layer: 111.2144546508789
Epoch: 170, Loss: 0.0219, Energy: 2597024.2500, Train: 99.59%, Valid: 76.60%, Test: 76.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.52713942527771
Norm after each mp layer: 2.719749927520752
Norm after each mp layer: 18.837064743041992
Norm after each mp layer: 111.2144546508789
Norm before input: 0.2552422881126404
Norm after input: 0.5273044109344482
Norm after each mp layer: 2.7234065532684326
Norm after each mp layer: 18.887348175048828
Norm after each mp layer: 111.52457427978516
Norm before input: 0.2552422881126404
Norm after input: 0.5273044109344482
Norm after each mp layer: 2.7234065532684326
Norm after each mp layer: 18.887348175048828
Norm after each mp layer: 111.52457427978516
Norm before input: 0.2552422881126404
Norm after input: 0.5274624824523926
Norm after each mp layer: 2.7266018390655518
Norm after each mp layer: 18.935983657836914
Norm after each mp layer: 111.95323944091797
Norm before input: 0.2552422881126404
Norm after input: 0.5274624824523926
Norm after each mp layer: 2.7266018390655518
Norm after each mp layer: 18.935983657836914
Norm after each mp layer: 111.95323944091797
Norm before input: 0.2552422881126404
Norm after input: 0.5276843905448914
Norm after each mp layer: 2.7304434776306152
Norm after each mp layer: 18.987960815429688
Norm after each mp layer: 112.29264068603516
Norm before input: 0.2552422881126404
Norm after input: 0.5276843905448914
Norm after each mp layer: 2.7304434776306152
Norm after each mp layer: 18.987958908081055
Norm after each mp layer: 112.29264068603516
Norm before input: 0.2552422881126404
Norm after input: 0.5280036330223083
Norm after each mp layer: 2.735262870788574
Norm after each mp layer: 19.047622680664062
Norm after each mp layer: 112.52429962158203
Norm before input: 0.2552422881126404
Norm after input: 0.5280036330223083
Norm after each mp layer: 2.735262870788574
Norm after each mp layer: 19.047622680664062
Norm after each mp layer: 112.52429962158203
Norm before input: 0.2552422881126404
Norm after input: 0.5282588005065918
Norm after each mp layer: 2.7395923137664795
Norm after each mp layer: 19.10231590270996
Norm after each mp layer: 112.68934631347656
Epoch: 175, Loss: 0.0201, Energy: 2648332.5000, Train: 99.59%, Valid: 76.80%, Test: 76.40%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5282588005065918
Norm after each mp layer: 2.7395923137664795
Norm after each mp layer: 19.10231590270996
Norm after each mp layer: 112.68934631347656
Norm before input: 0.2552422881126404
Norm after input: 0.5283738374710083
Norm after each mp layer: 2.742751359939575
Norm after each mp layer: 19.145177841186523
Norm after each mp layer: 112.83968353271484
Norm before input: 0.2552422881126404
Norm after input: 0.5283738374710083
Norm after each mp layer: 2.742751359939575
Norm after each mp layer: 19.145177841186523
Norm after each mp layer: 112.83968353271484
Norm before input: 0.2552422881126404
Norm after input: 0.5285748243331909
Norm after each mp layer: 2.746485471725464
Norm after each mp layer: 19.195491790771484
Norm after each mp layer: 113.091552734375
Norm before input: 0.2552422881126404
Norm after input: 0.5285748243331909
Norm after each mp layer: 2.746485471725464
Norm after each mp layer: 19.195491790771484
Norm after each mp layer: 113.091552734375
Norm before input: 0.2552422881126404
Norm after input: 0.5289190411567688
Norm after each mp layer: 2.751309633255005
Norm after each mp layer: 19.257617950439453
Norm after each mp layer: 113.39295196533203
Norm before input: 0.2552422881126404
Norm after input: 0.5289190411567688
Norm after each mp layer: 2.751309633255005
Norm after each mp layer: 19.257617950439453
Norm after each mp layer: 113.39295196533203
Norm before input: 0.2552422881126404
Norm after input: 0.5291334390640259
Norm after each mp layer: 2.7551684379577637
Norm after each mp layer: 19.306921005249023
Norm after each mp layer: 113.56452941894531
Norm before input: 0.2552422881126404
Norm after input: 0.5291334390640259
Norm after each mp layer: 2.7551684379577637
Norm after each mp layer: 19.306921005249023
Norm after each mp layer: 113.56452941894531
Norm before input: 0.2552422881126404
Norm after input: 0.5292216539382935
Norm after each mp layer: 2.757897138595581
Norm after each mp layer: 19.344724655151367
Norm after each mp layer: 113.7170639038086
Epoch: 180, Loss: 0.0186, Energy: 2674344.2500, Train: 99.67%, Valid: 76.80%, Test: 76.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5292216539382935
Norm after each mp layer: 2.757897138595581
Norm after each mp layer: 19.344724655151367
Norm after each mp layer: 113.7170639038086
Norm before input: 0.2552422881126404
Norm after input: 0.5294087529182434
Norm after each mp layer: 2.7611560821533203
Norm after each mp layer: 19.39195442199707
Norm after each mp layer: 114.0295181274414
Norm before input: 0.2552422881126404
Norm after input: 0.5294087529182434
Norm after each mp layer: 2.7611560821533203
Norm after each mp layer: 19.39195442199707
Norm after each mp layer: 114.0295181274414
Norm before input: 0.2552422881126404
Norm after input: 0.5296469330787659
Norm after each mp layer: 2.7649178504943848
Norm after each mp layer: 19.443557739257812
Norm after each mp layer: 114.34629821777344
Norm before input: 0.2552422881126404
Norm after input: 0.5296469330787659
Norm after each mp layer: 2.7649178504943848
Norm after each mp layer: 19.443557739257812
Norm after each mp layer: 114.34629821777344
Norm before input: 0.2552422881126404
Norm after input: 0.5298592448234558
Norm after each mp layer: 2.7686891555786133
Norm after each mp layer: 19.492116928100586
Norm after each mp layer: 114.53163146972656
Norm before input: 0.2552422881126404
Norm after input: 0.5298592448234558
Norm after each mp layer: 2.7686891555786133
Norm after each mp layer: 19.492116928100586
Norm after each mp layer: 114.53163146972656
Norm before input: 0.2552422881126404
Norm after input: 0.5300694704055786
Norm after each mp layer: 2.772418737411499
Norm after each mp layer: 19.539997100830078
Norm after each mp layer: 114.6514663696289
Norm before input: 0.2552422881126404
Norm after input: 0.5300694704055786
Norm after each mp layer: 2.772418737411499
Norm after each mp layer: 19.539997100830078
Norm after each mp layer: 114.6514663696289
Norm before input: 0.2552422881126404
Norm after input: 0.5302324891090393
Norm after each mp layer: 2.7756245136260986
Norm after each mp layer: 19.58283233642578
Norm after each mp layer: 114.77995300292969
Epoch: 185, Loss: 0.0172, Energy: 2699935.5000, Train: 99.67%, Valid: 76.80%, Test: 75.80%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5302324891090393
Norm after each mp layer: 2.7756245136260986
Norm after each mp layer: 19.58283233642578
Norm after each mp layer: 114.77995300292969
Norm before input: 0.2552422881126404
Norm after input: 0.5304405093193054
Norm after each mp layer: 2.7789297103881836
Norm after each mp layer: 19.62934684753418
Norm after each mp layer: 115.03118896484375
Norm before input: 0.2552422881126404
Norm after input: 0.5304405093193054
Norm after each mp layer: 2.7789297103881836
Norm after each mp layer: 19.62934684753418
Norm after each mp layer: 115.03118896484375
Norm before input: 0.2552422881126404
Norm after input: 0.5306841135025024
Norm after each mp layer: 2.7824392318725586
Norm after each mp layer: 19.679092407226562
Norm after each mp layer: 115.3293228149414
Norm before input: 0.2552422881126404
Norm after input: 0.5306841135025024
Norm after each mp layer: 2.7824392318725586
Norm after each mp layer: 19.679092407226562
Norm after each mp layer: 115.3293228149414
Norm before input: 0.2552422881126404
Norm after input: 0.5308059453964233
Norm after each mp layer: 2.785231590270996
Norm after each mp layer: 19.71735382080078
Norm after each mp layer: 115.48724365234375
Norm before input: 0.2552422881126404
Norm after input: 0.5308059453964233
Norm after each mp layer: 2.785231590270996
Norm after each mp layer: 19.71735382080078
Norm after each mp layer: 115.48724365234375
Norm before input: 0.2552422881126404
Norm after input: 0.5309486985206604
Norm after each mp layer: 2.7882654666900635
Norm after each mp layer: 19.75763511657715
Norm after each mp layer: 115.62601470947266
Norm before input: 0.2552422881126404
Norm after input: 0.5309486985206604
Norm after each mp layer: 2.7882654666900635
Norm after each mp layer: 19.75763511657715
Norm after each mp layer: 115.62601470947266
Norm before input: 0.2552422881126404
Norm after input: 0.5311607718467712
Norm after each mp layer: 2.791782855987549
Norm after each mp layer: 19.80413246154785
Norm after each mp layer: 115.82955169677734
Epoch: 190, Loss: 0.0160, Energy: 2713964.5000, Train: 99.67%, Valid: 76.60%, Test: 75.90%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5311607718467712
Norm after each mp layer: 2.791782855987549
Norm after each mp layer: 19.80413246154785
Norm after each mp layer: 115.82955169677734
Norm before input: 0.2552422881126404
Norm after input: 0.5313859581947327
Norm after each mp layer: 2.7953145503997803
Norm after each mp layer: 19.851314544677734
Norm after each mp layer: 116.0599136352539
Norm before input: 0.2552422881126404
Norm after input: 0.5313859581947327
Norm after each mp layer: 2.7953145503997803
Norm after each mp layer: 19.851314544677734
Norm after each mp layer: 116.0599136352539
Norm before input: 0.2552422881126404
Norm after input: 0.531624972820282
Norm after each mp layer: 2.7988572120666504
Norm after each mp layer: 19.899484634399414
Norm after each mp layer: 116.27517700195312
Norm before input: 0.2552422881126404
Norm after input: 0.531624972820282
Norm after each mp layer: 2.7988572120666504
Norm after each mp layer: 19.899484634399414
Norm after each mp layer: 116.27517700195312
Norm before input: 0.2552422881126404
Norm after input: 0.5317609310150146
Norm after each mp layer: 2.8017001152038574
Norm after each mp layer: 19.93746566772461
Norm after each mp layer: 116.37577056884766
Norm before input: 0.2552422881126404
Norm after input: 0.5317609310150146
Norm after each mp layer: 2.8017001152038574
Norm after each mp layer: 19.93746566772461
Norm after each mp layer: 116.37577056884766
Norm before input: 0.2552422881126404
Norm after input: 0.5319436192512512
Norm after each mp layer: 2.804828405380249
Norm after each mp layer: 19.979923248291016
Norm after each mp layer: 116.5269775390625
Norm before input: 0.2552422881126404
Norm after input: 0.5319436192512512
Norm after each mp layer: 2.804828405380249
Norm after each mp layer: 19.979923248291016
Norm after each mp layer: 116.5269775390625
Norm before input: 0.2552422881126404
Norm after input: 0.5321061611175537
Norm after each mp layer: 2.807746648788452
Norm after each mp layer: 20.020526885986328
Norm after each mp layer: 116.72533416748047
Epoch: 195, Loss: 0.0150, Energy: 2724090.0000, Train: 99.67%, Valid: 76.60%, Test: 76.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5321061611175537
Norm after each mp layer: 2.807746648788452
Norm after each mp layer: 20.020526885986328
Norm after each mp layer: 116.72533416748047
Norm before input: 0.2552422881126404
Norm after input: 0.5322664976119995
Norm after each mp layer: 2.81061053276062
Norm after each mp layer: 20.0609188079834
Norm after each mp layer: 116.94387817382812
Norm before input: 0.2552422881126404
Norm after input: 0.5322664976119995
Norm after each mp layer: 2.81061053276062
Norm after each mp layer: 20.0609188079834
Norm after each mp layer: 116.9438705444336
Norm before input: 0.2552422881126404
Norm after input: 0.5324228405952454
Norm after each mp layer: 2.8135428428649902
Norm after each mp layer: 20.100444793701172
Norm after each mp layer: 117.1065444946289
Norm before input: 0.2552422881126404
Norm after input: 0.5324228405952454
Norm after each mp layer: 2.8135428428649902
Norm after each mp layer: 20.100444793701172
Norm after each mp layer: 117.1065444946289
Norm before input: 0.2552422881126404
Norm after input: 0.532599687576294
Norm after each mp layer: 2.816725969314575
Norm after each mp layer: 20.141183853149414
Norm after each mp layer: 117.2309799194336
Norm before input: 0.2552422881126404
Norm after input: 0.532599687576294
Norm after each mp layer: 2.816725969314575
Norm after each mp layer: 20.141183853149414
Norm after each mp layer: 117.2309799194336
Norm before input: 0.2552422881126404
Norm after input: 0.5328367352485657
Norm after each mp layer: 2.8202431201934814
Norm after each mp layer: 20.187362670898438
Norm after each mp layer: 117.42061614990234
Norm before input: 0.2552422881126404
Norm after input: 0.5328367352485657
Norm after each mp layer: 2.8202431201934814
Norm after each mp layer: 20.187362670898438
Norm after each mp layer: 117.42061614990234
Norm before input: 0.2552422881126404
Norm after input: 0.5329926013946533
Norm after each mp layer: 2.8230438232421875
Norm after each mp layer: 20.225662231445312
Norm after each mp layer: 117.60639190673828
Epoch: 200, Loss: 0.0141, Energy: 2728985.7500, Train: 99.67%, Valid: 76.40%, Test: 76.20%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5329926013946533
Norm after each mp layer: 2.8230438232421875
Norm after each mp layer: 20.225662231445312
Norm after each mp layer: 117.60639190673828
Norm before input: 0.2552422881126404
Norm after input: 0.5331847667694092
Norm after each mp layer: 2.825984239578247
Norm after each mp layer: 20.267515182495117
Norm after each mp layer: 117.81822967529297
Norm before input: 0.2552422881126404
Norm after input: 0.5331847667694092
Norm after each mp layer: 2.825984239578247
Norm after each mp layer: 20.267515182495117
Norm after each mp layer: 117.81822967529297
Norm before input: 0.2552422881126404
Norm after input: 0.5332411527633667
Norm after each mp layer: 2.8280844688415527
Norm after each mp layer: 20.29566764831543
Norm after each mp layer: 117.88970947265625
Norm before input: 0.2552422881126404
Norm after input: 0.5332411527633667
Norm after each mp layer: 2.8280844688415527
Norm after each mp layer: 20.29566764831543
Norm after each mp layer: 117.88970947265625
Norm before input: 0.2552422881126404
Norm after input: 0.5334929823875427
Norm after each mp layer: 2.8314156532287598
Norm after each mp layer: 20.342247009277344
Norm after each mp layer: 118.09530639648438
Norm before input: 0.2552422881126404
Norm after input: 0.5334929823875427
Norm after each mp layer: 2.8314156532287598
Norm after each mp layer: 20.342247009277344
Norm after each mp layer: 118.09530639648438
Norm before input: 0.2552422881126404
Norm after input: 0.5335445404052734
Norm after each mp layer: 2.8333380222320557
Norm after each mp layer: 20.369129180908203
Norm after each mp layer: 118.19758605957031
Norm before input: 0.2552422881126404
Norm after input: 0.5335445404052734
Norm after each mp layer: 2.8333380222320557
Norm after each mp layer: 20.369129180908203
Norm after each mp layer: 118.19758605957031
Norm before input: 0.2552422881126404
Norm after input: 0.5337596535682678
Norm after each mp layer: 2.8363053798675537
Norm after each mp layer: 20.411413192749023
Norm after each mp layer: 118.41626739501953
Epoch: 205, Loss: 0.0134, Energy: 2722364.5000, Train: 99.67%, Valid: 75.80%, Test: 76.20%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5337596535682678
Norm after each mp layer: 2.8363053798675537
Norm after each mp layer: 20.411413192749023
Norm after each mp layer: 118.41626739501953
Norm before input: 0.2552422881126404
Norm after input: 0.533943772315979
Norm after each mp layer: 2.8392112255096436
Norm after each mp layer: 20.450496673583984
Norm after each mp layer: 118.56367492675781
Norm before input: 0.2552422881126404
Norm after input: 0.533943772315979
Norm after each mp layer: 2.8392112255096436
Norm after each mp layer: 20.450496673583984
Norm after each mp layer: 118.56367492675781
Norm before input: 0.2552422881126404
Norm after input: 0.5340510010719299
Norm after each mp layer: 2.8416810035705566
Norm after each mp layer: 20.482006072998047
Norm after each mp layer: 118.63365173339844
Norm before input: 0.2552422881126404
Norm after input: 0.5340510010719299
Norm after each mp layer: 2.8416810035705566
Norm after each mp layer: 20.482006072998047
Norm after each mp layer: 118.63365173339844
Norm before input: 0.2552422881126404
Norm after input: 0.5342919230461121
Norm after each mp layer: 2.844872236251831
Norm after each mp layer: 20.526260375976562
Norm after each mp layer: 118.83445739746094
Norm before input: 0.2552422881126404
Norm after input: 0.5342919230461121
Norm after each mp layer: 2.844872236251831
Norm after each mp layer: 20.526260375976562
Norm after each mp layer: 118.83445739746094
Norm before input: 0.2552422881126404
Norm after input: 0.5343443155288696
Norm after each mp layer: 2.846722364425659
Norm after each mp layer: 20.55204963684082
Norm after each mp layer: 118.94437408447266
Norm before input: 0.2552422881126404
Norm after input: 0.5343443155288696
Norm after each mp layer: 2.846722364425659
Norm after each mp layer: 20.55204963684082
Norm after each mp layer: 118.94437408447266
Norm before input: 0.2552422881126404
Norm after input: 0.5345245003700256
Norm after each mp layer: 2.84938383102417
Norm after each mp layer: 20.589664459228516
Norm after each mp layer: 119.12606811523438
Epoch: 210, Loss: 0.0126, Energy: 2718498.5000, Train: 99.75%, Valid: 75.80%, Test: 76.40%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5345245003700256
Norm after each mp layer: 2.84938383102417
Norm after each mp layer: 20.589664459228516
Norm after each mp layer: 119.12606811523438
Norm before input: 0.2552422881126404
Norm after input: 0.534683108329773
Norm after each mp layer: 2.851938247680664
Norm after each mp layer: 20.624906539916992
Norm after each mp layer: 119.26509094238281
Norm before input: 0.2552422881126404
Norm after input: 0.534683108329773
Norm after each mp layer: 2.851938486099243
Norm after each mp layer: 20.624906539916992
Norm after each mp layer: 119.26509094238281
Norm before input: 0.2552422881126404
Norm after input: 0.5347503423690796
Norm after each mp layer: 2.853893756866455
Norm after each mp layer: 20.651214599609375
Norm after each mp layer: 119.34363555908203
Norm before input: 0.2552422881126404
Norm after input: 0.5347503423690796
Norm after each mp layer: 2.853893756866455
Norm after each mp layer: 20.651214599609375
Norm after each mp layer: 119.34363555908203
Norm before input: 0.2552422881126404
Norm after input: 0.5349759459495544
Norm after each mp layer: 2.8568389415740967
Norm after each mp layer: 20.692562103271484
Norm after each mp layer: 119.5387954711914
Norm before input: 0.2552422881126404
Norm after input: 0.5349759459495544
Norm after each mp layer: 2.8568389415740967
Norm after each mp layer: 20.692562103271484
Norm after each mp layer: 119.53880310058594
Norm before input: 0.2552422881126404
Norm after input: 0.5350619554519653
Norm after each mp layer: 2.858914852142334
Norm after each mp layer: 20.720245361328125
Norm after each mp layer: 119.63986206054688
Norm before input: 0.2552422881126404
Norm after input: 0.5350619554519653
Norm after each mp layer: 2.858914852142334
Norm after each mp layer: 20.720245361328125
Norm after each mp layer: 119.63986206054688
Norm before input: 0.2552422881126404
Norm after input: 0.5352365970611572
Norm after each mp layer: 2.8615386486053467
Norm after each mp layer: 20.756181716918945
Norm after each mp layer: 119.78677368164062
Epoch: 215, Loss: 0.0119, Energy: 2711124.2500, Train: 99.75%, Valid: 75.60%, Test: 76.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5352365970611572
Norm after each mp layer: 2.8615386486053467
Norm after each mp layer: 20.756181716918945
Norm after each mp layer: 119.78677368164062
Norm before input: 0.2552422881126404
Norm after input: 0.535384476184845
Norm after each mp layer: 2.863924264907837
Norm after each mp layer: 20.789281845092773
Norm after each mp layer: 119.91404724121094
Norm before input: 0.2552422881126404
Norm after input: 0.535384476184845
Norm after each mp layer: 2.863924264907837
Norm after each mp layer: 20.789281845092773
Norm after each mp layer: 119.91404724121094
Norm before input: 0.2552422881126404
Norm after input: 0.5354536771774292
Norm after each mp layer: 2.865750312805176
Norm after each mp layer: 20.814489364624023
Norm after each mp layer: 119.9960708618164
Norm before input: 0.2552422881126404
Norm after input: 0.5354536771774292
Norm after each mp layer: 2.865750312805176
Norm after each mp layer: 20.814489364624023
Norm after each mp layer: 119.9960708618164
Norm before input: 0.2552422881126404
Norm after input: 0.5356586575508118
Norm after each mp layer: 2.868391275405884
Norm after each mp layer: 20.852481842041016
Norm after each mp layer: 120.16998291015625
Norm before input: 0.2552422881126404
Norm after input: 0.5356586575508118
Norm after each mp layer: 2.868391275405884
Norm after each mp layer: 20.852481842041016
Norm after each mp layer: 120.16998291015625
Norm before input: 0.2552422881126404
Norm after input: 0.5357189774513245
Norm after each mp layer: 2.870084524154663
Norm after each mp layer: 20.876361846923828
Norm after each mp layer: 120.25297546386719
Norm before input: 0.2552422881126404
Norm after input: 0.5357189774513245
Norm after each mp layer: 2.870084285736084
Norm after each mp layer: 20.876361846923828
Norm after each mp layer: 120.25297546386719
Norm before input: 0.2552422881126404
Norm after input: 0.5358718633651733
Norm after each mp layer: 2.872373580932617
Norm after each mp layer: 20.90899658203125
Norm after each mp layer: 120.37862396240234
Epoch: 220, Loss: 0.0113, Energy: 2697708.5000, Train: 99.75%, Valid: 75.80%, Test: 76.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5358718633651733
Norm after each mp layer: 2.872373580932617
Norm after each mp layer: 20.90899658203125
Norm after each mp layer: 120.37862396240234
Norm before input: 0.2552422881126404
Norm after input: 0.5359994769096375
Norm after each mp layer: 2.874532699584961
Norm after each mp layer: 20.93896484375
Norm after each mp layer: 120.47518920898438
Norm before input: 0.2552422881126404
Norm after input: 0.5359994769096375
Norm after each mp layer: 2.874532699584961
Norm after each mp layer: 20.93896484375
Norm after each mp layer: 120.47518920898438
Norm before input: 0.2552422881126404
Norm after input: 0.5360971689224243
Norm after each mp layer: 2.8764827251434326
Norm after each mp layer: 20.96578598022461
Norm after each mp layer: 120.5616226196289
Norm before input: 0.2552422881126404
Norm after input: 0.5360971689224243
Norm after each mp layer: 2.8764827251434326
Norm after each mp layer: 20.96578598022461
Norm after each mp layer: 120.5616226196289
Norm before input: 0.2552422881126404
Norm after input: 0.5362826585769653
Norm after each mp layer: 2.878925323486328
Norm after each mp layer: 21.00080108642578
Norm after each mp layer: 120.71858978271484
Norm before input: 0.2552422881126404
Norm after input: 0.5362826585769653
Norm after each mp layer: 2.878925323486328
Norm after each mp layer: 21.00080108642578
Norm after each mp layer: 120.71858978271484
Norm before input: 0.2552422881126404
Norm after input: 0.536342442035675
Norm after each mp layer: 2.8805646896362305
Norm after each mp layer: 21.02349281311035
Norm after each mp layer: 120.79638671875
Norm before input: 0.2552422881126404
Norm after input: 0.536342442035675
Norm after each mp layer: 2.8805646896362305
Norm after each mp layer: 21.02349281311035
Norm after each mp layer: 120.79638671875
Norm before input: 0.2552422881126404
Norm after input: 0.5365185737609863
Norm after each mp layer: 2.8829619884490967
Norm after each mp layer: 21.05724334716797
Norm after each mp layer: 120.92755889892578
Epoch: 225, Loss: 0.0107, Energy: 2680000.5000, Train: 99.83%, Valid: 75.80%, Test: 76.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5365185737609863
Norm after each mp layer: 2.8829619884490967
Norm after each mp layer: 21.05724334716797
Norm after each mp layer: 120.92755126953125
Norm before input: 0.2552422881126404
Norm after input: 0.5366025567054749
Norm after each mp layer: 2.884748697280884
Norm after each mp layer: 21.08197021484375
Norm after each mp layer: 121.00615692138672
Norm before input: 0.2552422881126404
Norm after input: 0.5366025567054749
Norm after each mp layer: 2.884748697280884
Norm after each mp layer: 21.08197021484375
Norm after each mp layer: 121.00615692138672
Norm before input: 0.2552422881126404
Norm after input: 0.5367217063903809
Norm after each mp layer: 2.8867123126983643
Norm after each mp layer: 21.109926223754883
Norm after each mp layer: 121.1148681640625
Norm before input: 0.2552422881126404
Norm after input: 0.5367217063903809
Norm after each mp layer: 2.8867123126983643
Norm after each mp layer: 21.109926223754883
Norm after each mp layer: 121.1148681640625
Norm before input: 0.2552422881126404
Norm after input: 0.5368602275848389
Norm after each mp layer: 2.888794183731079
Norm after each mp layer: 21.139543533325195
Norm after each mp layer: 121.23155975341797
Norm before input: 0.2552422881126404
Norm after input: 0.5368602275848389
Norm after each mp layer: 2.888794183731079
Norm after each mp layer: 21.139543533325195
Norm after each mp layer: 121.23155975341797
Norm before input: 0.2552422881126404
Norm after input: 0.5369420647621155
Norm after each mp layer: 2.89052677154541
Norm after each mp layer: 21.163496017456055
Norm after each mp layer: 121.3062973022461
Norm before input: 0.2552422881126404
Norm after input: 0.5369420647621155
Norm after each mp layer: 2.89052677154541
Norm after each mp layer: 21.163496017456055
Norm after each mp layer: 121.3062973022461
Norm before input: 0.2552422881126404
Norm after input: 0.5371081233024597
Norm after each mp layer: 2.8927786350250244
Norm after each mp layer: 21.1954288482666
Norm after each mp layer: 121.42670440673828
Epoch: 230, Loss: 0.0102, Energy: 2660876.0000, Train: 99.83%, Valid: 75.60%, Test: 76.30%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5371081233024597
Norm after each mp layer: 2.8927786350250244
Norm after each mp layer: 21.1954288482666
Norm after each mp layer: 121.42670440673828
Norm before input: 0.2552422881126404
Norm after input: 0.5371678471565247
Norm after each mp layer: 2.894352674484253
Norm after each mp layer: 21.216922760009766
Norm after each mp layer: 121.48947143554688
Norm before input: 0.2552422881126404
Norm after input: 0.5371678471565247
Norm after each mp layer: 2.894352674484253
Norm after each mp layer: 21.216922760009766
Norm after each mp layer: 121.48947143554688
Norm before input: 0.2552422881126404
Norm after input: 0.5373383164405823
Norm after each mp layer: 2.8966176509857178
Norm after each mp layer: 21.248943328857422
Norm after each mp layer: 121.61719512939453
Norm before input: 0.2552422881126404
Norm after input: 0.5373383164405823
Norm after each mp layer: 2.8966176509857178
Norm after each mp layer: 21.248943328857422
Norm after each mp layer: 121.61719512939453
Norm before input: 0.2552422881126404
Norm after input: 0.5374126434326172
Norm after each mp layer: 2.8982460498809814
Norm after each mp layer: 21.271512985229492
Norm after each mp layer: 121.6944580078125
Norm before input: 0.2552422881126404
Norm after input: 0.5374126434326172
Norm after each mp layer: 2.8982460498809814
Norm after each mp layer: 21.271512985229492
Norm after each mp layer: 121.6944580078125
Norm before input: 0.2552422881126404
Norm after input: 0.5375466346740723
Norm after each mp layer: 2.900240659713745
Norm after each mp layer: 21.299684524536133
Norm after each mp layer: 121.79671478271484
Norm before input: 0.2552422881126404
Norm after input: 0.5375466346740723
Norm after each mp layer: 2.900240659713745
Norm after each mp layer: 21.299684524536133
Norm after each mp layer: 121.79671478271484
Norm before input: 0.2552422881126404
Norm after input: 0.5376513004302979
Norm after each mp layer: 2.9020473957061768
Norm after each mp layer: 21.3248233795166
Norm after each mp layer: 121.87516784667969
Epoch: 235, Loss: 0.0097, Energy: 2642081.5000, Train: 99.92%, Valid: 75.60%, Test: 76.20%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5376513004302979
Norm after each mp layer: 2.9020473957061768
Norm after each mp layer: 21.3248233795166
Norm after each mp layer: 121.87516784667969
Norm before input: 0.2552422881126404
Norm after input: 0.5377547144889832
Norm after each mp layer: 2.9038119316101074
Norm after each mp layer: 21.34964942932129
Norm after each mp layer: 121.95819091796875
Norm before input: 0.2552422881126404
Norm after input: 0.5377547144889832
Norm after each mp layer: 2.9038119316101074
Norm after each mp layer: 21.34964942932129
Norm after each mp layer: 121.95819091796875
Norm before input: 0.2552422881126404
Norm after input: 0.5378835201263428
Norm after each mp layer: 2.90570330619812
Norm after each mp layer: 21.376768112182617
Norm after each mp layer: 122.0619125366211
Norm before input: 0.2552422881126404
Norm after input: 0.5378835201263428
Norm after each mp layer: 2.90570330619812
Norm after each mp layer: 21.376768112182617
Norm after each mp layer: 122.0619125366211
Norm before input: 0.2552422881126404
Norm after input: 0.5379602313041687
Norm after each mp layer: 2.907275915145874
Norm after each mp layer: 21.398714065551758
Norm after each mp layer: 122.13337707519531
Norm before input: 0.2552422881126404
Norm after input: 0.5379602313041687
Norm after each mp layer: 2.907275915145874
Norm after each mp layer: 21.398714065551758
Norm after each mp layer: 122.13337707519531
Norm before input: 0.2552422881126404
Norm after input: 0.5381098985671997
Norm after each mp layer: 2.9093196392059326
Norm after each mp layer: 21.42761993408203
Norm after each mp layer: 122.23674774169922
Norm before input: 0.2552422881126404
Norm after input: 0.5381098985671997
Norm after each mp layer: 2.9093196392059326
Norm after each mp layer: 21.42761993408203
Norm after each mp layer: 122.23674774169922
Norm before input: 0.2552422881126404
Norm after input: 0.5381662845611572
Norm after each mp layer: 2.910754442214966
Norm after each mp layer: 21.44732093811035
Norm after each mp layer: 122.29359436035156
Epoch: 240, Loss: 0.0093, Energy: 2620512.0000, Train: 99.83%, Valid: 75.60%, Test: 76.20%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5381662845611572
Norm after each mp layer: 2.910754442214966
Norm after each mp layer: 21.44732093811035
Norm after each mp layer: 122.29359436035156
Norm before input: 0.2552422881126404
Norm after input: 0.5383278727531433
Norm after each mp layer: 2.912843704223633
Norm after each mp layer: 21.47702407836914
Norm after each mp layer: 122.40478515625
Norm before input: 0.2552422881126404
Norm after input: 0.5383278727531433
Norm after each mp layer: 2.912843704223633
Norm after each mp layer: 21.47702407836914
Norm after each mp layer: 122.40478515625
Norm before input: 0.2552422881126404
Norm after input: 0.5383843183517456
Norm after each mp layer: 2.914249897003174
Norm after each mp layer: 21.496337890625
Norm after each mp layer: 122.46272277832031
Norm before input: 0.2552422881126404
Norm after input: 0.5383843183517456
Norm after each mp layer: 2.914249897003174
Norm after each mp layer: 21.496337890625
Norm after each mp layer: 122.46272277832031
Norm before input: 0.2552422881126404
Norm after input: 0.538541316986084
Norm after each mp layer: 2.91628098487854
Norm after each mp layer: 21.5252628326416
Norm after each mp layer: 122.56790161132812
Norm before input: 0.2552422881126404
Norm after input: 0.538541316986084
Norm after each mp layer: 2.91628098487854
Norm after each mp layer: 21.5252628326416
Norm after each mp layer: 122.56790161132812
Norm before input: 0.2552422881126404
Norm after input: 0.5385990738868713
Norm after each mp layer: 2.9176697731018066
Norm after each mp layer: 21.544431686401367
Norm after each mp layer: 122.62144470214844
Norm before input: 0.2552422881126404
Norm after input: 0.5385990738868713
Norm after each mp layer: 2.9176697731018066
Norm after each mp layer: 21.544431686401367
Norm after each mp layer: 122.62144470214844
Norm before input: 0.2552422881126404
Norm after input: 0.538737952709198
Norm after each mp layer: 2.919569253921509
Norm after each mp layer: 21.571321487426758
Norm after each mp layer: 122.71253967285156
Epoch: 245, Loss: 0.0089, Energy: 2595635.7500, Train: 99.92%, Valid: 75.40%, Test: 76.30%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.538737952709198
Norm after each mp layer: 2.919569253921509
Norm after each mp layer: 21.571321487426758
Norm after each mp layer: 122.71253967285156
Norm before input: 0.2552422881126404
Norm after input: 0.5388103127479553
Norm after each mp layer: 2.9210119247436523
Norm after each mp layer: 21.59157943725586
Norm after each mp layer: 122.778076171875
Norm before input: 0.2552422881126404
Norm after input: 0.5388103127479553
Norm after each mp layer: 2.9210119247436523
Norm after each mp layer: 21.59157943725586
Norm after each mp layer: 122.778076171875
Norm before input: 0.2552422881126404
Norm after input: 0.5389266014099121
Norm after each mp layer: 2.9227187633514404
Norm after each mp layer: 21.615938186645508
Norm after each mp layer: 122.86534881591797
Norm before input: 0.2552422881126404
Norm after input: 0.5389266014099121
Norm after each mp layer: 2.9227187633514404
Norm after each mp layer: 21.61594009399414
Norm after each mp layer: 122.86534881591797
Norm before input: 0.2552422881126404
Norm after input: 0.5390185117721558
Norm after each mp layer: 2.9242727756500244
Norm after each mp layer: 21.63779067993164
Norm after each mp layer: 122.93558502197266
Norm before input: 0.2552422881126404
Norm after input: 0.5390185117721558
Norm after each mp layer: 2.9242727756500244
Norm after each mp layer: 21.63779067993164
Norm after each mp layer: 122.93558502197266
Norm before input: 0.2552422881126404
Norm after input: 0.5391180515289307
Norm after each mp layer: 2.9258666038513184
Norm after each mp layer: 21.660236358642578
Norm after each mp layer: 123.00613403320312
Norm before input: 0.2552422881126404
Norm after input: 0.5391180515289307
Norm after each mp layer: 2.9258666038513184
Norm after each mp layer: 21.660236358642578
Norm after each mp layer: 123.00613403320312
Norm before input: 0.2552422881126404
Norm after input: 0.5392237901687622
Norm after each mp layer: 2.9274847507476807
Norm after each mp layer: 21.68314552307129
Norm after each mp layer: 123.07978057861328
Epoch: 250, Loss: 0.0085, Energy: 2572261.0000, Train: 99.92%, Valid: 75.40%, Test: 76.40%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5392237901687622
Norm after each mp layer: 2.9274847507476807
Norm after each mp layer: 21.68314552307129
Norm after each mp layer: 123.07978057861328
Norm before input: 0.2552422881126404
Norm after input: 0.5393093228340149
Norm after each mp layer: 2.928961753845215
Norm after each mp layer: 21.703941345214844
Norm after each mp layer: 123.14434814453125
Norm before input: 0.2552422881126404
Norm after input: 0.5393093228340149
Norm after each mp layer: 2.928961753845215
Norm after each mp layer: 21.703941345214844
Norm after each mp layer: 123.14434814453125
Norm before input: 0.2552422881126404
Norm after input: 0.5394268035888672
Norm after each mp layer: 2.930626153945923
Norm after each mp layer: 21.727705001831055
Norm after each mp layer: 123.2237548828125
Norm before input: 0.2552422881126404
Norm after input: 0.5394268035888672
Norm after each mp layer: 2.930626153945923
Norm after each mp layer: 21.727705001831055
Norm after each mp layer: 123.2237548828125
Norm before input: 0.2552422881126404
Norm after input: 0.5394935011863708
Norm after each mp layer: 2.931947946548462
Norm after each mp layer: 21.746387481689453
Norm after each mp layer: 123.27974700927734
Norm before input: 0.2552422881126404
Norm after input: 0.5394935011863708
Norm after each mp layer: 2.931947946548462
Norm after each mp layer: 21.746387481689453
Norm after each mp layer: 123.27974700927734
Norm before input: 0.2552422881126404
Norm after input: 0.5396196842193604
Norm after each mp layer: 2.933652400970459
Norm after each mp layer: 21.77072525024414
Norm after each mp layer: 123.3576431274414
Norm before input: 0.2552422881126404
Norm after input: 0.5396196842193604
Norm after each mp layer: 2.933652400970459
Norm after each mp layer: 21.77072525024414
Norm after each mp layer: 123.3576431274414
Norm before input: 0.2552422881126404
Norm after input: 0.5396727323532104
Norm after each mp layer: 2.9348647594451904
Norm after each mp layer: 21.787771224975586
Norm after each mp layer: 123.40596771240234
Epoch: 255, Loss: 0.0082, Energy: 2547740.7500, Train: 99.92%, Valid: 75.40%, Test: 76.30%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5396727323532104
Norm after each mp layer: 2.9348647594451904
Norm after each mp layer: 21.787771224975586
Norm after each mp layer: 123.40596771240234
Norm before input: 0.2552422881126404
Norm after input: 0.5398098826408386
Norm after each mp layer: 2.936613082885742
Norm after each mp layer: 21.81287956237793
Norm after each mp layer: 123.4902572631836
Norm before input: 0.2552422881126404
Norm after input: 0.5398098826408386
Norm after each mp layer: 2.936613082885742
Norm after each mp layer: 21.81287956237793
Norm after each mp layer: 123.4902572631836
Norm before input: 0.2552422881126404
Norm after input: 0.5398529767990112
Norm after each mp layer: 2.9377453327178955
Norm after each mp layer: 21.828676223754883
Norm after each mp layer: 123.53427124023438
Norm before input: 0.2552422881126404
Norm after input: 0.5398529767990112
Norm after each mp layer: 2.9377453327178955
Norm after each mp layer: 21.828676223754883
Norm after each mp layer: 123.53427124023438
Norm before input: 0.2552422881126404
Norm after input: 0.5400007367134094
Norm after each mp layer: 2.9395668506622314
Norm after each mp layer: 21.85458755493164
Norm after each mp layer: 123.61617279052734
Norm before input: 0.2552422881126404
Norm after input: 0.5400007367134094
Norm after each mp layer: 2.9395668506622314
Norm after each mp layer: 21.85458755493164
Norm after each mp layer: 123.61617279052734
Norm before input: 0.2552422881126404
Norm after input: 0.5400350689888
Norm after each mp layer: 2.940627336502075
Norm after each mp layer: 21.869287490844727
Norm after each mp layer: 123.65411376953125
Norm before input: 0.2552422881126404
Norm after input: 0.5400350689888
Norm after each mp layer: 2.940627336502075
Norm after each mp layer: 21.869287490844727
Norm after each mp layer: 123.65411376953125
Norm before input: 0.2552422881126404
Norm after input: 0.5401834845542908
Norm after each mp layer: 2.9424400329589844
Norm after each mp layer: 21.89499282836914
Norm after each mp layer: 123.73275756835938
Epoch: 260, Loss: 0.0079, Energy: 2519733.5000, Train: 99.92%, Valid: 75.20%, Test: 76.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5401834845542908
Norm after each mp layer: 2.9424400329589844
Norm after each mp layer: 21.89499282836914
Norm after each mp layer: 123.73275756835938
Norm before input: 0.2552422881126404
Norm after input: 0.5402204394340515
Norm after each mp layer: 2.9434828758239746
Norm after each mp layer: 21.909643173217773
Norm after each mp layer: 123.77438354492188
Norm before input: 0.2552422881126404
Norm after input: 0.5402204394340515
Norm after each mp layer: 2.9434828758239746
Norm after each mp layer: 21.909643173217773
Norm after each mp layer: 123.77438354492188
Norm before input: 0.2552422881126404
Norm after input: 0.5403539538383484
Norm after each mp layer: 2.945162773132324
Norm after each mp layer: 21.933603286743164
Norm after each mp layer: 123.84827423095703
Norm before input: 0.2552422881126404
Norm after input: 0.5403539538383484
Norm after each mp layer: 2.945162773132324
Norm after each mp layer: 21.933603286743164
Norm after each mp layer: 123.84827423095703
Norm before input: 0.2552422881126404
Norm after input: 0.5404049754142761
Norm after each mp layer: 2.9462742805480957
Norm after each mp layer: 21.949371337890625
Norm after each mp layer: 123.89251708984375
Norm before input: 0.2552422881126404
Norm after input: 0.5404049754142761
Norm after each mp layer: 2.9462742805480957
Norm after each mp layer: 21.949371337890625
Norm after each mp layer: 123.89251708984375
Norm before input: 0.2552422881126404
Norm after input: 0.5405146479606628
Norm after each mp layer: 2.94777512550354
Norm after each mp layer: 21.97076988220215
Norm after each mp layer: 123.95378875732422
Norm before input: 0.2552422881126404
Norm after input: 0.5405146479606628
Norm after each mp layer: 2.94777512550354
Norm after each mp layer: 21.97076988220215
Norm after each mp layer: 123.95378875732422
Norm before input: 0.2552422881126404
Norm after input: 0.5405871272087097
Norm after each mp layer: 2.949007034301758
Norm after each mp layer: 21.988399505615234
Norm after each mp layer: 124.00362396240234
Epoch: 265, Loss: 0.0076, Energy: 2494664.0000, Train: 99.92%, Valid: 75.20%, Test: 76.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5405871272087097
Norm after each mp layer: 2.949007034301758
Norm after each mp layer: 21.988399505615234
Norm after each mp layer: 124.00360870361328
Norm before input: 0.2552422881126404
Norm after input: 0.5406728386878967
Norm after each mp layer: 2.9503211975097656
Norm after each mp layer: 22.00720977783203
Norm after each mp layer: 124.05667114257812
Norm before input: 0.2552422881126404
Norm after input: 0.5406728386878967
Norm after each mp layer: 2.9503211975097656
Norm after each mp layer: 22.00720977783203
Norm after each mp layer: 124.05667114257812
Norm before input: 0.2552422881126404
Norm after input: 0.5407674312591553
Norm after each mp layer: 2.951683282852173
Norm after each mp layer: 22.026763916015625
Norm after each mp layer: 124.11228942871094
Norm before input: 0.2552422881126404
Norm after input: 0.5407674312591553
Norm after each mp layer: 2.951683282852173
Norm after each mp layer: 22.026763916015625
Norm after each mp layer: 124.11228942871094
Norm before input: 0.2552422881126404
Norm after input: 0.5408313274383545
Norm after each mp layer: 2.9528284072875977
Norm after each mp layer: 22.043197631835938
Norm after each mp layer: 124.15708923339844
Norm before input: 0.2552422881126404
Norm after input: 0.5408313274383545
Norm after each mp layer: 2.9528284072875977
Norm after each mp layer: 22.043197631835938
Norm after each mp layer: 124.15708923339844
Norm before input: 0.2552422881126404
Norm after input: 0.5409409403800964
Norm after each mp layer: 2.954284906387329
Norm after each mp layer: 22.06401252746582
Norm after each mp layer: 124.21292114257812
Norm before input: 0.2552422881126404
Norm after input: 0.5409409403800964
Norm after each mp layer: 2.954284906387329
Norm after each mp layer: 22.06401252746582
Norm after each mp layer: 124.21292114257812
Norm before input: 0.2552422881126404
Norm after input: 0.5409906506538391
Norm after each mp layer: 2.955310583114624
Norm after each mp layer: 22.078811645507812
Norm after each mp layer: 124.25343322753906
Epoch: 270, Loss: 0.0073, Energy: 2467492.0000, Train: 99.92%, Valid: 75.20%, Test: 76.20%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5409906506538391
Norm after each mp layer: 2.955310583114624
Norm after each mp layer: 22.078811645507812
Norm after each mp layer: 124.25343322753906
Norm before input: 0.2552422881126404
Norm after input: 0.5411055684089661
Norm after each mp layer: 2.9567878246307373
Norm after each mp layer: 22.09992218017578
Norm after each mp layer: 124.31016540527344
Norm before input: 0.2552422881126404
Norm after input: 0.5411055684089661
Norm after each mp layer: 2.9567878246307373
Norm after each mp layer: 22.09992218017578
Norm after each mp layer: 124.31016540527344
Norm before input: 0.2552422881126404
Norm after input: 0.5411521792411804
Norm after each mp layer: 2.957773447036743
Norm after each mp layer: 22.114198684692383
Norm after each mp layer: 124.35026550292969
Norm before input: 0.2552422881126404
Norm after input: 0.5411521792411804
Norm after each mp layer: 2.957773447036743
Norm after each mp layer: 22.114198684692383
Norm after each mp layer: 124.35026550292969
Norm before input: 0.2552422881126404
Norm after input: 0.5412619113922119
Norm after each mp layer: 2.9592056274414062
Norm after each mp layer: 22.13460350036621
Norm after each mp layer: 124.40263366699219
Norm before input: 0.2552422881126404
Norm after input: 0.5412619113922119
Norm after each mp layer: 2.9592056274414062
Norm after each mp layer: 22.13460350036621
Norm after each mp layer: 124.40263366699219
Norm before input: 0.2552422881126404
Norm after input: 0.5413155555725098
Norm after each mp layer: 2.960228443145752
Norm after each mp layer: 22.149362564086914
Norm after each mp layer: 124.44218444824219
Norm before input: 0.2552422881126404
Norm after input: 0.5413155555725098
Norm after each mp layer: 2.960228204727173
Norm after each mp layer: 22.149362564086914
Norm after each mp layer: 124.44218444824219
Norm before input: 0.2552422881126404
Norm after input: 0.5414122343063354
Norm after each mp layer: 2.961557626724243
Norm after each mp layer: 22.168283462524414
Norm after each mp layer: 124.4890365600586
Epoch: 275, Loss: 0.0071, Energy: 2438921.0000, Train: 99.92%, Valid: 75.40%, Test: 76.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5414122343063354
Norm after each mp layer: 2.961557626724243
Norm after each mp layer: 22.168283462524414
Norm after each mp layer: 124.4890365600586
Norm before input: 0.2552422881126404
Norm after input: 0.5414796471595764
Norm after each mp layer: 2.962661027908325
Norm after each mp layer: 22.184192657470703
Norm after each mp layer: 124.53152465820312
Norm before input: 0.2552422881126404
Norm after input: 0.5414796471595764
Norm after each mp layer: 2.962661027908325
Norm after each mp layer: 22.184192657470703
Norm after each mp layer: 124.53152465820312
Norm before input: 0.2552422881126404
Norm after input: 0.5415590405464172
Norm after each mp layer: 2.963848352432251
Norm after each mp layer: 22.201196670532227
Norm after each mp layer: 124.57466125488281
Norm before input: 0.2552422881126404
Norm after input: 0.5415590405464172
Norm after each mp layer: 2.963848352432251
Norm after each mp layer: 22.201196670532227
Norm after each mp layer: 124.57466125488281
Norm before input: 0.2552422881126404
Norm after input: 0.5416409969329834
Norm after each mp layer: 2.9650442600250244
Norm after each mp layer: 22.218345642089844
Norm after each mp layer: 124.61759185791016
Norm before input: 0.2552422881126404
Norm after input: 0.5416409969329834
Norm after each mp layer: 2.9650442600250244
Norm after each mp layer: 22.218345642089844
Norm after each mp layer: 124.61759185791016
Norm before input: 0.2552422881126404
Norm after input: 0.5417038798332214
Norm after each mp layer: 2.9660935401916504
Norm after each mp layer: 22.2335147857666
Norm after each mp layer: 124.65612030029297
Norm before input: 0.2552422881126404
Norm after input: 0.5417038798332214
Norm after each mp layer: 2.9660935401916504
Norm after each mp layer: 22.2335147857666
Norm after each mp layer: 124.65612030029297
Norm before input: 0.2552422881126404
Norm after input: 0.5417959094047546
Norm after each mp layer: 2.9673519134521484
Norm after each mp layer: 22.2514705657959
Norm after each mp layer: 124.69754028320312
Epoch: 280, Loss: 0.0068, Energy: 2411355.7500, Train: 99.92%, Valid: 75.40%, Test: 76.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5417959094047546
Norm after each mp layer: 2.9673519134521484
Norm after each mp layer: 22.2514705657959
Norm after each mp layer: 124.69754028320312
Norm before input: 0.2552422881126404
Norm after input: 0.5418488383293152
Norm after each mp layer: 2.9683046340942383
Norm after each mp layer: 22.26543426513672
Norm after each mp layer: 124.734375
Norm before input: 0.2552422881126404
Norm after input: 0.5418488383293152
Norm after each mp layer: 2.9683046340942383
Norm after each mp layer: 22.26543426513672
Norm after each mp layer: 124.734375
Norm before input: 0.2552422881126404
Norm after input: 0.5419434905052185
Norm after each mp layer: 2.9695661067962646
Norm after each mp layer: 22.283449172973633
Norm after each mp layer: 124.7742691040039
Norm before input: 0.2552422881126404
Norm after input: 0.5419434905052185
Norm after each mp layer: 2.9695661067962646
Norm after each mp layer: 22.283449172973633
Norm after each mp layer: 124.7742691040039
Norm before input: 0.2552422881126404
Norm after input: 0.5419944524765015
Norm after each mp layer: 2.9704837799072266
Norm after each mp layer: 22.297000885009766
Norm after each mp layer: 124.80982208251953
Norm before input: 0.2552422881126404
Norm after input: 0.5419944524765015
Norm after each mp layer: 2.9704837799072266
Norm after each mp layer: 22.297000885009766
Norm after each mp layer: 124.80982208251953
Norm before input: 0.2552422881126404
Norm after input: 0.5420838594436646
Norm after each mp layer: 2.9716944694519043
Norm after each mp layer: 22.314294815063477
Norm after each mp layer: 124.84591674804688
Norm before input: 0.2552422881126404
Norm after input: 0.5420838594436646
Norm after each mp layer: 2.9716944694519043
Norm after each mp layer: 22.314294815063477
Norm after each mp layer: 124.84591674804688
Norm before input: 0.2552422881126404
Norm after input: 0.5421398282051086
Norm after each mp layer: 2.9726343154907227
Norm after each mp layer: 22.328147888183594
Norm after each mp layer: 124.8802719116211
Epoch: 285, Loss: 0.0066, Energy: 2383784.2500, Train: 99.92%, Valid: 75.60%, Test: 76.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5421398282051086
Norm after each mp layer: 2.9726343154907227
Norm after each mp layer: 22.328147888183594
Norm after each mp layer: 124.8802719116211
Norm before input: 0.2552422881126404
Norm after input: 0.542218804359436
Norm after each mp layer: 2.973752021789551
Norm after each mp layer: 22.34421730041504
Norm after each mp layer: 124.91386413574219
Norm before input: 0.2552422881126404
Norm after input: 0.542218804359436
Norm after each mp layer: 2.973752021789551
Norm after each mp layer: 22.34421730041504
Norm after each mp layer: 124.91386413574219
Norm before input: 0.2552422881126404
Norm after input: 0.5422840118408203
Norm after each mp layer: 2.9747474193573
Norm after each mp layer: 22.358795166015625
Norm after each mp layer: 124.94770050048828
Norm before input: 0.2552422881126404
Norm after input: 0.5422840118408203
Norm after each mp layer: 2.9747474193573
Norm after each mp layer: 22.358795166015625
Norm after each mp layer: 124.94770050048828
Norm before input: 0.2552422881126404
Norm after input: 0.5423510670661926
Norm after each mp layer: 2.975755453109741
Norm after each mp layer: 22.373470306396484
Norm after each mp layer: 124.9794692993164
Norm before input: 0.2552422881126404
Norm after input: 0.5423510670661926
Norm after each mp layer: 2.975755453109741
Norm after each mp layer: 22.373470306396484
Norm after each mp layer: 124.9794692993164
Norm before input: 0.2552422881126404
Norm after input: 0.5424251556396484
Norm after each mp layer: 2.9768106937408447
Norm after each mp layer: 22.388761520385742
Norm after each mp layer: 125.0104751586914
Norm before input: 0.2552422881126404
Norm after input: 0.5424251556396484
Norm after each mp layer: 2.9768106937408447
Norm after each mp layer: 22.388761520385742
Norm after each mp layer: 125.0104751586914
Norm before input: 0.2552422881126404
Norm after input: 0.5424817204475403
Norm after each mp layer: 2.9777190685272217
Norm after each mp layer: 22.40220069885254
Norm after each mp layer: 125.04080200195312
Epoch: 290, Loss: 0.0064, Energy: 2355522.2500, Train: 99.92%, Valid: 76.00%, Test: 75.90%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5424817204475403
Norm after each mp layer: 2.9777190685272217
Norm after each mp layer: 22.40220069885254
Norm after each mp layer: 125.04080200195312
Norm before input: 0.2552422881126404
Norm after input: 0.5425610542297363
Norm after each mp layer: 2.978806972503662
Norm after each mp layer: 22.417850494384766
Norm after each mp layer: 125.06906127929688
Norm before input: 0.2552422881126404
Norm after input: 0.5425610542297363
Norm after each mp layer: 2.978806972503662
Norm after each mp layer: 22.417850494384766
Norm after each mp layer: 125.06906127929688
Norm before input: 0.2552422881126404
Norm after input: 0.5426114797592163
Norm after each mp layer: 2.9796462059020996
Norm after each mp layer: 22.430477142333984
Norm after each mp layer: 125.0995864868164
Norm before input: 0.2552422881126404
Norm after input: 0.5426114797592163
Norm after each mp layer: 2.9796462059020996
Norm after each mp layer: 22.430477142333984
Norm after each mp layer: 125.0995864868164
Norm before input: 0.2552422881126404
Norm after input: 0.5426913499832153
Norm after each mp layer: 2.9807283878326416
Norm after each mp layer: 22.44600486755371
Norm after each mp layer: 125.12578582763672
Norm before input: 0.2552422881126404
Norm after input: 0.5426913499832153
Norm after each mp layer: 2.9807283878326416
Norm after each mp layer: 22.44600486755371
Norm after each mp layer: 125.12578582763672
Norm before input: 0.2552422881126404
Norm after input: 0.5427404046058655
Norm after each mp layer: 2.981541633605957
Norm after each mp layer: 22.458316802978516
Norm after each mp layer: 125.15532684326172
Norm before input: 0.2552422881126404
Norm after input: 0.5427404046058655
Norm after each mp layer: 2.981541633605957
Norm after each mp layer: 22.458316802978516
Norm after each mp layer: 125.15532684326172
Norm before input: 0.2552422881126404
Norm after input: 0.5428167581558228
Norm after each mp layer: 2.982585906982422
Norm after each mp layer: 22.473331451416016
Norm after each mp layer: 125.17928314208984
Epoch: 295, Loss: 0.0062, Energy: 2327150.7500, Train: 99.92%, Valid: 76.00%, Test: 75.90%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5428167581558228
Norm after each mp layer: 2.982585906982422
Norm after each mp layer: 22.473331451416016
Norm after each mp layer: 125.17928314208984
Norm before input: 0.2552422881126404
Norm after input: 0.5428681969642639
Norm after each mp layer: 2.983407497406006
Norm after each mp layer: 22.485719680786133
Norm after each mp layer: 125.20697784423828
Norm before input: 0.2552422881126404
Norm after input: 0.5428681969642639
Norm after each mp layer: 2.983407497406006
Norm after each mp layer: 22.485719680786133
Norm after each mp layer: 125.20697784423828
Norm before input: 0.2552422881126404
Norm after input: 0.5429388880729675
Norm after each mp layer: 2.9843921661376953
Norm after each mp layer: 22.49999237060547
Norm after each mp layer: 125.23001098632812
Norm before input: 0.2552422881126404
Norm after input: 0.5429388880729675
Norm after each mp layer: 2.9843921661376953
Norm after each mp layer: 22.49999237060547
Norm after each mp layer: 125.23001098632812
Norm before input: 0.2552422881126404
Norm after input: 0.5429943203926086
Norm after each mp layer: 2.9852347373962402
Norm after each mp layer: 22.512617111206055
Norm after each mp layer: 125.25572204589844
Norm before input: 0.2552422881126404
Norm after input: 0.5429943203926086
Norm after each mp layer: 2.9852347373962402
Norm after each mp layer: 22.512617111206055
Norm after each mp layer: 125.25572204589844
Norm before input: 0.2552422881126404
Norm after input: 0.5430585145950317
Norm after each mp layer: 2.986150026321411
Norm after each mp layer: 22.52605628967285
Norm after each mp layer: 125.27812957763672
Norm before input: 0.2552422881126404
Norm after input: 0.5430585145950317
Norm after each mp layer: 2.986150026321411
Norm after each mp layer: 22.52605628967285
Norm after each mp layer: 125.27812957763672
Norm before input: 0.2552422881126404
Norm after input: 0.5431174039840698
Norm after each mp layer: 2.9870126247406006
Norm after each mp layer: 22.538883209228516
Norm after each mp layer: 125.30091094970703
Epoch: 300, Loss: 0.0061, Energy: 2299254.2500, Train: 99.92%, Valid: 75.80%, Test: 75.90%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5431174039840698
Norm after each mp layer: 2.9870126247406006
Norm after each mp layer: 22.538883209228516
Norm after each mp layer: 125.30091094970703
Norm before input: 0.2552422881126404
Norm after input: 0.543175220489502
Norm after each mp layer: 2.987861156463623
Norm after each mp layer: 22.551525115966797
Norm after each mp layer: 125.32274627685547
Norm before input: 0.2552422881126404
Norm after input: 0.543175220489502
Norm after each mp layer: 2.987861156463623
Norm after each mp layer: 22.551525115966797
Norm after each mp layer: 125.32274627685547
Norm before input: 0.2552422881126404
Norm after input: 0.5432368516921997
Norm after each mp layer: 2.9887373447418213
Norm after each mp layer: 22.564481735229492
Norm after each mp layer: 125.34298706054688
Norm before input: 0.2552422881126404
Norm after input: 0.5432368516921997
Norm after each mp layer: 2.9887373447418213
Norm after each mp layer: 22.564481735229492
Norm after each mp layer: 125.34298706054688
Norm before input: 0.2552422881126404
Norm after input: 0.5432894825935364
Norm after each mp layer: 2.9895265102386475
Norm after each mp layer: 22.576433181762695
Norm after each mp layer: 125.3648910522461
Norm before input: 0.2552422881126404
Norm after input: 0.5432894825935364
Norm after each mp layer: 2.9895265102386475
Norm after each mp layer: 22.576433181762695
Norm after each mp layer: 125.3648910522461
Norm before input: 0.2552422881126404
Norm after input: 0.5433534979820251
Norm after each mp layer: 2.9904134273529053
Norm after each mp layer: 22.589479446411133
Norm after each mp layer: 125.38263702392578
Norm before input: 0.2552422881126404
Norm after input: 0.5433534979820251
Norm after each mp layer: 2.9904134273529053
Norm after each mp layer: 22.589479446411133
Norm after each mp layer: 125.38263702392578
Norm before input: 0.2552422881126404
Norm after input: 0.5434014797210693
Norm after each mp layer: 2.9911491870880127
Norm after each mp layer: 22.600801467895508
Norm after each mp layer: 125.40421295166016
Epoch: 305, Loss: 0.0059, Energy: 2271335.2500, Train: 99.92%, Valid: 75.80%, Test: 76.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5434014797210693
Norm after each mp layer: 2.9911491870880127
Norm after each mp layer: 22.600801467895508
Norm after each mp layer: 125.40422821044922
Norm before input: 0.2552422881126404
Norm after input: 0.5434678196907043
Norm after each mp layer: 2.9920496940612793
Norm after each mp layer: 22.613956451416016
Norm after each mp layer: 125.41903686523438
Norm before input: 0.2552422881126404
Norm after input: 0.5434678196907043
Norm after each mp layer: 2.9920496940612793
Norm after each mp layer: 22.613956451416016
Norm after each mp layer: 125.41903686523438
Norm before input: 0.2552422881126404
Norm after input: 0.5435106158256531
Norm after each mp layer: 2.9927263259887695
Norm after each mp layer: 22.624588012695312
Norm after each mp layer: 125.4407730102539
Norm before input: 0.2552422881126404
Norm after input: 0.5435106158256531
Norm after each mp layer: 2.9927263259887695
Norm after each mp layer: 22.624588012695312
Norm after each mp layer: 125.4407730102539
Norm before input: 0.2552422881126404
Norm after input: 0.5435804724693298
Norm after each mp layer: 2.993650436401367
Norm after each mp layer: 22.637969970703125
Norm after each mp layer: 125.45268249511719
Norm before input: 0.2552422881126404
Norm after input: 0.5435804724693298
Norm after each mp layer: 2.993650436401367
Norm after each mp layer: 22.637969970703125
Norm after each mp layer: 125.45268249511719
Norm before input: 0.2552422881126404
Norm after input: 0.5436158180236816
Norm after each mp layer: 2.9942471981048584
Norm after each mp layer: 22.647687911987305
Norm after each mp layer: 125.4758071899414
Norm before input: 0.2552422881126404
Norm after input: 0.5436158180236816
Norm after each mp layer: 2.9942471981048584
Norm after each mp layer: 22.647687911987305
Norm after each mp layer: 125.47582244873047
Norm before input: 0.2552422881126404
Norm after input: 0.5436923503875732
Norm after each mp layer: 2.995225667953491
Norm after each mp layer: 22.661630630493164
Norm after each mp layer: 125.48342895507812
Epoch: 310, Loss: 0.0057, Energy: 2243512.5000, Train: 99.92%, Valid: 75.80%, Test: 76.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5436923503875732
Norm after each mp layer: 2.995225667953491
Norm after each mp layer: 22.661630630493164
Norm after each mp layer: 125.48342895507812
Norm before input: 0.2552422881126404
Norm after input: 0.5437154769897461
Norm after each mp layer: 2.9956982135772705
Norm after each mp layer: 22.669939041137695
Norm after each mp layer: 125.50950622558594
Norm before input: 0.2552422881126404
Norm after input: 0.5437154769897461
Norm after each mp layer: 2.9956982135772705
Norm after each mp layer: 22.669939041137695
Norm after each mp layer: 125.50950622558594
Norm before input: 0.2552422881126404
Norm after input: 0.5438054800033569
Norm after each mp layer: 2.9967973232269287
Norm after each mp layer: 22.685163497924805
Norm after each mp layer: 125.50975799560547
Norm before input: 0.2552422881126404
Norm after input: 0.5438054800033569
Norm after each mp layer: 2.9967973232269287
Norm after each mp layer: 22.685163497924805
Norm after each mp layer: 125.50975799560547
Norm before input: 0.2552422881126404
Norm after input: 0.5438070893287659
Norm after each mp layer: 2.997056245803833
Norm after each mp layer: 22.691082000732422
Norm after each mp layer: 125.54280090332031
Norm before input: 0.2552422881126404
Norm after input: 0.5438070893287659
Norm after each mp layer: 2.997056245803833
Norm after each mp layer: 22.691082000732422
Norm after each mp layer: 125.54280090332031
Norm before input: 0.2552422881126404
Norm after input: 0.5439239144325256
Norm after each mp layer: 2.998403549194336
Norm after each mp layer: 22.708993911743164
Norm after each mp layer: 125.5305404663086
Norm before input: 0.2552422881126404
Norm after input: 0.5439239144325256
Norm after each mp layer: 2.998403549194336
Norm after each mp layer: 22.708993911743164
Norm after each mp layer: 125.5305404663086
Norm before input: 0.2552422881126404
Norm after input: 0.5438867211341858
Norm after each mp layer: 2.998281955718994
Norm after each mp layer: 22.71068000793457
Norm after each mp layer: 125.57783508300781
Epoch: 315, Loss: 0.0056, Energy: 2216429.5000, Train: 99.92%, Valid: 76.00%, Test: 76.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5438867211341858
Norm after each mp layer: 2.998281955718994
Norm after each mp layer: 22.71068000793457
Norm after each mp layer: 125.57783508300781
Norm before input: 0.2552422881126404
Norm after input: 0.5440518260002136
Norm after each mp layer: 3.0000808238983154
Norm after each mp layer: 22.733509063720703
Norm after each mp layer: 125.54454040527344
Norm before input: 0.2552422881126404
Norm after input: 0.5440518260002136
Norm after each mp layer: 3.0000808238983154
Norm after each mp layer: 22.733509063720703
Norm after each mp layer: 125.54454040527344
Norm before input: 0.2552422881126404
Norm after input: 0.5439542531967163
Norm after each mp layer: 2.9993808269500732
Norm after each mp layer: 22.728679656982422
Norm after each mp layer: 125.61336517333984
Norm before input: 0.2552422881126404
Norm after input: 0.5439542531967163
Norm after each mp layer: 2.9993808269500732
Norm after each mp layer: 22.728679656982422
Norm after each mp layer: 125.61336517333984
Norm before input: 0.2552422881126404
Norm after input: 0.5441789627075195
Norm after each mp layer: 3.001725435256958
Norm after each mp layer: 22.757551193237305
Norm after each mp layer: 125.55728149414062
Norm before input: 0.2552422881126404
Norm after input: 0.5441789627075195
Norm after each mp layer: 3.001725435256958
Norm after each mp layer: 22.757551193237305
Norm after each mp layer: 125.55728149414062
Norm before input: 0.2552422881126404
Norm after input: 0.544034481048584
Norm after each mp layer: 3.000598430633545
Norm after each mp layer: 22.74759292602539
Norm after each mp layer: 125.6368408203125
Norm before input: 0.2552422881126404
Norm after input: 0.544034481048584
Norm after each mp layer: 3.000598430633545
Norm after each mp layer: 22.74759292602539
Norm after each mp layer: 125.6368408203125
Norm before input: 0.2552422881126404
Norm after input: 0.5442669987678528
Norm after each mp layer: 3.002969264984131
Norm after each mp layer: 22.776981353759766
Norm after each mp layer: 125.58428192138672
Epoch: 320, Loss: 0.0056, Energy: 2188435.0000, Train: 99.92%, Valid: 76.00%, Test: 76.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5442669987678528
Norm after each mp layer: 3.002969264984131
Norm after each mp layer: 22.776981353759766
Norm after each mp layer: 125.58428192138672
Norm before input: 0.2552422881126404
Norm after input: 0.5441552996635437
Norm after each mp layer: 3.00217604637146
Norm after each mp layer: 22.770313262939453
Norm after each mp layer: 125.6422348022461
Norm before input: 0.2552422881126404
Norm after input: 0.5441552996635437
Norm after each mp layer: 3.00217604637146
Norm after each mp layer: 22.770313262939453
Norm after each mp layer: 125.6422348022461
Norm before input: 0.2552422881126404
Norm after input: 0.5443149209022522
Norm after each mp layer: 3.003854274749756
Norm after each mp layer: 22.79193687438965
Norm after each mp layer: 125.61396789550781
Norm before input: 0.2552422881126404
Norm after input: 0.5443149209022522
Norm after each mp layer: 3.003854274749756
Norm after each mp layer: 22.79193687438965
Norm after each mp layer: 125.61396789550781
Norm before input: 0.2552422881126404
Norm after input: 0.5442929267883301
Norm after each mp layer: 3.0038437843322754
Norm after each mp layer: 22.79449462890625
Norm after each mp layer: 125.6468734741211
Norm before input: 0.2552422881126404
Norm after input: 0.5442929267883301
Norm after each mp layer: 3.0038437843322754
Norm after each mp layer: 22.79449462890625
Norm after each mp layer: 125.6468734741211
Norm before input: 0.2552422881126404
Norm after input: 0.5443552136421204
Norm after each mp layer: 3.0046584606170654
Norm after each mp layer: 22.80596923828125
Norm after each mp layer: 125.63758850097656
Norm before input: 0.2552422881126404
Norm after input: 0.5443552136421204
Norm after each mp layer: 3.0046584606170654
Norm after each mp layer: 22.805971145629883
Norm after each mp layer: 125.63758850097656
Norm before input: 0.2552422881126404
Norm after input: 0.5444197654724121
Norm after each mp layer: 3.005410671234131
Norm after each mp layer: 22.817508697509766
Norm after each mp layer: 125.64364624023438
Epoch: 325, Loss: 0.0053, Energy: 2162289.7500, Train: 99.92%, Valid: 76.00%, Test: 76.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5444197654724121
Norm after each mp layer: 3.005410671234131
Norm after each mp layer: 22.817508697509766
Norm after each mp layer: 125.64364624023438
Norm before input: 0.2552422881126404
Norm after input: 0.5444042086601257
Norm after each mp layer: 3.0054633617401123
Norm after each mp layer: 22.820697784423828
Norm after each mp layer: 125.66600799560547
Norm before input: 0.2552422881126404
Norm after input: 0.5444042086601257
Norm after each mp layer: 3.0054633617401123
Norm after each mp layer: 22.820697784423828
Norm after each mp layer: 125.66602325439453
Norm before input: 0.2552422881126404
Norm after input: 0.5445324182510376
Norm after each mp layer: 3.006873607635498
Norm after each mp layer: 22.83901023864746
Norm after each mp layer: 125.6308364868164
Norm before input: 0.2552422881126404
Norm after input: 0.5445324182510376
Norm after each mp layer: 3.006873607635498
Norm after each mp layer: 22.83901023864746
Norm after each mp layer: 125.6308364868164
Norm before input: 0.2552422881126404
Norm after input: 0.5444626212120056
Norm after each mp layer: 3.006330728530884
Norm after each mp layer: 22.836193084716797
Norm after each mp layer: 125.69071960449219
Norm before input: 0.2552422881126404
Norm after input: 0.5444626212120056
Norm after each mp layer: 3.006330728530884
Norm after each mp layer: 22.836193084716797
Norm after each mp layer: 125.69071960449219
Norm before input: 0.2552422881126404
Norm after input: 0.544630765914917
Norm after each mp layer: 3.008131504058838
Norm after each mp layer: 22.85862159729004
Norm after each mp layer: 125.63385772705078
Norm before input: 0.2552422881126404
Norm after input: 0.544630765914917
Norm after each mp layer: 3.008131504058838
Norm after each mp layer: 22.85862159729004
Norm after each mp layer: 125.63385772705078
Norm before input: 0.2552422881126404
Norm after input: 0.544538140296936
Norm after each mp layer: 3.0073931217193604
Norm after each mp layer: 22.853267669677734
Norm after each mp layer: 125.69563293457031
Epoch: 330, Loss: 0.0053, Energy: 2135716.5000, Train: 99.92%, Valid: 76.00%, Test: 75.70%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.544538140296936
Norm after each mp layer: 3.0073931217193604
Norm after each mp layer: 22.853267669677734
Norm after each mp layer: 125.69563293457031
Norm before input: 0.2552422881126404
Norm after input: 0.5447025895118713
Norm after each mp layer: 3.0090949535369873
Norm after each mp layer: 22.875057220458984
Norm after each mp layer: 125.65208435058594
Norm before input: 0.2552422881126404
Norm after input: 0.5447025895118713
Norm after each mp layer: 3.0090949535369873
Norm after each mp layer: 22.875057220458984
Norm after each mp layer: 125.65208435058594
Norm before input: 0.2552422881126404
Norm after input: 0.5446353554725647
Norm after each mp layer: 3.0086371898651123
Norm after each mp layer: 22.872356414794922
Norm after each mp layer: 125.69271087646484
Norm before input: 0.2552422881126404
Norm after input: 0.5446353554725647
Norm after each mp layer: 3.008636951446533
Norm after each mp layer: 22.872356414794922
Norm after each mp layer: 125.69271087646484
Norm before input: 0.2552422881126404
Norm after input: 0.5447535514831543
Norm after each mp layer: 3.0099077224731445
Norm after each mp layer: 22.88920021057129
Norm after each mp layer: 125.66431427001953
Norm before input: 0.2552422881126404
Norm after input: 0.5447535514831543
Norm after each mp layer: 3.0099077224731445
Norm after each mp layer: 22.88920021057129
Norm after each mp layer: 125.66431427001953
Norm before input: 0.2552422881126404
Norm after input: 0.5447404384613037
Norm after each mp layer: 3.0099077224731445
Norm after each mp layer: 22.892057418823242
Norm after each mp layer: 125.69425964355469
Norm before input: 0.2552422881126404
Norm after input: 0.5447404384613037
Norm after each mp layer: 3.0099077224731445
Norm after each mp layer: 22.892057418823242
Norm after each mp layer: 125.69425964355469
Norm before input: 0.2552422881126404
Norm after input: 0.5448020696640015
Norm after each mp layer: 3.0106823444366455
Norm after each mp layer: 22.90297508239746
Norm after each mp layer: 125.6780776977539
Epoch: 335, Loss: 0.0051, Energy: 2109343.0000, Train: 99.92%, Valid: 76.00%, Test: 75.60%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5448020696640015
Norm after each mp layer: 3.0106823444366455
Norm after each mp layer: 22.90297508239746
Norm after each mp layer: 125.6780776977539
Norm before input: 0.2552422881126404
Norm after input: 0.544837236404419
Norm after each mp layer: 3.0111398696899414
Norm after each mp layer: 22.910934448242188
Norm after each mp layer: 125.6866455078125
Norm before input: 0.2552422881126404
Norm after input: 0.544837236404419
Norm after each mp layer: 3.0111398696899414
Norm after each mp layer: 22.910934448242188
Norm after each mp layer: 125.6866455078125
Norm before input: 0.2552422881126404
Norm after input: 0.544857919216156
Norm after each mp layer: 3.0114808082580566
Norm after each mp layer: 22.917402267456055
Norm after each mp layer: 125.69571685791016
Norm before input: 0.2552422881126404
Norm after input: 0.544857919216156
Norm after each mp layer: 3.0114808082580566
Norm after each mp layer: 22.917402267456055
Norm after each mp layer: 125.69571685791016
Norm before input: 0.2552422881126404
Norm after input: 0.5449281930923462
Norm after each mp layer: 3.0123350620269775
Norm after each mp layer: 22.929218292236328
Norm after each mp layer: 125.67404174804688
Norm before input: 0.2552422881126404
Norm after input: 0.5449281930923462
Norm after each mp layer: 3.0123350620269775
Norm after each mp layer: 22.929218292236328
Norm after each mp layer: 125.67404174804688
Norm before input: 0.2552422881126404
Norm after input: 0.5449144244194031
Norm after each mp layer: 3.012314558029175
Norm after each mp layer: 22.931900024414062
Norm after each mp layer: 125.70401000976562
Norm before input: 0.2552422881126404
Norm after input: 0.5449144244194031
Norm after each mp layer: 3.012314558029175
Norm after each mp layer: 22.931900024414062
Norm after each mp layer: 125.70401000976562
Norm before input: 0.2552422881126404
Norm after input: 0.5450208783149719
Norm after each mp layer: 3.0134940147399902
Norm after each mp layer: 22.947471618652344
Norm after each mp layer: 125.67028045654297
Epoch: 340, Loss: 0.0050, Energy: 2083312.1250, Train: 99.92%, Valid: 76.00%, Test: 75.70%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5450208783149719
Norm after each mp layer: 3.0134940147399902
Norm after each mp layer: 22.947471618652344
Norm after each mp layer: 125.67028045654297
Norm before input: 0.2552422881126404
Norm after input: 0.5449703931808472
Norm after each mp layer: 3.0131518840789795
Norm after each mp layer: 22.946224212646484
Norm after each mp layer: 125.70809936523438
Norm before input: 0.2552422881126404
Norm after input: 0.5449703931808472
Norm after each mp layer: 3.0131518840789795
Norm after each mp layer: 22.946224212646484
Norm after each mp layer: 125.70809936523438
Norm before input: 0.2552422881126404
Norm after input: 0.5451080203056335
Norm after each mp layer: 3.0146067142486572
Norm after each mp layer: 22.96500015258789
Norm after each mp layer: 125.66401672363281
Norm before input: 0.2552422881126404
Norm after input: 0.5451080203056335
Norm after each mp layer: 3.0146067142486572
Norm after each mp layer: 22.96500015258789
Norm after each mp layer: 125.66401672363281
Norm before input: 0.2552422881126404
Norm after input: 0.5450305938720703
Norm after each mp layer: 3.0139851570129395
Norm after each mp layer: 22.96072006225586
Norm after each mp layer: 125.71609497070312
Norm before input: 0.2552422881126404
Norm after input: 0.5450305938720703
Norm after each mp layer: 3.0139851570129395
Norm after each mp layer: 22.96072006225586
Norm after each mp layer: 125.71609497070312
Norm before input: 0.2552422881126404
Norm after input: 0.5451833009719849
Norm after each mp layer: 3.0156192779541016
Norm after each mp layer: 22.98110580444336
Norm after each mp layer: 125.65465545654297
Norm before input: 0.2552422881126404
Norm after input: 0.5451833009719849
Norm after each mp layer: 3.0156192779541016
Norm after each mp layer: 22.98110580444336
Norm after each mp layer: 125.65465545654297
Norm before input: 0.2552422881126404
Norm after input: 0.545094907283783
Norm after each mp layer: 3.0148541927337646
Norm after each mp layer: 22.975439071655273
Norm after each mp layer: 125.71735382080078
Epoch: 345, Loss: 0.0050, Energy: 2058224.8750, Train: 99.92%, Valid: 76.00%, Test: 75.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.545094907283783
Norm after each mp layer: 3.0148541927337646
Norm after each mp layer: 22.975439071655273
Norm after each mp layer: 125.71735382080078
Norm before input: 0.2552422881126404
Norm after input: 0.5452461242675781
Norm after each mp layer: 3.016458034515381
Norm after each mp layer: 22.995534896850586
Norm after each mp layer: 125.65666198730469
Norm before input: 0.2552422881126404
Norm after input: 0.5452461242675781
Norm after each mp layer: 3.016458034515381
Norm after each mp layer: 22.99553680419922
Norm after each mp layer: 125.65666198730469
Norm before input: 0.2552422881126404
Norm after input: 0.5451655387878418
Norm after each mp layer: 3.015789747238159
Norm after each mp layer: 22.99065399169922
Norm after each mp layer: 125.70800018310547
Norm before input: 0.2552422881126404
Norm after input: 0.5451655387878418
Norm after each mp layer: 3.015789747238159
Norm after each mp layer: 22.99065399169922
Norm after each mp layer: 125.70800018310547
Norm before input: 0.2552422881126404
Norm after input: 0.5452971458435059
Norm after each mp layer: 3.017174243927002
Norm after each mp layer: 23.008522033691406
Norm after each mp layer: 125.65978240966797
Norm before input: 0.2552422881126404
Norm after input: 0.5452971458435059
Norm after each mp layer: 3.017174243927002
Norm after each mp layer: 23.008522033691406
Norm after each mp layer: 125.65978240966797
Norm before input: 0.2552422881126404
Norm after input: 0.5452425479888916
Norm after each mp layer: 3.0167527198791504
Norm after each mp layer: 23.006362915039062
Norm after each mp layer: 125.69767761230469
Norm before input: 0.2552422881126404
Norm after input: 0.5452425479888916
Norm after each mp layer: 3.0167527198791504
Norm after each mp layer: 23.006362915039062
Norm after each mp layer: 125.69767761230469
Norm before input: 0.2552422881126404
Norm after input: 0.5453434586524963
Norm after each mp layer: 3.0178606510162354
Norm after each mp layer: 23.020957946777344
Norm after each mp layer: 125.65632629394531
Epoch: 350, Loss: 0.0048, Energy: 2033269.3750, Train: 99.92%, Valid: 76.00%, Test: 75.60%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5453434586524963
Norm after each mp layer: 3.0178606510162354
Norm after each mp layer: 23.020957946777344
Norm after each mp layer: 125.65632629394531
Norm before input: 0.2552422881126404
Norm after input: 0.5453169345855713
Norm after each mp layer: 3.017683267593384
Norm after each mp layer: 23.021705627441406
Norm after each mp layer: 125.68480682373047
Norm before input: 0.2552422881126404
Norm after input: 0.5453169345855713
Norm after each mp layer: 3.017683267593384
Norm after each mp layer: 23.021705627441406
Norm after each mp layer: 125.68480682373047
Norm before input: 0.2552422881126404
Norm after input: 0.5453943610191345
Norm after each mp layer: 3.0185546875
Norm after each mp layer: 23.03373908996582
Norm after each mp layer: 125.65421295166016
Norm before input: 0.2552422881126404
Norm after input: 0.5453943610191345
Norm after each mp layer: 3.0185546875
Norm after each mp layer: 23.03373908996582
Norm after each mp layer: 125.65421295166016
Norm before input: 0.2552422881126404
Norm after input: 0.5453832149505615
Norm after each mp layer: 3.0185470581054688
Norm after each mp layer: 23.036163330078125
Norm after each mp layer: 125.6688232421875
Norm before input: 0.2552422881126404
Norm after input: 0.5453832149505615
Norm after each mp layer: 3.0185470581054688
Norm after each mp layer: 23.036163330078125
Norm after each mp layer: 125.6688232421875
Norm before input: 0.2552422881126404
Norm after input: 0.5454476475715637
Norm after each mp layer: 3.0192666053771973
Norm after each mp layer: 23.046701431274414
Norm after each mp layer: 125.64803314208984
Norm before input: 0.2552422881126404
Norm after input: 0.5454476475715637
Norm after each mp layer: 3.0192666053771973
Norm after each mp layer: 23.046701431274414
Norm after each mp layer: 125.64803314208984
Norm before input: 0.2552422881126404
Norm after input: 0.5454451441764832
Norm after each mp layer: 3.0193400382995605
Norm after each mp layer: 23.05000877380371
Norm after each mp layer: 125.65738677978516
Epoch: 355, Loss: 0.0047, Energy: 2008725.5000, Train: 99.92%, Valid: 76.00%, Test: 75.70%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5454451441764832
Norm after each mp layer: 3.0193400382995605
Norm after each mp layer: 23.05000877380371
Norm after each mp layer: 125.65738677978516
Norm before input: 0.2552422881126404
Norm after input: 0.545501172542572
Norm after each mp layer: 3.019993543624878
Norm after each mp layer: 23.059646606445312
Norm after each mp layer: 125.63570404052734
Norm before input: 0.2552422881126404
Norm after input: 0.545501172542572
Norm after each mp layer: 3.019993543624878
Norm after each mp layer: 23.059646606445312
Norm after each mp layer: 125.63570404052734
Norm before input: 0.2552422881126404
Norm after input: 0.5455005764961243
Norm after each mp layer: 3.020064115524292
Norm after each mp layer: 23.06305694580078
Norm after each mp layer: 125.64794158935547
Norm before input: 0.2552422881126404
Norm after input: 0.5455005764961243
Norm after each mp layer: 3.020064115524292
Norm after each mp layer: 23.06305694580078
Norm after each mp layer: 125.64794158935547
Norm before input: 0.2552422881126404
Norm after input: 0.5455597043037415
Norm after each mp layer: 3.0207431316375732
Norm after each mp layer: 23.07297134399414
Norm after each mp layer: 125.62464904785156
Norm before input: 0.2552422881126404
Norm after input: 0.5455597043037415
Norm after each mp layer: 3.0207431316375732
Norm after each mp layer: 23.07297134399414
Norm after each mp layer: 125.62464904785156
Norm before input: 0.2552422881126404
Norm after input: 0.5455482602119446
Norm after each mp layer: 3.020719528198242
Norm after each mp layer: 23.07518196105957
Norm after each mp layer: 125.63871765136719
Norm before input: 0.2552422881126404
Norm after input: 0.5455482602119446
Norm after each mp layer: 3.020719528198242
Norm after each mp layer: 23.07518196105957
Norm after each mp layer: 125.63871765136719
Norm before input: 0.2552422881126404
Norm after input: 0.5456225275993347
Norm after each mp layer: 3.021533489227295
Norm after each mp layer: 23.086660385131836
Norm after each mp layer: 125.60973358154297
Epoch: 360, Loss: 0.0047, Energy: 1984378.6250, Train: 99.92%, Valid: 76.00%, Test: 75.70%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5456225275993347
Norm after each mp layer: 3.021533489227295
Norm after each mp layer: 23.086660385131836
Norm after each mp layer: 125.60973358154297
Norm before input: 0.2552422881126404
Norm after input: 0.5455901622772217
Norm after each mp layer: 3.0212960243225098
Norm after each mp layer: 23.086524963378906
Norm after each mp layer: 125.63507080078125
Norm before input: 0.2552422881126404
Norm after input: 0.5455901622772217
Norm after each mp layer: 3.0212960243225098
Norm after each mp layer: 23.086524963378906
Norm after each mp layer: 125.63507080078125
Norm before input: 0.2552422881126404
Norm after input: 0.5456914305686951
Norm after each mp layer: 3.022393226623535
Norm after each mp layer: 23.100936889648438
Norm after each mp layer: 125.58704376220703
Norm before input: 0.2552422881126404
Norm after input: 0.5456914305686951
Norm after each mp layer: 3.022392988204956
Norm after each mp layer: 23.100936889648438
Norm after each mp layer: 125.58704376220703
Norm before input: 0.2552422881126404
Norm after input: 0.5456206798553467
Norm after each mp layer: 3.021756649017334
Norm after each mp layer: 23.096500396728516
Norm after each mp layer: 125.63540649414062
Norm before input: 0.2552422881126404
Norm after input: 0.5456206798553467
Norm after each mp layer: 3.021756649017334
Norm after each mp layer: 23.096500396728516
Norm after each mp layer: 125.63540649414062
Norm before input: 0.2552422881126404
Norm after input: 0.5457712411880493
Norm after each mp layer: 3.0233378410339355
Norm after each mp layer: 23.1162109375
Norm after each mp layer: 125.56046295166016
Norm before input: 0.2552422881126404
Norm after input: 0.5457712411880493
Norm after each mp layer: 3.0233378410339355
Norm after each mp layer: 23.1162109375
Norm after each mp layer: 125.56046295166016
Norm before input: 0.2552422881126404
Norm after input: 0.545641303062439
Norm after each mp layer: 3.022120952606201
Norm after each mp layer: 23.105226516723633
Norm after each mp layer: 125.63677978515625
Epoch: 365, Loss: 0.0046, Energy: 1961019.0000, Train: 99.92%, Valid: 76.00%, Test: 75.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.545641303062439
Norm after each mp layer: 3.022120952606201
Norm after each mp layer: 23.105226516723633
Norm after each mp layer: 125.63677978515625
Norm before input: 0.2552422881126404
Norm after input: 0.5458467602729797
Norm after each mp layer: 3.0242295265197754
Norm after each mp layer: 23.13075828552246
Norm after each mp layer: 125.53343963623047
Norm before input: 0.2552422881126404
Norm after input: 0.5458467602729797
Norm after each mp layer: 3.0242295265197754
Norm after each mp layer: 23.13075828552246
Norm after each mp layer: 125.53343963623047
Norm before input: 0.2552422881126404
Norm after input: 0.5456787943840027
Norm after each mp layer: 3.022631883621216
Norm after each mp layer: 23.11544418334961
Norm after each mp layer: 125.6279296875
Norm before input: 0.2552422881126404
Norm after input: 0.5456787943840027
Norm after each mp layer: 3.022631883621216
Norm after each mp layer: 23.11544418334961
Norm after each mp layer: 125.6279296875
Norm before input: 0.2552422881126404
Norm after input: 0.5458834767341614
Norm after each mp layer: 3.024728536605835
Norm after each mp layer: 23.140745162963867
Norm after each mp layer: 125.52201843261719
Norm before input: 0.2552422881126404
Norm after input: 0.5458834767341614
Norm after each mp layer: 3.024728536605835
Norm after each mp layer: 23.140745162963867
Norm after each mp layer: 125.52201843261719
Norm before input: 0.2552422881126404
Norm after input: 0.545750617980957
Norm after each mp layer: 3.023468255996704
Norm after each mp layer: 23.12909698486328
Norm after each mp layer: 125.59657287597656
Norm before input: 0.2552422881126404
Norm after input: 0.545750617980957
Norm after each mp layer: 3.023468255996704
Norm after each mp layer: 23.12909698486328
Norm after each mp layer: 125.59657287597656
Norm before input: 0.2552422881126404
Norm after input: 0.5458896160125732
Norm after each mp layer: 3.0249125957489014
Norm after each mp layer: 23.14717674255371
Norm after each mp layer: 125.52222442626953
Epoch: 370, Loss: 0.0046, Energy: 1937892.3750, Train: 99.92%, Valid: 76.00%, Test: 75.60%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5458896160125732
Norm after each mp layer: 3.0249125957489014
Norm after each mp layer: 23.14717674255371
Norm after each mp layer: 125.52222442626953
Norm before input: 0.2552422881126404
Norm after input: 0.5458300113677979
Norm after each mp layer: 3.0243706703186035
Norm after each mp layer: 23.14350128173828
Norm after each mp layer: 125.55656433105469
Norm before input: 0.2552422881126404
Norm after input: 0.5458300113677979
Norm after each mp layer: 3.0243706703186035
Norm after each mp layer: 23.14350128173828
Norm after each mp layer: 125.55657958984375
Norm before input: 0.2552422881126404
Norm after input: 0.5458968877792358
Norm after each mp layer: 3.025097608566284
Norm after each mp layer: 23.153724670410156
Norm after each mp layer: 125.51885223388672
Norm before input: 0.2552422881126404
Norm after input: 0.5458968877792358
Norm after each mp layer: 3.025097608566284
Norm after each mp layer: 23.153724670410156
Norm after each mp layer: 125.51885223388672
Norm before input: 0.2552422881126404
Norm after input: 0.545898973941803
Norm after each mp layer: 3.0251691341400146
Norm after each mp layer: 23.156837463378906
Norm after each mp layer: 125.5180435180664
Norm before input: 0.2552422881126404
Norm after input: 0.545898973941803
Norm after each mp layer: 3.0251691341400146
Norm after each mp layer: 23.156837463378906
Norm after each mp layer: 125.5180435180664
Norm before input: 0.2552422881126404
Norm after input: 0.5459116697311401
Norm after each mp layer: 3.02534556388855
Norm after each mp layer: 23.161121368408203
Norm after each mp layer: 125.51103210449219
Norm before input: 0.2552422881126404
Norm after input: 0.5459116697311401
Norm after each mp layer: 3.02534556388855
Norm after each mp layer: 23.161121368408203
Norm after each mp layer: 125.51103210449219
Norm before input: 0.2552422881126404
Norm after input: 0.5459613800048828
Norm after each mp layer: 3.025895833969116
Norm after each mp layer: 23.169479370117188
Norm after each mp layer: 125.4821548461914
Epoch: 375, Loss: 0.0044, Energy: 1914731.3750, Train: 99.92%, Valid: 76.00%, Test: 75.60%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5459613800048828
Norm after each mp layer: 3.025895833969116
Norm after each mp layer: 23.169479370117188
Norm after each mp layer: 125.4821548461914
Norm before input: 0.2552422881126404
Norm after input: 0.5459277033805847
Norm after each mp layer: 3.025606155395508
Norm after each mp layer: 23.168651580810547
Norm after each mp layer: 125.50108337402344
Norm before input: 0.2552422881126404
Norm after input: 0.5459277033805847
Norm after each mp layer: 3.025606155395508
Norm after each mp layer: 23.168651580810547
Norm after each mp layer: 125.50108337402344
Norm before input: 0.2552422881126404
Norm after input: 0.5460239052772522
Norm after each mp layer: 3.0266177654266357
Norm after each mp layer: 23.182085037231445
Norm after each mp layer: 125.44664764404297
Norm before input: 0.2552422881126404
Norm after input: 0.5460239052772522
Norm after each mp layer: 3.0266177654266357
Norm after each mp layer: 23.182085037231445
Norm after each mp layer: 125.44664764404297
Norm before input: 0.2552422881126404
Norm after input: 0.5459415316581726
Norm after each mp layer: 3.025836229324341
Norm after each mp layer: 23.175811767578125
Norm after each mp layer: 125.49342346191406
Norm before input: 0.2552422881126404
Norm after input: 0.5459415316581726
Norm after each mp layer: 3.025836229324341
Norm after each mp layer: 23.175811767578125
Norm after each mp layer: 125.49342346191406
Norm before input: 0.2552422881126404
Norm after input: 0.5460872054100037
Norm after each mp layer: 3.0273423194885254
Norm after each mp layer: 23.194618225097656
Norm after each mp layer: 125.41084289550781
Norm before input: 0.2552422881126404
Norm after input: 0.5460872054100037
Norm after each mp layer: 3.0273423194885254
Norm after each mp layer: 23.194618225097656
Norm after each mp layer: 125.41084289550781
Norm before input: 0.2552422881126404
Norm after input: 0.5459570288658142
Norm after each mp layer: 3.0260744094848633
Norm after each mp layer: 23.18294906616211
Norm after each mp layer: 125.48573303222656
Epoch: 380, Loss: 0.0044, Energy: 1892119.7500, Train: 99.92%, Valid: 76.00%, Test: 75.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5459570288658142
Norm after each mp layer: 3.0260744094848633
Norm after each mp layer: 23.18294906616211
Norm after each mp layer: 125.48573303222656
Norm before input: 0.2552422881126404
Norm after input: 0.5461387038230896
Norm after each mp layer: 3.0279340744018555
Norm after each mp layer: 23.205595016479492
Norm after each mp layer: 125.38277435302734
Norm before input: 0.2552422881126404
Norm after input: 0.5461387038230896
Norm after each mp layer: 3.0279340744018555
Norm after each mp layer: 23.205595016479492
Norm after each mp layer: 125.38277435302734
Norm before input: 0.2552422881126404
Norm after input: 0.5459899306297302
Norm after each mp layer: 3.0264811515808105
Norm after each mp layer: 23.191741943359375
Norm after each mp layer: 125.4662094116211
Norm before input: 0.2552422881126404
Norm after input: 0.5459899306297302
Norm after each mp layer: 3.0264811515808105
Norm after each mp layer: 23.191741943359375
Norm after each mp layer: 125.4662094116211
Norm before input: 0.2552422881126404
Norm after input: 0.5461612343788147
Norm after each mp layer: 3.0282304286956787
Norm after each mp layer: 23.213138580322266
Norm after each mp layer: 125.3681640625
Norm before input: 0.2552422881126404
Norm after input: 0.5461612343788147
Norm after each mp layer: 3.0282304286956787
Norm after each mp layer: 23.213138580322266
Norm after each mp layer: 125.3681640625
Norm before input: 0.2552422881126404
Norm after input: 0.546044647693634
Norm after each mp layer: 3.0270955562591553
Norm after each mp layer: 23.202749252319336
Norm after each mp layer: 125.4323959350586
Norm before input: 0.2552422881126404
Norm after input: 0.546044647693634
Norm after each mp layer: 3.0270955562591553
Norm after each mp layer: 23.202749252319336
Norm after each mp layer: 125.4323959350586
Norm before input: 0.2552422881126404
Norm after input: 0.5461663007736206
Norm after each mp layer: 3.0283563137054443
Norm after each mp layer: 23.21868896484375
Norm after each mp layer: 125.35842895507812
Epoch: 385, Loss: 0.0043, Energy: 1870564.3750, Train: 99.92%, Valid: 76.00%, Test: 75.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5461663007736206
Norm after each mp layer: 3.0283563137054443
Norm after each mp layer: 23.21868896484375
Norm after each mp layer: 125.35842895507812
Norm before input: 0.2552422881126404
Norm after input: 0.5461024641990662
Norm after each mp layer: 3.0277440547943115
Norm after each mp layer: 23.214122772216797
Norm after each mp layer: 125.39256286621094
Norm before input: 0.2552422881126404
Norm after input: 0.5461024641990662
Norm after each mp layer: 3.0277440547943115
Norm after each mp layer: 23.214122772216797
Norm after each mp layer: 125.39256286621094
Norm before input: 0.2552422881126404
Norm after input: 0.5461744666099548
Norm after each mp layer: 3.028503894805908
Norm after each mp layer: 23.224611282348633
Norm after each mp layer: 125.34613800048828
Norm before input: 0.2552422881126404
Norm after input: 0.5461744666099548
Norm after each mp layer: 3.028503894805908
Norm after each mp layer: 23.224611282348633
Norm after each mp layer: 125.34613800048828
Norm before input: 0.2552422881126404
Norm after input: 0.5461533665657043
Norm after each mp layer: 3.028327465057373
Norm after each mp layer: 23.224822998046875
Norm after each mp layer: 125.35325622558594
Norm before input: 0.2552422881126404
Norm after input: 0.5461533665657043
Norm after each mp layer: 3.028327465057373
Norm after each mp layer: 23.224822998046875
Norm after each mp layer: 125.35325622558594
Norm before input: 0.2552422881126404
Norm after input: 0.5461890697479248
Norm after each mp layer: 3.0287206172943115
Norm after each mp layer: 23.231334686279297
Norm after each mp layer: 125.32762908935547
Norm before input: 0.2552422881126404
Norm after input: 0.5461890697479248
Norm after each mp layer: 3.0287206172943115
Norm after each mp layer: 23.231334686279297
Norm after each mp layer: 125.32762908935547
Norm before input: 0.2552422881126404
Norm after input: 0.546198844909668
Norm after each mp layer: 3.0288524627685547
Norm after each mp layer: 23.234983444213867
Norm after each mp layer: 125.316650390625
Epoch: 390, Loss: 0.0042, Energy: 1848818.0000, Train: 99.92%, Valid: 76.00%, Test: 75.40%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.546198844909668
Norm after each mp layer: 3.0288524627685547
Norm after each mp layer: 23.234983444213867
Norm after each mp layer: 125.316650390625
Norm before input: 0.2552422881126404
Norm after input: 0.5462073683738708
Norm after each mp layer: 3.0289766788482666
Norm after each mp layer: 23.23851203918457
Norm after each mp layer: 125.30543518066406
Norm before input: 0.2552422881126404
Norm after input: 0.5462073683738708
Norm after each mp layer: 3.0289766788482666
Norm after each mp layer: 23.23851203918457
Norm after each mp layer: 125.30543518066406
Norm before input: 0.2552422881126404
Norm after input: 0.5462403893470764
Norm after each mp layer: 3.029343366622925
Norm after each mp layer: 23.244739532470703
Norm after each mp layer: 125.28058624267578
Norm before input: 0.2552422881126404
Norm after input: 0.5462403893470764
Norm after each mp layer: 3.029343366622925
Norm after each mp layer: 23.244739532470703
Norm after each mp layer: 125.28058624267578
Norm before input: 0.2552422881126404
Norm after input: 0.5462257266044617
Norm after each mp layer: 3.0292246341705322
Norm after each mp layer: 23.245643615722656
Norm after each mp layer: 125.28461456298828
Norm before input: 0.2552422881126404
Norm after input: 0.5462257266044617
Norm after each mp layer: 3.0292246341705322
Norm after each mp layer: 23.245643615722656
Norm after each mp layer: 125.28461456298828
Norm before input: 0.2552422881126404
Norm after input: 0.5462824702262878
Norm after each mp layer: 3.0298311710357666
Norm after each mp layer: 23.25448226928711
Norm after each mp layer: 125.24543762207031
Norm before input: 0.2552422881126404
Norm after input: 0.5462824702262878
Norm after each mp layer: 3.0298311710357666
Norm after each mp layer: 23.25448226928711
Norm after each mp layer: 125.24543762207031
Norm before input: 0.2552422881126404
Norm after input: 0.5462384223937988
Norm after each mp layer: 3.0294137001037598
Norm after each mp layer: 23.252073287963867
Norm after each mp layer: 125.26725769042969
Epoch: 395, Loss: 0.0042, Energy: 1827430.8750, Train: 99.92%, Valid: 76.00%, Test: 75.40%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5462384223937988
Norm after each mp layer: 3.0294137001037598
Norm after each mp layer: 23.252073287963867
Norm after each mp layer: 125.26725769042969
Norm before input: 0.2552422881126404
Norm after input: 0.5463290810585022
Norm after each mp layer: 3.0303566455841064
Norm after each mp layer: 23.264625549316406
Norm after each mp layer: 125.20868682861328
Norm before input: 0.2552422881126404
Norm after input: 0.5463290810585022
Norm after each mp layer: 3.0303566455841064
Norm after each mp layer: 23.264625549316406
Norm after each mp layer: 125.20868682861328
Norm before input: 0.2552422881126404
Norm after input: 0.5462430119514465
Norm after each mp layer: 3.029510021209717
Norm after each mp layer: 23.25745964050293
Norm after each mp layer: 125.25581359863281
Norm before input: 0.2552422881126404
Norm after input: 0.5462430119514465
Norm after each mp layer: 3.029510021209717
Norm after each mp layer: 23.25745964050293
Norm after each mp layer: 125.25581359863281
Norm before input: 0.2552422881126404
Norm after input: 0.5463817119598389
Norm after each mp layer: 3.0309412479400635
Norm after each mp layer: 23.27532386779785
Norm after each mp layer: 125.16722106933594
Norm before input: 0.2552422881126404
Norm after input: 0.5463817119598389
Norm after each mp layer: 3.0309412479400635
Norm after each mp layer: 23.27532386779785
Norm after each mp layer: 125.16722106933594
Norm before input: 0.2552422881126404
Norm after input: 0.5462419390678406
Norm after each mp layer: 3.0295445919036865
Norm after each mp layer: 23.262052536010742
Norm after each mp layer: 125.24689483642578
Norm before input: 0.2552422881126404
Norm after input: 0.5462419390678406
Norm after each mp layer: 3.0295445919036865
Norm after each mp layer: 23.262052536010742
Norm after each mp layer: 125.24689483642578
Norm before input: 0.2552422881126404
Norm after input: 0.5464304685592651
Norm after each mp layer: 3.0314712524414062
Norm after each mp layer: 23.28534698486328
Norm after each mp layer: 125.12841033935547
Epoch: 400, Loss: 0.0042, Energy: 1807207.0000, Train: 99.92%, Valid: 76.00%, Test: 75.40%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5464304685592651
Norm after each mp layer: 3.0314712524414062
Norm after each mp layer: 23.28534698486328
Norm after each mp layer: 125.12841033935547
Norm before input: 0.2552422881126404
Norm after input: 0.5462557077407837
Norm after each mp layer: 3.0297200679779053
Norm after each mp layer: 23.26801300048828
Norm after each mp layer: 125.2269515991211
Norm before input: 0.2552422881126404
Norm after input: 0.5462557077407837
Norm after each mp layer: 3.0297200679779053
Norm after each mp layer: 23.26801300048828
Norm after each mp layer: 125.2269515991211
Norm before input: 0.2552422881126404
Norm after input: 0.5464483499526978
Norm after each mp layer: 3.031681537628174
Norm after each mp layer: 23.291641235351562
Norm after each mp layer: 125.10504150390625
Norm before input: 0.2552422881126404
Norm after input: 0.5464483499526978
Norm after each mp layer: 3.031681537628174
Norm after each mp layer: 23.291641235351562
Norm after each mp layer: 125.10505676269531
Norm before input: 0.2552422881126404
Norm after input: 0.5462989807128906
Norm after each mp layer: 3.0301833152770996
Norm after each mp layer: 23.277036666870117
Norm after each mp layer: 125.1868896484375
Norm before input: 0.2552422881126404
Norm after input: 0.5462989807128906
Norm after each mp layer: 3.0301833152770996
Norm after each mp layer: 23.277036666870117
Norm after each mp layer: 125.1868896484375
Norm before input: 0.2552422881126404
Norm after input: 0.546440064907074
Norm after each mp layer: 3.0316295623779297
Norm after each mp layer: 23.29490089416504
Norm after each mp layer: 125.09126281738281
Norm before input: 0.2552422881126404
Norm after input: 0.546440064907074
Norm after each mp layer: 3.0316295623779297
Norm after each mp layer: 23.29490089416504
Norm after each mp layer: 125.09126281738281
Norm before input: 0.2552422881126404
Norm after input: 0.5463497638702393
Norm after each mp layer: 3.030723810195923
Norm after each mp layer: 23.286884307861328
Norm after each mp layer: 125.13651275634766
Epoch: 405, Loss: 0.0041, Energy: 1787162.6250, Train: 99.92%, Valid: 75.80%, Test: 75.60%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5463497638702393
Norm after each mp layer: 3.030723810195923
Norm after each mp layer: 23.286884307861328
Norm after each mp layer: 125.13651275634766
Norm before input: 0.2552422881126404
Norm after input: 0.5464329719543457
Norm after each mp layer: 3.0315823554992676
Norm after each mp layer: 23.298320770263672
Norm after each mp layer: 125.07426452636719
Norm before input: 0.2552422881126404
Norm after input: 0.5464329719543457
Norm after each mp layer: 3.0315823554992676
Norm after each mp layer: 23.298320770263672
Norm after each mp layer: 125.07426452636719
Norm before input: 0.2552422881126404
Norm after input: 0.5463913679122925
Norm after each mp layer: 3.0311739444732666
Norm after each mp layer: 23.295812606811523
Norm after each mp layer: 125.08804321289062
Norm before input: 0.2552422881126404
Norm after input: 0.5463913679122925
Norm after each mp layer: 3.0311739444732666
Norm after each mp layer: 23.295812606811523
Norm after each mp layer: 125.08804321289062
Norm before input: 0.2552422881126404
Norm after input: 0.546433687210083
Norm after each mp layer: 3.0316147804260254
Norm after each mp layer: 23.302711486816406
Norm after each mp layer: 125.05066680908203
Norm before input: 0.2552422881126404
Norm after input: 0.546433687210083
Norm after each mp layer: 3.0316147804260254
Norm after each mp layer: 23.302711486816406
Norm after each mp layer: 125.05066680908203
Norm before input: 0.2552422881126404
Norm after input: 0.5464246869087219
Norm after each mp layer: 3.031532049179077
Norm after each mp layer: 23.303855895996094
Norm after each mp layer: 125.04507446289062
Norm before input: 0.2552422881126404
Norm after input: 0.5464246869087219
Norm after each mp layer: 3.031532049179077
Norm after each mp layer: 23.303855895996094
Norm after each mp layer: 125.04507446289062
Norm before input: 0.2552422881126404
Norm after input: 0.5464403033256531
Norm after each mp layer: 3.031705379486084
Norm after each mp layer: 23.307796478271484
Norm after each mp layer: 125.02337646484375
Epoch: 410, Loss: 0.0040, Energy: 1767007.6250, Train: 99.92%, Valid: 75.80%, Test: 75.50%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5464403033256531
Norm after each mp layer: 3.031705379486084
Norm after each mp layer: 23.307796478271484
Norm after each mp layer: 125.02337646484375
Norm before input: 0.2552422881126404
Norm after input: 0.5464524030685425
Norm after each mp layer: 3.0318405628204346
Norm after each mp layer: 23.311328887939453
Norm after each mp layer: 125.00440216064453
Norm before input: 0.2552422881126404
Norm after input: 0.5464524030685425
Norm after each mp layer: 3.0318405628204346
Norm after each mp layer: 23.311328887939453
Norm after each mp layer: 125.00440216064453
Norm before input: 0.2552422881126404
Norm after input: 0.5464491844177246
Norm after each mp layer: 3.031817674636841
Norm after each mp layer: 23.313121795654297
Norm after each mp layer: 124.99571990966797
Norm before input: 0.2552422881126404
Norm after input: 0.5464491844177246
Norm after each mp layer: 3.031817674636841
Norm after each mp layer: 23.313121795654297
Norm after each mp layer: 124.99571990966797
Norm before input: 0.2552422881126404
Norm after input: 0.5464798212051392
Norm after each mp layer: 3.032142162322998
Norm after each mp layer: 23.3187198638916
Norm after each mp layer: 124.96581268310547
Norm before input: 0.2552422881126404
Norm after input: 0.5464798212051392
Norm after each mp layer: 3.032142162322998
Norm after each mp layer: 23.3187198638916
Norm after each mp layer: 124.96581268310547
Norm before input: 0.2552422881126404
Norm after input: 0.5464560389518738
Norm after each mp layer: 3.0319087505340576
Norm after each mp layer: 23.318161010742188
Norm after each mp layer: 124.97073364257812
Norm before input: 0.2552422881126404
Norm after input: 0.5464560389518738
Norm after each mp layer: 3.0319087505340576
Norm after each mp layer: 23.318161010742188
Norm after each mp layer: 124.97073364257812
Norm before input: 0.2552422881126404
Norm after input: 0.5465108752250671
Norm after each mp layer: 3.0324745178222656
Norm after each mp layer: 23.326425552368164
Norm after each mp layer: 124.92717742919922
Epoch: 415, Loss: 0.0040, Energy: 1747408.5000, Train: 99.92%, Valid: 75.80%, Test: 75.30%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5465108752250671
Norm after each mp layer: 3.0324745178222656
Norm after each mp layer: 23.326425552368164
Norm after each mp layer: 124.92717742919922
Norm before input: 0.2552422881126404
Norm after input: 0.5464572310447693
Norm after each mp layer: 3.0319321155548096
Norm after each mp layer: 23.322452545166016
Norm after each mp layer: 124.95198822021484
Norm before input: 0.2552422881126404
Norm after input: 0.5464572310447693
Norm after each mp layer: 3.0319321155548096
Norm after each mp layer: 23.322452545166016
Norm after each mp layer: 124.95198822021484
Norm before input: 0.2552422881126404
Norm after input: 0.5465497374534607
Norm after each mp layer: 3.0328845977783203
Norm after each mp layer: 23.33494758605957
Norm after each mp layer: 124.88394165039062
Norm before input: 0.2552422881126404
Norm after input: 0.5465497374534607
Norm after each mp layer: 3.0328845977783203
Norm after each mp layer: 23.33494758605957
Norm after each mp layer: 124.88394927978516
Norm before input: 0.2552422881126404
Norm after input: 0.5464469790458679
Norm after each mp layer: 3.03183913230896
Norm after each mp layer: 23.32538414001465
Norm after each mp layer: 124.93955993652344
Norm before input: 0.2552422881126404
Norm after input: 0.5464469790458679
Norm after each mp layer: 3.03183913230896
Norm after each mp layer: 23.32538414001465
Norm after each mp layer: 124.93955993652344
Norm before input: 0.2552422881126404
Norm after input: 0.5465987324714661
Norm after each mp layer: 3.033393383026123
Norm after each mp layer: 23.344505310058594
Norm after each mp layer: 124.83380126953125
Norm before input: 0.2552422881126404
Norm after input: 0.5465987324714661
Norm after each mp layer: 3.033393383026123
Norm after each mp layer: 23.344505310058594
Norm after each mp layer: 124.83380126953125
Norm before input: 0.2552422881126404
Norm after input: 0.546430766582489
Norm after each mp layer: 3.03167986869812
Norm after each mp layer: 23.327463150024414
Norm after each mp layer: 124.92958068847656
Epoch: 420, Loss: 0.0040, Energy: 1728664.3750, Train: 99.92%, Valid: 75.80%, Test: 75.40%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.546430766582489
Norm after each mp layer: 3.03167986869812
Norm after each mp layer: 23.327463150024414
Norm after each mp layer: 124.92958068847656
Norm before input: 0.2552422881126404
Norm after input: 0.5466390252113342
Norm after each mp layer: 3.033803939819336
Norm after each mp layer: 23.352825164794922
Norm after each mp layer: 124.78768920898438
Norm before input: 0.2552422881126404
Norm after input: 0.5466390252113342
Norm after each mp layer: 3.033803939819336
Norm after each mp layer: 23.352825164794922
Norm after each mp layer: 124.78768920898438
Norm before input: 0.2552422881126404
Norm after input: 0.5464386343955994
Norm after each mp layer: 3.03175687789917
Norm after each mp layer: 23.331954956054688
Norm after each mp layer: 124.90218353271484
Norm before input: 0.2552422881126404
Norm after input: 0.5464386343955994
Norm after each mp layer: 3.03175687789917
Norm after each mp layer: 23.331954956054688
Norm after each mp layer: 124.90218353271484
Norm before input: 0.2552422881126404
Norm after input: 0.5466361045837402
Norm after each mp layer: 3.0337626934051514
Norm after each mp layer: 23.35594367980957
Norm after each mp layer: 124.76553344726562
Norm before input: 0.2552422881126404
Norm after input: 0.5466361045837402
Norm after each mp layer: 3.0337626934051514
Norm after each mp layer: 23.35594367980957
Norm after each mp layer: 124.76553344726562
Norm before input: 0.2552422881126404
Norm after input: 0.546481192111969
Norm after each mp layer: 3.032177448272705
Norm after each mp layer: 23.340116500854492
Norm after each mp layer: 124.8490982055664
Norm before input: 0.2552422881126404
Norm after input: 0.546481192111969
Norm after each mp layer: 3.032177448272705
Norm after each mp layer: 23.340116500854492
Norm after each mp layer: 124.8490982055664
Norm before input: 0.2552422881126404
Norm after input: 0.5466095805168152
Norm after each mp layer: 3.03348970413208
Norm after each mp layer: 23.356353759765625
Norm after each mp layer: 124.7508544921875
Epoch: 425, Loss: 0.0039, Energy: 1710787.6250, Train: 99.92%, Valid: 75.80%, Test: 75.30%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5466095805168152
Norm after each mp layer: 3.03348970413208
Norm after each mp layer: 23.356353759765625
Norm after each mp layer: 124.7508544921875
Norm before input: 0.2552422881126404
Norm after input: 0.5465247631072998
Norm after each mp layer: 3.0326149463653564
Norm after each mp layer: 23.348466873168945
Norm after each mp layer: 124.78913879394531
Norm before input: 0.2552422881126404
Norm after input: 0.5465247631072998
Norm after each mp layer: 3.0326149463653564
Norm after each mp layer: 23.348466873168945
Norm after each mp layer: 124.78913879394531
Norm before input: 0.2552422881126404
Norm after input: 0.5465914011001587
Norm after each mp layer: 3.0332932472229004
Norm after each mp layer: 23.357778549194336
Norm after each mp layer: 124.729248046875
Norm before input: 0.2552422881126404
Norm after input: 0.5465914011001587
Norm after each mp layer: 3.0332932472229004
Norm after each mp layer: 23.357778549194336
Norm after each mp layer: 124.729248046875
Norm before input: 0.2552422881126404
Norm after input: 0.5465558767318726
Norm after each mp layer: 3.032925844192505
Norm after each mp layer: 23.355541229248047
Norm after each mp layer: 124.73480987548828
Norm before input: 0.2552422881126404
Norm after input: 0.5465558767318726
Norm after each mp layer: 3.032925844192505
Norm after each mp layer: 23.355541229248047
Norm after each mp layer: 124.73480987548828
Norm before input: 0.2552422881126404
Norm after input: 0.5465830564498901
Norm after each mp layer: 3.033195734024048
Norm after each mp layer: 23.360416412353516
Norm after each mp layer: 124.70097351074219
Norm before input: 0.2552422881126404
Norm after input: 0.5465830564498901
Norm after each mp layer: 3.033195734024048
Norm after each mp layer: 23.360416412353516
Norm after each mp layer: 124.70097351074219
Norm before input: 0.2552422881126404
Norm after input: 0.546579122543335
Norm after each mp layer: 3.033146381378174
Norm after each mp layer: 23.36177635192871
Norm after each mp layer: 124.68755340576172
Epoch: 430, Loss: 0.0038, Energy: 1691924.0000, Train: 99.92%, Valid: 75.80%, Test: 75.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.546579122543335
Norm after each mp layer: 3.033146381378174
Norm after each mp layer: 23.36177635192871
Norm after each mp layer: 124.68755340576172
Norm before input: 0.2552422881126404
Norm after input: 0.5465799570083618
Norm after each mp layer: 3.0331504344940186
Norm after each mp layer: 23.363698959350586
Norm after each mp layer: 124.67023468017578
Norm before input: 0.2552422881126404
Norm after input: 0.5465799570083618
Norm after each mp layer: 3.0331504344940186
Norm after each mp layer: 23.363698959350586
Norm after each mp layer: 124.67023468017578
Norm before input: 0.2552422881126404
Norm after input: 0.5465981960296631
Norm after each mp layer: 3.0333328247070312
Norm after each mp layer: 23.367591857910156
Norm after each mp layer: 124.64189147949219
Norm before input: 0.2552422881126404
Norm after input: 0.5465981960296631
Norm after each mp layer: 3.0333328247070312
Norm after each mp layer: 23.367591857910156
Norm after each mp layer: 124.64189147949219
Norm before input: 0.2552422881126404
Norm after input: 0.5465772151947021
Norm after each mp layer: 3.0331101417541504
Norm after each mp layer: 23.367015838623047
Norm after each mp layer: 124.64013671875
Norm before input: 0.2552422881126404
Norm after input: 0.5465772151947021
Norm after each mp layer: 3.0331101417541504
Norm after each mp layer: 23.367015838623047
Norm after each mp layer: 124.64013671875
Norm before input: 0.2552422881126404
Norm after input: 0.5466196537017822
Norm after each mp layer: 3.0335402488708496
Norm after each mp layer: 23.373638153076172
Norm after each mp layer: 124.59732818603516
Norm before input: 0.2552422881126404
Norm after input: 0.5466196537017822
Norm after each mp layer: 3.0335402488708496
Norm after each mp layer: 23.373638153076172
Norm after each mp layer: 124.59732818603516
Norm before input: 0.2552422881126404
Norm after input: 0.5465700030326843
Norm after each mp layer: 3.033022403717041
Norm after each mp layer: 23.3697452545166
Norm after each mp layer: 124.61537170410156
Epoch: 435, Loss: 0.0038, Energy: 1673755.0000, Train: 99.92%, Valid: 75.80%, Test: 75.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5465700030326843
Norm after each mp layer: 3.033022403717041
Norm after each mp layer: 23.3697452545166
Norm after each mp layer: 124.61537170410156
Norm before input: 0.2552422881126404
Norm after input: 0.5466483235359192
Norm after each mp layer: 3.0338165760040283
Norm after each mp layer: 23.38039207458496
Norm after each mp layer: 124.55077362060547
Norm before input: 0.2552422881126404
Norm after input: 0.5466483235359192
Norm after each mp layer: 3.0338165760040283
Norm after each mp layer: 23.380393981933594
Norm after each mp layer: 124.55077362060547
Norm before input: 0.2552422881126404
Norm after input: 0.5465541481971741
Norm after each mp layer: 3.0328330993652344
Norm after each mp layer: 23.371347427368164
Norm after each mp layer: 124.5999526977539
Norm before input: 0.2552422881126404
Norm after input: 0.5465541481971741
Norm after each mp layer: 3.0328330993652344
Norm after each mp layer: 23.371347427368164
Norm after each mp layer: 124.5999526977539
Norm before input: 0.2552422881126404
Norm after input: 0.5466880202293396
Norm after each mp layer: 3.03419828414917
Norm after each mp layer: 23.388277053833008
Norm after each mp layer: 124.49856567382812
Norm before input: 0.2552422881126404
Norm after input: 0.5466880202293396
Norm after each mp layer: 3.03419828414917
Norm after each mp layer: 23.388277053833008
Norm after each mp layer: 124.49856567382812
Norm before input: 0.2552422881126404
Norm after input: 0.546528697013855
Norm after each mp layer: 3.03254771232605
Norm after each mp layer: 23.371736526489258
Norm after each mp layer: 124.58936309814453
Norm before input: 0.2552422881126404
Norm after input: 0.546528697013855
Norm after each mp layer: 3.03254771232605
Norm after each mp layer: 23.371736526489258
Norm after each mp layer: 124.58936309814453
Norm before input: 0.2552422881126404
Norm after input: 0.5467262268066406
Norm after each mp layer: 3.0345609188079834
Norm after each mp layer: 23.395809173583984
Norm after each mp layer: 124.4459457397461
Epoch: 440, Loss: 0.0038, Energy: 1657112.8750, Train: 99.92%, Valid: 76.20%, Test: 75.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5467262268066406
Norm after each mp layer: 3.0345609188079834
Norm after each mp layer: 23.395809173583984
Norm after each mp layer: 124.4459457397461
Norm before input: 0.2552422881126404
Norm after input: 0.5465192198753357
Norm after each mp layer: 3.0324172973632812
Norm after each mp layer: 23.3736629486084
Norm after each mp layer: 124.5662841796875
Norm before input: 0.2552422881126404
Norm after input: 0.5465192198753357
Norm after each mp layer: 3.0324172973632812
Norm after each mp layer: 23.3736629486084
Norm after each mp layer: 124.5662841796875
Norm before input: 0.2552422881126404
Norm after input: 0.5467262268066406
Norm after each mp layer: 3.034522294998169
Norm after each mp layer: 23.398670196533203
Norm after each mp layer: 124.41490173339844
Norm before input: 0.2552422881126404
Norm after input: 0.5467262268066406
Norm after each mp layer: 3.034522294998169
Norm after each mp layer: 23.398670196533203
Norm after each mp layer: 124.41490173339844
Norm before input: 0.2552422881126404
Norm after input: 0.5465471744537354
Norm after each mp layer: 3.032662868499756
Norm after each mp layer: 23.379592895507812
Norm after each mp layer: 124.5151596069336
Norm before input: 0.2552422881126404
Norm after input: 0.5465471744537354
Norm after each mp layer: 3.032662868499756
Norm after each mp layer: 23.379592895507812
Norm after each mp layer: 124.5151596069336
Norm before input: 0.2552422881126404
Norm after input: 0.5466941595077515
Norm after each mp layer: 3.0341553688049316
Norm after each mp layer: 23.39774513244629
Norm after each mp layer: 124.39927673339844
Norm before input: 0.2552422881126404
Norm after input: 0.5466941595077515
Norm after each mp layer: 3.0341553688049316
Norm after each mp layer: 23.39774513244629
Norm after each mp layer: 124.39927673339844
Norm before input: 0.2552422881126404
Norm after input: 0.5465844869613647
Norm after each mp layer: 3.033006429672241
Norm after each mp layer: 23.386600494384766
Norm after each mp layer: 124.45238494873047
Epoch: 445, Loss: 0.0037, Energy: 1640021.6250, Train: 99.92%, Valid: 75.80%, Test: 75.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5465844869613647
Norm after each mp layer: 3.033006429672241
Norm after each mp layer: 23.386600494384766
Norm after each mp layer: 124.45238494873047
Norm before input: 0.2552422881126404
Norm after input: 0.5466664433479309
Norm after each mp layer: 3.033834457397461
Norm after each mp layer: 23.397415161132812
Norm after each mp layer: 124.37691497802734
Norm before input: 0.2552422881126404
Norm after input: 0.5466664433479309
Norm after each mp layer: 3.033834457397461
Norm after each mp layer: 23.397415161132812
Norm after each mp layer: 124.37691497802734
Norm before input: 0.2552422881126404
Norm after input: 0.5466102957725525
Norm after each mp layer: 3.0332396030426025
Norm after each mp layer: 23.39247703552246
Norm after each mp layer: 124.39325714111328
Norm before input: 0.2552422881126404
Norm after input: 0.5466102957725525
Norm after each mp layer: 3.0332396030426025
Norm after each mp layer: 23.39247703552246
Norm after each mp layer: 124.39325714111328
Norm before input: 0.2552422881126404
Norm after input: 0.5466508865356445
Norm after each mp layer: 3.0336382389068604
Norm after each mp layer: 23.39862060546875
Norm after each mp layer: 124.34555053710938
Norm before input: 0.2552422881126404
Norm after input: 0.5466508865356445
Norm after each mp layer: 3.0336382389068604
Norm after each mp layer: 23.39862060546875
Norm after each mp layer: 124.34555053710938
Norm before input: 0.2552422881126404
Norm after input: 0.5466262698173523
Norm after each mp layer: 3.033365249633789
Norm after each mp layer: 23.39733123779297
Norm after each mp layer: 124.34164428710938
Norm before input: 0.2552422881126404
Norm after input: 0.5466262698173523
Norm after each mp layer: 3.033365249633789
Norm after each mp layer: 23.39733123779297
Norm after each mp layer: 124.34164428710938
Norm before input: 0.2552422881126404
Norm after input: 0.5466433167457581
Norm after each mp layer: 3.0335235595703125
Norm after each mp layer: 23.40082359313965
Norm after each mp layer: 124.3097152709961
Epoch: 450, Loss: 0.0037, Energy: 1622963.3750, Train: 99.92%, Valid: 75.80%, Test: 75.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5466433167457581
Norm after each mp layer: 3.0335235595703125
Norm after each mp layer: 23.40082359313965
Norm after each mp layer: 124.3097152709961
Norm before input: 0.2552422881126404
Norm after input: 0.5466358065605164
Norm after each mp layer: 3.033428192138672
Norm after each mp layer: 23.401506423950195
Norm after each mp layer: 124.294677734375
Norm before input: 0.2552422881126404
Norm after input: 0.5466358065605164
Norm after each mp layer: 3.033428192138672
Norm after each mp layer: 23.401506423950195
Norm after each mp layer: 124.294677734375
Norm before input: 0.2552422881126404
Norm after input: 0.5466395020484924
Norm after each mp layer: 3.0334482192993164
Norm after each mp layer: 23.403470993041992
Norm after each mp layer: 124.2725830078125
Norm before input: 0.2552422881126404
Norm after input: 0.5466395020484924
Norm after each mp layer: 3.0334482192993164
Norm after each mp layer: 23.40346908569336
Norm after each mp layer: 124.2725830078125
Norm before input: 0.2552422881126404
Norm after input: 0.5466426014900208
Norm after each mp layer: 3.033463478088379
Norm after each mp layer: 23.405363082885742
Norm after each mp layer: 124.25115203857422
Norm before input: 0.2552422881126404
Norm after input: 0.5466426014900208
Norm after each mp layer: 3.033463478088379
Norm after each mp layer: 23.405363082885742
Norm after each mp layer: 124.25115203857422
Norm before input: 0.2552422881126404
Norm after input: 0.5466362833976746
Norm after each mp layer: 3.0333800315856934
Norm after each mp layer: 23.406150817871094
Norm after each mp layer: 124.23683166503906
Norm before input: 0.2552422881126404
Norm after input: 0.5466362833976746
Norm after each mp layer: 3.0333800315856934
Norm after each mp layer: 23.406150817871094
Norm after each mp layer: 124.23683166503906
Norm before input: 0.2552422881126404
Norm after input: 0.5466494560241699
Norm after each mp layer: 3.0334959030151367
Norm after each mp layer: 23.409156799316406
Norm after each mp layer: 124.21039581298828
Epoch: 455, Loss: 0.0036, Energy: 1606229.3750, Train: 99.92%, Valid: 76.00%, Test: 75.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5466494560241699
Norm after each mp layer: 3.0334959030151367
Norm after each mp layer: 23.409156799316406
Norm after each mp layer: 124.21039581298828
Norm before input: 0.2552422881126404
Norm after input: 0.5466321110725403
Norm after each mp layer: 3.033294200897217
Norm after each mp layer: 23.408632278442383
Norm after each mp layer: 124.2054214477539
Norm before input: 0.2552422881126404
Norm after input: 0.5466321110725403
Norm after each mp layer: 3.033294200897217
Norm after each mp layer: 23.408632278442383
Norm after each mp layer: 124.2054214477539
Norm before input: 0.2552422881126404
Norm after input: 0.5466594696044922
Norm after each mp layer: 3.033555030822754
Norm after each mp layer: 23.413238525390625
Norm after each mp layer: 124.17021942138672
Norm before input: 0.2552422881126404
Norm after input: 0.5466594696044922
Norm after each mp layer: 3.033555030822754
Norm after each mp layer: 23.413238525390625
Norm after each mp layer: 124.17021942138672
Norm before input: 0.2552422881126404
Norm after input: 0.5466232895851135
Norm after each mp layer: 3.033158779144287
Norm after each mp layer: 23.41053581237793
Norm after each mp layer: 124.17831420898438
Norm before input: 0.2552422881126404
Norm after input: 0.5466232895851135
Norm after each mp layer: 3.033158779144287
Norm after each mp layer: 23.41053581237793
Norm after each mp layer: 124.17831420898438
Norm before input: 0.2552422881126404
Norm after input: 0.5466763973236084
Norm after each mp layer: 3.0336873531341553
Norm after each mp layer: 23.41811180114746
Norm after each mp layer: 124.12533569335938
Norm before input: 0.2552422881126404
Norm after input: 0.5466763973236084
Norm after each mp layer: 3.0336873531341553
Norm after each mp layer: 23.41811180114746
Norm after each mp layer: 124.1253433227539
Norm before input: 0.2552422881126404
Norm after input: 0.5466047525405884
Norm after each mp layer: 3.0329246520996094
Norm after each mp layer: 23.411317825317383
Norm after each mp layer: 124.15717315673828
Epoch: 460, Loss: 0.0036, Energy: 1590506.3750, Train: 99.92%, Valid: 75.80%, Test: 75.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5466047525405884
Norm after each mp layer: 3.0329246520996094
Norm after each mp layer: 23.411317825317383
Norm after each mp layer: 124.15717315673828
Norm before input: 0.2552422881126404
Norm after input: 0.54670649766922
Norm after each mp layer: 3.0339596271514893
Norm after each mp layer: 23.424503326416016
Norm after each mp layer: 124.06986236572266
Norm before input: 0.2552422881126404
Norm after input: 0.54670649766922
Norm after each mp layer: 3.0339596271514893
Norm after each mp layer: 23.424503326416016
Norm after each mp layer: 124.0698471069336
Norm before input: 0.2552422881126404
Norm after input: 0.5465704202651978
Norm after each mp layer: 3.03252911567688
Norm after each mp layer: 23.410240173339844
Norm after each mp layer: 124.1453857421875
Norm before input: 0.2552422881126404
Norm after input: 0.5465704202651978
Norm after each mp layer: 3.03252911567688
Norm after each mp layer: 23.410240173339844
Norm after each mp layer: 124.1453857421875
Norm before input: 0.2552422881126404
Norm after input: 0.5467513203620911
Norm after each mp layer: 3.034376859664917
Norm after each mp layer: 23.432449340820312
Norm after each mp layer: 124.0040512084961
Norm before input: 0.2552422881126404
Norm after input: 0.5467513203620911
Norm after each mp layer: 3.034376859664917
Norm after each mp layer: 23.432449340820312
Norm after each mp layer: 124.0040512084961
Norm before input: 0.2552422881126404
Norm after input: 0.5465336441993713
Norm after each mp layer: 3.032097339630127
Norm after each mp layer: 23.408613204956055
Norm after each mp layer: 124.13450622558594
Norm before input: 0.2552422881126404
Norm after input: 0.5465336441993713
Norm after each mp layer: 3.032097339630127
Norm after each mp layer: 23.408613204956055
Norm after each mp layer: 124.13450622558594
Norm before input: 0.2552422881126404
Norm after input: 0.5467723608016968
Norm after each mp layer: 3.034532308578491
Norm after each mp layer: 23.437288284301758
Norm after each mp layer: 123.95344543457031
Epoch: 465, Loss: 0.0037, Energy: 1575666.2500, Train: 99.92%, Valid: 76.20%, Test: 75.20%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5467723608016968
Norm after each mp layer: 3.034532308578491
Norm after each mp layer: 23.437288284301758
Norm after each mp layer: 123.95344543457031
Norm before input: 0.2552422881126404
Norm after input: 0.5465413331985474
Norm after each mp layer: 3.0321121215820312
Norm after each mp layer: 23.411701202392578
Norm after each mp layer: 124.09012603759766
Norm before input: 0.2552422881126404
Norm after input: 0.5465413331985474
Norm after each mp layer: 3.0321121215820312
Norm after each mp layer: 23.411701202392578
Norm after each mp layer: 124.09012603759766
Norm before input: 0.2552422881126404
Norm after input: 0.5467325448989868
Norm after each mp layer: 3.034055471420288
Norm after each mp layer: 23.434778213500977
Norm after each mp layer: 123.93745422363281
Norm before input: 0.2552422881126404
Norm after input: 0.5467325448989868
Norm after each mp layer: 3.034055471420288
Norm after each mp layer: 23.434778213500977
Norm after each mp layer: 123.93745422363281
Norm before input: 0.2552422881126404
Norm after input: 0.54658043384552
Norm after each mp layer: 3.0324478149414062
Norm after each mp layer: 23.418251037597656
Norm after each mp layer: 124.01797485351562
Norm before input: 0.2552422881126404
Norm after input: 0.54658043384552
Norm after each mp layer: 3.0324478149414062
Norm after each mp layer: 23.418251037597656
Norm after each mp layer: 124.01797485351562
Norm before input: 0.2552422881126404
Norm after input: 0.5466843843460083
Norm after each mp layer: 3.033500909805298
Norm after each mp layer: 23.431371688842773
Norm after each mp layer: 123.91967010498047
Norm before input: 0.2552422881126404
Norm after input: 0.5466843843460083
Norm after each mp layer: 3.033500909805298
Norm after each mp layer: 23.431371688842773
Norm after each mp layer: 123.91967010498047
Norm before input: 0.2552422881126404
Norm after input: 0.5466063022613525
Norm after each mp layer: 3.0326614379882812
Norm after each mp layer: 23.423473358154297
Norm after each mp layer: 123.94758605957031
Epoch: 470, Loss: 0.0035, Energy: 1559683.5000, Train: 99.92%, Valid: 76.00%, Test: 75.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5466063022613525
Norm after each mp layer: 3.0326614379882812
Norm after each mp layer: 23.423473358154297
Norm after each mp layer: 123.94758605957031
Norm before input: 0.2552422881126404
Norm after input: 0.5466532707214355
Norm after each mp layer: 3.033113718032837
Norm after each mp layer: 23.4300594329834
Norm after each mp layer: 123.89028930664062
Norm before input: 0.2552422881126404
Norm after input: 0.5466532707214355
Norm after each mp layer: 3.033113718032837
Norm after each mp layer: 23.4300594329834
Norm after each mp layer: 123.89028930664062
Norm before input: 0.2552422881126404
Norm after input: 0.5466187000274658
Norm after each mp layer: 3.0327179431915283
Norm after each mp layer: 23.42722511291504
Norm after each mp layer: 123.89008331298828
Norm before input: 0.2552422881126404
Norm after input: 0.5466187000274658
Norm after each mp layer: 3.0327179431915283
Norm after each mp layer: 23.42722511291504
Norm after each mp layer: 123.89008331298828
Norm before input: 0.2552422881126404
Norm after input: 0.5466340780258179
Norm after each mp layer: 3.032841682434082
Norm after each mp layer: 23.43019676208496
Norm after each mp layer: 123.85530090332031
Norm before input: 0.2552422881126404
Norm after input: 0.5466340780258179
Norm after each mp layer: 3.032841682434082
Norm after each mp layer: 23.43019676208496
Norm after each mp layer: 123.85530090332031
Norm before input: 0.2552422881126404
Norm after input: 0.5466229319572449
Norm after each mp layer: 3.0326895713806152
Norm after each mp layer: 23.430116653442383
Norm after each mp layer: 123.83970642089844
Norm before input: 0.2552422881126404
Norm after input: 0.5466229319572449
Norm after each mp layer: 3.0326895713806152
Norm after each mp layer: 23.430116653442383
Norm after each mp layer: 123.83970642089844
Norm before input: 0.2552422881126404
Norm after input: 0.5466208457946777
Norm after each mp layer: 3.0326318740844727
Norm after each mp layer: 23.431089401245117
Norm after each mp layer: 123.8179702758789
Epoch: 475, Loss: 0.0035, Energy: 1544100.7500, Train: 99.92%, Valid: 76.00%, Test: 75.00%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5466208457946777
Norm after each mp layer: 3.0326318740844727
Norm after each mp layer: 23.431089401245117
Norm after each mp layer: 123.8179702758789
Norm before input: 0.2552422881126404
Norm after input: 0.5466246008872986
Norm after each mp layer: 3.032642364501953
Norm after each mp layer: 23.432777404785156
Norm after each mp layer: 123.79129028320312
Norm before input: 0.2552422881126404
Norm after input: 0.5466246008872986
Norm after each mp layer: 3.032642364501953
Norm after each mp layer: 23.432777404785156
Norm after each mp layer: 123.79129028320312
Norm before input: 0.2552422881126404
Norm after input: 0.546607494354248
Norm after each mp layer: 3.0324363708496094
Norm after each mp layer: 23.43202781677246
Norm after each mp layer: 123.77960968017578
Norm before input: 0.2552422881126404
Norm after input: 0.546607494354248
Norm after each mp layer: 3.0324363708496094
Norm after each mp layer: 23.43202781677246
Norm after each mp layer: 123.77960968017578
Norm before input: 0.2552422881126404
Norm after input: 0.5466277003288269
Norm after each mp layer: 3.0326144695281982
Norm after each mp layer: 23.435583114624023
Norm after each mp layer: 123.74351501464844
Norm before input: 0.2552422881126404
Norm after input: 0.5466277003288269
Norm after each mp layer: 3.0326144695281982
Norm after each mp layer: 23.435583114624023
Norm after each mp layer: 123.74351501464844
Norm before input: 0.2552422881126404
Norm after input: 0.5465924143791199
Norm after each mp layer: 3.032212495803833
Norm after each mp layer: 23.43266487121582
Norm after each mp layer: 123.74771118164062
Norm before input: 0.2552422881126404
Norm after input: 0.5465924143791199
Norm after each mp layer: 3.032212495803833
Norm after each mp layer: 23.43266487121582
Norm after each mp layer: 123.74771118164062
Norm before input: 0.2552422881126404
Norm after input: 0.5466374158859253
Norm after each mp layer: 3.032646894454956
Norm after each mp layer: 23.439064025878906
Norm after each mp layer: 123.69596862792969
Epoch: 480, Loss: 0.0035, Energy: 1529156.3750, Train: 99.92%, Valid: 76.20%, Test: 74.90%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5466374158859253
Norm after each mp layer: 3.032646894454956
Norm after each mp layer: 23.439064025878906
Norm after each mp layer: 123.69596862792969
Norm before input: 0.2552422881126404
Norm after input: 0.5465688705444336
Norm after each mp layer: 3.0318965911865234
Norm after each mp layer: 23.432235717773438
Norm after each mp layer: 123.72483825683594
Norm before input: 0.2552422881126404
Norm after input: 0.5465688705444336
Norm after each mp layer: 3.0318965911865234
Norm after each mp layer: 23.432235717773438
Norm after each mp layer: 123.72483825683594
Norm before input: 0.2552422881126404
Norm after input: 0.5466595888137817
Norm after each mp layer: 3.032803535461426
Norm after each mp layer: 23.443918228149414
Norm after each mp layer: 123.64163970947266
Norm before input: 0.2552422881126404
Norm after input: 0.5466595888137817
Norm after each mp layer: 3.032803535461426
Norm after each mp layer: 23.443918228149414
Norm after each mp layer: 123.64163970947266
Norm before input: 0.2552422881126404
Norm after input: 0.5465307831764221
Norm after each mp layer: 3.0314269065856934
Norm after each mp layer: 23.430065155029297
Norm after each mp layer: 123.7123794555664
Norm before input: 0.2552422881126404
Norm after input: 0.5465307831764221
Norm after each mp layer: 3.0314269065856934
Norm after each mp layer: 23.430065155029297
Norm after each mp layer: 123.7123794555664
Norm before input: 0.2552422881126404
Norm after input: 0.5466960072517395
Norm after each mp layer: 3.033114433288574
Norm after each mp layer: 23.450401306152344
Norm after each mp layer: 123.57452392578125
Norm before input: 0.2552422881126404
Norm after input: 0.5466960072517395
Norm after each mp layer: 3.033114433288574
Norm after each mp layer: 23.450401306152344
Norm after each mp layer: 123.57452392578125
Norm before input: 0.2552422881126404
Norm after input: 0.5464844703674316
Norm after each mp layer: 3.030877113342285
Norm after each mp layer: 23.426816940307617
Norm after each mp layer: 123.70191192626953
Epoch: 485, Loss: 0.0035, Energy: 1514649.8750, Train: 99.92%, Valid: 76.00%, Test: 75.10%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5464844703674316
Norm after each mp layer: 3.030877113342285
Norm after each mp layer: 23.426816940307617
Norm after each mp layer: 123.70191192626953
Norm before input: 0.2552422881126404
Norm after input: 0.5467182397842407
Norm after each mp layer: 3.033264398574829
Norm after each mp layer: 23.454927444458008
Norm after each mp layer: 123.5162124633789
Norm before input: 0.2552422881126404
Norm after input: 0.5467182397842407
Norm after each mp layer: 3.033264398574829
Norm after each mp layer: 23.454927444458008
Norm after each mp layer: 123.51619720458984
Norm before input: 0.2552422881126404
Norm after input: 0.546476423740387
Norm after each mp layer: 3.0307061672210693
Norm after each mp layer: 23.427595138549805
Norm after each mp layer: 123.6632308959961
Norm before input: 0.2552422881126404
Norm after input: 0.546476423740387
Norm after each mp layer: 3.0307061672210693
Norm after each mp layer: 23.427595138549805
Norm after each mp layer: 123.6632308959961
Norm before input: 0.2552422881126404
Norm after input: 0.5466808080673218
Norm after each mp layer: 3.0327839851379395
Norm after each mp layer: 23.452136993408203
Norm after each mp layer: 123.4954833984375
Norm before input: 0.2552422881126404
Norm after input: 0.5466808080673218
Norm after each mp layer: 3.0327839851379395
Norm after each mp layer: 23.452136993408203
Norm after each mp layer: 123.4954833984375
Norm before input: 0.2552422881126404
Norm after input: 0.5465075373649597
Norm after each mp layer: 3.0309336185455322
Norm after each mp layer: 23.432701110839844
Norm after each mp layer: 123.59258270263672
Norm before input: 0.2552422881126404
Norm after input: 0.5465075373649597
Norm after each mp layer: 3.0309336185455322
Norm after each mp layer: 23.432701110839844
Norm after each mp layer: 123.59258270263672
Norm before input: 0.2552422881126404
Norm after input: 0.5466273427009583
Norm after each mp layer: 3.0321435928344727
Norm after each mp layer: 23.447494506835938
Norm after each mp layer: 123.47929382324219
Epoch: 490, Loss: 0.0034, Energy: 1501296.6250, Train: 99.92%, Valid: 76.20%, Test: 74.90%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5466273427009583
Norm after each mp layer: 3.0321435928344727
Norm after each mp layer: 23.447494506835938
Norm after each mp layer: 123.47929382324219
Norm before input: 0.2552422881126404
Norm after input: 0.5465304255485535
Norm after each mp layer: 3.031095266342163
Norm after each mp layer: 23.437068939208984
Norm after each mp layer: 123.51918029785156
Norm before input: 0.2552422881126404
Norm after input: 0.5465304255485535
Norm after each mp layer: 3.031095266342163
Norm after each mp layer: 23.437068939208984
Norm after each mp layer: 123.51918029785156
Norm before input: 0.2552422881126404
Norm after input: 0.5465897917747498
Norm after each mp layer: 3.031675100326538
Norm after each mp layer: 23.444929122924805
Norm after each mp layer: 123.44792938232422
Norm before input: 0.2552422881126404
Norm after input: 0.5465897917747498
Norm after each mp layer: 3.031675100326538
Norm after each mp layer: 23.444929122924805
Norm after each mp layer: 123.44794464111328
Norm before input: 0.2552422881126404
Norm after input: 0.5465380549430847
Norm after each mp layer: 3.031094551086426
Norm after each mp layer: 23.43985366821289
Norm after each mp layer: 123.45660400390625
Norm before input: 0.2552422881126404
Norm after input: 0.5465380549430847
Norm after each mp layer: 3.031094551086426
Norm after each mp layer: 23.43985366821289
Norm after each mp layer: 123.45660400390625
Norm before input: 0.2552422881126404
Norm after input: 0.5465667247772217
Norm after each mp layer: 3.031350612640381
Norm after each mp layer: 23.444181442260742
Norm after each mp layer: 123.40846252441406
Norm before input: 0.2552422881126404
Norm after input: 0.5465667247772217
Norm after each mp layer: 3.031350612640381
Norm after each mp layer: 23.444181442260742
Norm after each mp layer: 123.40846252441406
Norm before input: 0.2552422881126404
Norm after input: 0.5465356111526489
Norm after each mp layer: 3.0309808254241943
Norm after each mp layer: 23.441532135009766
Norm after each mp layer: 123.4040756225586
Epoch: 495, Loss: 0.0034, Energy: 1486158.2500, Train: 99.92%, Valid: 76.00%, Test: 74.90%, Best Valid: 78.60%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5465356111526489
Norm after each mp layer: 3.0309808254241943
Norm after each mp layer: 23.441532135009766
Norm after each mp layer: 123.4040756225586
Norm before input: 0.2552422881126404
Norm after input: 0.5465524792671204
Norm after each mp layer: 3.0311121940612793
Norm after each mp layer: 23.444515228271484
Norm after each mp layer: 123.36581420898438
Norm before input: 0.2552422881126404
Norm after input: 0.5465524792671204
Norm after each mp layer: 3.0311121940612793
Norm after each mp layer: 23.444515228271484
Norm after each mp layer: 123.36581420898438
Norm before input: 0.2552422881126404
Norm after input: 0.5465269088745117
Norm after each mp layer: 3.0308034420013428
Norm after each mp layer: 23.442543029785156
Norm after each mp layer: 123.35780334472656
Norm before input: 0.2552422881126404
Norm after input: 0.5465269088745117
Norm after each mp layer: 3.0308034420013428
Norm after each mp layer: 23.442541122436523
Norm after each mp layer: 123.35780334472656
Norm before input: 0.2552422881126404
Norm after input: 0.5465430617332458
Norm after each mp layer: 3.0309340953826904
Norm after each mp layer: 23.445459365844727
Norm after each mp layer: 123.319580078125
Norm before input: 0.2552422881126404
Norm after input: 0.5465430617332458
Norm after each mp layer: 3.0309340953826904
Norm after each mp layer: 23.445459365844727
Norm after each mp layer: 123.319580078125
Norm before input: 0.2552422881126404
Norm after input: 0.5465123653411865
Norm after each mp layer: 3.03057599067688
Norm after each mp layer: 23.442895889282227
Norm after each mp layer: 123.31575012207031
train_accuracy_list: [0.28228476821192056, 0.28228476821192056, 0.16225165562913907, 0.16225165562913907, 0.28228476821192056, 0.1630794701986755, 0.12748344370860928, 0.16225165562913907, 0.28228476821192056, 0.28228476821192056, 0.326158940397351, 0.16225165562913907, 0.2185430463576159, 0.29221854304635764, 0.25496688741721857, 0.22350993377483444, 0.3758278145695364, 0.3783112582781457, 0.37665562913907286, 0.3956953642384106, 0.43625827814569534, 0.4544701986754967, 0.40397350993377484, 0.37251655629139074, 0.4420529801324503, 0.4644039735099338, 0.47102649006622516, 0.4693708609271523, 0.4693708609271523, 0.4776490066225166, 0.48509933774834435, 0.5620860927152318, 0.5587748344370861, 0.5496688741721855, 0.5206953642384106, 0.5529801324503312, 0.5711920529801324, 0.5927152317880795, 0.6341059602649006, 0.6415562913907285, 0.6349337748344371, 0.6465231788079471, 0.6365894039735099, 0.6034768211920529, 0.6208609271523179, 0.6713576158940397, 0.6945364238410596, 0.668046357615894, 0.6945364238410596, 0.6978476821192053, 0.7110927152317881, 0.7243377483443708, 0.7408940397350994, 0.7549668874172185, 0.7508278145695364, 0.7541390728476821, 0.7615894039735099, 0.7649006622516556, 0.7748344370860927, 0.7913907284768212, 0.7864238410596026, 0.8004966887417219, 0.7814569536423841, 0.7690397350993378, 0.8112582781456954, 0.8245033112582781, 0.8062913907284768, 0.8211920529801324, 0.8344370860927153, 0.8336092715231788, 0.8377483443708609, 0.8485099337748344, 0.8427152317880795, 0.8584437086092715, 0.8650662251655629, 0.8493377483443708, 0.875, 0.8733443708609272, 0.8650662251655629, 0.8799668874172185, 0.8824503311258278, 0.8832781456953642, 0.8915562913907285, 0.8874172185430463, 0.9031456953642384, 0.9064569536423841, 0.9139072847682119, 0.9188741721854304, 0.918046357615894, 0.9263245033112583, 0.9370860927152318, 0.9370860927152318, 0.9312913907284768, 0.9288079470198676, 0.9403973509933775, 0.9379139072847682, 0.9445364238410596, 0.9586092715231788, 0.9420529801324503, 0.9470198675496688, 0.956953642384106, 0.9519867549668874, 0.9619205298013245, 0.9644039735099338, 0.9586092715231788, 0.9701986754966887, 0.9652317880794702, 0.9594370860927153, 0.9660596026490066, 0.9718543046357616, 0.9710264900662252, 0.9627483443708609, 0.9677152317880795, 0.9718543046357616, 0.9652317880794702, 0.9751655629139073, 0.9751655629139073, 0.9735099337748344, 0.9660596026490066, 0.9776490066225165, 0.9768211920529801, 0.9768211920529801, 0.9751655629139073, 0.9776490066225165, 0.9776490066225165, 0.9784768211920529, 0.9776490066225165, 0.9801324503311258, 0.9801324503311258, 0.9776490066225165, 0.9817880794701986, 0.9826158940397351, 0.9850993377483444, 0.9834437086092715, 0.9867549668874173, 0.9826158940397351, 0.984271523178808, 0.9859271523178808, 0.9867549668874173, 0.9867549668874173, 0.9884105960264901, 0.9908940397350994, 0.9925496688741722, 0.9908940397350994, 0.9925496688741722, 0.9917218543046358, 0.9925496688741722, 0.9917218543046358, 0.9917218543046358, 0.9933774834437086, 0.9925496688741722, 0.9917218543046358, 0.9917218543046358, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9925496688741722, 0.9925496688741722, 0.9933774834437086, 0.9925496688741722, 0.9933774834437086, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9950331125827815, 0.9950331125827815, 0.9942052980132451, 0.9950331125827815, 0.9950331125827815, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9983443708609272, 0.9966887417218543, 0.9966887417218543, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9975165562913907, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636]
valid_accuracy_list: [0.316, 0.316, 0.156, 0.162, 0.316, 0.156, 0.138, 0.154, 0.316, 0.316, 0.352, 0.162, 0.204, 0.272, 0.236, 0.242, 0.392, 0.404, 0.404, 0.412, 0.442, 0.458, 0.392, 0.338, 0.39, 0.45, 0.462, 0.466, 0.464, 0.48, 0.49, 0.5, 0.496, 0.512, 0.506, 0.522, 0.534, 0.546, 0.574, 0.572, 0.558, 0.582, 0.584, 0.56, 0.558, 0.588, 0.604, 0.582, 0.6, 0.616, 0.628, 0.628, 0.618, 0.634, 0.626, 0.632, 0.626, 0.634, 0.642, 0.644, 0.66, 0.656, 0.66, 0.636, 0.666, 0.68, 0.66, 0.67, 0.678, 0.678, 0.684, 0.684, 0.694, 0.69, 0.692, 0.694, 0.696, 0.704, 0.708, 0.712, 0.716, 0.718, 0.714, 0.724, 0.72, 0.728, 0.732, 0.722, 0.736, 0.726, 0.738, 0.752, 0.736, 0.748, 0.742, 0.746, 0.754, 0.762, 0.746, 0.756, 0.762, 0.75, 0.77, 0.766, 0.756, 0.766, 0.774, 0.758, 0.758, 0.774, 0.77, 0.764, 0.776, 0.778, 0.756, 0.774, 0.782, 0.772, 0.762, 0.778, 0.786, 0.772, 0.77, 0.778, 0.778, 0.774, 0.776, 0.782, 0.776, 0.766, 0.78, 0.784, 0.774, 0.77, 0.774, 0.78, 0.778, 0.774, 0.77, 0.776, 0.778, 0.78, 0.78, 0.78, 0.774, 0.776, 0.774, 0.78, 0.778, 0.77, 0.77, 0.776, 0.776, 0.774, 0.768, 0.77, 0.772, 0.768, 0.766, 0.768, 0.768, 0.764, 0.762, 0.764, 0.764, 0.762, 0.764, 0.764, 0.764, 0.766, 0.766, 0.766, 0.768, 0.766, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.768, 0.768, 0.768, 0.766, 0.764, 0.766, 0.76, 0.762, 0.76, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.756, 0.756, 0.756, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.758, 0.76, 0.758, 0.76, 0.758, 0.76, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.762, 0.758, 0.762, 0.758, 0.762, 0.758, 0.762, 0.758, 0.76, 0.758, 0.76, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.76, 0.758, 0.76, 0.758, 0.762, 0.758, 0.762, 0.758, 0.762, 0.758, 0.762, 0.758, 0.762, 0.76, 0.762, 0.76, 0.762, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.762, 0.76, 0.762, 0.76, 0.762, 0.76, 0.762, 0.76, 0.762, 0.76, 0.762, 0.76, 0.762, 0.76, 0.762, 0.76, 0.762, 0.76, 0.762, 0.76, 0.762, 0.76]
test_accuracy_list: [0.319, 0.319, 0.144, 0.149, 0.319, 0.144, 0.122, 0.145, 0.319, 0.319, 0.349, 0.149, 0.188, 0.266, 0.219, 0.233, 0.397, 0.394, 0.391, 0.415, 0.456, 0.461, 0.397, 0.347, 0.419, 0.462, 0.467, 0.473, 0.462, 0.472, 0.478, 0.506, 0.499, 0.515, 0.512, 0.525, 0.547, 0.56, 0.575, 0.57, 0.571, 0.592, 0.604, 0.569, 0.574, 0.601, 0.601, 0.588, 0.592, 0.596, 0.604, 0.611, 0.625, 0.619, 0.621, 0.618, 0.626, 0.626, 0.636, 0.641, 0.643, 0.634, 0.627, 0.626, 0.641, 0.651, 0.657, 0.639, 0.648, 0.661, 0.65, 0.665, 0.669, 0.664, 0.665, 0.676, 0.669, 0.68, 0.689, 0.683, 0.694, 0.698, 0.685, 0.704, 0.696, 0.704, 0.713, 0.708, 0.724, 0.718, 0.733, 0.735, 0.736, 0.741, 0.748, 0.746, 0.755, 0.756, 0.754, 0.752, 0.76, 0.763, 0.763, 0.759, 0.77, 0.765, 0.762, 0.768, 0.769, 0.763, 0.767, 0.767, 0.763, 0.766, 0.771, 0.77, 0.763, 0.769, 0.763, 0.76, 0.763, 0.771, 0.77, 0.767, 0.763, 0.767, 0.768, 0.769, 0.774, 0.774, 0.776, 0.771, 0.775, 0.762, 0.77, 0.77, 0.774, 0.773, 0.775, 0.773, 0.773, 0.769, 0.771, 0.775, 0.774, 0.774, 0.774, 0.775, 0.774, 0.77, 0.771, 0.774, 0.775, 0.774, 0.77, 0.772, 0.772, 0.773, 0.77, 0.771, 0.77, 0.769, 0.77, 0.769, 0.769, 0.766, 0.765, 0.766, 0.766, 0.765, 0.763, 0.764, 0.766, 0.766, 0.764, 0.763, 0.764, 0.764, 0.76, 0.761, 0.76, 0.76, 0.759, 0.759, 0.758, 0.759, 0.759, 0.759, 0.758, 0.759, 0.759, 0.76, 0.759, 0.759, 0.761, 0.761, 0.761, 0.761, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.763, 0.764, 0.763, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.765, 0.765, 0.764, 0.765, 0.765, 0.765, 0.765, 0.765, 0.764, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.763, 0.763, 0.763, 0.762, 0.762, 0.762, 0.762, 0.763, 0.762, 0.762, 0.762, 0.762, 0.763, 0.762, 0.763, 0.763, 0.763, 0.764, 0.764, 0.764, 0.764, 0.764, 0.765, 0.764, 0.764, 0.763, 0.764, 0.763, 0.765, 0.763, 0.765, 0.763, 0.765, 0.765, 0.765, 0.765, 0.764, 0.764, 0.762, 0.762, 0.762, 0.762, 0.761, 0.761, 0.762, 0.761, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.761, 0.761, 0.761, 0.761, 0.761, 0.76, 0.76, 0.76, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.76, 0.76, 0.76, 0.76, 0.76, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.76, 0.761, 0.76, 0.758, 0.757, 0.757, 0.757, 0.757, 0.757, 0.758, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.757, 0.756, 0.757, 0.756, 0.756, 0.755, 0.756, 0.755, 0.756, 0.756, 0.756, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.756, 0.757, 0.756, 0.756, 0.755, 0.756, 0.755, 0.756, 0.755, 0.756, 0.755, 0.756, 0.755, 0.755, 0.756, 0.755, 0.756, 0.755, 0.756, 0.755, 0.756, 0.754, 0.755, 0.754, 0.755, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.755, 0.754, 0.754, 0.754, 0.756, 0.753, 0.756, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.753, 0.754, 0.753, 0.753, 0.753, 0.754, 0.753, 0.754, 0.753, 0.754, 0.753, 0.753, 0.753, 0.753, 0.752, 0.751, 0.752, 0.751, 0.751, 0.751, 0.751, 0.752, 0.751, 0.752, 0.752, 0.752, 0.753, 0.751, 0.752, 0.751, 0.752, 0.751, 0.751, 0.751, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.751, 0.749, 0.752, 0.749, 0.752, 0.749, 0.75, 0.749, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.749, 0.75, 0.749, 0.75, 0.75, 0.751, 0.751, 0.75, 0.75, 0.751, 0.749, 0.75, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749]
best validation: 0.786
best test: 0.776
Experiment run
dataset: cora
num_mp_layers: 3
mp_hidden_dim: 4000
optimizer_lr: 0.001
loss_func: CrossEntropyLoss
total_epoch: 500
energy_lambda: 1e-08
Norm before input: 0.2552422881126404
Norm after input: 0.9947524070739746
Norm after each mp layer: 0.6566787362098694
Norm after each mp layer: 0.6222918033599854
Norm after each mp layer: 0.6184736490249634
Norm before input: 0.2552422881126404
Norm after input: 1.0018221139907837
Norm after each mp layer: 1.8246667385101318
Norm after each mp layer: 1.639543056488037
Norm after each mp layer: 1.6299399137496948
Norm before input: 0.2552422881126404
Norm after input: 1.0018221139907837
Norm after each mp layer: 1.8246667385101318
Norm after each mp layer: 1.639543056488037
Norm after each mp layer: 1.6299399137496948
Norm before input: 0.2552422881126404
Norm after input: 1.054318904876709
Norm after each mp layer: 2.7995738983154297
Norm after each mp layer: 5.275686264038086
Norm after each mp layer: 14.31612491607666
Norm before input: 0.2552422881126404
Norm after input: 1.054318904876709
Norm after each mp layer: 2.7995738983154297
Norm after each mp layer: 5.275686264038086
Norm after each mp layer: 14.31612491607666
Norm before input: 0.2552422881126404
Norm after input: 1.1246620416641235
Norm after each mp layer: 4.17412805557251
Norm after each mp layer: 12.078486442565918
Norm after each mp layer: 44.79237365722656
Norm before input: 0.2552422881126404
Norm after input: 1.1246620416641235
Norm after each mp layer: 4.17412805557251
Norm after each mp layer: 12.078486442565918
Norm after each mp layer: 44.79237365722656
Norm before input: 0.2552422881126404
Norm after input: 1.1184871196746826
Norm after each mp layer: 4.358199119567871
Norm after each mp layer: 13.652045249938965
Norm after each mp layer: 52.38709259033203
Norm before input: 0.2552422881126404
Norm after input: 1.1184871196746826
Norm after each mp layer: 4.358199119567871
Norm after each mp layer: 13.652045249938965
Norm after each mp layer: 52.38709259033203
Norm before input: 0.2552422881126404
Norm after input: 1.0753118991851807
Norm after each mp layer: 4.000727653503418
Norm after each mp layer: 10.615105628967285
Norm after each mp layer: 38.52912139892578
Epoch: 05, Loss: 3.9554, Energy: 416936.6562, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 1.0753118991851807
Norm after each mp layer: 4.000727653503418
Norm after each mp layer: 10.615105628967285
Norm after each mp layer: 38.52912139892578
Norm before input: 0.2552422881126404
Norm after input: 1.015260934829712
Norm after each mp layer: 3.5883500576019287
Norm after each mp layer: 7.9358439445495605
Norm after each mp layer: 17.681640625
Norm before input: 0.2552422881126404
Norm after input: 1.015260934829712
Norm after each mp layer: 3.5883500576019287
Norm after each mp layer: 7.9358439445495605
Norm after each mp layer: 17.681640625
Norm before input: 0.2552422881126404
Norm after input: 0.9769848585128784
Norm after each mp layer: 3.404332160949707
Norm after each mp layer: 8.588080406188965
Norm after each mp layer: 23.568655014038086
Norm before input: 0.2552422881126404
Norm after input: 0.9769848585128784
Norm after each mp layer: 3.404332160949707
Norm after each mp layer: 8.588080406188965
Norm after each mp layer: 23.568655014038086
Norm before input: 0.2552422881126404
Norm after input: 0.9444510340690613
Norm after each mp layer: 3.301267623901367
Norm after each mp layer: 9.590017318725586
Norm after each mp layer: 26.295188903808594
Norm before input: 0.2552422881126404
Norm after input: 0.9444510340690613
Norm after each mp layer: 3.301267623901367
Norm after each mp layer: 9.590017318725586
Norm after each mp layer: 26.295188903808594
Norm before input: 0.2552422881126404
Norm after input: 0.9070664048194885
Norm after each mp layer: 3.1242635250091553
Norm after each mp layer: 9.539223670959473
Norm after each mp layer: 31.312782287597656
Norm before input: 0.2552422881126404
Norm after input: 0.9070664048194885
Norm after each mp layer: 3.1242635250091553
Norm after each mp layer: 9.539223670959473
Norm after each mp layer: 31.312782287597656
Norm before input: 0.2552422881126404
Norm after input: 0.8700322508811951
Norm after each mp layer: 2.931838274002075
Norm after each mp layer: 9.397398948669434
Norm after each mp layer: 33.12223815917969
Epoch: 10, Loss: 2.3446, Energy: 136704.5000, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.8700322508811951
Norm after each mp layer: 2.931838274002075
Norm after each mp layer: 9.397398948669434
Norm after each mp layer: 33.12223815917969
Norm before input: 0.2552422881126404
Norm after input: 0.8285218477249146
Norm after each mp layer: 2.675774574279785
Norm after each mp layer: 8.741368293762207
Norm after each mp layer: 31.45713996887207
Norm before input: 0.2552422881126404
Norm after input: 0.8285218477249146
Norm after each mp layer: 2.675774574279785
Norm after each mp layer: 8.741368293762207
Norm after each mp layer: 31.45713996887207
Norm before input: 0.2552422881126404
Norm after input: 0.7899544835090637
Norm after each mp layer: 2.4507088661193848
Norm after each mp layer: 8.21245288848877
Norm after each mp layer: 29.209131240844727
Norm before input: 0.2552422881126404
Norm after input: 0.7899544835090637
Norm after each mp layer: 2.4507088661193848
Norm after each mp layer: 8.21245288848877
Norm after each mp layer: 29.20913314819336
Norm before input: 0.2552422881126404
Norm after input: 0.7454478144645691
Norm after each mp layer: 2.165947437286377
Norm after each mp layer: 7.1545233726501465
Norm after each mp layer: 26.411907196044922
Norm before input: 0.2552422881126404
Norm after input: 0.7454478144645691
Norm after each mp layer: 2.165947437286377
Norm after each mp layer: 7.1545233726501465
Norm after each mp layer: 26.411903381347656
Norm before input: 0.2552422881126404
Norm after input: 0.7049221992492676
Norm after each mp layer: 1.9067394733428955
Norm after each mp layer: 6.187184810638428
Norm after each mp layer: 23.9352970123291
Norm before input: 0.2552422881126404
Norm after input: 0.7049221992492676
Norm after each mp layer: 1.9067394733428955
Norm after each mp layer: 6.187184810638428
Norm after each mp layer: 23.9352970123291
Norm before input: 0.2552422881126404
Norm after input: 0.6737143397331238
Norm after each mp layer: 1.734493613243103
Norm after each mp layer: 5.585045337677002
Norm after each mp layer: 21.89496421813965
Epoch: 15, Loss: 1.9883, Energy: 75837.9531, Train: 24.42%, Valid: 22.00%, Test: 21.20%, Best Valid: 35.00%, Best Test: 34.10%
Norm before input: 0.2552422881126404
Norm after input: 0.6737143397331238
Norm after each mp layer: 1.734493613243103
Norm after each mp layer: 5.585045337677002
Norm after each mp layer: 21.89496421813965
Norm before input: 0.2552422881126404
Norm after input: 0.6452664732933044
Norm after each mp layer: 1.5807546377182007
Norm after each mp layer: 5.03884744644165
Norm after each mp layer: 20.139314651489258
Norm before input: 0.2552422881126404
Norm after input: 0.6452664732933044
Norm after each mp layer: 1.5807546377182007
Norm after each mp layer: 5.03884744644165
Norm after each mp layer: 20.139314651489258
Norm before input: 0.2552422881126404
Norm after input: 0.6167340874671936
Norm after each mp layer: 1.415996789932251
Norm after each mp layer: 4.4424896240234375
Norm after each mp layer: 18.222461700439453
Norm before input: 0.2552422881126404
Norm after input: 0.6167340874671936
Norm after each mp layer: 1.415996789932251
Norm after each mp layer: 4.4424896240234375
Norm after each mp layer: 18.222461700439453
Norm before input: 0.2552422881126404
Norm after input: 0.5933011770248413
Norm after each mp layer: 1.287160873413086
Norm after each mp layer: 4.017289161682129
Norm after each mp layer: 16.840492248535156
Norm before input: 0.2552422881126404
Norm after input: 0.5933011770248413
Norm after each mp layer: 1.287160873413086
Norm after each mp layer: 4.017289161682129
Norm after each mp layer: 16.840492248535156
Norm before input: 0.2552422881126404
Norm after input: 0.5786209106445312
Norm after each mp layer: 1.236201286315918
Norm after each mp layer: 3.900972604751587
Norm after each mp layer: 16.628068923950195
Norm before input: 0.2552422881126404
Norm after input: 0.5786209106445312
Norm after each mp layer: 1.236201286315918
Norm after each mp layer: 3.900972604751587
Norm after each mp layer: 16.628068923950195
Norm before input: 0.2552422881126404
Norm after input: 0.5705357789993286
Norm after each mp layer: 1.2406615018844604
Norm after each mp layer: 4.019590377807617
Norm after each mp layer: 17.48223114013672
Epoch: 20, Loss: 1.6100, Energy: 34006.4180, Train: 41.56%, Valid: 42.40%, Test: 44.20%, Best Valid: 42.40%, Best Test: 44.20%
Norm before input: 0.2552422881126404
Norm after input: 0.5705357789993286
Norm after each mp layer: 1.2406615018844604
Norm after each mp layer: 4.019590377807617
Norm after each mp layer: 17.48223114013672
Norm before input: 0.2552422881126404
Norm after input: 0.5644089579582214
Norm after each mp layer: 1.256758689880371
Norm after each mp layer: 4.215203762054443
Norm after each mp layer: 18.73978614807129
Norm before input: 0.2552422881126404
Norm after input: 0.5644089579582214
Norm after each mp layer: 1.256758689880371
Norm after each mp layer: 4.215203762054443
Norm after each mp layer: 18.73978614807129
Norm before input: 0.2552422881126404
Norm after input: 0.5574293732643127
Norm after each mp layer: 1.2594572305679321
Norm after each mp layer: 4.360208511352539
Norm after each mp layer: 19.728254318237305
Norm before input: 0.2552422881126404
Norm after input: 0.5574293732643127
Norm after each mp layer: 1.2594572305679321
Norm after each mp layer: 4.360208511352539
Norm after each mp layer: 19.728254318237305
Norm before input: 0.2552422881126404
Norm after input: 0.5510925650596619
Norm after each mp layer: 1.265832543373108
Norm after each mp layer: 4.510524272918701
Norm after each mp layer: 20.720703125
Norm before input: 0.2552422881126404
Norm after input: 0.5510925650596619
Norm after each mp layer: 1.2658326625823975
Norm after each mp layer: 4.510524272918701
Norm after each mp layer: 20.720703125
Norm before input: 0.2552422881126404
Norm after input: 0.546840488910675
Norm after each mp layer: 1.288613200187683
Norm after each mp layer: 4.716184139251709
Norm after each mp layer: 22.09368133544922
Norm before input: 0.2552422881126404
Norm after input: 0.546840488910675
Norm after each mp layer: 1.288613200187683
Norm after each mp layer: 4.716184139251709
Norm after each mp layer: 22.09368133544922
Norm before input: 0.2552422881126404
Norm after input: 0.5447372198104858
Norm after each mp layer: 1.325898289680481
Norm after each mp layer: 4.983579158782959
Norm after each mp layer: 23.977127075195312
Epoch: 25, Loss: 1.3618, Energy: 59146.6094, Train: 46.44%, Valid: 44.40%, Test: 46.10%, Best Valid: 46.00%, Best Test: 46.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5447372198104858
Norm after each mp layer: 1.325898289680481
Norm after each mp layer: 4.983579158782959
Norm after each mp layer: 23.977127075195312
Norm before input: 0.2552422881126404
Norm after input: 0.5431987047195435
Norm after each mp layer: 1.3658946752548218
Norm after each mp layer: 5.273682594299316
Norm after each mp layer: 26.08194923400879
Norm before input: 0.2552422881126404
Norm after input: 0.5431987047195435
Norm after each mp layer: 1.3658946752548218
Norm after each mp layer: 5.273682594299316
Norm after each mp layer: 26.08194923400879
Norm before input: 0.2552422881126404
Norm after input: 0.5392230749130249
Norm after each mp layer: 1.3840725421905518
Norm after each mp layer: 5.45286750793457
Norm after each mp layer: 27.46898078918457
Norm before input: 0.2552422881126404
Norm after input: 0.5392230749130249
Norm after each mp layer: 1.3840725421905518
Norm after each mp layer: 5.45286750793457
Norm after each mp layer: 27.46898078918457
Norm before input: 0.2552422881126404
Norm after input: 0.5332277417182922
Norm after each mp layer: 1.382925271987915
Norm after each mp layer: 5.516386032104492
Norm after each mp layer: 27.999359130859375
Norm before input: 0.2552422881126404
Norm after input: 0.5332277417182922
Norm after each mp layer: 1.382925271987915
Norm after each mp layer: 5.516386032104492
Norm after each mp layer: 27.999359130859375
Norm before input: 0.2552422881126404
Norm after input: 0.529434323310852
Norm after each mp layer: 1.3934671878814697
Norm after each mp layer: 5.63006067276001
Norm after each mp layer: 28.78000259399414
Norm before input: 0.2552422881126404
Norm after input: 0.529434323310852
Norm after each mp layer: 1.3934671878814697
Norm after each mp layer: 5.63006067276001
Norm after each mp layer: 28.78000259399414
Norm before input: 0.2552422881126404
Norm after input: 0.5296842455863953
Norm after each mp layer: 1.4272263050079346
Norm after each mp layer: 5.869441032409668
Norm after each mp layer: 30.423002243041992
Epoch: 30, Loss: 1.2097, Energy: 97887.0625, Train: 47.52%, Valid: 47.40%, Test: 47.40%, Best Valid: 47.40%, Best Test: 47.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5296842455863953
Norm after each mp layer: 1.4272263050079346
Norm after each mp layer: 5.869441032409668
Norm after each mp layer: 30.423002243041992
Norm before input: 0.2552422881126404
Norm after input: 0.5314163565635681
Norm after each mp layer: 1.4680790901184082
Norm after each mp layer: 6.150847911834717
Norm after each mp layer: 32.324806213378906
Norm before input: 0.2552422881126404
Norm after input: 0.5314163565635681
Norm after each mp layer: 1.4680790901184082
Norm after each mp layer: 6.150847911834717
Norm after each mp layer: 32.324806213378906
Norm before input: 0.2552422881126404
Norm after input: 0.530532717704773
Norm after each mp layer: 1.4881458282470703
Norm after each mp layer: 6.300929546356201
Norm after each mp layer: 33.330955505371094
Norm before input: 0.2552422881126404
Norm after input: 0.530532717704773
Norm after each mp layer: 1.4881458282470703
Norm after each mp layer: 6.300929546356201
Norm after each mp layer: 33.330955505371094
Norm before input: 0.2552422881126404
Norm after input: 0.5273799300193787
Norm after each mp layer: 1.4928029775619507
Norm after each mp layer: 6.34353494644165
Norm after each mp layer: 33.58273696899414
Norm before input: 0.2552422881126404
Norm after input: 0.5273799300193787
Norm after each mp layer: 1.4928029775619507
Norm after each mp layer: 6.34353494644165
Norm after each mp layer: 33.58273696899414
Norm before input: 0.2552422881126404
Norm after input: 0.5257869362831116
Norm after each mp layer: 1.5082037448883057
Norm after each mp layer: 6.4507527351379395
Norm after each mp layer: 34.30125427246094
Norm before input: 0.2552422881126404
Norm after input: 0.5257869362831116
Norm after each mp layer: 1.5082037448883057
Norm after each mp layer: 6.4507527351379395
Norm after each mp layer: 34.30125427246094
Norm before input: 0.2552422881126404
Norm after input: 0.5264434218406677
Norm after each mp layer: 1.5356253385543823
Norm after each mp layer: 6.6460089683532715
Norm after each mp layer: 35.7531852722168
Epoch: 35, Loss: 1.0705, Energy: 134969.3125, Train: 50.99%, Valid: 50.40%, Test: 49.50%, Best Valid: 50.40%, Best Test: 50.00%
Norm before input: 0.2552422881126404
Norm after input: 0.5264434218406677
Norm after each mp layer: 1.5356253385543823
Norm after each mp layer: 6.6460089683532715
Norm after each mp layer: 35.7531852722168
Norm before input: 0.2552422881126404
Norm after input: 0.5258578658103943
Norm after each mp layer: 1.5519179105758667
Norm after each mp layer: 6.770954608917236
Norm after each mp layer: 36.615291595458984
Norm before input: 0.2552422881126404
Norm after input: 0.5258578658103943
Norm after each mp layer: 1.5519179105758667
Norm after each mp layer: 6.770954608917236
Norm after each mp layer: 36.615291595458984
Norm before input: 0.2552422881126404
Norm after input: 0.52393639087677
Norm after each mp layer: 1.5578428506851196
Norm after each mp layer: 6.828713417053223
Norm after each mp layer: 36.91599655151367
Norm before input: 0.2552422881126404
Norm after input: 0.52393639087677
Norm after each mp layer: 1.5578428506851196
Norm after each mp layer: 6.828713417053223
Norm after each mp layer: 36.91599655151367
Norm before input: 0.2552422881126404
Norm after input: 0.522916316986084
Norm after each mp layer: 1.5687787532806396
Norm after each mp layer: 6.9271769523620605
Norm after each mp layer: 37.55196762084961
Norm before input: 0.2552422881126404
Norm after input: 0.522916316986084
Norm after each mp layer: 1.5687787532806396
Norm after each mp layer: 6.9271769523620605
Norm after each mp layer: 37.55196762084961
Norm before input: 0.2552422881126404
Norm after input: 0.5228118300437927
Norm after each mp layer: 1.5859100818634033
Norm after each mp layer: 7.071537017822266
Norm after each mp layer: 38.56877517700195
Norm before input: 0.2552422881126404
Norm after input: 0.5228118300437927
Norm after each mp layer: 1.5859100818634033
Norm after each mp layer: 7.071537017822266
Norm after each mp layer: 38.56877517700195
Norm before input: 0.2552422881126404
Norm after input: 0.5213211178779602
Norm after each mp layer: 1.595220923423767
Norm after each mp layer: 7.148097515106201
Norm after each mp layer: 39.03832244873047
Epoch: 40, Loss: 0.9712, Energy: 161753.6406, Train: 63.08%, Valid: 55.40%, Test: 56.60%, Best Valid: 56.60%, Best Test: 56.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5213211178779602
Norm after each mp layer: 1.595220923423767
Norm after each mp layer: 7.148097515106201
Norm after each mp layer: 39.03832244873047
Norm before input: 0.2552422881126404
Norm after input: 0.5197470784187317
Norm after each mp layer: 1.6046512126922607
Norm after each mp layer: 7.211841583251953
Norm after each mp layer: 39.38698196411133
Norm before input: 0.2552422881126404
Norm after input: 0.5197470784187317
Norm after each mp layer: 1.6046512126922607
Norm after each mp layer: 7.211841583251953
Norm after each mp layer: 39.38698196411133
Norm before input: 0.2552422881126404
Norm after input: 0.5203467607498169
Norm after each mp layer: 1.6261088848114014
Norm after each mp layer: 7.3630805015563965
Norm after each mp layer: 40.474727630615234
Norm before input: 0.2552422881126404
Norm after input: 0.5203467607498169
Norm after each mp layer: 1.6261088848114014
Norm after each mp layer: 7.3630805015563965
Norm after each mp layer: 40.474727630615234
Norm before input: 0.2552422881126404
Norm after input: 0.5206291675567627
Norm after each mp layer: 1.6438512802124023
Norm after each mp layer: 7.480955600738525
Norm after each mp layer: 41.2309684753418
Norm before input: 0.2552422881126404
Norm after input: 0.5206291675567627
Norm after each mp layer: 1.6438512802124023
Norm after each mp layer: 7.480955600738525
Norm after each mp layer: 41.2309684753418
Norm before input: 0.2552422881126404
Norm after input: 0.519917905330658
Norm after each mp layer: 1.6542651653289795
Norm after each mp layer: 7.538139343261719
Norm after each mp layer: 41.409767150878906
Norm before input: 0.2552422881126404
Norm after input: 0.519917905330658
Norm after each mp layer: 1.6542651653289795
Norm after each mp layer: 7.538139343261719
Norm after each mp layer: 41.409767150878906
Norm before input: 0.2552422881126404
Norm after input: 0.520711362361908
Norm after each mp layer: 1.671221375465393
Norm after each mp layer: 7.658957481384277
Norm after each mp layer: 42.207115173339844
Epoch: 45, Loss: 0.8980, Energy: 166907.9688, Train: 66.39%, Valid: 58.80%, Test: 59.80%, Best Valid: 58.80%, Best Test: 59.80%
Norm before input: 0.2552422881126404
Norm after input: 0.520711362361908
Norm after each mp layer: 1.671221375465393
Norm after each mp layer: 7.658957481384277
Norm after each mp layer: 42.207115173339844
Norm before input: 0.2552422881126404
Norm after input: 0.5211710929870605
Norm after each mp layer: 1.685639500617981
Norm after each mp layer: 7.762231349945068
Norm after each mp layer: 42.853118896484375
Norm before input: 0.2552422881126404
Norm after input: 0.5211710929870605
Norm after each mp layer: 1.685639500617981
Norm after each mp layer: 7.762231349945068
Norm after each mp layer: 42.853118896484375
Norm before input: 0.2552422881126404
Norm after input: 0.5201848745346069
Norm after each mp layer: 1.6930131912231445
Norm after each mp layer: 7.7996602058410645
Norm after each mp layer: 42.89910125732422
Norm before input: 0.2552422881126404
Norm after input: 0.5201848745346069
Norm after each mp layer: 1.6930131912231445
Norm after each mp layer: 7.799661159515381
Norm after each mp layer: 42.89910125732422
Norm before input: 0.2552422881126404
Norm after input: 0.5206513404846191
Norm after each mp layer: 1.7081942558288574
Norm after each mp layer: 7.912949562072754
Norm after each mp layer: 43.70772933959961
Norm before input: 0.2552422881126404
Norm after input: 0.5206513404846191
Norm after each mp layer: 1.7081942558288574
Norm after each mp layer: 7.912949562072754
Norm after each mp layer: 43.70772933959961
Norm before input: 0.2552422881126404
Norm after input: 0.5212014317512512
Norm after each mp layer: 1.7246544361114502
Norm after each mp layer: 8.048469543457031
Norm after each mp layer: 44.73350143432617
Norm before input: 0.2552422881126404
Norm after input: 0.5212014317512512
Norm after each mp layer: 1.7246544361114502
Norm after each mp layer: 8.048469543457031
Norm after each mp layer: 44.73350143432617
Norm before input: 0.2552422881126404
Norm after input: 0.5192835330963135
Norm after each mp layer: 1.7291780710220337
Norm after each mp layer: 8.084992408752441
Norm after each mp layer: 44.82243347167969
Epoch: 50, Loss: 0.8341, Energy: 205482.3281, Train: 67.22%, Valid: 59.00%, Test: 59.20%, Best Valid: 60.00%, Best Test: 60.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5192835330963135
Norm after each mp layer: 1.7291780710220337
Norm after each mp layer: 8.084992408752441
Norm after each mp layer: 44.82243347167969
Norm before input: 0.2552422881126404
Norm after input: 0.5184420943260193
Norm after each mp layer: 1.7388652563095093
Norm after each mp layer: 8.17847728729248
Norm after each mp layer: 45.465606689453125
Norm before input: 0.2552422881126404
Norm after input: 0.5184420943260193
Norm after each mp layer: 1.7388653755187988
Norm after each mp layer: 8.17847728729248
Norm after each mp layer: 45.465606689453125
Norm before input: 0.2552422881126404
Norm after input: 0.5185547471046448
Norm after each mp layer: 1.7530912160873413
Norm after each mp layer: 8.313209533691406
Norm after each mp layer: 46.46901321411133
Norm before input: 0.2552422881126404
Norm after input: 0.5185547471046448
Norm after each mp layer: 1.7530912160873413
Norm after each mp layer: 8.313209533691406
Norm after each mp layer: 46.46901321411133
Norm before input: 0.2552422881126404
Norm after input: 0.516708493232727
Norm after each mp layer: 1.756487488746643
Norm after each mp layer: 8.342190742492676
Norm after each mp layer: 46.43788146972656
Norm before input: 0.2552422881126404
Norm after input: 0.516708493232727
Norm after each mp layer: 1.756487488746643
Norm after each mp layer: 8.342190742492676
Norm after each mp layer: 46.43788146972656
Norm before input: 0.2552422881126404
Norm after input: 0.5177346467971802
Norm after each mp layer: 1.773857593536377
Norm after each mp layer: 8.487783432006836
Norm after each mp layer: 47.46231460571289
Norm before input: 0.2552422881126404
Norm after input: 0.5177346467971802
Norm after each mp layer: 1.773857593536377
Norm after each mp layer: 8.487783432006836
Norm after each mp layer: 47.462310791015625
Norm before input: 0.2552422881126404
Norm after input: 0.5179232954978943
Norm after each mp layer: 1.7869980335235596
Norm after each mp layer: 8.586162567138672
Norm after each mp layer: 48.030113220214844
Epoch: 55, Loss: 0.7691, Energy: 223204.5312, Train: 74.34%, Valid: 62.40%, Test: 61.90%, Best Valid: 62.40%, Best Test: 61.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5179232954978943
Norm after each mp layer: 1.7869980335235596
Norm after each mp layer: 8.586162567138672
Norm after each mp layer: 48.030113220214844
Norm before input: 0.2552422881126404
Norm after input: 0.5176078081130981
Norm after each mp layer: 1.7983111143112183
Norm after each mp layer: 8.672491073608398
Norm after each mp layer: 48.49319076538086
Norm before input: 0.2552422881126404
Norm after input: 0.5176078081130981
Norm after each mp layer: 1.7983111143112183
Norm after each mp layer: 8.672491073608398
Norm after each mp layer: 48.49319076538086
Norm before input: 0.2552422881126404
Norm after input: 0.5180749297142029
Norm after each mp layer: 1.8146121501922607
Norm after each mp layer: 8.829753875732422
Norm after each mp layer: 49.634368896484375
Norm before input: 0.2552422881126404
Norm after input: 0.5180749297142029
Norm after each mp layer: 1.8146121501922607
Norm after each mp layer: 8.829753875732422
Norm after each mp layer: 49.634368896484375
Norm before input: 0.2552422881126404
Norm after input: 0.5159791111946106
Norm after each mp layer: 1.818505883216858
Norm after each mp layer: 8.889894485473633
Norm after each mp layer: 49.90460968017578
Norm before input: 0.2552422881126404
Norm after input: 0.5159791111946106
Norm after each mp layer: 1.818505883216858
Norm after each mp layer: 8.889894485473633
Norm after each mp layer: 49.904605865478516
Norm before input: 0.2552422881126404
Norm after input: 0.5154978632926941
Norm after each mp layer: 1.8311704397201538
Norm after each mp layer: 9.048406600952148
Norm after each mp layer: 51.06365203857422
Norm before input: 0.2552422881126404
Norm after input: 0.5154978632926941
Norm after each mp layer: 1.8311704397201538
Norm after each mp layer: 9.048406600952148
Norm after each mp layer: 51.06365203857422
Norm before input: 0.2552422881126404
Norm after input: 0.513384997844696
Norm after each mp layer: 1.834566354751587
Norm after each mp layer: 9.112467765808105
Norm after each mp layer: 51.358028411865234
Epoch: 60, Loss: 0.6993, Energy: 250887.6562, Train: 77.48%, Valid: 62.80%, Test: 63.60%, Best Valid: 63.40%, Best Test: 63.60%
Norm before input: 0.2552422881126404
Norm after input: 0.513384997844696
Norm after each mp layer: 1.834566354751587
Norm after each mp layer: 9.112467765808105
Norm after each mp layer: 51.358028411865234
Norm before input: 0.2552422881126404
Norm after input: 0.5137447118759155
Norm after each mp layer: 1.8515702486038208
Norm after each mp layer: 9.28856372833252
Norm after each mp layer: 52.585121154785156
Norm before input: 0.2552422881126404
Norm after input: 0.5137447118759155
Norm after each mp layer: 1.8515702486038208
Norm after each mp layer: 9.28856372833252
Norm after each mp layer: 52.58512878417969
Norm before input: 0.2552422881126404
Norm after input: 0.5119067430496216
Norm after each mp layer: 1.856251835823059
Norm after each mp layer: 9.33182144165039
Norm after each mp layer: 52.626014709472656
Norm before input: 0.2552422881126404
Norm after input: 0.5119067430496216
Norm after each mp layer: 1.856251835823059
Norm after each mp layer: 9.33182144165039
Norm after each mp layer: 52.626014709472656
Norm before input: 0.2552422881126404
Norm after input: 0.5146142840385437
Norm after each mp layer: 1.8873424530029297
Norm after each mp layer: 9.617870330810547
Norm after each mp layer: 54.71321105957031
Norm before input: 0.2552422881126404
Norm after input: 0.5146142840385437
Norm after each mp layer: 1.8873424530029297
Norm after each mp layer: 9.617870330810547
Norm after each mp layer: 54.71321105957031
Norm before input: 0.2552422881126404
Norm after input: 0.5105515122413635
Norm after each mp layer: 1.881119728088379
Norm after each mp layer: 9.551200866699219
Norm after each mp layer: 53.78268051147461
Norm before input: 0.2552422881126404
Norm after input: 0.5105515122413635
Norm after each mp layer: 1.881119728088379
Norm after each mp layer: 9.551200866699219
Norm after each mp layer: 53.78268051147461
Norm before input: 0.2552422881126404
Norm after input: 0.5151550769805908
Norm after each mp layer: 1.921496868133545
Norm after each mp layer: 9.940329551696777
Norm after each mp layer: 56.735164642333984
Epoch: 65, Loss: 0.6631, Energy: 264637.8125, Train: 78.97%, Valid: 65.60%, Test: 63.80%, Best Valid: 65.60%, Best Test: 63.80%
Norm before input: 0.2552422881126404
Norm after input: 0.5151550769805908
Norm after each mp layer: 1.921496868133545
Norm after each mp layer: 9.940329551696777
Norm after each mp layer: 56.735164642333984
Norm before input: 0.2552422881126404
Norm after input: 0.509375810623169
Norm after each mp layer: 1.90494704246521
Norm after each mp layer: 9.835169792175293
Norm after each mp layer: 55.60201644897461
Norm before input: 0.2552422881126404
Norm after input: 0.509375810623169
Norm after each mp layer: 1.90494704246521
Norm after each mp layer: 9.835169792175293
Norm after each mp layer: 55.60201644897461
Norm before input: 0.2552422881126404
Norm after input: 0.5089536905288696
Norm after each mp layer: 1.917159080505371
Norm after each mp layer: 10.007722854614258
Norm after each mp layer: 56.784393310546875
Norm before input: 0.2552422881126404
Norm after input: 0.5089536905288696
Norm after each mp layer: 1.917159080505371
Norm after each mp layer: 10.007722854614258
Norm after each mp layer: 56.784393310546875
Norm before input: 0.2552422881126404
Norm after input: 0.5100978016853333
Norm after each mp layer: 1.9392677545547485
Norm after each mp layer: 10.271811485290527
Norm after each mp layer: 58.67985916137695
Norm before input: 0.2552422881126404
Norm after input: 0.5100978016853333
Norm after each mp layer: 1.9392677545547485
Norm after each mp layer: 10.271811485290527
Norm after each mp layer: 58.67985916137695
Norm before input: 0.2552422881126404
Norm after input: 0.5046793222427368
Norm after each mp layer: 1.922197699546814
Norm after each mp layer: 10.182137489318848
Norm after each mp layer: 57.744781494140625
Norm before input: 0.2552422881126404
Norm after input: 0.5046793222427368
Norm after each mp layer: 1.922197699546814
Norm after each mp layer: 10.182137489318848
Norm after each mp layer: 57.744781494140625
Norm before input: 0.2552422881126404
Norm after input: 0.5068143010139465
Norm after each mp layer: 1.9486384391784668
Norm after each mp layer: 10.443939208984375
Norm after each mp layer: 59.57078552246094
Epoch: 70, Loss: 0.5807, Energy: 317010.2812, Train: 83.61%, Valid: 67.60%, Test: 65.50%, Best Valid: 67.60%, Best Test: 65.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5068143010139465
Norm after each mp layer: 1.9486384391784668
Norm after each mp layer: 10.443939208984375
Norm after each mp layer: 59.57078552246094
Norm before input: 0.2552422881126404
Norm after input: 0.5058428645133972
Norm after each mp layer: 1.9558216333389282
Norm after each mp layer: 10.544075012207031
Norm after each mp layer: 60.10942077636719
Norm before input: 0.2552422881126404
Norm after input: 0.5058428645133972
Norm after each mp layer: 1.9558216333389282
Norm after each mp layer: 10.544075012207031
Norm after each mp layer: 60.10942077636719
Norm before input: 0.2552422881126404
Norm after input: 0.5026283860206604
Norm after each mp layer: 1.947929859161377
Norm after each mp layer: 10.529938697814941
Norm after each mp layer: 59.77653503417969
Norm before input: 0.2552422881126404
Norm after input: 0.5026283860206604
Norm after each mp layer: 1.947929859161377
Norm after each mp layer: 10.529938697814941
Norm after each mp layer: 59.77653503417969
Norm before input: 0.2552422881126404
Norm after input: 0.504314124584198
Norm after each mp layer: 1.9678723812103271
Norm after each mp layer: 10.770552635192871
Norm after each mp layer: 61.49172592163086
Norm before input: 0.2552422881126404
Norm after input: 0.504314124584198
Norm after each mp layer: 1.9678723812103271
Norm after each mp layer: 10.770552635192871
Norm after each mp layer: 61.49172592163086
Norm before input: 0.2552422881126404
Norm after input: 0.5026913285255432
Norm after each mp layer: 1.9659944772720337
Norm after each mp layer: 10.834400177001953
Norm after each mp layer: 61.8472900390625
Norm before input: 0.2552422881126404
Norm after input: 0.5026913285255432
Norm after each mp layer: 1.9659944772720337
Norm after each mp layer: 10.834400177001953
Norm after each mp layer: 61.8472900390625
Norm before input: 0.2552422881126404
Norm after input: 0.5007073879241943
Norm after each mp layer: 1.9619144201278687
Norm after each mp layer: 10.868882179260254
Norm after each mp layer: 61.946903228759766
Epoch: 75, Loss: 0.4824, Energy: 405891.6250, Train: 85.68%, Valid: 69.00%, Test: 65.90%, Best Valid: 69.00%, Best Test: 66.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5007073879241943
Norm after each mp layer: 1.9619144201278687
Norm after each mp layer: 10.868882179260254
Norm after each mp layer: 61.946903228759766
Norm before input: 0.2552422881126404
Norm after input: 0.5035427212715149
Norm after each mp layer: 1.992362380027771
Norm after each mp layer: 11.166084289550781
Norm after each mp layer: 63.961605072021484
Norm before input: 0.2552422881126404
Norm after input: 0.5035427212715149
Norm after each mp layer: 1.992362380027771
Norm after each mp layer: 11.166084289550781
Norm after each mp layer: 63.961605072021484
Norm before input: 0.2552422881126404
Norm after input: 0.5019572377204895
Norm after each mp layer: 1.99465811252594
Norm after each mp layer: 11.202234268188477
Norm after each mp layer: 63.936981201171875
Norm before input: 0.2552422881126404
Norm after input: 0.5019572377204895
Norm after each mp layer: 1.99465811252594
Norm after each mp layer: 11.202234268188477
Norm after each mp layer: 63.936981201171875
Norm before input: 0.2552422881126404
Norm after input: 0.502866804599762
Norm after each mp layer: 2.014169692993164
Norm after each mp layer: 11.390486717224121
Norm after each mp layer: 65.05414581298828
Norm before input: 0.2552422881126404
Norm after input: 0.502866804599762
Norm after each mp layer: 2.014169692993164
Norm after each mp layer: 11.390486717224121
Norm after each mp layer: 65.05414581298828
Norm before input: 0.2552422881126404
Norm after input: 0.5048261284828186
Norm after each mp layer: 2.039508819580078
Norm after each mp layer: 11.65158748626709
Norm after each mp layer: 66.76083374023438
Norm before input: 0.2552422881126404
Norm after input: 0.5048261284828186
Norm after each mp layer: 2.039508819580078
Norm after each mp layer: 11.65158748626709
Norm after each mp layer: 66.76083374023438
Norm before input: 0.2552422881126404
Norm after input: 0.5028366446495056
Norm after each mp layer: 2.036525249481201
Norm after each mp layer: 11.669677734375
Norm after each mp layer: 66.65447235107422
Epoch: 80, Loss: 0.4155, Energy: 529614.3750, Train: 88.25%, Valid: 70.60%, Test: 67.90%, Best Valid: 70.60%, Best Test: 69.00%
Norm before input: 0.2552422881126404
Norm after input: 0.5028366446495056
Norm after each mp layer: 2.036525249481201
Norm after each mp layer: 11.669677734375
Norm after each mp layer: 66.65447235107422
Norm before input: 0.2552422881126404
Norm after input: 0.5059962868690491
Norm after each mp layer: 2.070675849914551
Norm after each mp layer: 11.99609088897705
Norm after each mp layer: 68.82450866699219
Norm before input: 0.2552422881126404
Norm after input: 0.5059962868690491
Norm after each mp layer: 2.070675849914551
Norm after each mp layer: 11.99609088897705
Norm after each mp layer: 68.82450866699219
Norm before input: 0.2552422881126404
Norm after input: 0.5051906108856201
Norm after each mp layer: 2.077312707901001
Norm after each mp layer: 12.061375617980957
Norm after each mp layer: 69.0035171508789
Norm before input: 0.2552422881126404
Norm after input: 0.5051906108856201
Norm after each mp layer: 2.077312707901001
Norm after each mp layer: 12.061375617980957
Norm after each mp layer: 69.0035171508789
Norm before input: 0.2552422881126404
Norm after input: 0.5067837238311768
Norm after each mp layer: 2.100735664367676
Norm after each mp layer: 12.273893356323242
Norm after each mp layer: 70.28506469726562
Norm before input: 0.2552422881126404
Norm after input: 0.5067837238311768
Norm after each mp layer: 2.100735664367676
Norm after each mp layer: 12.273893356323242
Norm after each mp layer: 70.28506469726562
Norm before input: 0.2552422881126404
Norm after input: 0.508236289024353
Norm after each mp layer: 2.121368408203125
Norm after each mp layer: 12.477336883544922
Norm after each mp layer: 71.56327056884766
Norm before input: 0.2552422881126404
Norm after input: 0.508236289024353
Norm after each mp layer: 2.121368408203125
Norm after each mp layer: 12.477336883544922
Norm after each mp layer: 71.56327056884766
Norm before input: 0.2552422881126404
Norm after input: 0.5071405172348022
Norm after each mp layer: 2.1232340335845947
Norm after each mp layer: 12.504047393798828
Norm after each mp layer: 71.54793548583984
Epoch: 85, Loss: 0.3366, Energy: 651467.4375, Train: 90.81%, Valid: 72.20%, Test: 70.60%, Best Valid: 74.20%, Best Test: 71.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5071405172348022
Norm after each mp layer: 2.1232340335845947
Norm after each mp layer: 12.504047393798828
Norm after each mp layer: 71.54793548583984
Norm before input: 0.2552422881126404
Norm after input: 0.5112727880477905
Norm after each mp layer: 2.163518190383911
Norm after each mp layer: 12.869546890258789
Norm after each mp layer: 74.10215759277344
Norm before input: 0.2552422881126404
Norm after input: 0.5112727880477905
Norm after each mp layer: 2.163518190383911
Norm after each mp layer: 12.869546890258789
Norm after each mp layer: 74.10215759277344
Norm before input: 0.2552422881126404
Norm after input: 0.5087307691574097
Norm after each mp layer: 2.1550703048706055
Norm after each mp layer: 12.779886245727539
Norm after each mp layer: 73.22563934326172
Norm before input: 0.2552422881126404
Norm after input: 0.5087307691574097
Norm after each mp layer: 2.1550703048706055
Norm after each mp layer: 12.779886245727539
Norm after each mp layer: 73.22563934326172
Norm before input: 0.2552422881126404
Norm after input: 0.5103856325149536
Norm after each mp layer: 2.174088478088379
Norm after each mp layer: 12.958066940307617
Norm after each mp layer: 74.4666748046875
Norm before input: 0.2552422881126404
Norm after input: 0.5103856325149536
Norm after each mp layer: 2.174088478088379
Norm after each mp layer: 12.958066940307617
Norm after each mp layer: 74.4666748046875
Norm before input: 0.2552422881126404
Norm after input: 0.5119004249572754
Norm after each mp layer: 2.1899328231811523
Norm after each mp layer: 13.13443374633789
Norm after each mp layer: 75.79254150390625
Norm before input: 0.2552422881126404
Norm after input: 0.5119004249572754
Norm after each mp layer: 2.1899328231811523
Norm after each mp layer: 13.13443374633789
Norm after each mp layer: 75.79254150390625
Norm before input: 0.2552422881126404
Norm after input: 0.5096652507781982
Norm after each mp layer: 2.1805267333984375
Norm after each mp layer: 13.049259185791016
Norm after each mp layer: 74.98448944091797
Epoch: 90, Loss: 0.2697, Energy: 807491.6875, Train: 92.88%, Valid: 74.40%, Test: 73.10%, Best Valid: 75.20%, Best Test: 74.20%
Norm before input: 0.2552422881126404
Norm after input: 0.5096652507781982
Norm after each mp layer: 2.1805267333984375
Norm after each mp layer: 13.049259185791016
Norm after each mp layer: 74.98448944091797
Norm before input: 0.2552422881126404
Norm after input: 0.513474702835083
Norm after each mp layer: 2.222036361694336
Norm after each mp layer: 13.377352714538574
Norm after each mp layer: 77.21305847167969
Norm before input: 0.2552422881126404
Norm after input: 0.513474702835083
Norm after each mp layer: 2.222036361694336
Norm after each mp layer: 13.377352714538574
Norm after each mp layer: 77.21305847167969
Norm before input: 0.2552422881126404
Norm after input: 0.5139185786247253
Norm after each mp layer: 2.2370431423187256
Norm after each mp layer: 13.490110397338867
Norm after each mp layer: 77.89107513427734
Norm before input: 0.2552422881126404
Norm after input: 0.5139185786247253
Norm after each mp layer: 2.2370431423187256
Norm after each mp layer: 13.490110397338867
Norm after each mp layer: 77.89107513427734
Norm before input: 0.2552422881126404
Norm after input: 0.5120052695274353
Norm after each mp layer: 2.231904983520508
Norm after each mp layer: 13.456520080566406
Norm after each mp layer: 77.60609436035156
Norm before input: 0.2552422881126404
Norm after input: 0.5120052695274353
Norm after each mp layer: 2.231904983520508
Norm after each mp layer: 13.456520080566406
Norm after each mp layer: 77.60609436035156
Norm before input: 0.2552422881126404
Norm after input: 0.513454258441925
Norm after each mp layer: 2.24800968170166
Norm after each mp layer: 13.631315231323242
Norm after each mp layer: 79.12516784667969
Norm before input: 0.2552422881126404
Norm after input: 0.513454258441925
Norm after each mp layer: 2.24800968170166
Norm after each mp layer: 13.631315231323242
Norm after each mp layer: 79.12516784667969
Norm before input: 0.2552422881126404
Norm after input: 0.5133132934570312
Norm after each mp layer: 2.2551109790802
Norm after each mp layer: 13.678306579589844
Norm after each mp layer: 79.43505096435547
Epoch: 95, Loss: 0.1985, Energy: 915468.8125, Train: 95.12%, Valid: 76.20%, Test: 76.40%, Best Valid: 76.20%, Best Test: 76.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5133132934570312
Norm after each mp layer: 2.2551109790802
Norm after each mp layer: 13.678305625915527
Norm after each mp layer: 79.43505096435547
Norm before input: 0.2552422881126404
Norm after input: 0.5130029320716858
Norm after each mp layer: 2.260298728942871
Norm after each mp layer: 13.706969261169434
Norm after each mp layer: 79.58343505859375
Norm before input: 0.2552422881126404
Norm after input: 0.5130029320716858
Norm after each mp layer: 2.260298728942871
Norm after each mp layer: 13.706969261169434
Norm after each mp layer: 79.58343505859375
Norm before input: 0.2552422881126404
Norm after input: 0.5142338275909424
Norm after each mp layer: 2.274160146713257
Norm after each mp layer: 13.832806587219238
Norm after each mp layer: 80.6416244506836
Norm before input: 0.2552422881126404
Norm after input: 0.5142338275909424
Norm after each mp layer: 2.274160146713257
Norm after each mp layer: 13.832806587219238
Norm after each mp layer: 80.6416244506836
Norm before input: 0.2552422881126404
Norm after input: 0.5140960216522217
Norm after each mp layer: 2.277614116668701
Norm after each mp layer: 13.862651824951172
Norm after each mp layer: 80.85282897949219
Norm before input: 0.2552422881126404
Norm after input: 0.5140960216522217
Norm after each mp layer: 2.277614116668701
Norm after each mp layer: 13.862651824951172
Norm after each mp layer: 80.85282897949219
Norm before input: 0.2552422881126404
Norm after input: 0.5136846899986267
Norm after each mp layer: 2.2788355350494385
Norm after each mp layer: 13.872337341308594
Norm after each mp layer: 80.80682373046875
Norm before input: 0.2552422881126404
Norm after input: 0.5136846899986267
Norm after each mp layer: 2.2788355350494385
Norm after each mp layer: 13.872337341308594
Norm after each mp layer: 80.80682373046875
Norm before input: 0.2552422881126404
Norm after input: 0.5150391459465027
Norm after each mp layer: 2.293802261352539
Norm after each mp layer: 14.005741119384766
Norm after each mp layer: 81.74095916748047
Epoch: 100, Loss: 0.1546, Energy: 981996.5000, Train: 96.36%, Valid: 76.40%, Test: 76.50%, Best Valid: 77.20%, Best Test: 76.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5150391459465027
Norm after each mp layer: 2.293802261352539
Norm after each mp layer: 14.005741119384766
Norm after each mp layer: 81.74095916748047
Norm before input: 0.2552422881126404
Norm after input: 0.5144587159156799
Norm after each mp layer: 2.29646372795105
Norm after each mp layer: 14.017127990722656
Norm after each mp layer: 81.5860366821289
Norm before input: 0.2552422881126404
Norm after input: 0.5144587159156799
Norm after each mp layer: 2.29646372795105
Norm after each mp layer: 14.017127990722656
Norm after each mp layer: 81.5860366821289
Norm before input: 0.2552422881126404
Norm after input: 0.5161296129226685
Norm after each mp layer: 2.315863609313965
Norm after each mp layer: 14.18847942352295
Norm after each mp layer: 82.84178161621094
Norm before input: 0.2552422881126404
Norm after input: 0.5161296129226685
Norm after each mp layer: 2.315863609313965
Norm after each mp layer: 14.18847942352295
Norm after each mp layer: 82.84178161621094
Norm before input: 0.2552422881126404
Norm after input: 0.5155355334281921
Norm after each mp layer: 2.3179774284362793
Norm after each mp layer: 14.211087226867676
Norm after each mp layer: 82.92273712158203
Norm before input: 0.2552422881126404
Norm after input: 0.5155355334281921
Norm after each mp layer: 2.3179774284362793
Norm after each mp layer: 14.211087226867676
Norm after each mp layer: 82.92273712158203
Norm before input: 0.2552422881126404
Norm after input: 0.5160655975341797
Norm after each mp layer: 2.3260693550109863
Norm after each mp layer: 14.300251960754395
Norm after each mp layer: 83.50556945800781
Norm before input: 0.2552422881126404
Norm after input: 0.5160655975341797
Norm after each mp layer: 2.3260693550109863
Norm after each mp layer: 14.300251960754395
Norm after each mp layer: 83.50556945800781
Norm before input: 0.2552422881126404
Norm after input: 0.5170684456825256
Norm after each mp layer: 2.3367738723754883
Norm after each mp layer: 14.410998344421387
Norm after each mp layer: 84.07184600830078
Epoch: 105, Loss: 0.1246, Energy: 1072831.0000, Train: 97.27%, Valid: 77.20%, Test: 76.90%, Best Valid: 77.20%, Best Test: 77.00%
Norm before input: 0.2552422881126404
Norm after input: 0.5170684456825256
Norm after each mp layer: 2.3367738723754883
Norm after each mp layer: 14.410998344421387
Norm after each mp layer: 84.07184600830078
Norm before input: 0.2552422881126404
Norm after input: 0.5162577033042908
Norm after each mp layer: 2.3358163833618164
Norm after each mp layer: 14.392871856689453
Norm after each mp layer: 83.41219329833984
Norm before input: 0.2552422881126404
Norm after input: 0.5162577033042908
Norm after each mp layer: 2.3358163833618164
Norm after each mp layer: 14.392871856689453
Norm after each mp layer: 83.41219329833984
Norm before input: 0.2552422881126404
Norm after input: 0.5184735655784607
Norm after each mp layer: 2.3558859825134277
Norm after each mp layer: 14.586041450500488
Norm after each mp layer: 84.85106658935547
Norm before input: 0.2552422881126404
Norm after input: 0.5184735655784607
Norm after each mp layer: 2.3558859825134277
Norm after each mp layer: 14.586041450500488
Norm after each mp layer: 84.85106658935547
Norm before input: 0.2552422881126404
Norm after input: 0.5173301696777344
Norm after each mp layer: 2.3509299755096436
Norm after each mp layer: 14.552428245544434
Norm after each mp layer: 84.61925506591797
Norm before input: 0.2552422881126404
Norm after input: 0.5173301696777344
Norm after each mp layer: 2.3509299755096436
Norm after each mp layer: 14.552428245544434
Norm after each mp layer: 84.61925506591797
Norm before input: 0.2552422881126404
Norm after input: 0.5160107016563416
Norm after each mp layer: 2.343412399291992
Norm after each mp layer: 14.505172729492188
Norm after each mp layer: 84.34803771972656
Norm before input: 0.2552422881126404
Norm after input: 0.5160107016563416
Norm after each mp layer: 2.343412399291992
Norm after each mp layer: 14.505172729492188
Norm after each mp layer: 84.34803771972656
Norm before input: 0.2552422881126404
Norm after input: 0.5191062688827515
Norm after each mp layer: 2.368568181991577
Norm after each mp layer: 14.739654541015625
Norm after each mp layer: 85.86417388916016
Epoch: 110, Loss: 0.1127, Energy: 1061174.0000, Train: 97.10%, Valid: 78.20%, Test: 76.90%, Best Valid: 78.20%, Best Test: 77.00%
Norm before input: 0.2552422881126404
Norm after input: 0.5191062688827515
Norm after each mp layer: 2.368568181991577
Norm after each mp layer: 14.739654541015625
Norm after each mp layer: 85.86417388916016
Norm before input: 0.2552422881126404
Norm after input: 0.5182251334190369
Norm after each mp layer: 2.364743947982788
Norm after each mp layer: 14.694291114807129
Norm after each mp layer: 84.86012268066406
Norm before input: 0.2552422881126404
Norm after input: 0.5182251334190369
Norm after each mp layer: 2.364743947982788
Norm after each mp layer: 14.694291114807129
Norm after each mp layer: 84.86012268066406
Norm before input: 0.2552422881126404
Norm after input: 0.5162330865859985
Norm after each mp layer: 2.351792812347412
Norm after each mp layer: 14.593668937683105
Norm after each mp layer: 83.92803192138672
Norm before input: 0.2552422881126404
Norm after input: 0.5162330865859985
Norm after each mp layer: 2.351792812347412
Norm after each mp layer: 14.593668937683105
Norm after each mp layer: 83.92803192138672
Norm before input: 0.2552422881126404
Norm after input: 0.5189988613128662
Norm after each mp layer: 2.3724398612976074
Norm after each mp layer: 14.816792488098145
Norm after each mp layer: 85.8893051147461
Norm before input: 0.2552422881126404
Norm after input: 0.5189988613128662
Norm after each mp layer: 2.3724398612976074
Norm after each mp layer: 14.816792488098145
Norm after each mp layer: 85.8893051147461
Norm before input: 0.2552422881126404
Norm after input: 0.5184158086776733
Norm after each mp layer: 2.370548725128174
Norm after each mp layer: 14.805667877197266
Norm after each mp layer: 85.57827758789062
Norm before input: 0.2552422881126404
Norm after input: 0.5184158086776733
Norm after each mp layer: 2.370548725128174
Norm after each mp layer: 14.805667877197266
Norm after each mp layer: 85.57827758789062
Norm before input: 0.2552422881126404
Norm after input: 0.5162699818611145
Norm after each mp layer: 2.3583567142486572
Norm after each mp layer: 14.688206672668457
Norm after each mp layer: 84.15926361083984
Epoch: 115, Loss: 0.0939, Energy: 1097414.7500, Train: 97.02%, Valid: 76.40%, Test: 77.10%, Best Valid: 78.20%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5162699818611145
Norm after each mp layer: 2.3583567142486572
Norm after each mp layer: 14.688206672668457
Norm after each mp layer: 84.15926361083984
Norm before input: 0.2552422881126404
Norm after input: 0.5189226865768433
Norm after each mp layer: 2.380815267562866
Norm after each mp layer: 14.88036823272705
Norm after each mp layer: 84.9381332397461
Norm before input: 0.2552422881126404
Norm after input: 0.5189226865768433
Norm after each mp layer: 2.380815267562866
Norm after each mp layer: 14.88036823272705
Norm after each mp layer: 84.9381332397461
Norm before input: 0.2552422881126404
Norm after input: 0.5197269916534424
Norm after each mp layer: 2.3884124755859375
Norm after each mp layer: 14.960308074951172
Norm after each mp layer: 85.40593719482422
Norm before input: 0.2552422881126404
Norm after input: 0.5197269916534424
Norm after each mp layer: 2.3884124755859375
Norm after each mp layer: 14.960308074951172
Norm after each mp layer: 85.40593719482422
Norm before input: 0.2552422881126404
Norm after input: 0.5175464749336243
Norm after each mp layer: 2.373077392578125
Norm after each mp layer: 14.838496208190918
Norm after each mp layer: 84.71501159667969
Norm before input: 0.2552422881126404
Norm after input: 0.5175464749336243
Norm after each mp layer: 2.373077392578125
Norm after each mp layer: 14.838496208190918
Norm after each mp layer: 84.71501159667969
Norm before input: 0.2552422881126404
Norm after input: 0.5166974663734436
Norm after each mp layer: 2.3675498962402344
Norm after each mp layer: 14.803019523620605
Norm after each mp layer: 84.54718780517578
Norm before input: 0.2552422881126404
Norm after input: 0.5166974663734436
Norm after each mp layer: 2.3675498962402344
Norm after each mp layer: 14.803019523620605
Norm after each mp layer: 84.54718780517578
Norm before input: 0.2552422881126404
Norm after input: 0.5190820693969727
Norm after each mp layer: 2.3870437145233154
Norm after each mp layer: 14.977190971374512
Norm after each mp layer: 85.41252136230469
Epoch: 120, Loss: 0.0836, Energy: 998224.8125, Train: 98.01%, Valid: 78.00%, Test: 76.80%, Best Valid: 78.20%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5190820693969727
Norm after each mp layer: 2.3870437145233154
Norm after each mp layer: 14.977190971374512
Norm after each mp layer: 85.41252136230469
Norm before input: 0.2552422881126404
Norm after input: 0.5189601182937622
Norm after each mp layer: 2.3901031017303467
Norm after each mp layer: 14.979754447937012
Norm after each mp layer: 84.89513397216797
Norm before input: 0.2552422881126404
Norm after input: 0.5189601182937622
Norm after each mp layer: 2.3901031017303467
Norm after each mp layer: 14.979754447937012
Norm after each mp layer: 84.89513397216797
Norm before input: 0.2552422881126404
Norm after input: 0.5174981355667114
Norm after each mp layer: 2.3824687004089355
Norm after each mp layer: 14.906649589538574
Norm after each mp layer: 84.46033477783203
Norm before input: 0.2552422881126404
Norm after input: 0.5174981355667114
Norm after each mp layer: 2.3824687004089355
Norm after each mp layer: 14.906647682189941
Norm after each mp layer: 84.46033477783203
Norm before input: 0.2552422881126404
Norm after input: 0.5177549123764038
Norm after each mp layer: 2.384831428527832
Norm after each mp layer: 14.952498435974121
Norm after each mp layer: 85.44734191894531
Norm before input: 0.2552422881126404
Norm after input: 0.5177549123764038
Norm after each mp layer: 2.384831428527832
Norm after each mp layer: 14.952498435974121
Norm after each mp layer: 85.44734191894531
Norm before input: 0.2552422881126404
Norm after input: 0.5185564756393433
Norm after each mp layer: 2.3915953636169434
Norm after each mp layer: 15.031391143798828
Norm after each mp layer: 86.10883331298828
Norm before input: 0.2552422881126404
Norm after input: 0.5185564756393433
Norm after each mp layer: 2.3915953636169434
Norm after each mp layer: 15.031391143798828
Norm after each mp layer: 86.10883331298828
Norm before input: 0.2552422881126404
Norm after input: 0.518112301826477
Norm after each mp layer: 2.391206979751587
Norm after each mp layer: 15.00732707977295
Norm after each mp layer: 84.85569763183594
Epoch: 125, Loss: 0.0718, Energy: 1042730.8750, Train: 98.10%, Valid: 76.80%, Test: 76.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.518112301826477
Norm after each mp layer: 2.391206979751587
Norm after each mp layer: 15.00732707977295
Norm after each mp layer: 84.85569763183594
Norm before input: 0.2552422881126404
Norm after input: 0.5182730555534363
Norm after each mp layer: 2.395160436630249
Norm after each mp layer: 15.032767295837402
Norm after each mp layer: 84.19007110595703
Norm before input: 0.2552422881126404
Norm after input: 0.5182730555534363
Norm after each mp layer: 2.395160436630249
Norm after each mp layer: 15.032767295837402
Norm after each mp layer: 84.19007110595703
Norm before input: 0.2552422881126404
Norm after input: 0.519206702709198
Norm after each mp layer: 2.402588129043579
Norm after each mp layer: 15.120013236999512
Norm after each mp layer: 85.02143096923828
Norm before input: 0.2552422881126404
Norm after input: 0.519206702709198
Norm after each mp layer: 2.402588129043579
Norm after each mp layer: 15.120013236999512
Norm after each mp layer: 85.02143096923828
Norm before input: 0.2552422881126404
Norm after input: 0.5189390778541565
Norm after each mp layer: 2.4016170501708984
Norm after each mp layer: 15.132740020751953
Norm after each mp layer: 85.65830993652344
Norm before input: 0.2552422881126404
Norm after input: 0.5189390778541565
Norm after each mp layer: 2.4016170501708984
Norm after each mp layer: 15.132740020751953
Norm after each mp layer: 85.65831756591797
Norm before input: 0.2552422881126404
Norm after input: 0.5180005431175232
Norm after each mp layer: 2.3972325325012207
Norm after each mp layer: 15.090482711791992
Norm after each mp layer: 85.32945251464844
Norm before input: 0.2552422881126404
Norm after input: 0.5180005431175232
Norm after each mp layer: 2.3972325325012207
Norm after each mp layer: 15.090482711791992
Norm after each mp layer: 85.32945251464844
Norm before input: 0.2552422881126404
Norm after input: 0.518819272518158
Norm after each mp layer: 2.406893253326416
Norm after each mp layer: 15.156708717346191
Norm after each mp layer: 84.9614486694336
Epoch: 130, Loss: 0.0620, Energy: 967995.9375, Train: 98.59%, Valid: 77.20%, Test: 77.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.518819272518158
Norm after each mp layer: 2.406893253326416
Norm after each mp layer: 15.156708717346191
Norm after each mp layer: 84.9614486694336
Norm before input: 0.2552422881126404
Norm after input: 0.5196971893310547
Norm after each mp layer: 2.4160616397857666
Norm after each mp layer: 15.232242584228516
Norm after each mp layer: 85.05279541015625
Norm before input: 0.2552422881126404
Norm after input: 0.5196971893310547
Norm after each mp layer: 2.4160616397857666
Norm after each mp layer: 15.232242584228516
Norm after each mp layer: 85.05279541015625
Norm before input: 0.2552422881126404
Norm after input: 0.5193758606910706
Norm after each mp layer: 2.4151978492736816
Norm after each mp layer: 15.2359037399292
Norm after each mp layer: 85.39688110351562
Norm before input: 0.2552422881126404
Norm after input: 0.5193758606910706
Norm after each mp layer: 2.4151978492736816
Norm after each mp layer: 15.2359037399292
Norm after each mp layer: 85.39688110351562
Norm before input: 0.2552422881126404
Norm after input: 0.5186251997947693
Norm after each mp layer: 2.4108762741088867
Norm after each mp layer: 15.213504791259766
Norm after each mp layer: 85.71102905273438
Norm before input: 0.2552422881126404
Norm after input: 0.5186251997947693
Norm after each mp layer: 2.4108762741088867
Norm after each mp layer: 15.213504791259766
Norm after each mp layer: 85.71102905273438
Norm before input: 0.2552422881126404
Norm after input: 0.5190049409866333
Norm after each mp layer: 2.415349245071411
Norm after each mp layer: 15.260973930358887
Norm after each mp layer: 85.879638671875
Norm before input: 0.2552422881126404
Norm after input: 0.5190049409866333
Norm after each mp layer: 2.415349245071411
Norm after each mp layer: 15.260973930358887
Norm after each mp layer: 85.879638671875
Norm before input: 0.2552422881126404
Norm after input: 0.5197800397872925
Norm after each mp layer: 2.423457384109497
Norm after each mp layer: 15.330366134643555
Norm after each mp layer: 85.64418029785156
Epoch: 135, Loss: 0.0540, Energy: 967217.6875, Train: 98.68%, Valid: 77.60%, Test: 77.30%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5197800397872925
Norm after each mp layer: 2.423457384109497
Norm after each mp layer: 15.330366134643555
Norm after each mp layer: 85.64418029785156
Norm before input: 0.2552422881126404
Norm after input: 0.5199495553970337
Norm after each mp layer: 2.427338123321533
Norm after each mp layer: 15.362147331237793
Norm after each mp layer: 85.3073501586914
Norm before input: 0.2552422881126404
Norm after input: 0.5199495553970337
Norm after each mp layer: 2.427338123321533
Norm after each mp layer: 15.362147331237793
Norm after each mp layer: 85.3073501586914
Norm before input: 0.2552422881126404
Norm after input: 0.5196519494056702
Norm after each mp layer: 2.4269731044769287
Norm after each mp layer: 15.369939804077148
Norm after each mp layer: 85.54852294921875
Norm before input: 0.2552422881126404
Norm after input: 0.5196519494056702
Norm after each mp layer: 2.4269731044769287
Norm after each mp layer: 15.369939804077148
Norm after each mp layer: 85.54852294921875
Norm before input: 0.2552422881126404
Norm after input: 0.5195319056510925
Norm after each mp layer: 2.4276063442230225
Norm after each mp layer: 15.39291000366211
Norm after each mp layer: 86.15812683105469
Norm before input: 0.2552422881126404
Norm after input: 0.5195319056510925
Norm after each mp layer: 2.4276063442230225
Norm after each mp layer: 15.39291000366211
Norm after each mp layer: 86.15812683105469
Norm before input: 0.2552422881126404
Norm after input: 0.5197506546974182
Norm after each mp layer: 2.4314730167388916
Norm after each mp layer: 15.435440063476562
Norm after each mp layer: 86.43568420410156
Norm before input: 0.2552422881126404
Norm after input: 0.5197506546974182
Norm after each mp layer: 2.4314730167388916
Norm after each mp layer: 15.435440063476562
Norm after each mp layer: 86.43568420410156
Norm before input: 0.2552422881126404
Norm after input: 0.5201351642608643
Norm after each mp layer: 2.437425374984741
Norm after each mp layer: 15.48419189453125
Norm after each mp layer: 86.14347076416016
Epoch: 140, Loss: 0.0482, Energy: 952491.7500, Train: 99.17%, Valid: 76.80%, Test: 76.50%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5201351642608643
Norm after each mp layer: 2.437425374984741
Norm after each mp layer: 15.484189987182617
Norm after each mp layer: 86.14347076416016
Norm before input: 0.2552422881126404
Norm after input: 0.5205454230308533
Norm after each mp layer: 2.443462371826172
Norm after each mp layer: 15.537337303161621
Norm after each mp layer: 85.95913696289062
Norm before input: 0.2552422881126404
Norm after input: 0.5205454230308533
Norm after each mp layer: 2.443462371826172
Norm after each mp layer: 15.537337303161621
Norm after each mp layer: 85.95913696289062
Norm before input: 0.2552422881126404
Norm after input: 0.5207988619804382
Norm after each mp layer: 2.447324752807617
Norm after each mp layer: 15.58468246459961
Norm after each mp layer: 86.2945327758789
Norm before input: 0.2552422881126404
Norm after input: 0.5207988619804382
Norm after each mp layer: 2.447324752807617
Norm after each mp layer: 15.58468246459961
Norm after each mp layer: 86.2945327758789
Norm before input: 0.2552422881126404
Norm after input: 0.5208498239517212
Norm after each mp layer: 2.4493069648742676
Norm after each mp layer: 15.6194486618042
Norm after each mp layer: 86.72029113769531
Norm before input: 0.2552422881126404
Norm after input: 0.5208498239517212
Norm after each mp layer: 2.4493069648742676
Norm after each mp layer: 15.6194486618042
Norm after each mp layer: 86.72029113769531
Norm before input: 0.2552422881126404
Norm after input: 0.5210614204406738
Norm after each mp layer: 2.452911138534546
Norm after each mp layer: 15.660038948059082
Norm after each mp layer: 86.72606658935547
Norm before input: 0.2552422881126404
Norm after input: 0.5210614204406738
Norm after each mp layer: 2.452911138534546
Norm after each mp layer: 15.660038948059082
Norm after each mp layer: 86.72606658935547
Norm before input: 0.2552422881126404
Norm after input: 0.521344006061554
Norm after each mp layer: 2.457326889038086
Norm after each mp layer: 15.70329761505127
Norm after each mp layer: 86.54205322265625
Epoch: 145, Loss: 0.0431, Energy: 935090.4375, Train: 99.17%, Valid: 76.40%, Test: 76.50%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.521344006061554
Norm after each mp layer: 2.457326889038086
Norm after each mp layer: 15.70329761505127
Norm after each mp layer: 86.54205322265625
Norm before input: 0.2552422881126404
Norm after input: 0.5216076374053955
Norm after each mp layer: 2.4611828327178955
Norm after each mp layer: 15.747681617736816
Norm after each mp layer: 86.75155639648438
Norm before input: 0.2552422881126404
Norm after input: 0.5216076374053955
Norm after each mp layer: 2.4611828327178955
Norm after each mp layer: 15.747681617736816
Norm after each mp layer: 86.75155639648438
Norm before input: 0.2552422881126404
Norm after input: 0.5217028260231018
Norm after each mp layer: 2.46362042427063
Norm after each mp layer: 15.782605171203613
Norm after each mp layer: 87.18575286865234
Norm before input: 0.2552422881126404
Norm after input: 0.5217028260231018
Norm after each mp layer: 2.46362042427063
Norm after each mp layer: 15.782605171203613
Norm after each mp layer: 87.18575286865234
Norm before input: 0.2552422881126404
Norm after input: 0.5217205882072449
Norm after each mp layer: 2.465848207473755
Norm after each mp layer: 15.811639785766602
Norm after each mp layer: 87.43840789794922
Norm before input: 0.2552422881126404
Norm after input: 0.5217205882072449
Norm after each mp layer: 2.465848207473755
Norm after each mp layer: 15.811639785766602
Norm after each mp layer: 87.43840789794922
Norm before input: 0.2552422881126404
Norm after input: 0.5219960808753967
Norm after each mp layer: 2.4702882766723633
Norm after each mp layer: 15.856782913208008
Norm after each mp layer: 87.48191833496094
Norm before input: 0.2552422881126404
Norm after input: 0.5219960808753967
Norm after each mp layer: 2.4702882766723633
Norm after each mp layer: 15.856783866882324
Norm after each mp layer: 87.48191833496094
Norm before input: 0.2552422881126404
Norm after input: 0.5224462151527405
Norm after each mp layer: 2.4758896827697754
Norm after each mp layer: 15.913902282714844
Norm after each mp layer: 87.50829315185547
Epoch: 150, Loss: 0.0391, Energy: 921685.7500, Train: 99.42%, Valid: 76.80%, Test: 76.60%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5224462151527405
Norm after each mp layer: 2.4758894443511963
Norm after each mp layer: 15.913902282714844
Norm after each mp layer: 87.50829315185547
Norm before input: 0.2552422881126404
Norm after input: 0.5226749777793884
Norm after each mp layer: 2.479398488998413
Norm after each mp layer: 15.956896781921387
Norm after each mp layer: 87.62918853759766
Norm before input: 0.2552422881126404
Norm after input: 0.5226749777793884
Norm after each mp layer: 2.479398488998413
Norm after each mp layer: 15.956896781921387
Norm after each mp layer: 87.62918853759766
Norm before input: 0.2552422881126404
Norm after input: 0.5226342678070068
Norm after each mp layer: 2.480504274368286
Norm after each mp layer: 15.981483459472656
Norm after each mp layer: 87.83009338378906
Norm before input: 0.2552422881126404
Norm after input: 0.5226342678070068
Norm after each mp layer: 2.480504274368286
Norm after each mp layer: 15.981483459472656
Norm after each mp layer: 87.83009338378906
Norm before input: 0.2552422881126404
Norm after input: 0.522704005241394
Norm after each mp layer: 2.4823408126831055
Norm after each mp layer: 16.011987686157227
Norm after each mp layer: 88.02798461914062
Norm before input: 0.2552422881126404
Norm after input: 0.522704005241394
Norm after each mp layer: 2.4823408126831055
Norm after each mp layer: 16.011987686157227
Norm after each mp layer: 88.02798461914062
Norm before input: 0.2552422881126404
Norm after input: 0.5230821371078491
Norm after each mp layer: 2.4867184162139893
Norm after each mp layer: 16.06153678894043
Norm after each mp layer: 88.12205505371094
Norm before input: 0.2552422881126404
Norm after input: 0.5230821371078491
Norm after each mp layer: 2.4867184162139893
Norm after each mp layer: 16.06153678894043
Norm after each mp layer: 88.12205505371094
Norm before input: 0.2552422881126404
Norm after input: 0.5233820080757141
Norm after each mp layer: 2.4907827377319336
Norm after each mp layer: 16.105642318725586
Norm after each mp layer: 88.1537094116211
Epoch: 155, Loss: 0.0357, Energy: 905704.2500, Train: 99.17%, Valid: 76.60%, Test: 76.60%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5233820080757141
Norm after each mp layer: 2.4907827377319336
Norm after each mp layer: 16.105642318725586
Norm after each mp layer: 88.1537094116211
Norm before input: 0.2552422881126404
Norm after input: 0.5234568119049072
Norm after each mp layer: 2.4931466579437256
Norm after each mp layer: 16.13572883605957
Norm after each mp layer: 88.29202270507812
Norm before input: 0.2552422881126404
Norm after input: 0.5234568119049072
Norm after each mp layer: 2.4931466579437256
Norm after each mp layer: 16.13572883605957
Norm after each mp layer: 88.29202270507812
Norm before input: 0.2552422881126404
Norm after input: 0.5235621929168701
Norm after each mp layer: 2.4955649375915527
Norm after each mp layer: 16.168228149414062
Norm after each mp layer: 88.51714324951172
Norm before input: 0.2552422881126404
Norm after input: 0.5235621929168701
Norm after each mp layer: 2.4955649375915527
Norm after each mp layer: 16.168228149414062
Norm after each mp layer: 88.51714324951172
Norm before input: 0.2552422881126404
Norm after input: 0.5238357186317444
Norm after each mp layer: 2.4991559982299805
Norm after each mp layer: 16.210323333740234
Norm after each mp layer: 88.62606811523438
Norm before input: 0.2552422881126404
Norm after input: 0.5238357186317444
Norm after each mp layer: 2.4991559982299805
Norm after each mp layer: 16.210323333740234
Norm after each mp layer: 88.62606811523438
Norm before input: 0.2552422881126404
Norm after input: 0.5240896940231323
Norm after each mp layer: 2.5024852752685547
Norm after each mp layer: 16.249181747436523
Norm after each mp layer: 88.6010513305664
Norm before input: 0.2552422881126404
Norm after input: 0.5240896940231323
Norm after each mp layer: 2.5024852752685547
Norm after each mp layer: 16.249181747436523
Norm after each mp layer: 88.6010513305664
Norm before input: 0.2552422881126404
Norm after input: 0.5242562294006348
Norm after each mp layer: 2.504833936691284
Norm after each mp layer: 16.281991958618164
Norm after each mp layer: 88.65374755859375
Epoch: 160, Loss: 0.0329, Energy: 883982.0000, Train: 99.42%, Valid: 76.20%, Test: 76.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5242562294006348
Norm after each mp layer: 2.5048341751098633
Norm after each mp layer: 16.281991958618164
Norm after each mp layer: 88.65374755859375
Norm before input: 0.2552422881126404
Norm after input: 0.5243746638298035
Norm after each mp layer: 2.506542444229126
Norm after each mp layer: 16.312223434448242
Norm after each mp layer: 88.8603515625
Norm before input: 0.2552422881126404
Norm after input: 0.5243746638298035
Norm after each mp layer: 2.506542444229126
Norm after each mp layer: 16.312223434448242
Norm after each mp layer: 88.8603515625
Norm before input: 0.2552422881126404
Norm after input: 0.5244587063789368
Norm after each mp layer: 2.508138418197632
Norm after each mp layer: 16.339689254760742
Norm after each mp layer: 89.01834106445312
Norm before input: 0.2552422881126404
Norm after input: 0.5244587063789368
Norm after each mp layer: 2.508138418197632
Norm after each mp layer: 16.339689254760742
Norm after each mp layer: 89.01834869384766
Norm before input: 0.2552422881126404
Norm after input: 0.5246082544326782
Norm after each mp layer: 2.5105173587799072
Norm after each mp layer: 16.370649337768555
Norm after each mp layer: 89.0464859008789
Norm before input: 0.2552422881126404
Norm after input: 0.5246082544326782
Norm after each mp layer: 2.5105173587799072
Norm after each mp layer: 16.370649337768555
Norm after each mp layer: 89.0464859008789
Norm before input: 0.2552422881126404
Norm after input: 0.5248727798461914
Norm after each mp layer: 2.513768434524536
Norm after each mp layer: 16.409454345703125
Norm after each mp layer: 89.11378479003906
Norm before input: 0.2552422881126404
Norm after input: 0.5248727798461914
Norm after each mp layer: 2.513768434524536
Norm after each mp layer: 16.409454345703125
Norm after each mp layer: 89.11378479003906
Norm before input: 0.2552422881126404
Norm after input: 0.5250685811042786
Norm after each mp layer: 2.5163352489471436
Norm after each mp layer: 16.443443298339844
Norm after each mp layer: 89.27484130859375
Epoch: 165, Loss: 0.0305, Energy: 864246.8125, Train: 99.59%, Valid: 76.40%, Test: 76.70%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5250685811042786
Norm after each mp layer: 2.5163352489471436
Norm after each mp layer: 16.443443298339844
Norm after each mp layer: 89.27484130859375
Norm before input: 0.2552422881126404
Norm after input: 0.525111973285675
Norm after each mp layer: 2.517688751220703
Norm after each mp layer: 16.466005325317383
Norm after each mp layer: 89.40010070800781
Norm before input: 0.2552422881126404
Norm after input: 0.525111973285675
Norm after each mp layer: 2.517688751220703
Norm after each mp layer: 16.466005325317383
Norm after each mp layer: 89.40010070800781
Norm before input: 0.2552422881126404
Norm after input: 0.5252196192741394
Norm after each mp layer: 2.519453763961792
Norm after each mp layer: 16.492626190185547
Norm after each mp layer: 89.47962188720703
Norm before input: 0.2552422881126404
Norm after input: 0.5252196192741394
Norm after each mp layer: 2.519453763961792
Norm after each mp layer: 16.492626190185547
Norm after each mp layer: 89.47962188720703
Norm before input: 0.2552422881126404
Norm after input: 0.5254572033882141
Norm after each mp layer: 2.522108554840088
Norm after each mp layer: 16.528539657592773
Norm after each mp layer: 89.54796600341797
Norm before input: 0.2552422881126404
Norm after input: 0.5254572033882141
Norm after each mp layer: 2.522108554840088
Norm after each mp layer: 16.528539657592773
Norm after each mp layer: 89.54796600341797
Norm before input: 0.2552422881126404
Norm after input: 0.5255913734436035
Norm after each mp layer: 2.5239877700805664
Norm after each mp layer: 16.55706787109375
Norm after each mp layer: 89.57859802246094
Norm before input: 0.2552422881126404
Norm after input: 0.5255913734436035
Norm after each mp layer: 2.5239877700805664
Norm after each mp layer: 16.55706787109375
Norm after each mp layer: 89.57859802246094
Norm before input: 0.2552422881126404
Norm after input: 0.5256157517433167
Norm after each mp layer: 2.5250086784362793
Norm after each mp layer: 16.577497482299805
Norm after each mp layer: 89.62145233154297
Epoch: 170, Loss: 0.0283, Energy: 843937.2500, Train: 99.59%, Valid: 76.20%, Test: 76.50%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5256157517433167
Norm after each mp layer: 2.5250086784362793
Norm after each mp layer: 16.577497482299805
Norm after each mp layer: 89.62145233154297
Norm before input: 0.2552422881126404
Norm after input: 0.5257589817047119
Norm after each mp layer: 2.5268356800079346
Norm after each mp layer: 16.605836868286133
Norm after each mp layer: 89.73422241210938
Norm before input: 0.2552422881126404
Norm after input: 0.5257589817047119
Norm after each mp layer: 2.5268356800079346
Norm after each mp layer: 16.605836868286133
Norm after each mp layer: 89.73422241210938
Norm before input: 0.2552422881126404
Norm after input: 0.52599036693573
Norm after each mp layer: 2.5293946266174316
Norm after each mp layer: 16.63936424255371
Norm after each mp layer: 89.81397247314453
Norm before input: 0.2552422881126404
Norm after input: 0.52599036693573
Norm after each mp layer: 2.5293946266174316
Norm after each mp layer: 16.63936424255371
Norm after each mp layer: 89.81397247314453
Norm before input: 0.2552422881126404
Norm after input: 0.526160717010498
Norm after each mp layer: 2.531585693359375
Norm after each mp layer: 16.667940139770508
Norm after each mp layer: 89.84326171875
Norm before input: 0.2552422881126404
Norm after input: 0.526160717010498
Norm after each mp layer: 2.531585693359375
Norm after each mp layer: 16.667940139770508
Norm after each mp layer: 89.84326171875
Norm before input: 0.2552422881126404
Norm after input: 0.5262787938117981
Norm after each mp layer: 2.5332841873168945
Norm after each mp layer: 16.69317626953125
Norm after each mp layer: 89.93380737304688
Norm before input: 0.2552422881126404
Norm after input: 0.5262787938117981
Norm after each mp layer: 2.5332841873168945
Norm after each mp layer: 16.69317626953125
Norm after each mp layer: 89.93380737304688
Norm before input: 0.2552422881126404
Norm after input: 0.5263572335243225
Norm after each mp layer: 2.5345823764801025
Norm after each mp layer: 16.71533203125
Norm after each mp layer: 90.0428466796875
Epoch: 175, Loss: 0.0265, Energy: 824556.5000, Train: 99.59%, Valid: 76.40%, Test: 76.40%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5263572335243225
Norm after each mp layer: 2.5345823764801025
Norm after each mp layer: 16.71533203125
Norm after each mp layer: 90.0428466796875
Norm before input: 0.2552422881126404
Norm after input: 0.5264593958854675
Norm after each mp layer: 2.536020517349243
Norm after each mp layer: 16.73855972290039
Norm after each mp layer: 90.0989761352539
Norm before input: 0.2552422881126404
Norm after input: 0.5264593958854675
Norm after each mp layer: 2.536020517349243
Norm after each mp layer: 16.73855972290039
Norm after each mp layer: 90.0989761352539
Norm before input: 0.2552422881126404
Norm after input: 0.5266278386116028
Norm after each mp layer: 2.5378940105438232
Norm after each mp layer: 16.766563415527344
Norm after each mp layer: 90.13468933105469
Norm before input: 0.2552422881126404
Norm after input: 0.5266278386116028
Norm after each mp layer: 2.5378940105438232
Norm after each mp layer: 16.766563415527344
Norm after each mp layer: 90.13468933105469
Norm before input: 0.2552422881126404
Norm after input: 0.5267390012741089
Norm after each mp layer: 2.53930926322937
Norm after each mp layer: 16.790103912353516
Norm after each mp layer: 90.1617660522461
Norm before input: 0.2552422881126404
Norm after input: 0.5267390012741089
Norm after each mp layer: 2.53930926322937
Norm after each mp layer: 16.790103912353516
Norm after each mp layer: 90.1617660522461
Norm before input: 0.2552422881126404
Norm after input: 0.5268208384513855
Norm after each mp layer: 2.5404727458953857
Norm after each mp layer: 16.81128692626953
Norm after each mp layer: 90.22384643554688
Norm before input: 0.2552422881126404
Norm after input: 0.5268208384513855
Norm after each mp layer: 2.540472984313965
Norm after each mp layer: 16.81128692626953
Norm after each mp layer: 90.22384643554688
Norm before input: 0.2552422881126404
Norm after input: 0.5269611477851868
Norm after each mp layer: 2.542100667953491
Norm after each mp layer: 16.836854934692383
Norm after each mp layer: 90.30957794189453
Epoch: 180, Loss: 0.0249, Energy: 803602.8750, Train: 99.67%, Valid: 76.00%, Test: 76.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5269611477851868
Norm after each mp layer: 2.542100667953491
Norm after each mp layer: 16.836854934692383
Norm after each mp layer: 90.30957794189453
Norm before input: 0.2552422881126404
Norm after input: 0.5270713567733765
Norm after each mp layer: 2.543630361557007
Norm after each mp layer: 16.85970687866211
Norm after each mp layer: 90.32929229736328
Norm before input: 0.2552422881126404
Norm after input: 0.5270713567733765
Norm after each mp layer: 2.543630361557007
Norm after each mp layer: 16.85970687866211
Norm after each mp layer: 90.32929229736328
Norm before input: 0.2552422881126404
Norm after input: 0.5271775126457214
Norm after each mp layer: 2.5450992584228516
Norm after each mp layer: 16.88222312927246
Norm after each mp layer: 90.36761474609375
Norm before input: 0.2552422881126404
Norm after input: 0.5271775126457214
Norm after each mp layer: 2.5450992584228516
Norm after each mp layer: 16.88222312927246
Norm after each mp layer: 90.36761474609375
Norm before input: 0.2552422881126404
Norm after input: 0.5272778272628784
Norm after each mp layer: 2.546409845352173
Norm after each mp layer: 16.904104232788086
Norm after each mp layer: 90.45341491699219
Norm before input: 0.2552422881126404
Norm after input: 0.5272778272628784
Norm after each mp layer: 2.546409845352173
Norm after each mp layer: 16.904104232788086
Norm after each mp layer: 90.45341491699219
Norm before input: 0.2552422881126404
Norm after input: 0.5273745656013489
Norm after each mp layer: 2.5476770401000977
Norm after each mp layer: 16.925437927246094
Norm after each mp layer: 90.51676177978516
Norm before input: 0.2552422881126404
Norm after input: 0.5273745656013489
Norm after each mp layer: 2.5476770401000977
Norm after each mp layer: 16.925437927246094
Norm after each mp layer: 90.51676177978516
Norm before input: 0.2552422881126404
Norm after input: 0.5275225043296814
Norm after each mp layer: 2.5493557453155518
Norm after each mp layer: 16.951000213623047
Norm after each mp layer: 90.54716491699219
Epoch: 185, Loss: 0.0234, Energy: 784903.5625, Train: 99.67%, Valid: 75.60%, Test: 76.20%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5275225043296814
Norm after each mp layer: 2.5493557453155518
Norm after each mp layer: 16.951000213623047
Norm after each mp layer: 90.54716491699219
Norm before input: 0.2552422881126404
Norm after input: 0.5276153683662415
Norm after each mp layer: 2.5506622791290283
Norm after each mp layer: 16.972084045410156
Norm after each mp layer: 90.53123474121094
Norm before input: 0.2552422881126404
Norm after input: 0.5276153683662415
Norm after each mp layer: 2.5506622791290283
Norm after each mp layer: 16.972084045410156
Norm after each mp layer: 90.53123474121094
Norm before input: 0.2552422881126404
Norm after input: 0.5277245044708252
Norm after each mp layer: 2.5519680976867676
Norm after each mp layer: 16.994295120239258
Norm after each mp layer: 90.58465576171875
Norm before input: 0.2552422881126404
Norm after input: 0.5277245044708252
Norm after each mp layer: 2.5519680976867676
Norm after each mp layer: 16.994295120239258
Norm after each mp layer: 90.58465576171875
Norm before input: 0.2552422881126404
Norm after input: 0.5277954339981079
Norm after each mp layer: 2.5529541969299316
Norm after each mp layer: 17.012659072875977
Norm after each mp layer: 90.64253997802734
Norm before input: 0.2552422881126404
Norm after input: 0.5277954339981079
Norm after each mp layer: 2.5529541969299316
Norm after each mp layer: 17.012659072875977
Norm after each mp layer: 90.64253997802734
Norm before input: 0.2552422881126404
Norm after input: 0.5279115438461304
Norm after each mp layer: 2.5542845726013184
Norm after each mp layer: 17.034143447875977
Norm after each mp layer: 90.69947814941406
Norm before input: 0.2552422881126404
Norm after input: 0.5279115438461304
Norm after each mp layer: 2.5542845726013184
Norm after each mp layer: 17.034143447875977
Norm after each mp layer: 90.69947814941406
Norm before input: 0.2552422881126404
Norm after input: 0.5279996991157532
Norm after each mp layer: 2.5554592609405518
Norm after each mp layer: 17.05293083190918
Norm after each mp layer: 90.71955108642578
Epoch: 190, Loss: 0.0221, Energy: 763805.3125, Train: 99.67%, Valid: 75.60%, Test: 76.00%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5279996991157532
Norm after each mp layer: 2.5554592609405518
Norm after each mp layer: 17.05293083190918
Norm after each mp layer: 90.71955871582031
Norm before input: 0.2552422881126404
Norm after input: 0.5281155705451965
Norm after each mp layer: 2.556772470474243
Norm after each mp layer: 17.073930740356445
Norm after each mp layer: 90.78170776367188
Norm before input: 0.2552422881126404
Norm after input: 0.5281155705451965
Norm after each mp layer: 2.556772470474243
Norm after each mp layer: 17.073930740356445
Norm after each mp layer: 90.78170776367188
Norm before input: 0.2552422881126404
Norm after input: 0.528181254863739
Norm after each mp layer: 2.557711362838745
Norm after each mp layer: 17.090604782104492
Norm after each mp layer: 90.815673828125
Norm before input: 0.2552422881126404
Norm after input: 0.528181254863739
Norm after each mp layer: 2.557711362838745
Norm after each mp layer: 17.090604782104492
Norm after each mp layer: 90.81566619873047
Norm before input: 0.2552422881126404
Norm after input: 0.5283209681510925
Norm after each mp layer: 2.5591516494750977
Norm after each mp layer: 17.113332748413086
Norm after each mp layer: 90.8597183227539
Norm before input: 0.2552422881126404
Norm after input: 0.5283209681510925
Norm after each mp layer: 2.5591516494750977
Norm after each mp layer: 17.113332748413086
Norm after each mp layer: 90.8597183227539
Norm before input: 0.2552422881126404
Norm after input: 0.5283554792404175
Norm after each mp layer: 2.5598576068878174
Norm after each mp layer: 17.12685775756836
Norm after each mp layer: 90.82146453857422
Norm before input: 0.2552422881126404
Norm after input: 0.5283554792404175
Norm after each mp layer: 2.5598576068878174
Norm after each mp layer: 17.12685775756836
Norm after each mp layer: 90.82146453857422
Norm before input: 0.2552422881126404
Norm after input: 0.5285452604293823
Norm after each mp layer: 2.561478614807129
Norm after each mp layer: 17.152929306030273
Norm after each mp layer: 90.9189224243164
Epoch: 195, Loss: 0.0210, Energy: 742566.6250, Train: 99.75%, Valid: 75.20%, Test: 75.60%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5285452604293823
Norm after each mp layer: 2.561478614807129
Norm after each mp layer: 17.152929306030273
Norm after each mp layer: 90.9189224243164
Norm before input: 0.2552422881126404
Norm after input: 0.5284851789474487
Norm after each mp layer: 2.56141996383667
Norm after each mp layer: 17.158002853393555
Norm after each mp layer: 90.87640380859375
Norm before input: 0.2552422881126404
Norm after input: 0.5284851789474487
Norm after each mp layer: 2.56141996383667
Norm after each mp layer: 17.158002853393555
Norm after each mp layer: 90.87640380859375
Norm before input: 0.2552422881126404
Norm after input: 0.5286761522293091
Norm after each mp layer: 2.5630056858062744
Norm after each mp layer: 17.182958602905273
Norm after each mp layer: 90.97248077392578
Norm before input: 0.2552422881126404
Norm after input: 0.5286761522293091
Norm after each mp layer: 2.5630056858062744
Norm after each mp layer: 17.182958602905273
Norm after each mp layer: 90.97248077392578
Norm before input: 0.2552422881126404
Norm after input: 0.5287321209907532
Norm after each mp layer: 2.5637712478637695
Norm after each mp layer: 17.196887969970703
Norm after each mp layer: 90.95743560791016
Norm before input: 0.2552422881126404
Norm after input: 0.5287321209907532
Norm after each mp layer: 2.5637712478637695
Norm after each mp layer: 17.196887969970703
Norm after each mp layer: 90.95743560791016
Norm before input: 0.2552422881126404
Norm after input: 0.5287374258041382
Norm after each mp layer: 2.56416392326355
Norm after each mp layer: 17.206254959106445
Norm after each mp layer: 90.927978515625
Norm before input: 0.2552422881126404
Norm after input: 0.5287374258041382
Norm after each mp layer: 2.56416392326355
Norm after each mp layer: 17.206254959106445
Norm after each mp layer: 90.927978515625
Norm before input: 0.2552422881126404
Norm after input: 0.5289497375488281
Norm after each mp layer: 2.565786361694336
Norm after each mp layer: 17.231609344482422
Norm after each mp layer: 91.05431365966797
Epoch: 200, Loss: 0.0200, Energy: 722221.5000, Train: 99.75%, Valid: 75.20%, Test: 75.60%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5289497375488281
Norm after each mp layer: 2.565786361694336
Norm after each mp layer: 17.231609344482422
Norm after each mp layer: 91.05431365966797
Norm before input: 0.2552422881126404
Norm after input: 0.5289308428764343
Norm after each mp layer: 2.5659396648406982
Norm after each mp layer: 17.238264083862305
Norm after each mp layer: 91.01255798339844
Norm before input: 0.2552422881126404
Norm after input: 0.5289308428764343
Norm after each mp layer: 2.5659396648406982
Norm after each mp layer: 17.238264083862305
Norm after each mp layer: 91.01255798339844
Norm before input: 0.2552422881126404
Norm after input: 0.5290135741233826
Norm after each mp layer: 2.5667507648468018
Norm after each mp layer: 17.25328826904297
Norm after each mp layer: 91.01493835449219
Norm before input: 0.2552422881126404
Norm after input: 0.5290135741233826
Norm after each mp layer: 2.5667507648468018
Norm after each mp layer: 17.25328826904297
Norm after each mp layer: 91.01493835449219
Norm before input: 0.2552422881126404
Norm after input: 0.5291405320167542
Norm after each mp layer: 2.567795991897583
Norm after each mp layer: 17.271799087524414
Norm after each mp layer: 91.06167602539062
Norm before input: 0.2552422881126404
Norm after input: 0.5291405320167542
Norm after each mp layer: 2.567795991897583
Norm after each mp layer: 17.271799087524414
Norm after each mp layer: 91.06167602539062
Norm before input: 0.2552422881126404
Norm after input: 0.5290977358818054
Norm after each mp layer: 2.567732572555542
Norm after each mp layer: 17.276336669921875
Norm after each mp layer: 91.01641082763672
Norm before input: 0.2552422881126404
Norm after input: 0.5290977358818054
Norm after each mp layer: 2.567732572555542
Norm after each mp layer: 17.276336669921875
Norm after each mp layer: 91.01641082763672
Norm before input: 0.2552422881126404
Norm after input: 0.5292599201202393
Norm after each mp layer: 2.5689773559570312
Norm after each mp layer: 17.296964645385742
Norm after each mp layer: 91.08151245117188
Epoch: 205, Loss: 0.0190, Energy: 702855.1875, Train: 99.75%, Valid: 75.40%, Test: 75.30%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5292599201202393
Norm after each mp layer: 2.5689775943756104
Norm after each mp layer: 17.296964645385742
Norm after each mp layer: 91.08151245117188
Norm before input: 0.2552422881126404
Norm after input: 0.5293229818344116
Norm after each mp layer: 2.5696356296539307
Norm after each mp layer: 17.309642791748047
Norm after each mp layer: 91.0643539428711
Norm before input: 0.2552422881126404
Norm after input: 0.5293229818344116
Norm after each mp layer: 2.5696356296539307
Norm after each mp layer: 17.309642791748047
Norm after each mp layer: 91.0643539428711
Norm before input: 0.2552422881126404
Norm after input: 0.5293304324150085
Norm after each mp layer: 2.5699148178100586
Norm after each mp layer: 17.317678451538086
Norm after each mp layer: 91.0290298461914
Norm before input: 0.2552422881126404
Norm after input: 0.5293304324150085
Norm after each mp layer: 2.5699148178100586
Norm after each mp layer: 17.317678451538086
Norm after each mp layer: 91.0290298461914
Norm before input: 0.2552422881126404
Norm after input: 0.5294970273971558
Norm after each mp layer: 2.571129560470581
Norm after each mp layer: 17.338048934936523
Norm after each mp layer: 91.11795043945312
Norm before input: 0.2552422881126404
Norm after input: 0.5294970273971558
Norm after each mp layer: 2.571129560470581
Norm after each mp layer: 17.338048934936523
Norm after each mp layer: 91.11795043945312
Norm before input: 0.2552422881126404
Norm after input: 0.5294846892356873
Norm after each mp layer: 2.571221113204956
Norm after each mp layer: 17.344038009643555
Norm after each mp layer: 91.09288787841797
Norm before input: 0.2552422881126404
Norm after input: 0.5294846892356873
Norm after each mp layer: 2.571221113204956
Norm after each mp layer: 17.344038009643555
Norm after each mp layer: 91.09288787841797
Norm before input: 0.2552422881126404
Norm after input: 0.5295594930648804
Norm after each mp layer: 2.5718815326690674
Norm after each mp layer: 17.357135772705078
Norm after each mp layer: 91.10197448730469
Epoch: 210, Loss: 0.0180, Energy: 685125.0625, Train: 99.75%, Valid: 75.20%, Test: 75.20%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5295594930648804
Norm after each mp layer: 2.5718815326690674
Norm after each mp layer: 17.357135772705078
Norm after each mp layer: 91.10197448730469
Norm before input: 0.2552422881126404
Norm after input: 0.529657781124115
Norm after each mp layer: 2.5726702213287354
Norm after each mp layer: 17.371959686279297
Norm after each mp layer: 91.12998962402344
Norm before input: 0.2552422881126404
Norm after input: 0.529657781124115
Norm after each mp layer: 2.5726702213287354
Norm after each mp layer: 17.371959686279297
Norm after each mp layer: 91.1299819946289
Norm before input: 0.2552422881126404
Norm after input: 0.529649019241333
Norm after each mp layer: 2.5727624893188477
Norm after each mp layer: 17.37785530090332
Norm after each mp layer: 91.1070327758789
Norm before input: 0.2552422881126404
Norm after input: 0.529649019241333
Norm after each mp layer: 2.5727624893188477
Norm after each mp layer: 17.37785530090332
Norm after each mp layer: 91.1070327758789
Norm before input: 0.2552422881126404
Norm after input: 0.5298001766204834
Norm after each mp layer: 2.573870897293091
Norm after each mp layer: 17.39650535583496
Norm after each mp layer: 91.17170715332031
Norm before input: 0.2552422881126404
Norm after input: 0.5298001766204834
Norm after each mp layer: 2.573870897293091
Norm after each mp layer: 17.39650535583496
Norm after each mp layer: 91.17170715332031
Norm before input: 0.2552422881126404
Norm after input: 0.5298163890838623
Norm after each mp layer: 2.57417368888855
Norm after each mp layer: 17.40447425842285
Norm after each mp layer: 91.1400375366211
Norm before input: 0.2552422881126404
Norm after input: 0.5298163890838623
Norm after each mp layer: 2.57417368888855
Norm after each mp layer: 17.40447425842285
Norm after each mp layer: 91.1400375366211
Norm before input: 0.2552422881126404
Norm after input: 0.5298687815666199
Norm after each mp layer: 2.5746991634368896
Norm after each mp layer: 17.415302276611328
Norm after each mp layer: 91.13141632080078
Epoch: 215, Loss: 0.0171, Energy: 667075.6875, Train: 99.75%, Valid: 75.40%, Test: 75.30%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5298687815666199
Norm after each mp layer: 2.5746991634368896
Norm after each mp layer: 17.415302276611328
Norm after each mp layer: 91.13141632080078
Norm before input: 0.2552422881126404
Norm after input: 0.5299800038337708
Norm after each mp layer: 2.5755443572998047
Norm after each mp layer: 17.43048095703125
Norm after each mp layer: 91.17404174804688
Norm before input: 0.2552422881126404
Norm after input: 0.5299800038337708
Norm after each mp layer: 2.5755443572998047
Norm after each mp layer: 17.43048095703125
Norm after each mp layer: 91.17404174804688
Norm before input: 0.2552422881126404
Norm after input: 0.5299753546714783
Norm after each mp layer: 2.575641393661499
Norm after each mp layer: 17.436115264892578
Norm after each mp layer: 91.15330505371094
Norm before input: 0.2552422881126404
Norm after input: 0.5299753546714783
Norm after each mp layer: 2.575641393661499
Norm after each mp layer: 17.436115264892578
Norm after each mp layer: 91.15330505371094
Norm before input: 0.2552422881126404
Norm after input: 0.5301000475883484
Norm after each mp layer: 2.5765552520751953
Norm after each mp layer: 17.452190399169922
Norm after each mp layer: 91.19293212890625
Norm before input: 0.2552422881126404
Norm after input: 0.5301000475883484
Norm after each mp layer: 2.5765552520751953
Norm after each mp layer: 17.452190399169922
Norm after each mp layer: 91.19293975830078
Norm before input: 0.2552422881126404
Norm after input: 0.5301040410995483
Norm after each mp layer: 2.5767135620117188
Norm after each mp layer: 17.458465576171875
Norm after each mp layer: 91.1614761352539
Norm before input: 0.2552422881126404
Norm after input: 0.5301040410995483
Norm after each mp layer: 2.5767135620117188
Norm after each mp layer: 17.458465576171875
Norm after each mp layer: 91.1614761352539
Norm before input: 0.2552422881126404
Norm after input: 0.5301738977432251
Norm after each mp layer: 2.5772500038146973
Norm after each mp layer: 17.469736099243164
Norm after each mp layer: 91.18030548095703
Epoch: 220, Loss: 0.0164, Energy: 649178.0625, Train: 99.75%, Valid: 75.40%, Test: 75.60%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5301738977432251
Norm after each mp layer: 2.5772500038146973
Norm after each mp layer: 17.469736099243164
Norm after each mp layer: 91.18030548095703
Norm before input: 0.2552422881126404
Norm after input: 0.5302475690841675
Norm after each mp layer: 2.5777878761291504
Norm after each mp layer: 17.48109245300293
Norm after each mp layer: 91.2095947265625
Norm before input: 0.2552422881126404
Norm after input: 0.5302475690841675
Norm after each mp layer: 2.5777878761291504
Norm after each mp layer: 17.48109245300293
Norm after each mp layer: 91.2095947265625
Norm before input: 0.2552422881126404
Norm after input: 0.5302527546882629
Norm after each mp layer: 2.577914237976074
Norm after each mp layer: 17.48692512512207
Norm after each mp layer: 91.18993377685547
Norm before input: 0.2552422881126404
Norm after input: 0.5302527546882629
Norm after each mp layer: 2.577914237976074
Norm after each mp layer: 17.48692512512207
Norm after each mp layer: 91.18993377685547
Norm before input: 0.2552422881126404
Norm after input: 0.5303646326065063
Norm after each mp layer: 2.5787177085876465
Norm after each mp layer: 17.501323699951172
Norm after each mp layer: 91.21745300292969
Norm before input: 0.2552422881126404
Norm after input: 0.5303646326065063
Norm after each mp layer: 2.5787177085876465
Norm after each mp layer: 17.501323699951172
Norm after each mp layer: 91.21745300292969
Norm before input: 0.2552422881126404
Norm after input: 0.5303534865379333
Norm after each mp layer: 2.5787267684936523
Norm after each mp layer: 17.505535125732422
Norm after each mp layer: 91.18484497070312
Norm before input: 0.2552422881126404
Norm after input: 0.5303534865379333
Norm after each mp layer: 2.5787267684936523
Norm after each mp layer: 17.50553321838379
Norm after each mp layer: 91.18484497070312
Norm before input: 0.2552422881126404
Norm after input: 0.530470073223114
Norm after each mp layer: 2.579498529434204
Norm after each mp layer: 17.519878387451172
Norm after each mp layer: 91.23065948486328
Epoch: 225, Loss: 0.0157, Energy: 632453.9375, Train: 99.75%, Valid: 75.60%, Test: 75.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.530470073223114
Norm after each mp layer: 2.579498529434204
Norm after each mp layer: 17.519878387451172
Norm after each mp layer: 91.23065948486328
Norm before input: 0.2552422881126404
Norm after input: 0.5304682850837708
Norm after each mp layer: 2.579538583755493
Norm after each mp layer: 17.524696350097656
Norm after each mp layer: 91.2056884765625
Norm before input: 0.2552422881126404
Norm after input: 0.5304682850837708
Norm after each mp layer: 2.579538583755493
Norm after each mp layer: 17.524696350097656
Norm after each mp layer: 91.2056884765625
Norm before input: 0.2552422881126404
Norm after input: 0.5305452942848206
Norm after each mp layer: 2.580082654953003
Norm after each mp layer: 17.535911560058594
Norm after each mp layer: 91.21226501464844
Norm before input: 0.2552422881126404
Norm after input: 0.5305452942848206
Norm after each mp layer: 2.580082654953003
Norm after each mp layer: 17.535913467407227
Norm after each mp layer: 91.21226501464844
Norm before input: 0.2552422881126404
Norm after input: 0.5305748581886292
Norm after each mp layer: 2.5803029537200928
Norm after each mp layer: 17.543031692504883
Norm after each mp layer: 91.20315551757812
Norm before input: 0.2552422881126404
Norm after input: 0.5305748581886292
Norm after each mp layer: 2.5803029537200928
Norm after each mp layer: 17.543031692504883
Norm after each mp layer: 91.20315551757812
Norm before input: 0.2552422881126404
Norm after input: 0.5306212306022644
Norm after each mp layer: 2.5805931091308594
Norm after each mp layer: 17.551197052001953
Norm after each mp layer: 91.21569061279297
Norm before input: 0.2552422881126404
Norm after input: 0.5306212306022644
Norm after each mp layer: 2.5805931091308594
Norm after each mp layer: 17.551197052001953
Norm after each mp layer: 91.21569061279297
Norm before input: 0.2552422881126404
Norm after input: 0.5306813716888428
Norm after each mp layer: 2.5809803009033203
Norm after each mp layer: 17.560422897338867
Norm after each mp layer: 91.22933959960938
Epoch: 230, Loss: 0.0150, Energy: 617175.8750, Train: 99.75%, Valid: 75.40%, Test: 75.50%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5306813716888428
Norm after each mp layer: 2.5809803009033203
Norm after each mp layer: 17.560422897338867
Norm after each mp layer: 91.22933959960938
Norm before input: 0.2552422881126404
Norm after input: 0.5306978225708008
Norm after each mp layer: 2.5811169147491455
Norm after each mp layer: 17.566144943237305
Norm after each mp layer: 91.20973205566406
Norm before input: 0.2552422881126404
Norm after input: 0.5306978225708008
Norm after each mp layer: 2.5811169147491455
Norm after each mp layer: 17.566144943237305
Norm after each mp layer: 91.20973205566406
Norm before input: 0.2552422881126404
Norm after input: 0.530781626701355
Norm after each mp layer: 2.5816588401794434
Norm after each mp layer: 17.577232360839844
Norm after each mp layer: 91.22900390625
Norm before input: 0.2552422881126404
Norm after input: 0.530781626701355
Norm after each mp layer: 2.5816588401794434
Norm after each mp layer: 17.577232360839844
Norm after each mp layer: 91.22900390625
Norm before input: 0.2552422881126404
Norm after input: 0.5307651162147522
Norm after each mp layer: 2.581547737121582
Norm after each mp layer: 17.57994270324707
Norm after each mp layer: 91.20530700683594
Norm before input: 0.2552422881126404
Norm after input: 0.5307651162147522
Norm after each mp layer: 2.581547737121582
Norm after each mp layer: 17.57994270324707
Norm after each mp layer: 91.20530700683594
Norm before input: 0.2552422881126404
Norm after input: 0.5308921337127686
Norm after each mp layer: 2.5823259353637695
Norm after each mp layer: 17.594242095947266
Norm after each mp layer: 91.25189971923828
Norm before input: 0.2552422881126404
Norm after input: 0.5308921337127686
Norm after each mp layer: 2.5823259353637695
Norm after each mp layer: 17.594242095947266
Norm after each mp layer: 91.25189971923828
Norm before input: 0.2552422881126404
Norm after input: 0.5308235883712769
Norm after each mp layer: 2.581890821456909
Norm after each mp layer: 17.5926456451416
Norm after each mp layer: 91.19266510009766
Epoch: 235, Loss: 0.0145, Energy: 603312.9375, Train: 99.83%, Valid: 75.40%, Test: 75.60%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5308235883712769
Norm after each mp layer: 2.581890821456909
Norm after each mp layer: 17.5926456451416
Norm after each mp layer: 91.19266510009766
Norm before input: 0.2552422881126404
Norm after input: 0.5309919118881226
Norm after each mp layer: 2.5829193592071533
Norm after each mp layer: 17.610097885131836
Norm after each mp layer: 91.25096893310547
Norm before input: 0.2552422881126404
Norm after input: 0.5309919118881226
Norm after each mp layer: 2.5829193592071533
Norm after each mp layer: 17.610097885131836
Norm after each mp layer: 91.25096893310547
Norm before input: 0.2552422881126404
Norm after input: 0.5308986306190491
Norm after each mp layer: 2.5823018550872803
Norm after each mp layer: 17.606201171875
Norm after each mp layer: 91.1856460571289
Norm before input: 0.2552422881126404
Norm after input: 0.5308986306190491
Norm after each mp layer: 2.5823018550872803
Norm after each mp layer: 17.606201171875
Norm after each mp layer: 91.1856460571289
Norm before input: 0.2552422881126404
Norm after input: 0.5310491919517517
Norm after each mp layer: 2.5831971168518066
Norm after each mp layer: 17.62177848815918
Norm after each mp layer: 91.23778533935547
Norm before input: 0.2552422881126404
Norm after input: 0.5310491919517517
Norm after each mp layer: 2.5831971168518066
Norm after each mp layer: 17.62177848815918
Norm after each mp layer: 91.23778533935547
Norm before input: 0.2552422881126404
Norm after input: 0.5310187339782715
Norm after each mp layer: 2.582960844039917
Norm after each mp layer: 17.622760772705078
Norm after each mp layer: 91.20474243164062
Norm before input: 0.2552422881126404
Norm after input: 0.5310187339782715
Norm after each mp layer: 2.582960844039917
Norm after each mp layer: 17.622760772705078
Norm after each mp layer: 91.20474243164062
Norm before input: 0.2552422881126404
Norm after input: 0.5310730934143066
Norm after each mp layer: 2.583263397216797
Norm after each mp layer: 17.63064193725586
Norm after each mp layer: 91.2062759399414
Epoch: 240, Loss: 0.0139, Energy: 587103.1875, Train: 99.75%, Valid: 75.40%, Test: 75.60%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5310730934143066
Norm after each mp layer: 2.583263397216797
Norm after each mp layer: 17.63064193725586
Norm after each mp layer: 91.20628356933594
Norm before input: 0.2552422881126404
Norm after input: 0.5311295390129089
Norm after each mp layer: 2.5835745334625244
Norm after each mp layer: 17.638565063476562
Norm after each mp layer: 91.20893096923828
Norm before input: 0.2552422881126404
Norm after input: 0.5311295390129089
Norm after each mp layer: 2.5835745334625244
Norm after each mp layer: 17.638565063476562
Norm after each mp layer: 91.20893096923828
Norm before input: 0.2552422881126404
Norm after input: 0.5311112403869629
Norm after each mp layer: 2.5833890438079834
Norm after each mp layer: 17.640256881713867
Norm after each mp layer: 91.18883514404297
Norm before input: 0.2552422881126404
Norm after input: 0.5311112403869629
Norm after each mp layer: 2.5833890438079834
Norm after each mp layer: 17.640256881713867
Norm after each mp layer: 91.18883514404297
Norm before input: 0.2552422881126404
Norm after input: 0.5312278270721436
Norm after each mp layer: 2.5840401649475098
Norm after each mp layer: 17.652835845947266
Norm after each mp layer: 91.23160552978516
Norm before input: 0.2552422881126404
Norm after input: 0.5312278270721436
Norm after each mp layer: 2.5840401649475098
Norm after each mp layer: 17.652835845947266
Norm after each mp layer: 91.23160552978516
Norm before input: 0.2552422881126404
Norm after input: 0.5311500430107117
Norm after each mp layer: 2.5835087299346924
Norm after each mp layer: 17.649765014648438
Norm after each mp layer: 91.17179107666016
Norm before input: 0.2552422881126404
Norm after input: 0.5311500430107117
Norm after each mp layer: 2.5835087299346924
Norm after each mp layer: 17.649765014648438
Norm after each mp layer: 91.17179107666016
Norm before input: 0.2552422881126404
Norm after input: 0.5313047766685486
Norm after each mp layer: 2.5844180583953857
Norm after each mp layer: 17.66531753540039
Norm after each mp layer: 91.21818542480469
Epoch: 245, Loss: 0.0134, Energy: 572378.0000, Train: 99.83%, Valid: 75.40%, Test: 75.50%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5313047766685486
Norm after each mp layer: 2.5844180583953857
Norm after each mp layer: 17.66531753540039
Norm after each mp layer: 91.21818542480469
Norm before input: 0.2552422881126404
Norm after input: 0.5312207937240601
Norm after each mp layer: 2.583804130554199
Norm after each mp layer: 17.661401748657227
Norm after each mp layer: 91.16963195800781
Norm before input: 0.2552422881126404
Norm after input: 0.5312207937240601
Norm after each mp layer: 2.583804130554199
Norm after each mp layer: 17.661401748657227
Norm after each mp layer: 91.16963195800781
Norm before input: 0.2552422881126404
Norm after input: 0.5313380360603333
Norm after each mp layer: 2.584458112716675
Norm after each mp layer: 17.673643112182617
Norm after each mp layer: 91.20157623291016
Norm before input: 0.2552422881126404
Norm after input: 0.5313380360603333
Norm after each mp layer: 2.584458112716675
Norm after each mp layer: 17.673643112182617
Norm after each mp layer: 91.20157623291016
Norm before input: 0.2552422881126404
Norm after input: 0.5313220620155334
Norm after each mp layer: 2.5842790603637695
Norm after each mp layer: 17.675138473510742
Norm after each mp layer: 91.17375183105469
Norm before input: 0.2552422881126404
Norm after input: 0.5313220620155334
Norm after each mp layer: 2.5842790603637695
Norm after each mp layer: 17.675138473510742
Norm after each mp layer: 91.17375183105469
Norm before input: 0.2552422881126404
Norm after input: 0.5313558578491211
Norm after each mp layer: 2.5843944549560547
Norm after each mp layer: 17.68062973022461
Norm after each mp layer: 91.17230987548828
Norm before input: 0.2552422881126404
Norm after input: 0.5313558578491211
Norm after each mp layer: 2.5843944549560547
Norm after each mp layer: 17.68062973022461
Norm after each mp layer: 91.17230987548828
Norm before input: 0.2552422881126404
Norm after input: 0.5314030051231384
Norm after each mp layer: 2.5845978260040283
Norm after each mp layer: 17.68714141845703
Norm after each mp layer: 91.17363739013672
Epoch: 250, Loss: 0.0128, Energy: 560055.3750, Train: 99.83%, Valid: 75.40%, Test: 75.40%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5314030051231384
Norm after each mp layer: 2.5845978260040283
Norm after each mp layer: 17.68714141845703
Norm after each mp layer: 91.17363739013672
Norm before input: 0.2552422881126404
Norm after input: 0.5313833355903625
Norm after each mp layer: 2.584374189376831
Norm after each mp layer: 17.688129425048828
Norm after each mp layer: 91.14921569824219
Norm before input: 0.2552422881126404
Norm after input: 0.5313833355903625
Norm after each mp layer: 2.584374189376831
Norm after each mp layer: 17.688129425048828
Norm after each mp layer: 91.14921569824219
Norm before input: 0.2552422881126404
Norm after input: 0.5314798951148987
Norm after each mp layer: 2.5848605632781982
Norm after each mp layer: 17.69843292236328
Norm after each mp layer: 91.17948913574219
Norm before input: 0.2552422881126404
Norm after input: 0.5314798951148987
Norm after each mp layer: 2.5848605632781982
Norm after each mp layer: 17.69843292236328
Norm after each mp layer: 91.17948913574219
Norm before input: 0.2552422881126404
Norm after input: 0.5314063429832458
Norm after each mp layer: 2.5842981338500977
Norm after each mp layer: 17.69496726989746
Norm after each mp layer: 91.12962341308594
Norm before input: 0.2552422881126404
Norm after input: 0.5314063429832458
Norm after each mp layer: 2.5842981338500977
Norm after each mp layer: 17.69496726989746
Norm after each mp layer: 91.12962341308594
Norm before input: 0.2552422881126404
Norm after input: 0.5315465927124023
Norm after each mp layer: 2.5850751399993896
Norm after each mp layer: 17.70868682861328
Norm after each mp layer: 91.1643295288086
Norm before input: 0.2552422881126404
Norm after input: 0.5315465927124023
Norm after each mp layer: 2.5850751399993896
Norm after each mp layer: 17.70868682861328
Norm after each mp layer: 91.1643295288086
Norm before input: 0.2552422881126404
Norm after input: 0.5314486622810364
Norm after each mp layer: 2.584322214126587
Norm after each mp layer: 17.70294189453125
Norm after each mp layer: 91.11353302001953
Epoch: 255, Loss: 0.0125, Energy: 548222.1875, Train: 99.83%, Valid: 75.60%, Test: 75.30%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5314486622810364
Norm after each mp layer: 2.584322214126587
Norm after each mp layer: 17.70294189453125
Norm after each mp layer: 91.11353302001953
Norm before input: 0.2552422881126404
Norm after input: 0.5315762162208557
Norm after each mp layer: 2.58498477935791
Norm after each mp layer: 17.715314865112305
Norm after each mp layer: 91.14805603027344
Norm before input: 0.2552422881126404
Norm after input: 0.5315762162208557
Norm after each mp layer: 2.58498477935791
Norm after each mp layer: 17.715312957763672
Norm after each mp layer: 91.14805603027344
Norm before input: 0.2552422881126404
Norm after input: 0.5315131545066833
Norm after each mp layer: 2.584446668624878
Norm after each mp layer: 17.71225929260254
Norm after each mp layer: 91.10537719726562
Norm before input: 0.2552422881126404
Norm after input: 0.5315131545066833
Norm after each mp layer: 2.584446668624878
Norm after each mp layer: 17.71225929260254
Norm after each mp layer: 91.10537719726562
Norm before input: 0.2552422881126404
Norm after input: 0.5315825939178467
Norm after each mp layer: 2.5847318172454834
Norm after each mp layer: 17.71988868713379
Norm after each mp layer: 91.11531066894531
Norm before input: 0.2552422881126404
Norm after input: 0.5315825939178467
Norm after each mp layer: 2.5847318172454834
Norm after each mp layer: 17.71988868713379
Norm after each mp layer: 91.11531066894531
Norm before input: 0.2552422881126404
Norm after input: 0.5315698981285095
Norm after each mp layer: 2.584496021270752
Norm after each mp layer: 17.720781326293945
Norm after each mp layer: 91.09278106689453
Norm before input: 0.2552422881126404
Norm after input: 0.5315698981285095
Norm after each mp layer: 2.584496021270752
Norm after each mp layer: 17.720781326293945
Norm after each mp layer: 91.09278106689453
Norm before input: 0.2552422881126404
Norm after input: 0.5315930247306824
Norm after each mp layer: 2.584477186203003
Norm after each mp layer: 17.724464416503906
Norm after each mp layer: 91.08563232421875
Epoch: 260, Loss: 0.0119, Energy: 534247.4375, Train: 99.83%, Valid: 75.60%, Test: 75.50%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5315930247306824
Norm after each mp layer: 2.584477186203003
Norm after each mp layer: 17.724464416503906
Norm after each mp layer: 91.08563232421875
Norm before input: 0.2552422881126404
Norm after input: 0.5316232442855835
Norm after each mp layer: 2.584493637084961
Norm after each mp layer: 17.7286376953125
Norm after each mp layer: 91.08343505859375
Norm before input: 0.2552422881126404
Norm after input: 0.5316232442855835
Norm after each mp layer: 2.584493637084961
Norm after each mp layer: 17.7286376953125
Norm after each mp layer: 91.08343505859375
Norm before input: 0.2552422881126404
Norm after input: 0.531602144241333
Norm after each mp layer: 2.5841927528381348
Norm after each mp layer: 17.728635787963867
Norm after each mp layer: 91.05741119384766
Norm before input: 0.2552422881126404
Norm after input: 0.531602144241333
Norm after each mp layer: 2.5841927528381348
Norm after each mp layer: 17.728635787963867
Norm after each mp layer: 91.05741119384766
Norm before input: 0.2552422881126404
Norm after input: 0.5316717624664307
Norm after each mp layer: 2.5844595432281494
Norm after each mp layer: 17.73592185974121
Norm after each mp layer: 91.0640640258789
Norm before input: 0.2552422881126404
Norm after input: 0.5316717624664307
Norm after each mp layer: 2.5844595432281494
Norm after each mp layer: 17.73592185974121
Norm after each mp layer: 91.0640640258789
Norm before input: 0.2552422881126404
Norm after input: 0.5316050052642822
Norm after each mp layer: 2.583847761154175
Norm after each mp layer: 17.73199462890625
Norm after each mp layer: 91.02519226074219
Norm before input: 0.2552422881126404
Norm after input: 0.5316050052642822
Norm after each mp layer: 2.583847761154175
Norm after each mp layer: 17.73199462890625
Norm after each mp layer: 91.02519226074219
Norm before input: 0.2552422881126404
Norm after input: 0.53171706199646
Norm after each mp layer: 2.5843536853790283
Norm after each mp layer: 17.742443084716797
Norm after each mp layer: 91.05189514160156
Epoch: 265, Loss: 0.0116, Energy: 521274.5000, Train: 99.83%, Valid: 76.00%, Test: 75.50%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.53171706199646
Norm after each mp layer: 2.5843536853790283
Norm after each mp layer: 17.742443084716797
Norm after each mp layer: 91.05189514160156
Norm before input: 0.2552422881126404
Norm after input: 0.5316050052642822
Norm after each mp layer: 2.583440065383911
Norm after each mp layer: 17.734636306762695
Norm after each mp layer: 90.99514770507812
Norm before input: 0.2552422881126404
Norm after input: 0.5316050052642822
Norm after each mp layer: 2.583440065383911
Norm after each mp layer: 17.734636306762695
Norm after each mp layer: 90.99514770507812
Norm before input: 0.2552422881126404
Norm after input: 0.5317501425743103
Norm after each mp layer: 2.584150552749634
Norm after each mp layer: 17.7475643157959
Norm after each mp layer: 91.02560424804688
Norm before input: 0.2552422881126404
Norm after input: 0.5317501425743103
Norm after each mp layer: 2.584150552749634
Norm after each mp layer: 17.7475643157959
Norm after each mp layer: 91.02560424804688
Norm before input: 0.2552422881126404
Norm after input: 0.531620979309082
Norm after each mp layer: 2.583101987838745
Norm after each mp layer: 17.73811912536621
Norm after each mp layer: 90.96613311767578
Norm before input: 0.2552422881126404
Norm after input: 0.531620979309082
Norm after each mp layer: 2.583101987838745
Norm after each mp layer: 17.73811912536621
Norm after each mp layer: 90.96613311767578
Norm before input: 0.2552422881126404
Norm after input: 0.5317491888999939
Norm after each mp layer: 2.583695411682129
Norm after each mp layer: 17.749425888061523
Norm after each mp layer: 90.98550415039062
Norm before input: 0.2552422881126404
Norm after input: 0.5317491888999939
Norm after each mp layer: 2.583695411682129
Norm after each mp layer: 17.749425888061523
Norm after each mp layer: 90.98551177978516
Norm before input: 0.2552422881126404
Norm after input: 0.5316628217697144
Norm after each mp layer: 2.582892894744873
Norm after each mp layer: 17.743183135986328
Norm after each mp layer: 90.94440460205078
Epoch: 270, Loss: 0.0112, Energy: 510656.9688, Train: 99.83%, Valid: 75.80%, Test: 75.30%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5316628217697144
Norm after each mp layer: 2.582892894744873
Norm after each mp layer: 17.743183135986328
Norm after each mp layer: 90.94440460205078
Norm before input: 0.2552422881126404
Norm after input: 0.5317246317863464
Norm after each mp layer: 2.5830366611480713
Norm after each mp layer: 17.748973846435547
Norm after each mp layer: 90.94742584228516
Norm before input: 0.2552422881126404
Norm after input: 0.5317246317863464
Norm after each mp layer: 2.5830366611480713
Norm after each mp layer: 17.748973846435547
Norm after each mp layer: 90.94742584228516
Norm before input: 0.2552422881126404
Norm after input: 0.5316989421844482
Norm after each mp layer: 2.5826306343078613
Norm after each mp layer: 17.7476806640625
Norm after each mp layer: 90.91699981689453
Norm before input: 0.2552422881126404
Norm after input: 0.5316989421844482
Norm after each mp layer: 2.5826306343078613
Norm after each mp layer: 17.7476806640625
Norm after each mp layer: 90.91699981689453
Norm before input: 0.2552422881126404
Norm after input: 0.5317044854164124
Norm after each mp layer: 2.5823984146118164
Norm after each mp layer: 17.748802185058594
Norm after each mp layer: 90.90497589111328
Norm before input: 0.2552422881126404
Norm after input: 0.5317044854164124
Norm after each mp layer: 2.5823984146118164
Norm after each mp layer: 17.748802185058594
Norm after each mp layer: 90.90497589111328
Norm before input: 0.2552422881126404
Norm after input: 0.5317222476005554
Norm after each mp layer: 2.5822415351867676
Norm after each mp layer: 17.750864028930664
Norm after each mp layer: 90.89588165283203
Norm before input: 0.2552422881126404
Norm after input: 0.5317222476005554
Norm after each mp layer: 2.5822415351867676
Norm after each mp layer: 17.750864028930664
Norm after each mp layer: 90.89588165283203
Norm before input: 0.2552422881126404
Norm after input: 0.5316892862319946
Norm after each mp layer: 2.581768035888672
Norm after each mp layer: 17.748790740966797
Norm after each mp layer: 90.86663055419922
Epoch: 275, Loss: 0.0108, Energy: 498116.7188, Train: 99.83%, Valid: 75.80%, Test: 75.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5316892862319946
Norm after each mp layer: 2.581768035888672
Norm after each mp layer: 17.748790740966797
Norm after each mp layer: 90.86662292480469
Norm before input: 0.2552422881126404
Norm after input: 0.5317457318305969
Norm after each mp layer: 2.5818493366241455
Norm after each mp layer: 17.7539119720459
Norm after each mp layer: 90.86998748779297
Norm before input: 0.2552422881126404
Norm after input: 0.5317457318305969
Norm after each mp layer: 2.5818493366241455
Norm after each mp layer: 17.7539119720459
Norm after each mp layer: 90.86998748779297
Norm before input: 0.2552422881126404
Norm after input: 0.5316647291183472
Norm after each mp layer: 2.5810556411743164
Norm after each mp layer: 17.747812271118164
Norm after each mp layer: 90.82862091064453
Norm before input: 0.2552422881126404
Norm after input: 0.5316647291183472
Norm after each mp layer: 2.5810556411743164
Norm after each mp layer: 17.747812271118164
Norm after each mp layer: 90.82862091064453
Norm before input: 0.2552422881126404
Norm after input: 0.5317699909210205
Norm after each mp layer: 2.581453561782837
Norm after each mp layer: 17.756772994995117
Norm after each mp layer: 90.84101867675781
Norm before input: 0.2552422881126404
Norm after input: 0.5317699909210205
Norm after each mp layer: 2.581453561782837
Norm after each mp layer: 17.756772994995117
Norm after each mp layer: 90.84101867675781
Norm before input: 0.2552422881126404
Norm after input: 0.531642496585846
Norm after each mp layer: 2.5803399085998535
Norm after each mp layer: 17.74666404724121
Norm after each mp layer: 90.79230499267578
Norm before input: 0.2552422881126404
Norm after input: 0.531642496585846
Norm after each mp layer: 2.5803399085998535
Norm after each mp layer: 17.74666404724121
Norm after each mp layer: 90.79230499267578
Norm before input: 0.2552422881126404
Norm after input: 0.5317815542221069
Norm after each mp layer: 2.5809500217437744
Norm after each mp layer: 17.7581787109375
Norm after each mp layer: 90.81157684326172
Epoch: 280, Loss: 0.0105, Energy: 485405.3750, Train: 99.83%, Valid: 75.80%, Test: 75.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5317815542221069
Norm after each mp layer: 2.5809500217437744
Norm after each mp layer: 17.7581787109375
Norm after each mp layer: 90.81157684326172
Norm before input: 0.2552422881126404
Norm after input: 0.5316402912139893
Norm after each mp layer: 2.579744338989258
Norm after each mp layer: 17.746829986572266
Norm after each mp layer: 90.7569351196289
Norm before input: 0.2552422881126404
Norm after input: 0.5316402912139893
Norm after each mp layer: 2.579744338989258
Norm after each mp layer: 17.746829986572266
Norm after each mp layer: 90.7569351196289
Norm before input: 0.2552422881126404
Norm after input: 0.5317647457122803
Norm after each mp layer: 2.58025860786438
Norm after each mp layer: 17.757049560546875
Norm after each mp layer: 90.76790618896484
Norm before input: 0.2552422881126404
Norm after input: 0.5317647457122803
Norm after each mp layer: 2.58025860786438
Norm after each mp layer: 17.757049560546875
Norm after each mp layer: 90.76790618896484
Norm before input: 0.2552422881126404
Norm after input: 0.5316594243049622
Norm after each mp layer: 2.5792651176452637
Norm after each mp layer: 17.74848747253418
Norm after each mp layer: 90.72748565673828
Norm before input: 0.2552422881126404
Norm after input: 0.5316594243049622
Norm after each mp layer: 2.5792651176452637
Norm after each mp layer: 17.74848747253418
Norm after each mp layer: 90.72748565673828
Norm before input: 0.2552422881126404
Norm after input: 0.5317268967628479
Norm after each mp layer: 2.5793983936309814
Norm after each mp layer: 17.75404167175293
Norm after each mp layer: 90.72627258300781
Norm before input: 0.2552422881126404
Norm after input: 0.5317268967628479
Norm after each mp layer: 2.5793983936309814
Norm after each mp layer: 17.75404167175293
Norm after each mp layer: 90.72627258300781
Norm before input: 0.2552422881126404
Norm after input: 0.5316817164421082
Norm after each mp layer: 2.578794002532959
Norm after each mp layer: 17.750431060791016
Norm after each mp layer: 90.70000457763672
Epoch: 285, Loss: 0.0101, Energy: 475490.0625, Train: 99.92%, Valid: 75.40%, Test: 75.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5316817164421082
Norm after each mp layer: 2.578794002532959
Norm after each mp layer: 17.750431060791016
Norm after each mp layer: 90.70000457763672
Norm before input: 0.2552422881126404
Norm after input: 0.5316869616508484
Norm after each mp layer: 2.5785136222839355
Norm after each mp layer: 17.750967025756836
Norm after each mp layer: 90.68819427490234
Norm before input: 0.2552422881126404
Norm after input: 0.5316869616508484
Norm after each mp layer: 2.5785136222839355
Norm after each mp layer: 17.750967025756836
Norm after each mp layer: 90.68819427490234
Norm before input: 0.2552422881126404
Norm after input: 0.5316963195800781
Norm after each mp layer: 2.578270435333252
Norm after each mp layer: 17.751911163330078
Norm after each mp layer: 90.67394256591797
Norm before input: 0.2552422881126404
Norm after input: 0.5316963195800781
Norm after each mp layer: 2.578270435333252
Norm after each mp layer: 17.751911163330078
Norm after each mp layer: 90.67394256591797
Norm before input: 0.2552422881126404
Norm after input: 0.5316557884216309
Norm after each mp layer: 2.577698230743408
Norm after each mp layer: 17.7487735748291
Norm after each mp layer: 90.65128326416016
Norm before input: 0.2552422881126404
Norm after input: 0.5316557884216309
Norm after each mp layer: 2.577698230743408
Norm after each mp layer: 17.7487735748291
Norm after each mp layer: 90.65128326416016
Norm before input: 0.2552422881126404
Norm after input: 0.5317075252532959
Norm after each mp layer: 2.5777275562286377
Norm after each mp layer: 17.75318145751953
Norm after each mp layer: 90.65184783935547
Norm before input: 0.2552422881126404
Norm after input: 0.5317075252532959
Norm after each mp layer: 2.5777275562286377
Norm after each mp layer: 17.75318145751953
Norm after each mp layer: 90.65185546875
Norm before input: 0.2552422881126404
Norm after input: 0.5316289067268372
Norm after each mp layer: 2.576918601989746
Norm after each mp layer: 17.747005462646484
Norm after each mp layer: 90.62007141113281
Epoch: 290, Loss: 0.0098, Energy: 464797.9688, Train: 99.92%, Valid: 75.40%, Test: 75.00%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5316289067268372
Norm after each mp layer: 2.576918601989746
Norm after each mp layer: 17.747005462646484
Norm after each mp layer: 90.62007141113281
Norm before input: 0.2552422881126404
Norm after input: 0.5317157506942749
Norm after each mp layer: 2.57719349861145
Norm after each mp layer: 17.75436782836914
Norm after each mp layer: 90.62626647949219
Norm before input: 0.2552422881126404
Norm after input: 0.5317157506942749
Norm after each mp layer: 2.57719349861145
Norm after each mp layer: 17.754365921020508
Norm after each mp layer: 90.62626647949219
Norm before input: 0.2552422881126404
Norm after input: 0.5316070914268494
Norm after each mp layer: 2.5761866569519043
Norm after each mp layer: 17.745746612548828
Norm after each mp layer: 90.59356689453125
Norm before input: 0.2552422881126404
Norm after input: 0.5316070914268494
Norm after each mp layer: 2.5761866569519043
Norm after each mp layer: 17.745746612548828
Norm after each mp layer: 90.59356689453125
Norm before input: 0.2552422881126404
Norm after input: 0.5317130088806152
Norm after each mp layer: 2.5766098499298096
Norm after each mp layer: 17.754756927490234
Norm after each mp layer: 90.59906768798828
Norm before input: 0.2552422881126404
Norm after input: 0.5317130088806152
Norm after each mp layer: 2.5766098499298096
Norm after each mp layer: 17.754756927490234
Norm after each mp layer: 90.59906768798828
Norm before input: 0.2552422881126404
Norm after input: 0.5316017866134644
Norm after each mp layer: 2.575590133666992
Norm after each mp layer: 17.745962142944336
Norm after each mp layer: 90.570068359375
Norm before input: 0.2552422881126404
Norm after input: 0.5316017866134644
Norm after each mp layer: 2.575590133666992
Norm after each mp layer: 17.745962142944336
Norm after each mp layer: 90.570068359375
Norm before input: 0.2552422881126404
Norm after input: 0.5316935777664185
Norm after each mp layer: 2.5759336948394775
Norm after each mp layer: 17.753925323486328
Norm after each mp layer: 90.57286071777344
Epoch: 295, Loss: 0.0095, Energy: 453558.2500, Train: 99.92%, Valid: 75.20%, Test: 75.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5316935777664185
Norm after each mp layer: 2.5759336948394775
Norm after each mp layer: 17.753925323486328
Norm after each mp layer: 90.57286071777344
Norm before input: 0.2552422881126404
Norm after input: 0.5316107273101807
Norm after each mp layer: 2.575120687484741
Norm after each mp layer: 17.74761962890625
Norm after each mp layer: 90.54730224609375
Norm before input: 0.2552422881126404
Norm after input: 0.5316107273101807
Norm after each mp layer: 2.575120687484741
Norm after each mp layer: 17.74761962890625
Norm after each mp layer: 90.54730224609375
Norm before input: 0.2552422881126404
Norm after input: 0.53166264295578
Norm after each mp layer: 2.575207233428955
Norm after each mp layer: 17.752458572387695
Norm after each mp layer: 90.54574584960938
Norm before input: 0.2552422881126404
Norm after input: 0.53166264295578
Norm after each mp layer: 2.575207233428955
Norm after each mp layer: 17.752458572387695
Norm after each mp layer: 90.54574584960938
Norm before input: 0.2552422881126404
Norm after input: 0.5316216349601746
Norm after each mp layer: 2.5746803283691406
Norm after each mp layer: 17.749744415283203
Norm after each mp layer: 90.52934265136719
Norm before input: 0.2552422881126404
Norm after input: 0.5316216349601746
Norm after each mp layer: 2.5746803283691406
Norm after each mp layer: 17.749744415283203
Norm after each mp layer: 90.52934265136719
Norm before input: 0.2552422881126404
Norm after input: 0.5316282510757446
Norm after each mp layer: 2.574475049972534
Norm after each mp layer: 17.751026153564453
Norm after each mp layer: 90.5210952758789
Norm before input: 0.2552422881126404
Norm after input: 0.5316282510757446
Norm after each mp layer: 2.574475049972534
Norm after each mp layer: 17.751026153564453
Norm after each mp layer: 90.5210952758789
Norm before input: 0.2552422881126404
Norm after input: 0.531627893447876
Norm after each mp layer: 2.574223279953003
Norm after each mp layer: 17.751789093017578
Norm after each mp layer: 90.51386260986328
Epoch: 300, Loss: 0.0092, Energy: 444554.5938, Train: 99.92%, Valid: 75.20%, Test: 74.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.531627893447876
Norm after each mp layer: 2.574223279953003
Norm after each mp layer: 17.751789093017578
Norm after each mp layer: 90.51386260986328
Norm before input: 0.2552422881126404
Norm after input: 0.5315937399864197
Norm after each mp layer: 2.5737528800964355
Norm after each mp layer: 17.749834060668945
Norm after each mp layer: 90.49942016601562
Norm before input: 0.2552422881126404
Norm after input: 0.5315937399864197
Norm after each mp layer: 2.5737528800964355
Norm after each mp layer: 17.749834060668945
Norm after each mp layer: 90.49942016601562
Norm before input: 0.2552422881126404
Norm after input: 0.53162682056427
Norm after each mp layer: 2.57373309135437
Norm after each mp layer: 17.753440856933594
Norm after each mp layer: 90.49534606933594
Norm before input: 0.2552422881126404
Norm after input: 0.53162682056427
Norm after each mp layer: 2.57373309135437
Norm after each mp layer: 17.753440856933594
Norm after each mp layer: 90.49534606933594
Norm before input: 0.2552422881126404
Norm after input: 0.5315634608268738
Norm after each mp layer: 2.5730621814727783
Norm after each mp layer: 17.74908447265625
Norm after each mp layer: 90.48011016845703
Norm before input: 0.2552422881126404
Norm after input: 0.5315634608268738
Norm after each mp layer: 2.5730621814727783
Norm after each mp layer: 17.74908447265625
Norm after each mp layer: 90.48011016845703
Norm before input: 0.2552422881126404
Norm after input: 0.5316135287284851
Norm after each mp layer: 2.5731630325317383
Norm after each mp layer: 17.754114151000977
Norm after each mp layer: 90.47545623779297
Norm before input: 0.2552422881126404
Norm after input: 0.5316135287284851
Norm after each mp layer: 2.5731630325317383
Norm after each mp layer: 17.754114151000977
Norm after each mp layer: 90.47545623779297
Norm before input: 0.2552422881126404
Norm after input: 0.5315414667129517
Norm after each mp layer: 2.572427272796631
Norm after each mp layer: 17.749006271362305
Norm after each mp layer: 90.46076202392578
Epoch: 305, Loss: 0.0089, Energy: 435701.0312, Train: 99.92%, Valid: 75.20%, Test: 74.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5315414667129517
Norm after each mp layer: 2.572427272796631
Norm after each mp layer: 17.749006271362305
Norm after each mp layer: 90.46076202392578
Norm before input: 0.2552422881126404
Norm after input: 0.531584620475769
Norm after each mp layer: 2.5724804401397705
Norm after each mp layer: 17.75345802307129
Norm after each mp layer: 90.45359802246094
Norm before input: 0.2552422881126404
Norm after input: 0.531584620475769
Norm after each mp layer: 2.5724804401397705
Norm after each mp layer: 17.75345802307129
Norm after each mp layer: 90.45359802246094
Norm before input: 0.2552422881126404
Norm after input: 0.5315274596214294
Norm after each mp layer: 2.571842908859253
Norm after each mp layer: 17.74957847595215
Norm after each mp layer: 90.43949890136719
Norm before input: 0.2552422881126404
Norm after input: 0.5315274596214294
Norm after each mp layer: 2.571842908859253
Norm after each mp layer: 17.74957847595215
Norm after each mp layer: 90.43949890136719
Norm before input: 0.2552422881126404
Norm after input: 0.5315441489219666
Norm after each mp layer: 2.5717108249664307
Norm after each mp layer: 17.751840591430664
Norm after each mp layer: 90.429443359375
Norm before input: 0.2552422881126404
Norm after input: 0.5315441489219666
Norm after each mp layer: 2.5717108249664307
Norm after each mp layer: 17.751840591430664
Norm after each mp layer: 90.429443359375
Norm before input: 0.2552422881126404
Norm after input: 0.5315173864364624
Norm after each mp layer: 2.571275234222412
Norm after each mp layer: 17.750497817993164
Norm after each mp layer: 90.4179458618164
Norm before input: 0.2552422881126404
Norm after input: 0.5315173864364624
Norm after each mp layer: 2.571275234222412
Norm after each mp layer: 17.750497817993164
Norm after each mp layer: 90.4179458618164
Norm before input: 0.2552422881126404
Norm after input: 0.5314998030662537
Norm after each mp layer: 2.5709035396575928
Norm after each mp layer: 17.74994468688965
Norm after each mp layer: 90.40508270263672
Epoch: 310, Loss: 0.0086, Energy: 426393.8438, Train: 99.92%, Valid: 75.20%, Test: 74.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5314998030662537
Norm after each mp layer: 2.5709035396575928
Norm after each mp layer: 17.74994468688965
Norm after each mp layer: 90.40508270263672
Norm before input: 0.2552422881126404
Norm after input: 0.5315038561820984
Norm after each mp layer: 2.570676565170288
Norm after each mp layer: 17.751209259033203
Norm after each mp layer: 90.39450073242188
Norm before input: 0.2552422881126404
Norm after input: 0.5315038561820984
Norm after each mp layer: 2.570676565170288
Norm after each mp layer: 17.751209259033203
Norm after each mp layer: 90.39450073242188
Norm before input: 0.2552422881126404
Norm after input: 0.5314592123031616
Norm after each mp layer: 2.570110321044922
Norm after each mp layer: 17.748432159423828
Norm after each mp layer: 90.38227081298828
Norm before input: 0.2552422881126404
Norm after input: 0.5314592123031616
Norm after each mp layer: 2.570110321044922
Norm after each mp layer: 17.748432159423828
Norm after each mp layer: 90.38227081298828
Norm before input: 0.2552422881126404
Norm after input: 0.531479001045227
Norm after each mp layer: 2.569993734359741
Norm after each mp layer: 17.751060485839844
Norm after each mp layer: 90.3697509765625
Norm before input: 0.2552422881126404
Norm after input: 0.531479001045227
Norm after each mp layer: 2.569993734359741
Norm after each mp layer: 17.751060485839844
Norm after each mp layer: 90.3697509765625
Norm before input: 0.2552422881126404
Norm after input: 0.53142911195755
Norm after each mp layer: 2.5693798065185547
Norm after each mp layer: 17.747848510742188
Norm after each mp layer: 90.36153411865234
Norm before input: 0.2552422881126404
Norm after input: 0.53142911195755
Norm after each mp layer: 2.5693798065185547
Norm after each mp layer: 17.747848510742188
Norm after each mp layer: 90.36153411865234
Norm before input: 0.2552422881126404
Norm after input: 0.5314403772354126
Norm after each mp layer: 2.5692074298858643
Norm after each mp layer: 17.74983787536621
Norm after each mp layer: 90.34715270996094
Epoch: 315, Loss: 0.0084, Energy: 417489.0938, Train: 99.92%, Valid: 75.20%, Test: 74.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5314403772354126
Norm after each mp layer: 2.5692074298858643
Norm after each mp layer: 17.74983787536621
Norm after each mp layer: 90.34715270996094
Norm before input: 0.2552422881126404
Norm after input: 0.5314090251922607
Norm after each mp layer: 2.5687224864959717
Norm after each mp layer: 17.74822235107422
Norm after each mp layer: 90.33930206298828
Norm before input: 0.2552422881126404
Norm after input: 0.5314090251922607
Norm after each mp layer: 2.5687224864959717
Norm after each mp layer: 17.74822235107422
Norm after each mp layer: 90.33930206298828
Norm before input: 0.2552422881126404
Norm after input: 0.5313950777053833
Norm after each mp layer: 2.5683701038360596
Norm after each mp layer: 17.748136520385742
Norm after each mp layer: 90.326904296875
Norm before input: 0.2552422881126404
Norm after input: 0.5313950777053833
Norm after each mp layer: 2.5683701038360596
Norm after each mp layer: 17.748136520385742
Norm after each mp layer: 90.326904296875
Norm before input: 0.2552422881126404
Norm after input: 0.5313901901245117
Norm after each mp layer: 2.568081855773926
Norm after each mp layer: 17.748830795288086
Norm after each mp layer: 90.31484985351562
Norm before input: 0.2552422881126404
Norm after input: 0.5313901901245117
Norm after each mp layer: 2.568081855773926
Norm after each mp layer: 17.748830795288086
Norm after each mp layer: 90.31484985351562
Norm before input: 0.2552422881126404
Norm after input: 0.5313543677330017
Norm after each mp layer: 2.567568778991699
Norm after each mp layer: 17.746912002563477
Norm after each mp layer: 90.30728149414062
Norm before input: 0.2552422881126404
Norm after input: 0.5313543677330017
Norm after each mp layer: 2.567568778991699
Norm after each mp layer: 17.746912002563477
Norm after each mp layer: 90.30728149414062
Norm before input: 0.2552422881126404
Norm after input: 0.5313626527786255
Norm after each mp layer: 2.567383050918579
Norm after each mp layer: 17.748779296875
Norm after each mp layer: 90.29129028320312
Epoch: 320, Loss: 0.0081, Energy: 409016.8125, Train: 99.92%, Valid: 75.20%, Test: 74.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5313626527786255
Norm after each mp layer: 2.567383050918579
Norm after each mp layer: 17.748779296875
Norm after each mp layer: 90.29129028320312
Norm before input: 0.2552422881126404
Norm after input: 0.5313243865966797
Norm after each mp layer: 2.566850423812866
Norm after each mp layer: 17.74665641784668
Norm after each mp layer: 90.28533935546875
Norm before input: 0.2552422881126404
Norm after input: 0.5313243865966797
Norm after each mp layer: 2.566850423812866
Norm after each mp layer: 17.74665641784668
Norm after each mp layer: 90.28533935546875
Norm before input: 0.2552422881126404
Norm after input: 0.531324028968811
Norm after each mp layer: 2.56660532951355
Norm after each mp layer: 17.747812271118164
Norm after each mp layer: 90.2694320678711
Norm before input: 0.2552422881126404
Norm after input: 0.531324028968811
Norm after each mp layer: 2.56660532951355
Norm after each mp layer: 17.747812271118164
Norm after each mp layer: 90.2694320678711
Norm before input: 0.2552422881126404
Norm after input: 0.5313012003898621
Norm after each mp layer: 2.5661890506744385
Norm after each mp layer: 17.74703025817871
Norm after each mp layer: 90.25997161865234
Norm before input: 0.2552422881126404
Norm after input: 0.5313012003898621
Norm after each mp layer: 2.5661890506744385
Norm after each mp layer: 17.74703025817871
Norm after each mp layer: 90.25997161865234
Norm before input: 0.2552422881126404
Norm after input: 0.5312811732292175
Norm after each mp layer: 2.565796375274658
Norm after each mp layer: 17.746505737304688
Norm after each mp layer: 90.24842071533203
Norm before input: 0.2552422881126404
Norm after input: 0.5312811732292175
Norm after each mp layer: 2.565796375274658
Norm after each mp layer: 17.746505737304688
Norm after each mp layer: 90.24842071533203
Norm before input: 0.2552422881126404
Norm after input: 0.5312758684158325
Norm after each mp layer: 2.5655128955841064
Norm after each mp layer: 17.74724578857422
Norm after each mp layer: 90.2332534790039
Epoch: 325, Loss: 0.0079, Energy: 400874.7188, Train: 99.92%, Valid: 75.20%, Test: 74.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5312758684158325
Norm after each mp layer: 2.5655128955841064
Norm after each mp layer: 17.74724578857422
Norm after each mp layer: 90.2332534790039
Norm before input: 0.2552422881126404
Norm after input: 0.531242847442627
Norm after each mp layer: 2.5650177001953125
Norm after each mp layer: 17.745595932006836
Norm after each mp layer: 90.22590637207031
Norm before input: 0.2552422881126404
Norm after input: 0.531242847442627
Norm after each mp layer: 2.5650177001953125
Norm after each mp layer: 17.745595932006836
Norm after each mp layer: 90.22590637207031
Norm before input: 0.2552422881126404
Norm after input: 0.5312420129776001
Norm after each mp layer: 2.564769744873047
Norm after each mp layer: 17.746749877929688
Norm after each mp layer: 90.2071304321289
Norm before input: 0.2552422881126404
Norm after input: 0.5312420129776001
Norm after each mp layer: 2.564769744873047
Norm after each mp layer: 17.746749877929688
Norm after each mp layer: 90.2071304321289
Norm before input: 0.2552422881126404
Norm after input: 0.5312110185623169
Norm after each mp layer: 2.564286708831787
Norm after each mp layer: 17.74529266357422
Norm after each mp layer: 90.19930267333984
Norm before input: 0.2552422881126404
Norm after input: 0.5312110185623169
Norm after each mp layer: 2.564286708831787
Norm after each mp layer: 17.74529266357422
Norm after each mp layer: 90.19930267333984
Norm before input: 0.2552422881126404
Norm after input: 0.5312007665634155
Norm after each mp layer: 2.563965320587158
Norm after each mp layer: 17.74566078186035
Norm after each mp layer: 90.1826171875
Norm before input: 0.2552422881126404
Norm after input: 0.5312007665634155
Norm after each mp layer: 2.563965320587158
Norm after each mp layer: 17.74566078186035
Norm after each mp layer: 90.1826171875
Norm before input: 0.2552422881126404
Norm after input: 0.5311817526817322
Norm after each mp layer: 2.563573122024536
Norm after each mp layer: 17.745267868041992
Norm after each mp layer: 90.16963958740234
Epoch: 330, Loss: 0.0077, Energy: 392980.0938, Train: 99.92%, Valid: 75.00%, Test: 74.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5311817526817322
Norm after each mp layer: 2.563573122024536
Norm after each mp layer: 17.745267868041992
Norm after each mp layer: 90.16963195800781
Norm before input: 0.2552422881126404
Norm after input: 0.5311585664749146
Norm after each mp layer: 2.563148260116577
Norm after each mp layer: 17.744524002075195
Norm after each mp layer: 90.15787506103516
Norm before input: 0.2552422881126404
Norm after input: 0.5311585664749146
Norm after each mp layer: 2.563148260116577
Norm after each mp layer: 17.744524002075195
Norm after each mp layer: 90.15787506103516
Norm before input: 0.2552422881126404
Norm after input: 0.5311498045921326
Norm after each mp layer: 2.562837600708008
Norm after each mp layer: 17.745071411132812
Norm after each mp layer: 90.13945770263672
Norm before input: 0.2552422881126404
Norm after input: 0.5311498045921326
Norm after each mp layer: 2.562837600708008
Norm after each mp layer: 17.745071411132812
Norm after each mp layer: 90.13945770263672
Norm before input: 0.2552422881126404
Norm after input: 0.5311198234558105
Norm after each mp layer: 2.5623586177825928
Norm after each mp layer: 17.743757247924805
Norm after each mp layer: 90.12993621826172
Norm before input: 0.2552422881126404
Norm after input: 0.5311198234558105
Norm after each mp layer: 2.5623586177825928
Norm after each mp layer: 17.743757247924805
Norm after each mp layer: 90.12993621826172
Norm before input: 0.2552422881126404
Norm after input: 0.5311127305030823
Norm after each mp layer: 2.5620615482330322
Norm after each mp layer: 17.744474411010742
Norm after each mp layer: 90.10999298095703
Norm before input: 0.2552422881126404
Norm after input: 0.5311127305030823
Norm after each mp layer: 2.5620615482330322
Norm after each mp layer: 17.744474411010742
Norm after each mp layer: 90.10999298095703
Norm before input: 0.2552422881126404
Norm after input: 0.5310852527618408
Norm after each mp layer: 2.561602830886841
Norm after each mp layer: 17.743398666381836
Norm after each mp layer: 90.09894561767578
Epoch: 335, Loss: 0.0075, Energy: 385298.9062, Train: 99.92%, Valid: 75.00%, Test: 74.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5310852527618408
Norm after each mp layer: 2.561602830886841
Norm after each mp layer: 17.743398666381836
Norm after each mp layer: 90.09894561767578
Norm before input: 0.2552422881126404
Norm after input: 0.531071662902832
Norm after each mp layer: 2.56125545501709
Norm after each mp layer: 17.74355697631836
Norm after each mp layer: 90.08106231689453
Norm before input: 0.2552422881126404
Norm after input: 0.531071662902832
Norm after each mp layer: 2.56125545501709
Norm after each mp layer: 17.74355697631836
Norm after each mp layer: 90.08106231689453
Norm before input: 0.2552422881126404
Norm after input: 0.5310519337654114
Norm after each mp layer: 2.560859203338623
Norm after each mp layer: 17.74317741394043
Norm after each mp layer: 90.06600189208984
Norm before input: 0.2552422881126404
Norm after input: 0.5310519337654114
Norm after each mp layer: 2.560859203338623
Norm after each mp layer: 17.74317741394043
Norm after each mp layer: 90.06600189208984
Norm before input: 0.2552422881126404
Norm after input: 0.5310299396514893
Norm after each mp layer: 2.5604465007781982
Norm after each mp layer: 17.74260902404785
Norm after each mp layer: 90.05135345458984
Norm before input: 0.2552422881126404
Norm after input: 0.5310299396514893
Norm after each mp layer: 2.5604465007781982
Norm after each mp layer: 17.74260902404785
Norm after each mp layer: 90.05135345458984
Norm before input: 0.2552422881126404
Norm after input: 0.5310168266296387
Norm after each mp layer: 2.560105323791504
Norm after each mp layer: 17.74282455444336
Norm after each mp layer: 90.03196716308594
Norm before input: 0.2552422881126404
Norm after input: 0.5310168266296387
Norm after each mp layer: 2.560105323791504
Norm after each mp layer: 17.74282455444336
Norm after each mp layer: 90.03196716308594
Norm before input: 0.2552422881126404
Norm after input: 0.5309898853302002
Norm after each mp layer: 2.5596530437469482
Norm after each mp layer: 17.741817474365234
Norm after each mp layer: 90.01934814453125
Epoch: 340, Loss: 0.0073, Energy: 377867.5625, Train: 99.92%, Valid: 75.00%, Test: 74.70%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5309898853302002
Norm after each mp layer: 2.5596530437469482
Norm after each mp layer: 17.741817474365234
Norm after each mp layer: 90.01934814453125
Norm before input: 0.2552422881126404
Norm after input: 0.5309790968894958
Norm after each mp layer: 2.559330940246582
Norm after each mp layer: 17.742250442504883
Norm after each mp layer: 89.99800109863281
Norm before input: 0.2552422881126404
Norm after input: 0.5309790968894958
Norm after each mp layer: 2.559330940246582
Norm after each mp layer: 17.742250442504883
Norm after each mp layer: 89.99800109863281
Norm before input: 0.2552422881126404
Norm after input: 0.5309516787528992
Norm after each mp layer: 2.5588765144348145
Norm after each mp layer: 17.741220474243164
Norm after each mp layer: 89.98474884033203
Norm before input: 0.2552422881126404
Norm after input: 0.5309516787528992
Norm after each mp layer: 2.5588765144348145
Norm after each mp layer: 17.741220474243164
Norm after each mp layer: 89.98474884033203
Norm before input: 0.2552422881126404
Norm after input: 0.5309385657310486
Norm after each mp layer: 2.558537006378174
Norm after each mp layer: 17.741470336914062
Norm after each mp layer: 89.96371459960938
Norm before input: 0.2552422881126404
Norm after input: 0.5309385657310486
Norm after each mp layer: 2.558537006378174
Norm after each mp layer: 17.741470336914062
Norm after each mp layer: 89.96371459960938
Norm before input: 0.2552422881126404
Norm after input: 0.5309145450592041
Norm after each mp layer: 2.558108329772949
Norm after each mp layer: 17.740739822387695
Norm after each mp layer: 89.94810485839844
Norm before input: 0.2552422881126404
Norm after input: 0.5309145450592041
Norm after each mp layer: 2.558108329772949
Norm after each mp layer: 17.740739822387695
Norm after each mp layer: 89.94811248779297
Norm before input: 0.2552422881126404
Norm after input: 0.530896782875061
Norm after each mp layer: 2.5577311515808105
Norm after each mp layer: 17.740589141845703
Norm after each mp layer: 89.92855072021484
Epoch: 345, Loss: 0.0071, Energy: 370676.6250, Train: 99.92%, Valid: 74.80%, Test: 74.70%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.530896782875061
Norm after each mp layer: 2.5577311515808105
Norm after each mp layer: 17.740589141845703
Norm after each mp layer: 89.92855072021484
Norm before input: 0.2552422881126404
Norm after input: 0.5308769941329956
Norm after each mp layer: 2.5573360919952393
Norm after each mp layer: 17.740245819091797
Norm after each mp layer: 89.90999603271484
Norm before input: 0.2552422881126404
Norm after input: 0.5308769941329956
Norm after each mp layer: 2.5573360919952393
Norm after each mp layer: 17.740245819091797
Norm after each mp layer: 89.90999603271484
Norm before input: 0.2552422881126404
Norm after input: 0.5308550000190735
Norm after each mp layer: 2.5569236278533936
Norm after each mp layer: 17.739721298217773
Norm after each mp layer: 89.8923110961914
Norm before input: 0.2552422881126404
Norm after input: 0.5308550000190735
Norm after each mp layer: 2.5569236278533936
Norm after each mp layer: 17.739721298217773
Norm after each mp layer: 89.8923110961914
Norm before input: 0.2552422881126404
Norm after input: 0.5308386087417603
Norm after each mp layer: 2.55655574798584
Norm after each mp layer: 17.739688873291016
Norm after each mp layer: 89.87115478515625
Norm before input: 0.2552422881126404
Norm after input: 0.5308386087417603
Norm after each mp layer: 2.55655574798584
Norm after each mp layer: 17.739688873291016
Norm after each mp layer: 89.87115478515625
Norm before input: 0.2552422881126404
Norm after input: 0.5308135747909546
Norm after each mp layer: 2.5561184883117676
Norm after each mp layer: 17.738895416259766
Norm after each mp layer: 89.85448455810547
Norm before input: 0.2552422881126404
Norm after input: 0.5308135747909546
Norm after each mp layer: 2.5561184883117676
Norm after each mp layer: 17.738895416259766
Norm after each mp layer: 89.85448455810547
Norm before input: 0.2552422881126404
Norm after input: 0.530799150466919
Norm after each mp layer: 2.555767059326172
Norm after each mp layer: 17.73904800415039
Norm after each mp layer: 89.83168029785156
Epoch: 350, Loss: 0.0070, Energy: 363675.7500, Train: 99.92%, Valid: 74.80%, Test: 74.70%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.530799150466919
Norm after each mp layer: 2.555767059326172
Norm after each mp layer: 17.73904800415039
Norm after each mp layer: 89.83168029785156
Norm before input: 0.2552422881126404
Norm after input: 0.5307726860046387
Norm after each mp layer: 2.5553178787231445
Norm after each mp layer: 17.73812484741211
Norm after each mp layer: 89.81536865234375
Norm before input: 0.2552422881126404
Norm after input: 0.5307726860046387
Norm after each mp layer: 2.5553178787231445
Norm after each mp layer: 17.73812484741211
Norm after each mp layer: 89.81536865234375
Norm before input: 0.2552422881126404
Norm after input: 0.530758798122406
Norm after each mp layer: 2.554971933364868
Norm after each mp layer: 17.738330841064453
Norm after each mp layer: 89.79161071777344
Norm before input: 0.2552422881126404
Norm after input: 0.530758798122406
Norm after each mp layer: 2.554971933364868
Norm after each mp layer: 17.738330841064453
Norm after each mp layer: 89.79161071777344
Norm before input: 0.2552422881126404
Norm after input: 0.5307319760322571
Norm after each mp layer: 2.55452036857605
Norm after each mp layer: 17.737375259399414
Norm after each mp layer: 89.77518463134766
Norm before input: 0.2552422881126404
Norm after input: 0.5307319760322571
Norm after each mp layer: 2.55452036857605
Norm after each mp layer: 17.737375259399414
Norm after each mp layer: 89.77517700195312
Norm before input: 0.2552422881126404
Norm after input: 0.5307180881500244
Norm after each mp layer: 2.554175615310669
Norm after each mp layer: 17.737585067749023
Norm after each mp layer: 89.7509994506836
Norm before input: 0.2552422881126404
Norm after input: 0.5307180881500244
Norm after each mp layer: 2.554175615310669
Norm after each mp layer: 17.737585067749023
Norm after each mp layer: 89.7509994506836
Norm before input: 0.2552422881126404
Norm after input: 0.5306911468505859
Norm after each mp layer: 2.553724765777588
Norm after each mp layer: 17.73662757873535
Norm after each mp layer: 89.73406982421875
Epoch: 355, Loss: 0.0068, Energy: 356874.4062, Train: 99.92%, Valid: 74.80%, Test: 74.60%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5306911468505859
Norm after each mp layer: 2.553724765777588
Norm after each mp layer: 17.73662757873535
Norm after each mp layer: 89.73406982421875
Norm before input: 0.2552422881126404
Norm after input: 0.530677080154419
Norm after each mp layer: 2.553380012512207
Norm after each mp layer: 17.736825942993164
Norm after each mp layer: 89.70943450927734
Norm before input: 0.2552422881126404
Norm after input: 0.530677080154419
Norm after each mp layer: 2.553380012512207
Norm after each mp layer: 17.736825942993164
Norm after each mp layer: 89.70943450927734
Norm before input: 0.2552422881126404
Norm after input: 0.5306500792503357
Norm after each mp layer: 2.552929639816284
Norm after each mp layer: 17.73586082458496
Norm after each mp layer: 89.69232177734375
Norm before input: 0.2552422881126404
Norm after input: 0.5306500792503357
Norm after each mp layer: 2.552929639816284
Norm after each mp layer: 17.73586082458496
Norm after each mp layer: 89.69232177734375
Norm before input: 0.2552422881126404
Norm after input: 0.5306362509727478
Norm after each mp layer: 2.552588701248169
Norm after each mp layer: 17.736085891723633
Norm after each mp layer: 89.66714477539062
Norm before input: 0.2552422881126404
Norm after input: 0.5306362509727478
Norm after each mp layer: 2.552588701248169
Norm after each mp layer: 17.736085891723633
Norm after each mp layer: 89.66714477539062
Norm before input: 0.2552422881126404
Norm after input: 0.5306082367897034
Norm after each mp layer: 2.552131175994873
Norm after each mp layer: 17.735027313232422
Norm after each mp layer: 89.65019226074219
Norm before input: 0.2552422881126404
Norm after input: 0.5306082367897034
Norm after each mp layer: 2.552131175994873
Norm after each mp layer: 17.735027313232422
Norm after each mp layer: 89.65019226074219
Norm before input: 0.2552422881126404
Norm after input: 0.530595600605011
Norm after each mp layer: 2.5518007278442383
Norm after each mp layer: 17.73535919189453
Norm after each mp layer: 89.62391662597656
Epoch: 360, Loss: 0.0067, Energy: 350301.5312, Train: 99.92%, Valid: 74.80%, Test: 74.50%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.530595600605011
Norm after each mp layer: 2.5518007278442383
Norm after each mp layer: 17.73535919189453
Norm after each mp layer: 89.62391662597656
Norm before input: 0.2552422881126404
Norm after input: 0.5305654406547546
Norm after each mp layer: 2.5513265132904053
Norm after each mp layer: 17.734098434448242
Norm after each mp layer: 89.60797882080078
Norm before input: 0.2552422881126404
Norm after input: 0.5305654406547546
Norm after each mp layer: 2.5513265132904053
Norm after each mp layer: 17.734100341796875
Norm after each mp layer: 89.60797882080078
Norm before input: 0.2552422881126404
Norm after input: 0.5305554270744324
Norm after each mp layer: 2.5510189533233643
Norm after each mp layer: 17.734664916992188
Norm after each mp layer: 89.57960510253906
Norm before input: 0.2552422881126404
Norm after input: 0.5305554270744324
Norm after each mp layer: 2.5510189533233643
Norm after each mp layer: 17.734664916992188
Norm after each mp layer: 89.57960510253906
Norm before input: 0.2552422881126404
Norm after input: 0.5305209159851074
Norm after each mp layer: 2.550508737564087
Norm after each mp layer: 17.733001708984375
Norm after each mp layer: 89.56594848632812
Norm before input: 0.2552422881126404
Norm after input: 0.5305209159851074
Norm after each mp layer: 2.550508737564087
Norm after each mp layer: 17.733001708984375
Norm after each mp layer: 89.56594848632812
Norm before input: 0.2552422881126404
Norm after input: 0.530516505241394
Norm after each mp layer: 2.5502471923828125
Norm after each mp layer: 17.734060287475586
Norm after each mp layer: 89.53379821777344
Norm before input: 0.2552422881126404
Norm after input: 0.530516505241394
Norm after each mp layer: 2.5502471923828125
Norm after each mp layer: 17.734060287475586
Norm after each mp layer: 89.53379821777344
Norm before input: 0.2552422881126404
Norm after input: 0.5304737687110901
Norm after each mp layer: 2.5496695041656494
Norm after each mp layer: 17.731637954711914
Norm after each mp layer: 89.52484130859375
Epoch: 365, Loss: 0.0066, Energy: 343910.8438, Train: 99.92%, Valid: 74.80%, Test: 74.30%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5304737687110901
Norm after each mp layer: 2.5496695041656494
Norm after each mp layer: 17.731637954711914
Norm after each mp layer: 89.52484130859375
Norm before input: 0.2552422881126404
Norm after input: 0.5304800868034363
Norm after each mp layer: 2.54949688911438
Norm after each mp layer: 17.733657836914062
Norm after each mp layer: 89.48551177978516
Norm before input: 0.2552422881126404
Norm after input: 0.5304800868034363
Norm after each mp layer: 2.54949688911438
Norm after each mp layer: 17.733657836914062
Norm after each mp layer: 89.48551177978516
Norm before input: 0.2552422881126404
Norm after input: 0.5304219126701355
Norm after each mp layer: 2.5487897396087646
Norm after each mp layer: 17.72980499267578
Norm after each mp layer: 89.4857177734375
Norm before input: 0.2552422881126404
Norm after input: 0.5304219126701355
Norm after each mp layer: 2.5487897396087646
Norm after each mp layer: 17.72980308532715
Norm after each mp layer: 89.48571014404297
Norm before input: 0.2552422881126404
Norm after input: 0.5304486751556396
Norm after each mp layer: 2.5487875938415527
Norm after each mp layer: 17.73367691040039
Norm after each mp layer: 89.43316650390625
Norm before input: 0.2552422881126404
Norm after input: 0.5304486751556396
Norm after each mp layer: 2.5487875938415527
Norm after each mp layer: 17.73367691040039
Norm after each mp layer: 89.43316650390625
Norm before input: 0.2552422881126404
Norm after input: 0.5303622484207153
Norm after each mp layer: 2.5478439331054688
Norm after each mp layer: 17.72720718383789
Norm after each mp layer: 89.45056915283203
Norm before input: 0.2552422881126404
Norm after input: 0.5303622484207153
Norm after each mp layer: 2.5478439331054688
Norm after each mp layer: 17.72720718383789
Norm after each mp layer: 89.45056915283203
Norm before input: 0.2552422881126404
Norm after input: 0.530424952507019
Norm after each mp layer: 2.5481393337249756
Norm after each mp layer: 17.73431968688965
Norm after each mp layer: 89.3752212524414
Epoch: 370, Loss: 0.0064, Energy: 337809.5312, Train: 99.92%, Valid: 74.60%, Test: 74.50%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.530424952507019
Norm after each mp layer: 2.5481393337249756
Norm after each mp layer: 17.73431968688965
Norm after each mp layer: 89.3752212524414
Norm before input: 0.2552422881126404
Norm after input: 0.5302948951721191
Norm after each mp layer: 2.54682993888855
Norm after each mp layer: 17.72379493713379
Norm after each mp layer: 89.41905975341797
Norm before input: 0.2552422881126404
Norm after input: 0.5302948951721191
Norm after each mp layer: 2.54682993888855
Norm after each mp layer: 17.72379493713379
Norm after each mp layer: 89.41905975341797
Norm before input: 0.2552422881126404
Norm after input: 0.5304014682769775
Norm after each mp layer: 2.5474863052368164
Norm after each mp layer: 17.73482322692871
Norm after each mp layer: 89.31664276123047
Norm before input: 0.2552422881126404
Norm after input: 0.5304014682769775
Norm after each mp layer: 2.5474863052368164
Norm after each mp layer: 17.73482322692871
Norm after each mp layer: 89.31664276123047
Norm before input: 0.2552422881126404
Norm after input: 0.5302385091781616
Norm after each mp layer: 2.545900821685791
Norm after each mp layer: 17.721208572387695
Norm after each mp layer: 89.37872314453125
Norm before input: 0.2552422881126404
Norm after input: 0.5302385091781616
Norm after each mp layer: 2.545900821685791
Norm after each mp layer: 17.721208572387695
Norm after each mp layer: 89.37872314453125
Norm before input: 0.2552422881126404
Norm after input: 0.530346691608429
Norm after each mp layer: 2.5465641021728516
Norm after each mp layer: 17.732267379760742
Norm after each mp layer: 89.27531433105469
Norm before input: 0.2552422881126404
Norm after input: 0.530346691608429
Norm after each mp layer: 2.5465641021728516
Norm after each mp layer: 17.732267379760742
Norm after each mp layer: 89.27531433105469
Norm before input: 0.2552422881126404
Norm after input: 0.5302186608314514
Norm after each mp layer: 2.5452659130096436
Norm after each mp layer: 17.721773147583008
Norm after each mp layer: 89.31471252441406
Epoch: 375, Loss: 0.0064, Energy: 331648.2188, Train: 99.92%, Valid: 74.60%, Test: 74.30%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5302186608314514
Norm after each mp layer: 2.5452659130096436
Norm after each mp layer: 17.721773147583008
Norm after each mp layer: 89.31471252441406
Norm before input: 0.2552422881126404
Norm after input: 0.5302579998970032
Norm after each mp layer: 2.5453600883483887
Norm after each mp layer: 17.726579666137695
Norm after each mp layer: 89.2509765625
Norm before input: 0.2552422881126404
Norm after input: 0.5302579998970032
Norm after each mp layer: 2.5453600883483887
Norm after each mp layer: 17.726579666137695
Norm after each mp layer: 89.25096893310547
Norm before input: 0.2552422881126404
Norm after input: 0.530213475227356
Norm after each mp layer: 2.544754981994629
Norm after each mp layer: 17.7237491607666
Norm after each mp layer: 89.23985290527344
Norm before input: 0.2552422881126404
Norm after input: 0.530213475227356
Norm after each mp layer: 2.544754981994629
Norm after each mp layer: 17.7237491607666
Norm after each mp layer: 89.23985290527344
Norm before input: 0.2552422881126404
Norm after input: 0.5301695466041565
Norm after each mp layer: 2.544154405593872
Norm after each mp layer: 17.720983505249023
Norm after each mp layer: 89.22882080078125
Norm before input: 0.2552422881126404
Norm after input: 0.5301695466041565
Norm after each mp layer: 2.544154405593872
Norm after each mp layer: 17.720983505249023
Norm after each mp layer: 89.22882080078125
Norm before input: 0.2552422881126404
Norm after input: 0.5302006602287292
Norm after each mp layer: 2.5441818237304688
Norm after each mp layer: 17.725114822387695
Norm after each mp layer: 89.17135620117188
Norm before input: 0.2552422881126404
Norm after input: 0.5302006602287292
Norm after each mp layer: 2.5441818237304688
Norm after each mp layer: 17.725116729736328
Norm after each mp layer: 89.17135620117188
Norm before input: 0.2552422881126404
Norm after input: 0.5300948023796082
Norm after each mp layer: 2.5430660247802734
Norm after each mp layer: 17.71669578552246
Norm after each mp layer: 89.19954681396484
Epoch: 380, Loss: 0.0062, Energy: 325762.0312, Train: 99.92%, Valid: 74.40%, Test: 74.20%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5300948023796082
Norm after each mp layer: 2.5430660247802734
Norm after each mp layer: 17.71669578552246
Norm after each mp layer: 89.19954681396484
Norm before input: 0.2552422881126404
Norm after input: 0.5301678776741028
Norm after each mp layer: 2.5434436798095703
Norm after each mp layer: 17.724653244018555
Norm after each mp layer: 89.11683654785156
Norm before input: 0.2552422881126404
Norm after input: 0.5301678776741028
Norm after each mp layer: 2.5434436798095703
Norm after each mp layer: 17.724653244018555
Norm after each mp layer: 89.11683654785156
Norm before input: 0.2552422881126404
Norm after input: 0.5300460457801819
Norm after each mp layer: 2.5421969890594482
Norm after each mp layer: 17.71474838256836
Norm after each mp layer: 89.15435791015625
Norm before input: 0.2552422881126404
Norm after input: 0.5300460457801819
Norm after each mp layer: 2.5421969890594482
Norm after each mp layer: 17.71474838256836
Norm after each mp layer: 89.15435791015625
Norm before input: 0.2552422881126404
Norm after input: 0.5301040410995483
Norm after each mp layer: 2.542452573776245
Norm after each mp layer: 17.721315383911133
Norm after each mp layer: 89.08015441894531
Norm before input: 0.2552422881126404
Norm after input: 0.5301040410995483
Norm after each mp layer: 2.542452573776245
Norm after each mp layer: 17.721315383911133
Norm after each mp layer: 89.08015441894531
Norm before input: 0.2552422881126404
Norm after input: 0.5300213098526001
Norm after each mp layer: 2.5415337085723877
Norm after each mp layer: 17.71498680114746
Norm after each mp layer: 89.09413146972656
Norm before input: 0.2552422881126404
Norm after input: 0.5300213098526001
Norm after each mp layer: 2.5415337085723877
Norm after each mp layer: 17.71498680114746
Norm after each mp layer: 89.09413146972656
Norm before input: 0.2552422881126404
Norm after input: 0.530024528503418
Norm after each mp layer: 2.5413339138031006
Norm after each mp layer: 17.716562271118164
Norm after each mp layer: 89.0550537109375
Epoch: 385, Loss: 0.0061, Energy: 320196.1250, Train: 99.92%, Valid: 74.40%, Test: 74.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.530024528503418
Norm after each mp layer: 2.5413339138031006
Norm after each mp layer: 17.716562271118164
Norm after each mp layer: 89.0550537109375
Norm before input: 0.2552422881126404
Norm after input: 0.5300020575523376
Norm after each mp layer: 2.5409202575683594
Norm after each mp layer: 17.715797424316406
Norm after each mp layer: 89.03262329101562
Norm before input: 0.2552422881126404
Norm after input: 0.5300020575523376
Norm after each mp layer: 2.5409202575683594
Norm after each mp layer: 17.715797424316406
Norm after each mp layer: 89.03262329101562
Norm before input: 0.2552422881126404
Norm after input: 0.5299488306045532
Norm after each mp layer: 2.5402517318725586
Norm after each mp layer: 17.71224594116211
Norm after each mp layer: 89.02958679199219
Norm before input: 0.2552422881126404
Norm after input: 0.5299488306045532
Norm after each mp layer: 2.5402517318725586
Norm after each mp layer: 17.71224594116211
Norm after each mp layer: 89.02958679199219
Norm before input: 0.2552422881126404
Norm after input: 0.5299748182296753
Norm after each mp layer: 2.5402493476867676
Norm after each mp layer: 17.71600914001465
Norm after each mp layer: 88.97665405273438
Norm before input: 0.2552422881126404
Norm after input: 0.5299748182296753
Norm after each mp layer: 2.5402493476867676
Norm after each mp layer: 17.71600914001465
Norm after each mp layer: 88.97665405273438
Norm before input: 0.2552422881126404
Norm after input: 0.5298858284950256
Norm after each mp layer: 2.5392847061157227
Norm after each mp layer: 17.709203720092773
Norm after each mp layer: 88.99644470214844
Norm before input: 0.2552422881126404
Norm after input: 0.5298858284950256
Norm after each mp layer: 2.5392847061157227
Norm after each mp layer: 17.709203720092773
Norm after each mp layer: 88.99644470214844
Norm before input: 0.2552422881126404
Norm after input: 0.5299328565597534
Norm after each mp layer: 2.539461612701416
Norm after each mp layer: 17.714923858642578
Norm after each mp layer: 88.93042755126953
Epoch: 390, Loss: 0.0060, Energy: 314781.2500, Train: 99.92%, Valid: 74.40%, Test: 74.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5299328565597534
Norm after each mp layer: 2.539461612701416
Norm after each mp layer: 17.714923858642578
Norm after each mp layer: 88.93042755126953
Norm before input: 0.2552422881126404
Norm after input: 0.5298393964767456
Norm after each mp layer: 2.5384609699249268
Norm after each mp layer: 17.707706451416016
Norm after each mp layer: 88.95337677001953
Norm before input: 0.2552422881126404
Norm after input: 0.5298393964767456
Norm after each mp layer: 2.5384609699249268
Norm after each mp layer: 17.707706451416016
Norm after each mp layer: 88.95337677001953
Norm before input: 0.2552422881126404
Norm after input: 0.5298744440078735
Norm after each mp layer: 2.5385398864746094
Norm after each mp layer: 17.71233558654785
Norm after each mp layer: 88.8951644897461
Norm before input: 0.2552422881126404
Norm after input: 0.5298744440078735
Norm after each mp layer: 2.5385398864746094
Norm after each mp layer: 17.71233558654785
Norm after each mp layer: 88.8951644897461
Norm before input: 0.2552422881126404
Norm after input: 0.529805064201355
Norm after each mp layer: 2.5377421379089355
Norm after each mp layer: 17.70734405517578
Norm after each mp layer: 88.90357971191406
Norm before input: 0.2552422881126404
Norm after input: 0.529805064201355
Norm after each mp layer: 2.5377421379089355
Norm after each mp layer: 17.70734405517578
Norm after each mp layer: 88.90357971191406
Norm before input: 0.2552422881126404
Norm after input: 0.5298079252243042
Norm after each mp layer: 2.53755259513855
Norm after each mp layer: 17.709028244018555
Norm after each mp layer: 88.86603546142578
Norm before input: 0.2552422881126404
Norm after input: 0.5298079252243042
Norm after each mp layer: 2.53755259513855
Norm after each mp layer: 17.709028244018555
Norm after each mp layer: 88.86603546142578
Norm before input: 0.2552422881126404
Norm after input: 0.5297735333442688
Norm after each mp layer: 2.537050724029541
Norm after each mp layer: 17.70728874206543
Norm after each mp layer: 88.85232543945312
Epoch: 395, Loss: 0.0059, Energy: 309431.0000, Train: 99.92%, Valid: 74.20%, Test: 74.20%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5297735333442688
Norm after each mp layer: 2.537050724029541
Norm after each mp layer: 17.70728874206543
Norm after each mp layer: 88.85232543945312
Norm before input: 0.2552422881126404
Norm after input: 0.5297418832778931
Norm after each mp layer: 2.5365726947784424
Norm after each mp layer: 17.7058162689209
Norm after each mp layer: 88.83709716796875
Norm before input: 0.2552422881126404
Norm after input: 0.5297418832778931
Norm after each mp layer: 2.5365726947784424
Norm after each mp layer: 17.7058162689209
Norm after each mp layer: 88.83709716796875
Norm before input: 0.2552422881126404
Norm after input: 0.5297389626502991
Norm after each mp layer: 2.5363364219665527
Norm after each mp layer: 17.707000732421875
Norm after each mp layer: 88.8038558959961
Norm before input: 0.2552422881126404
Norm after input: 0.5297389626502991
Norm after each mp layer: 2.5363364219665527
Norm after each mp layer: 17.707000732421875
Norm after each mp layer: 88.8038558959961
Norm before input: 0.2552422881126404
Norm after input: 0.5296808481216431
Norm after each mp layer: 2.535634994506836
Norm after each mp layer: 17.703079223632812
Norm after each mp layer: 88.8062744140625
Norm before input: 0.2552422881126404
Norm after input: 0.5296808481216431
Norm after each mp layer: 2.535634994506836
Norm after each mp layer: 17.703079223632812
Norm after each mp layer: 88.8062744140625
Norm before input: 0.2552422881126404
Norm after input: 0.5296993255615234
Norm after each mp layer: 2.5355796813964844
Norm after each mp layer: 17.70624542236328
Norm after each mp layer: 88.75939178466797
Norm before input: 0.2552422881126404
Norm after input: 0.5296993255615234
Norm after each mp layer: 2.5355796813964844
Norm after each mp layer: 17.70624542236328
Norm after each mp layer: 88.75939178466797
Norm before input: 0.2552422881126404
Norm after input: 0.5296252965927124
Norm after each mp layer: 2.53474497795105
Norm after each mp layer: 17.700841903686523
Norm after each mp layer: 88.7720718383789
Epoch: 400, Loss: 0.0058, Energy: 304326.8125, Train: 99.92%, Valid: 74.20%, Test: 74.20%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5296252965927124
Norm after each mp layer: 2.53474497795105
Norm after each mp layer: 17.700841903686523
Norm after each mp layer: 88.7720718383789
Norm before input: 0.2552422881126404
Norm after input: 0.5296538472175598
Norm after each mp layer: 2.5347752571105957
Norm after each mp layer: 17.704925537109375
Norm after each mp layer: 88.71865844726562
Norm before input: 0.2552422881126404
Norm after input: 0.5296538472175598
Norm after each mp layer: 2.5347752571105957
Norm after each mp layer: 17.704925537109375
Norm after each mp layer: 88.71865844726562
Norm before input: 0.2552422881126404
Norm after input: 0.5295751690864563
Norm after each mp layer: 2.533900499343872
Norm after each mp layer: 17.699068069458008
Norm after each mp layer: 88.73455810546875
Norm before input: 0.2552422881126404
Norm after input: 0.5295751690864563
Norm after each mp layer: 2.533900499343872
Norm after each mp layer: 17.699068069458008
Norm after each mp layer: 88.73455810546875
Norm before input: 0.2552422881126404
Norm after input: 0.5296034216880798
Norm after each mp layer: 2.533928394317627
Norm after each mp layer: 17.70310401916504
Norm after each mp layer: 88.6812973022461
Norm before input: 0.2552422881126404
Norm after input: 0.5296034216880798
Norm after each mp layer: 2.533928394317627
Norm after each mp layer: 17.70310401916504
Norm after each mp layer: 88.68128967285156
Norm before input: 0.2552422881126404
Norm after input: 0.5295290946960449
Norm after each mp layer: 2.533090114593506
Norm after each mp layer: 17.69762420654297
Norm after each mp layer: 88.69445037841797
Norm before input: 0.2552422881126404
Norm after input: 0.5295290946960449
Norm after each mp layer: 2.533090114593506
Norm after each mp layer: 17.697622299194336
Norm after each mp layer: 88.69445037841797
Norm before input: 0.2552422881126404
Norm after input: 0.5295499563217163
Norm after each mp layer: 2.5330562591552734
Norm after each mp layer: 17.70096778869629
Norm after each mp layer: 88.64582824707031
Epoch: 405, Loss: 0.0057, Energy: 299528.8438, Train: 99.92%, Valid: 74.20%, Test: 74.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5295499563217163
Norm after each mp layer: 2.5330562591552734
Norm after each mp layer: 17.70096778869629
Norm after each mp layer: 88.64582824707031
Norm before input: 0.2552422881126404
Norm after input: 0.5294852256774902
Norm after each mp layer: 2.5322976112365723
Norm after each mp layer: 17.696346282958984
Norm after each mp layer: 88.65288543701172
Norm before input: 0.2552422881126404
Norm after input: 0.5294852256774902
Norm after each mp layer: 2.5322976112365723
Norm after each mp layer: 17.696348190307617
Norm after each mp layer: 88.65288543701172
Norm before input: 0.2552422881126404
Norm after input: 0.5294959545135498
Norm after each mp layer: 2.5321784019470215
Norm after each mp layer: 17.69874382019043
Norm after each mp layer: 88.61056518554688
Norm before input: 0.2552422881126404
Norm after input: 0.5294959545135498
Norm after each mp layer: 2.5321784019470215
Norm after each mp layer: 17.69874382019043
Norm after each mp layer: 88.61056518554688
Norm before input: 0.2552422881126404
Norm after input: 0.5294411778450012
Norm after each mp layer: 2.531503438949585
Norm after each mp layer: 17.695037841796875
Norm after each mp layer: 88.61100006103516
Norm before input: 0.2552422881126404
Norm after input: 0.5294411778450012
Norm after each mp layer: 2.531503438949585
Norm after each mp layer: 17.695037841796875
Norm after each mp layer: 88.61100006103516
Norm before input: 0.2552422881126404
Norm after input: 0.5294426679611206
Norm after each mp layer: 2.5313048362731934
Norm after each mp layer: 17.696552276611328
Norm after each mp layer: 88.57475280761719
Norm before input: 0.2552422881126404
Norm after input: 0.5294426679611206
Norm after each mp layer: 2.5313048362731934
Norm after each mp layer: 17.696552276611328
Norm after each mp layer: 88.57475280761719
Norm before input: 0.2552422881126404
Norm after input: 0.5293959379196167
Norm after each mp layer: 2.530697822570801
Norm after each mp layer: 17.693578720092773
Norm after each mp layer: 88.57015991210938
Epoch: 410, Loss: 0.0056, Energy: 294694.8750, Train: 99.92%, Valid: 74.00%, Test: 74.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5293959379196167
Norm after each mp layer: 2.530697822570801
Norm after each mp layer: 17.693578720092773
Norm after each mp layer: 88.57015991210938
Norm before input: 0.2552422881126404
Norm after input: 0.5293907523155212
Norm after each mp layer: 2.530442953109741
Norm after each mp layer: 17.694461822509766
Norm after each mp layer: 88.5383529663086
Norm before input: 0.2552422881126404
Norm after input: 0.5293907523155212
Norm after each mp layer: 2.530442953109741
Norm after each mp layer: 17.694461822509766
Norm after each mp layer: 88.5383529663086
Norm before input: 0.2552422881126404
Norm after input: 0.5293489694595337
Norm after each mp layer: 2.529876947402954
Norm after each mp layer: 17.691923141479492
Norm after each mp layer: 88.5305404663086
Norm before input: 0.2552422881126404
Norm after input: 0.5293489694595337
Norm after each mp layer: 2.529876947402954
Norm after each mp layer: 17.691923141479492
Norm after each mp layer: 88.5305404663086
Norm before input: 0.2552422881126404
Norm after input: 0.5293405652046204
Norm after each mp layer: 2.5295941829681396
Norm after each mp layer: 17.69247817993164
Norm after each mp layer: 88.50079345703125
Norm before input: 0.2552422881126404
Norm after input: 0.5293405652046204
Norm after each mp layer: 2.5295941829681396
Norm after each mp layer: 17.69247817993164
Norm after each mp layer: 88.50079345703125
Norm before input: 0.2552422881126404
Norm after input: 0.5293004512786865
Norm after each mp layer: 2.5290420055389404
Norm after each mp layer: 17.690073013305664
Norm after each mp layer: 88.49198150634766
Norm before input: 0.2552422881126404
Norm after input: 0.5293004512786865
Norm after each mp layer: 2.5290420055389404
Norm after each mp layer: 17.690073013305664
Norm after each mp layer: 88.49198150634766
Norm before input: 0.2552422881126404
Norm after input: 0.5292922854423523
Norm after each mp layer: 2.528761625289917
Norm after each mp layer: 17.690637588500977
Norm after each mp layer: 88.46208190917969
Epoch: 415, Loss: 0.0055, Energy: 290131.0312, Train: 99.92%, Valid: 74.00%, Test: 74.00%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5292922854423523
Norm after each mp layer: 2.528761625289917
Norm after each mp layer: 17.690637588500977
Norm after each mp layer: 88.46208190917969
Norm before input: 0.2552422881126404
Norm after input: 0.5292501449584961
Norm after each mp layer: 2.5281918048858643
Norm after each mp layer: 17.688018798828125
Norm after each mp layer: 88.45465850830078
Norm before input: 0.2552422881126404
Norm after input: 0.5292501449584961
Norm after each mp layer: 2.5281918048858643
Norm after each mp layer: 17.688018798828125
Norm after each mp layer: 88.45465850830078
Norm before input: 0.2552422881126404
Norm after input: 0.5292462110519409
Norm after each mp layer: 2.527946949005127
Norm after each mp layer: 17.68895721435547
Norm after each mp layer: 88.4218978881836
Norm before input: 0.2552422881126404
Norm after input: 0.5292462110519409
Norm after each mp layer: 2.527946949005127
Norm after each mp layer: 17.68895721435547
Norm after each mp layer: 88.4218978881836
Norm before input: 0.2552422881126404
Norm after input: 0.5291973948478699
Norm after each mp layer: 2.527320623397827
Norm after each mp layer: 17.68569564819336
Norm after each mp layer: 88.4188461303711
Norm before input: 0.2552422881126404
Norm after input: 0.5291973948478699
Norm after each mp layer: 2.527320623397827
Norm after each mp layer: 17.68569564819336
Norm after each mp layer: 88.4188461303711
Norm before input: 0.2552422881126404
Norm after input: 0.5292033553123474
Norm after each mp layer: 2.5271594524383545
Norm after each mp layer: 17.687536239624023
Norm after each mp layer: 88.37945556640625
Norm before input: 0.2552422881126404
Norm after input: 0.5292033553123474
Norm after each mp layer: 2.5271594524383545
Norm after each mp layer: 17.687536239624023
Norm after each mp layer: 88.37945556640625
Norm before input: 0.2552422881126404
Norm after input: 0.5291408896446228
Norm after each mp layer: 2.5264153480529785
Norm after each mp layer: 17.68297004699707
Norm after each mp layer: 88.38542938232422
Epoch: 420, Loss: 0.0054, Energy: 285605.4688, Train: 99.92%, Valid: 74.00%, Test: 74.10%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5291408896446228
Norm after each mp layer: 2.5264153480529785
Norm after each mp layer: 17.68297004699707
Norm after each mp layer: 88.38542938232422
Norm before input: 0.2552422881126404
Norm after input: 0.5291656255722046
Norm after each mp layer: 2.5264148712158203
Norm after each mp layer: 17.68655014038086
Norm after each mp layer: 88.33332824707031
Norm before input: 0.2552422881126404
Norm after input: 0.5291656255722046
Norm after each mp layer: 2.5264148712158203
Norm after each mp layer: 17.686548233032227
Norm after each mp layer: 88.33332824707031
Norm before input: 0.2552422881126404
Norm after input: 0.52907794713974
Norm after each mp layer: 2.5254549980163574
Norm after each mp layer: 17.67958641052246
Norm after each mp layer: 88.35607147216797
Norm before input: 0.2552422881126404
Norm after input: 0.52907794713974
Norm after each mp layer: 2.5254549980163574
Norm after each mp layer: 17.67958641052246
Norm after each mp layer: 88.35607147216797
Norm before input: 0.2552422881126404
Norm after input: 0.5291357636451721
Norm after each mp layer: 2.5257346630096436
Norm after each mp layer: 17.68621253967285
Norm after each mp layer: 88.281982421875
Norm before input: 0.2552422881126404
Norm after input: 0.5291357636451721
Norm after each mp layer: 2.5257346630096436
Norm after each mp layer: 17.68621253967285
Norm after each mp layer: 88.281982421875
Norm before input: 0.2552422881126404
Norm after input: 0.5290072560310364
Norm after each mp layer: 2.5244247913360596
Norm after each mp layer: 17.675371170043945
Norm after each mp layer: 88.33170318603516
Norm before input: 0.2552422881126404
Norm after input: 0.5290072560310364
Norm after each mp layer: 2.5244247913360596
Norm after each mp layer: 17.675371170043945
Norm after each mp layer: 88.33170318603516
Norm before input: 0.2552422881126404
Norm after input: 0.5291106700897217
Norm after each mp layer: 2.5250911712646484
Norm after each mp layer: 17.686189651489258
Norm after each mp layer: 88.22718048095703
Epoch: 425, Loss: 0.0054, Energy: 281428.8750, Train: 99.92%, Valid: 73.80%, Test: 74.00%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5291106700897217
Norm after each mp layer: 2.5250911712646484
Norm after each mp layer: 17.686189651489258
Norm after each mp layer: 88.22718048095703
Norm before input: 0.2552422881126404
Norm after input: 0.5289388298988342
Norm after each mp layer: 2.523407220840454
Norm after each mp layer: 17.671175003051758
Norm after each mp layer: 88.30469512939453
Norm before input: 0.2552422881126404
Norm after input: 0.5289388298988342
Norm after each mp layer: 2.523407220840454
Norm after each mp layer: 17.671175003051758
Norm after each mp layer: 88.30469512939453
Norm before input: 0.2552422881126404
Norm after input: 0.5290684700012207
Norm after each mp layer: 2.524289846420288
Norm after each mp layer: 17.684303283691406
Norm after each mp layer: 88.18264770507812
Norm before input: 0.2552422881126404
Norm after input: 0.5290684700012207
Norm after each mp layer: 2.524289846420288
Norm after each mp layer: 17.684303283691406
Norm after each mp layer: 88.18264770507812
Norm before input: 0.2552422881126404
Norm after input: 0.5288980603218079
Norm after each mp layer: 2.522613763809204
Norm after each mp layer: 17.669313430786133
Norm after each mp layer: 88.25755310058594
Norm before input: 0.2552422881126404
Norm after input: 0.5288980603218079
Norm after each mp layer: 2.522613763809204
Norm after each mp layer: 17.669313430786133
Norm after each mp layer: 88.25755310058594
Norm before input: 0.2552422881126404
Norm after input: 0.5289907455444336
Norm after each mp layer: 2.523179292678833
Norm after each mp layer: 17.678918838500977
Norm after each mp layer: 88.1582260131836
Norm before input: 0.2552422881126404
Norm after input: 0.5289907455444336
Norm after each mp layer: 2.523179292678833
Norm after each mp layer: 17.678918838500977
Norm after each mp layer: 88.1582260131836
Norm before input: 0.2552422881126404
Norm after input: 0.5288814902305603
Norm after each mp layer: 2.5220227241516113
Norm after each mp layer: 17.669639587402344
Norm after each mp layer: 88.19213104248047
Epoch: 430, Loss: 0.0053, Energy: 276931.5938, Train: 99.92%, Valid: 73.80%, Test: 74.00%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5288814902305603
Norm after each mp layer: 2.5220227241516113
Norm after each mp layer: 17.669639587402344
Norm after each mp layer: 88.19213104248047
Norm before input: 0.2552422881126404
Norm after input: 0.5289022326469421
Norm after each mp layer: 2.521975040435791
Norm after each mp layer: 17.672550201416016
Norm after each mp layer: 88.13983917236328
Norm before input: 0.2552422881126404
Norm after input: 0.5289022326469421
Norm after each mp layer: 2.521975040435791
Norm after each mp layer: 17.672550201416016
Norm after each mp layer: 88.13983917236328
Norm before input: 0.2552422881126404
Norm after input: 0.5288658142089844
Norm after each mp layer: 2.5214388370513916
Norm after each mp layer: 17.67012596130371
Norm after each mp layer: 88.12612915039062
Norm before input: 0.2552422881126404
Norm after input: 0.5288658142089844
Norm after each mp layer: 2.5214388370513916
Norm after each mp layer: 17.67012596130371
Norm after each mp layer: 88.12612915039062
Norm before input: 0.2552422881126404
Norm after input: 0.5288200378417969
Norm after each mp layer: 2.520822048187256
Norm after each mp layer: 17.666833877563477
Norm after each mp layer: 88.11946868896484
Norm before input: 0.2552422881126404
Norm after input: 0.5288200378417969
Norm after each mp layer: 2.520822048187256
Norm after each mp layer: 17.666833877563477
Norm after each mp layer: 88.11946868896484
Norm before input: 0.2552422881126404
Norm after input: 0.5288445949554443
Norm after each mp layer: 2.5208077430725098
Norm after each mp layer: 17.67015266418457
Norm after each mp layer: 88.06643676757812
Norm before input: 0.2552422881126404
Norm after input: 0.5288445949554443
Norm after each mp layer: 2.5208077430725098
Norm after each mp layer: 17.67015266418457
Norm after each mp layer: 88.06643676757812
Norm before input: 0.2552422881126404
Norm after input: 0.5287455916404724
Norm after each mp layer: 2.519737720489502
Norm after each mp layer: 17.66187286376953
Norm after each mp layer: 88.09569549560547
Epoch: 435, Loss: 0.0052, Energy: 272759.9062, Train: 99.92%, Valid: 73.80%, Test: 73.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5287455916404724
Norm after each mp layer: 2.519737720489502
Norm after each mp layer: 17.66187286376953
Norm after each mp layer: 88.09569549560547
Norm before input: 0.2552422881126404
Norm after input: 0.5288147330284119
Norm after each mp layer: 2.5201077461242676
Norm after each mp layer: 17.66936492919922
Norm after each mp layer: 88.01272583007812
Norm before input: 0.2552422881126404
Norm after input: 0.5288147330284119
Norm after each mp layer: 2.5201077461242676
Norm after each mp layer: 17.66936492919922
Norm after each mp layer: 88.01272583007812
Norm before input: 0.2552422881126404
Norm after input: 0.5286825299263
Norm after each mp layer: 2.5187556743621826
Norm after each mp layer: 17.657928466796875
Norm after each mp layer: 88.06351470947266
Norm before input: 0.2552422881126404
Norm after input: 0.5286825299263
Norm after each mp layer: 2.5187556743621826
Norm after each mp layer: 17.657928466796875
Norm after each mp layer: 88.06351470947266
Norm before input: 0.2552422881126404
Norm after input: 0.528768002986908
Norm after each mp layer: 2.5192654132843018
Norm after each mp layer: 17.666887283325195
Norm after each mp layer: 87.96993255615234
Norm before input: 0.2552422881126404
Norm after input: 0.528768002986908
Norm after each mp layer: 2.5192654132843018
Norm after each mp layer: 17.666887283325195
Norm after each mp layer: 87.96993255615234
Norm before input: 0.2552422881126404
Norm after input: 0.5286383032798767
Norm after each mp layer: 2.5179336071014404
Norm after each mp layer: 17.655620574951172
Norm after each mp layer: 88.01943969726562
Norm before input: 0.2552422881126404
Norm after input: 0.5286383032798767
Norm after each mp layer: 2.5179336071014404
Norm after each mp layer: 17.655620574951172
Norm after each mp layer: 88.01943969726562
Norm before input: 0.2552422881126404
Norm after input: 0.5287025570869446
Norm after each mp layer: 2.5182619094848633
Norm after each mp layer: 17.662555694580078
Norm after each mp layer: 87.94001770019531
Epoch: 440, Loss: 0.0052, Energy: 268917.6875, Train: 99.92%, Valid: 73.80%, Test: 73.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5287025570869446
Norm after each mp layer: 2.5182619094848633
Norm after each mp layer: 17.662555694580078
Norm after each mp layer: 87.94001770019531
Norm before input: 0.2552422881126404
Norm after input: 0.5286081433296204
Norm after each mp layer: 2.5172317028045654
Norm after each mp layer: 17.65459632873535
Norm after each mp layer: 87.96609497070312
Norm before input: 0.2552422881126404
Norm after input: 0.5286081433296204
Norm after each mp layer: 2.5172317028045654
Norm after each mp layer: 17.65459632873535
Norm after each mp layer: 87.96609497070312
Norm before input: 0.2552422881126404
Norm after input: 0.5286301970481873
Norm after each mp layer: 2.5172009468078613
Norm after each mp layer: 17.657611846923828
Norm after each mp layer: 87.91435241699219
Norm before input: 0.2552422881126404
Norm after input: 0.5286301970481873
Norm after each mp layer: 2.5172009468078613
Norm after each mp layer: 17.657611846923828
Norm after each mp layer: 87.91435241699219
Norm before input: 0.2552422881126404
Norm after input: 0.5285810232162476
Norm after each mp layer: 2.5165603160858154
Norm after each mp layer: 17.653949737548828
Norm after each mp layer: 87.9103775024414
Norm before input: 0.2552422881126404
Norm after input: 0.5285810232162476
Norm after each mp layer: 2.5165603160858154
Norm after each mp layer: 17.653949737548828
Norm after each mp layer: 87.9103775024414
Norm before input: 0.2552422881126404
Norm after input: 0.5285608172416687
Norm after each mp layer: 2.5161702632904053
Norm after each mp layer: 17.65304946899414
Norm after each mp layer: 87.8872299194336
Norm before input: 0.2552422881126404
Norm after input: 0.5285608172416687
Norm after each mp layer: 2.5161702632904053
Norm after each mp layer: 17.65304946899414
Norm after each mp layer: 87.8872299194336
Norm before input: 0.2552422881126404
Norm after input: 0.5285516381263733
Norm after each mp layer: 2.515874147415161
Norm after each mp layer: 17.65318489074707
Norm after each mp layer: 87.857177734375
Epoch: 445, Loss: 0.0051, Energy: 264875.7500, Train: 99.92%, Valid: 73.80%, Test: 74.00%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5285516381263733
Norm after each mp layer: 2.515874147415161
Norm after each mp layer: 17.65318489074707
Norm after each mp layer: 87.857177734375
Norm before input: 0.2552422881126404
Norm after input: 0.5284963846206665
Norm after each mp layer: 2.515184164047241
Norm after each mp layer: 17.64900016784668
Norm after each mp layer: 87.85826873779297
Norm before input: 0.2552422881126404
Norm after input: 0.5284963846206665
Norm after each mp layer: 2.515184164047241
Norm after each mp layer: 17.64900016784668
Norm after each mp layer: 87.85826873779297
Norm before input: 0.2552422881126404
Norm after input: 0.5285188555717468
Norm after each mp layer: 2.5151631832122803
Norm after each mp layer: 17.652141571044922
Norm after each mp layer: 87.80745697021484
Norm before input: 0.2552422881126404
Norm after input: 0.5285188555717468
Norm after each mp layer: 2.5151631832122803
Norm after each mp layer: 17.652141571044922
Norm after each mp layer: 87.80745697021484
Norm before input: 0.2552422881126404
Norm after input: 0.5284357070922852
Norm after each mp layer: 2.514235496520996
Norm after each mp layer: 17.64532470703125
Norm after each mp layer: 87.82759094238281
Norm before input: 0.2552422881126404
Norm after input: 0.5284357070922852
Norm after each mp layer: 2.514235496520996
Norm after each mp layer: 17.64532470703125
Norm after each mp layer: 87.82759094238281
Norm before input: 0.2552422881126404
Norm after input: 0.52848219871521
Norm after each mp layer: 2.514422655105591
Norm after each mp layer: 17.65071678161621
Norm after each mp layer: 87.76081085205078
Norm before input: 0.2552422881126404
Norm after input: 0.52848219871521
Norm after each mp layer: 2.514422655105591
Norm after each mp layer: 17.65071678161621
Norm after each mp layer: 87.76081085205078
Norm before input: 0.2552422881126404
Norm after input: 0.5283789038658142
Norm after each mp layer: 2.513322114944458
Norm after each mp layer: 17.641971588134766
Norm after each mp layer: 87.79450988769531
Epoch: 450, Loss: 0.0050, Energy: 260952.4062, Train: 99.92%, Valid: 73.80%, Test: 73.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5283789038658142
Norm after each mp layer: 2.513322114944458
Norm after each mp layer: 17.641971588134766
Norm after each mp layer: 87.79450988769531
Norm before input: 0.2552422881126404
Norm after input: 0.5284401178359985
Norm after each mp layer: 2.5136356353759766
Norm after each mp layer: 17.648717880249023
Norm after each mp layer: 87.71804809570312
Norm before input: 0.2552422881126404
Norm after input: 0.5284401178359985
Norm after each mp layer: 2.5136356353759766
Norm after each mp layer: 17.648717880249023
Norm after each mp layer: 87.71804809570312
Norm before input: 0.2552422881126404
Norm after input: 0.5283276438713074
Norm after each mp layer: 2.5124542713165283
Norm after each mp layer: 17.639057159423828
Norm after each mp layer: 87.75819396972656
Norm before input: 0.2552422881126404
Norm after input: 0.5283276438713074
Norm after each mp layer: 2.5124542713165283
Norm after each mp layer: 17.639057159423828
Norm after each mp layer: 87.75819396972656
Norm before input: 0.2552422881126404
Norm after input: 0.5283915996551514
Norm after each mp layer: 2.5127899646759033
Norm after each mp layer: 17.646026611328125
Norm after each mp layer: 87.68000030517578
Norm before input: 0.2552422881126404
Norm after input: 0.5283915996551514
Norm after each mp layer: 2.5127899646759033
Norm after each mp layer: 17.646026611328125
Norm after each mp layer: 87.68000030517578
Norm before input: 0.2552422881126404
Norm after input: 0.5282828211784363
Norm after each mp layer: 2.5116384029388428
Norm after each mp layer: 17.636682510375977
Norm after each mp layer: 87.71756744384766
Norm before input: 0.2552422881126404
Norm after input: 0.5282828211784363
Norm after each mp layer: 2.5116384029388428
Norm after each mp layer: 17.636682510375977
Norm after each mp layer: 87.71756744384766
Norm before input: 0.2552422881126404
Norm after input: 0.5283377170562744
Norm after each mp layer: 2.5118961334228516
Norm after each mp layer: 17.64278793334961
Norm after each mp layer: 87.64494323730469
Epoch: 455, Loss: 0.0050, Energy: 257456.4375, Train: 99.92%, Valid: 74.00%, Test: 73.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5283377170562744
Norm after each mp layer: 2.5118961334228516
Norm after each mp layer: 17.64278793334961
Norm after each mp layer: 87.64494323730469
Norm before input: 0.2552422881126404
Norm after input: 0.5282424092292786
Norm after each mp layer: 2.510859251022339
Norm after each mp layer: 17.634702682495117
Norm after each mp layer: 87.67327117919922
Norm before input: 0.2552422881126404
Norm after input: 0.5282424092292786
Norm after each mp layer: 2.510859251022339
Norm after each mp layer: 17.634702682495117
Norm after each mp layer: 87.67327117919922
Norm before input: 0.2552422881126404
Norm after input: 0.5282818675041199
Norm after each mp layer: 2.510984182357788
Norm after each mp layer: 17.639352798461914
Norm after each mp layer: 87.6109390258789
Norm before input: 0.2552422881126404
Norm after input: 0.5282818675041199
Norm after each mp layer: 2.510984182357788
Norm after each mp layer: 17.639352798461914
Norm after each mp layer: 87.6109390258789
Norm before input: 0.2552422881126404
Norm after input: 0.5282033085823059
Norm after each mp layer: 2.5100901126861572
Norm after each mp layer: 17.632831573486328
Norm after each mp layer: 87.62835693359375
Norm before input: 0.2552422881126404
Norm after input: 0.5282033085823059
Norm after each mp layer: 2.5100901126861572
Norm after each mp layer: 17.632831573486328
Norm after each mp layer: 87.62835693359375
Norm before input: 0.2552422881126404
Norm after input: 0.5282272696495056
Norm after each mp layer: 2.5100808143615723
Norm after each mp layer: 17.636009216308594
Norm after each mp layer: 87.5768051147461
Norm before input: 0.2552422881126404
Norm after input: 0.5282272696495056
Norm after each mp layer: 2.5100808143615723
Norm after each mp layer: 17.636009216308594
Norm after each mp layer: 87.5768051147461
Norm before input: 0.2552422881126404
Norm after input: 0.528162956237793
Norm after each mp layer: 2.509310245513916
Norm after each mp layer: 17.630834579467773
Norm after each mp layer: 87.58487701416016
Epoch: 460, Loss: 0.0049, Energy: 253668.8594, Train: 99.92%, Valid: 74.00%, Test: 73.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.528162956237793
Norm after each mp layer: 2.509310245513916
Norm after each mp layer: 17.630834579467773
Norm after each mp layer: 87.58487701416016
Norm before input: 0.2552422881126404
Norm after input: 0.5281751751899719
Norm after each mp layer: 2.509199380874634
Norm after each mp layer: 17.632890701293945
Norm after each mp layer: 87.54144287109375
Norm before input: 0.2552422881126404
Norm after input: 0.5281751751899719
Norm after each mp layer: 2.509199380874634
Norm after each mp layer: 17.632890701293945
Norm after each mp layer: 87.54144287109375
Norm before input: 0.2552422881126404
Norm after input: 0.5281203389167786
Norm after each mp layer: 2.5085113048553467
Norm after each mp layer: 17.62860679626465
Norm after each mp layer: 87.54332733154297
Norm before input: 0.2552422881126404
Norm after input: 0.5281203389167786
Norm after each mp layer: 2.5085113048553467
Norm after each mp layer: 17.62860679626465
Norm after each mp layer: 87.54332733154297
Norm before input: 0.2552422881126404
Norm after input: 0.5281259417533875
Norm after each mp layer: 2.5083441734313965
Norm after each mp layer: 17.63003158569336
Norm after each mp layer: 87.50459289550781
Norm before input: 0.2552422881126404
Norm after input: 0.5281259417533875
Norm after each mp layer: 2.5083441734313965
Norm after each mp layer: 17.63003158569336
Norm after each mp layer: 87.50459289550781
Norm before input: 0.2552422881126404
Norm after input: 0.5280751585960388
Norm after each mp layer: 2.5076904296875
Norm after each mp layer: 17.626113891601562
Norm after each mp layer: 87.50410461425781
Norm before input: 0.2552422881126404
Norm after input: 0.5280751585960388
Norm after each mp layer: 2.5076904296875
Norm after each mp layer: 17.626113891601562
Norm after each mp layer: 87.50410461425781
Norm before input: 0.2552422881126404
Norm after input: 0.5280797481536865
Norm after each mp layer: 2.5075137615203857
Norm after each mp layer: 17.627422332763672
Norm after each mp layer: 87.46649932861328
Epoch: 465, Loss: 0.0048, Energy: 250218.4375, Train: 99.92%, Valid: 74.20%, Test: 73.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5280797481536865
Norm after each mp layer: 2.5075137615203857
Norm after each mp layer: 17.627422332763672
Norm after each mp layer: 87.46649932861328
Norm before input: 0.2552422881126404
Norm after input: 0.5280274748802185
Norm after each mp layer: 2.5068464279174805
Norm after each mp layer: 17.62334442138672
Norm after each mp layer: 87.46751403808594
Norm before input: 0.2552422881126404
Norm after input: 0.5280274748802185
Norm after each mp layer: 2.5068464279174805
Norm after each mp layer: 17.62334442138672
Norm after each mp layer: 87.46751403808594
Norm before input: 0.2552422881126404
Norm after input: 0.5280369520187378
Norm after each mp layer: 2.5067121982574463
Norm after each mp layer: 17.625110626220703
Norm after each mp layer: 87.4268798828125
Norm before input: 0.2552422881126404
Norm after input: 0.5280369520187378
Norm after each mp layer: 2.5067121982574463
Norm after each mp layer: 17.625110626220703
Norm after each mp layer: 87.4268798828125
Norm before input: 0.2552422881126404
Norm after input: 0.5279768109321594
Norm after each mp layer: 2.5059759616851807
Norm after each mp layer: 17.620267868041992
Norm after each mp layer: 87.43352508544922
Norm before input: 0.2552422881126404
Norm after input: 0.5279768109321594
Norm after each mp layer: 2.5059759616851807
Norm after each mp layer: 17.620267868041992
Norm after each mp layer: 87.43352508544922
Norm before input: 0.2552422881126404
Norm after input: 0.5279987454414368
Norm after each mp layer: 2.5059494972229004
Norm after each mp layer: 17.623197555541992
Norm after each mp layer: 87.38461303710938
Norm before input: 0.2552422881126404
Norm after input: 0.5279987454414368
Norm after each mp layer: 2.5059494972229004
Norm after each mp layer: 17.623197555541992
Norm after each mp layer: 87.38461303710938
Norm before input: 0.2552422881126404
Norm after input: 0.5279220342636108
Norm after each mp layer: 2.5050690174102783
Norm after each mp layer: 17.616764068603516
Norm after each mp layer: 87.40269470214844
Epoch: 470, Loss: 0.0048, Energy: 246706.3750, Train: 99.92%, Valid: 74.00%, Test: 73.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5279220342636108
Norm after each mp layer: 2.5050692558288574
Norm after each mp layer: 17.616762161254883
Norm after each mp layer: 87.40269470214844
Norm before input: 0.2552422881126404
Norm after input: 0.5279666781425476
Norm after each mp layer: 2.5052387714385986
Norm after each mp layer: 17.621826171875
Norm after each mp layer: 87.33859252929688
Norm before input: 0.2552422881126404
Norm after input: 0.5279666781425476
Norm after each mp layer: 2.5052387714385986
Norm after each mp layer: 17.621826171875
Norm after each mp layer: 87.33859252929688
Norm before input: 0.2552422881126404
Norm after input: 0.5278615951538086
Norm after each mp layer: 2.50411057472229
Norm after each mp layer: 17.61265754699707
Norm after each mp layer: 87.3761978149414
Norm before input: 0.2552422881126404
Norm after input: 0.5278615951538086
Norm after each mp layer: 2.50411057472229
Norm after each mp layer: 17.61265754699707
Norm after each mp layer: 87.3761978149414
Norm before input: 0.2552422881126404
Norm after input: 0.5279414057731628
Norm after each mp layer: 2.5045833587646484
Norm after each mp layer: 17.621009826660156
Norm after each mp layer: 87.28841400146484
Norm before input: 0.2552422881126404
Norm after input: 0.5279414057731628
Norm after each mp layer: 2.5045833587646484
Norm after each mp layer: 17.621009826660156
Norm after each mp layer: 87.28841400146484
Norm before input: 0.2552422881126404
Norm after input: 0.5277968645095825
Norm after each mp layer: 2.503110647201538
Norm after each mp layer: 17.608020782470703
Norm after each mp layer: 87.35260009765625
Norm before input: 0.2552422881126404
Norm after input: 0.5277968645095825
Norm after each mp layer: 2.503110647201538
Norm after each mp layer: 17.608020782470703
Norm after each mp layer: 87.35260009765625
Norm before input: 0.2552422881126404
Norm after input: 0.527915894985199
Norm after each mp layer: 2.5039196014404297
Norm after each mp layer: 17.620004653930664
Norm after each mp layer: 87.2381362915039
Epoch: 475, Loss: 0.0048, Energy: 243607.2969, Train: 99.92%, Valid: 74.20%, Test: 73.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.527915894985199
Norm after each mp layer: 2.5039196014404297
Norm after each mp layer: 17.620004653930664
Norm after each mp layer: 87.2381362915039
Norm before input: 0.2552422881126404
Norm after input: 0.5277400612831116
Norm after each mp layer: 2.5021700859069824
Norm after each mp layer: 17.60392951965332
Norm after each mp layer: 87.3226089477539
Norm before input: 0.2552422881126404
Norm after input: 0.5277400612831116
Norm after each mp layer: 2.5021700859069824
Norm after each mp layer: 17.60392951965332
Norm after each mp layer: 87.3226089477539
Norm before input: 0.2552422881126404
Norm after input: 0.5278711318969727
Norm after each mp layer: 2.503077983856201
Norm after each mp layer: 17.616933822631836
Norm after each mp layer: 87.19938659667969
Norm before input: 0.2552422881126404
Norm after input: 0.5278711318969727
Norm after each mp layer: 2.503077983856201
Norm after each mp layer: 17.616933822631836
Norm after each mp layer: 87.19938659667969
Norm before input: 0.2552422881126404
Norm after input: 0.5277063250541687
Norm after each mp layer: 2.501418113708496
Norm after each mp layer: 17.601795196533203
Norm after each mp layer: 87.27532196044922
Norm before input: 0.2552422881126404
Norm after input: 0.5277063250541687
Norm after each mp layer: 2.501418113708496
Norm after each mp layer: 17.601795196533203
Norm after each mp layer: 87.27532958984375
Norm before input: 0.2552422881126404
Norm after input: 0.5278024077415466
Norm after each mp layer: 2.5020196437835693
Norm after each mp layer: 17.611425399780273
Norm after each mp layer: 87.17447662353516
Norm before input: 0.2552422881126404
Norm after input: 0.5278024077415466
Norm after each mp layer: 2.5020196437835693
Norm after each mp layer: 17.611425399780273
Norm after each mp layer: 87.17448425292969
Norm before input: 0.2552422881126404
Norm after input: 0.5276869535446167
Norm after each mp layer: 2.5007853507995605
Norm after each mp layer: 17.600963592529297
Norm after each mp layer: 87.21664428710938
Epoch: 480, Loss: 0.0047, Energy: 240026.5625, Train: 99.92%, Valid: 74.00%, Test: 73.80%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5276869535446167
Norm after each mp layer: 2.5007853507995605
Norm after each mp layer: 17.600963592529297
Norm after each mp layer: 87.21664428710938
Norm before input: 0.2552422881126404
Norm after input: 0.5277288556098938
Norm after each mp layer: 2.500915288925171
Norm after each mp layer: 17.6054630279541
Norm after each mp layer: 87.15253448486328
Norm before input: 0.2552422881126404
Norm after input: 0.5277288556098938
Norm after each mp layer: 2.500915288925171
Norm after each mp layer: 17.6054630279541
Norm after each mp layer: 87.15253448486328
Norm before input: 0.2552422881126404
Norm after input: 0.5276668667793274
Norm after each mp layer: 2.5001425743103027
Norm after each mp layer: 17.600099563598633
Norm after each mp layer: 87.15945434570312
Norm before input: 0.2552422881126404
Norm after input: 0.5276668667793274
Norm after each mp layer: 2.5001425743103027
Norm after each mp layer: 17.600099563598633
Norm after each mp layer: 87.15945434570312
Norm before input: 0.2552422881126404
Norm after input: 0.5276613235473633
Norm after each mp layer: 2.499859571456909
Norm after each mp layer: 17.600116729736328
Norm after each mp layer: 87.12869262695312
Norm before input: 0.2552422881126404
Norm after input: 0.5276613235473633
Norm after each mp layer: 2.499859571456909
Norm after each mp layer: 17.600116729736328
Norm after each mp layer: 87.12869262695312
Norm before input: 0.2552422881126404
Norm after input: 0.5276423692703247
Norm after each mp layer: 2.4994614124298096
Norm after each mp layer: 17.598876953125
Norm after each mp layer: 87.10748291015625
Norm before input: 0.2552422881126404
Norm after input: 0.5276423692703247
Norm after each mp layer: 2.4994614124298096
Norm after each mp layer: 17.598876953125
Norm after each mp layer: 87.10748291015625
Norm before input: 0.2552422881126404
Norm after input: 0.527599036693573
Norm after each mp layer: 2.498852491378784
Norm after each mp layer: 17.595325469970703
Norm after each mp layer: 87.10310363769531
Epoch: 485, Loss: 0.0046, Energy: 236876.9531, Train: 99.92%, Valid: 74.00%, Test: 73.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.527599036693573
Norm after each mp layer: 2.498852491378784
Norm after each mp layer: 17.595325469970703
Norm after each mp layer: 87.10310363769531
Norm before input: 0.2552422881126404
Norm after input: 0.5276156067848206
Norm after each mp layer: 2.4987659454345703
Norm after each mp layer: 17.597484588623047
Norm after each mp layer: 87.05815887451172
Norm before input: 0.2552422881126404
Norm after input: 0.5276156067848206
Norm after each mp layer: 2.4987659454345703
Norm after each mp layer: 17.597484588623047
Norm after each mp layer: 87.05815124511719
Norm before input: 0.2552422881126404
Norm after input: 0.5275385975837708
Norm after each mp layer: 2.4978668689727783
Norm after each mp layer: 17.590734481811523
Norm after each mp layer: 87.07699584960938
Norm before input: 0.2552422881126404
Norm after input: 0.5275385975837708
Norm after each mp layer: 2.4978668689727783
Norm after each mp layer: 17.590734481811523
Norm after each mp layer: 87.07699584960938
Norm before input: 0.2552422881126404
Norm after input: 0.5275890231132507
Norm after each mp layer: 2.498077392578125
Norm after each mp layer: 17.59610366821289
Norm after each mp layer: 87.00961303710938
Norm before input: 0.2552422881126404
Norm after input: 0.5275890231132507
Norm after each mp layer: 2.498077392578125
Norm after each mp layer: 17.59610366821289
Norm after each mp layer: 87.00960540771484
Norm before input: 0.2552422881126404
Norm after input: 0.5274785161018372
Norm after each mp layer: 2.496886968612671
Norm after each mp layer: 17.586124420166016
Norm after each mp layer: 87.05192565917969
Norm before input: 0.2552422881126404
Norm after input: 0.5274785161018372
Norm after each mp layer: 2.496886968612671
Norm after each mp layer: 17.586124420166016
Norm after each mp layer: 87.05192565917969
Norm before input: 0.2552422881126404
Norm after input: 0.5275623798370361
Norm after each mp layer: 2.4973866939544678
Norm after each mp layer: 17.594619750976562
Norm after each mp layer: 86.96260070800781
Epoch: 490, Loss: 0.0046, Energy: 233898.0625, Train: 99.92%, Valid: 74.00%, Test: 74.00%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5275623798370361
Norm after each mp layer: 2.4973866939544678
Norm after each mp layer: 17.594619750976562
Norm after each mp layer: 86.96260070800781
Norm before input: 0.2552422881126404
Norm after input: 0.5274221897125244
Norm after each mp layer: 2.495936870574951
Norm after each mp layer: 17.58176040649414
Norm after each mp layer: 87.02516174316406
Norm before input: 0.2552422881126404
Norm after input: 0.5274221897125244
Norm after each mp layer: 2.495936870574951
Norm after each mp layer: 17.58176040649414
Norm after each mp layer: 87.02516174316406
Norm before input: 0.2552422881126404
Norm after input: 0.5275289416313171
Norm after each mp layer: 2.4966354370117188
Norm after each mp layer: 17.592382431030273
Norm after each mp layer: 86.92001342773438
Norm before input: 0.2552422881126404
Norm after input: 0.5275289416313171
Norm after each mp layer: 2.4966354370117188
Norm after each mp layer: 17.592382431030273
Norm after each mp layer: 86.92001342773438
Norm before input: 0.2552422881126404
Norm after input: 0.5273774266242981
Norm after each mp layer: 2.495084047317505
Norm after each mp layer: 17.578378677368164
Norm after each mp layer: 86.98949432373047
Norm before input: 0.2552422881126404
Norm after input: 0.5273774266242981
Norm after each mp layer: 2.495084047317505
Norm after each mp layer: 17.578378677368164
Norm after each mp layer: 86.98949432373047
Norm before input: 0.2552422881126404
Norm after input: 0.5274814963340759
Norm after each mp layer: 2.495758056640625
Norm after each mp layer: 17.588706970214844
Norm after each mp layer: 86.88526153564453
Norm before input: 0.2552422881126404
Norm after input: 0.5274814963340759
Norm after each mp layer: 2.495758056640625
Norm after each mp layer: 17.588706970214844
Norm after each mp layer: 86.88526153564453
Norm before input: 0.2552422881126404
Norm after input: 0.5273467302322388
Norm after each mp layer: 2.4943506717681885
Norm after each mp layer: 17.576261520385742
Norm after each mp layer: 86.94281768798828
Epoch: 495, Loss: 0.0046, Energy: 230606.9062, Train: 99.92%, Valid: 74.00%, Test: 73.90%, Best Valid: 78.20%, Best Test: 77.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5273467302322388
Norm after each mp layer: 2.4943506717681885
Norm after each mp layer: 17.576261520385742
Norm after each mp layer: 86.94281768798828
Norm before input: 0.2552422881126404
Norm after input: 0.527423620223999
Norm after each mp layer: 2.494786024093628
Norm after each mp layer: 17.583980560302734
Norm after each mp layer: 86.85674285888672
Norm before input: 0.2552422881126404
Norm after input: 0.527423620223999
Norm after each mp layer: 2.494786024093628
Norm after each mp layer: 17.583980560302734
Norm after each mp layer: 86.85674285888672
Norm before input: 0.2552422881126404
Norm after input: 0.5273225903511047
Norm after each mp layer: 2.4936702251434326
Norm after each mp layer: 17.57474708557129
Norm after each mp layer: 86.89169311523438
Norm before input: 0.2552422881126404
Norm after input: 0.5273225903511047
Norm after each mp layer: 2.4936702251434326
Norm after each mp layer: 17.57474708557129
Norm after each mp layer: 86.89169311523438
Norm before input: 0.2552422881126404
Norm after input: 0.5273649096488953
Norm after each mp layer: 2.4938035011291504
Norm after each mp layer: 17.579181671142578
Norm after each mp layer: 86.82952117919922
Norm before input: 0.2552422881126404
Norm after input: 0.5273649096488953
Norm after each mp layer: 2.4938035011291504
Norm after each mp layer: 17.579181671142578
Norm after each mp layer: 86.82952117919922
Norm before input: 0.2552422881126404
Norm after input: 0.5272971987724304
Norm after each mp layer: 2.4929776191711426
Norm after each mp layer: 17.573139190673828
Norm after each mp layer: 86.8424301147461
train_accuracy_list: [0.28228476821192056, 0.28228476821192056, 0.16225165562913907, 0.16225165562913907, 0.28228476821192056, 0.16225165562913907, 0.12251655629139073, 0.16225165562913907, 0.28228476821192056, 0.28228476821192056, 0.3195364238410596, 0.16225165562913907, 0.20198675496688742, 0.26986754966887416, 0.24420529801324503, 0.3253311258278146, 0.37665562913907286, 0.3741721854304636, 0.37665562913907286, 0.4155629139072848, 0.4337748344370861, 0.4544701986754967, 0.37251655629139074, 0.37996688741721857, 0.4644039735099338, 0.4652317880794702, 0.46605960264900664, 0.46605960264900664, 0.46357615894039733, 0.47516556291390727, 0.5206953642384106, 0.5463576158940397, 0.5132450331125827, 0.5173841059602649, 0.5099337748344371, 0.5372516556291391, 0.5670529801324503, 0.6241721854304636, 0.6208609271523179, 0.6307947019867549, 0.6506622516556292, 0.5968543046357616, 0.5993377483443708, 0.6026490066225165, 0.6639072847682119, 0.6564569536423841, 0.6672185430463576, 0.6821192052980133, 0.6639072847682119, 0.6721854304635762, 0.6928807947019867, 0.6995033112582781, 0.7135761589403974, 0.7110927152317881, 0.7433774834437086, 0.7524834437086093, 0.7582781456953642, 0.7591059602649006, 0.7723509933774835, 0.7748344370860927, 0.7913907284768212, 0.777317880794702, 0.7872516556291391, 0.7913907284768212, 0.7897350993377483, 0.8153973509933775, 0.8211920529801324, 0.8187086092715232, 0.8178807947019867, 0.8360927152317881, 0.8427152317880795, 0.8427152317880795, 0.8468543046357616, 0.8526490066225165, 0.8567880794701986, 0.8534768211920529, 0.8725165562913907, 0.8733443708609272, 0.8642384105960265, 0.8824503311258278, 0.8816225165562914, 0.8948675496688742, 0.9014900662251656, 0.9014900662251656, 0.9081125827814569, 0.8990066225165563, 0.9172185430463576, 0.9279801324503312, 0.9213576158940397, 0.9288079470198676, 0.9354304635761589, 0.9403973509933775, 0.9395695364238411, 0.9445364238410596, 0.9511589403973509, 0.9511589403973509, 0.9619205298013245, 0.9610927152317881, 0.9602649006622517, 0.9635761589403974, 0.9586092715231788, 0.9685430463576159, 0.9627483443708609, 0.9668874172185431, 0.972682119205298, 0.9627483443708609, 0.9685430463576159, 0.9718543046357616, 0.9660596026490066, 0.9710264900662252, 0.9751655629139073, 0.9644039735099338, 0.9735099337748344, 0.9768211920529801, 0.9701986754966887, 0.9793046357615894, 0.9759933774834437, 0.9776490066225165, 0.9768211920529801, 0.9801324503311258, 0.9784768211920529, 0.9776490066225165, 0.9817880794701986, 0.9809602649006622, 0.9809602649006622, 0.9801324503311258, 0.9809602649006622, 0.9826158940397351, 0.9826158940397351, 0.9859271523178808, 0.9826158940397351, 0.984271523178808, 0.9875827814569537, 0.9867549668874173, 0.9867549668874173, 0.9884105960264901, 0.9908940397350994, 0.9900662251655629, 0.9892384105960265, 0.9917218543046358, 0.9917218543046358, 0.9917218543046358, 0.9908940397350994, 0.9908940397350994, 0.9917218543046358, 0.9917218543046358, 0.9925496688741722, 0.9933774834437086, 0.9933774834437086, 0.9942052980132451, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9917218543046358, 0.9942052980132451, 0.9933774834437086, 0.9933774834437086, 0.9942052980132451, 0.9942052980132451, 0.9950331125827815, 0.9950331125827815, 0.9950331125827815, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9966887417218543, 0.9966887417218543, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9966887417218543, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636]
valid_accuracy_list: [0.316, 0.316, 0.156, 0.162, 0.316, 0.156, 0.134, 0.154, 0.316, 0.316, 0.35, 0.162, 0.186, 0.268, 0.22, 0.364, 0.388, 0.406, 0.404, 0.424, 0.442, 0.46, 0.364, 0.364, 0.444, 0.45, 0.462, 0.456, 0.464, 0.474, 0.5, 0.498, 0.492, 0.486, 0.504, 0.506, 0.538, 0.566, 0.554, 0.554, 0.588, 0.548, 0.548, 0.558, 0.588, 0.582, 0.59, 0.598, 0.6, 0.59, 0.606, 0.6, 0.622, 0.622, 0.624, 0.63, 0.62, 0.634, 0.63, 0.628, 0.626, 0.638, 0.642, 0.654, 0.656, 0.662, 0.668, 0.666, 0.668, 0.676, 0.69, 0.69, 0.684, 0.69, 0.69, 0.696, 0.706, 0.706, 0.704, 0.706, 0.716, 0.728, 0.732, 0.742, 0.722, 0.742, 0.732, 0.744, 0.752, 0.744, 0.754, 0.756, 0.748, 0.762, 0.762, 0.752, 0.772, 0.762, 0.758, 0.764, 0.758, 0.766, 0.76, 0.764, 0.772, 0.762, 0.778, 0.768, 0.76, 0.782, 0.774, 0.756, 0.778, 0.78, 0.764, 0.776, 0.778, 0.776, 0.774, 0.78, 0.774, 0.77, 0.776, 0.78, 0.768, 0.768, 0.774, 0.778, 0.776, 0.772, 0.772, 0.776, 0.776, 0.778, 0.776, 0.764, 0.766, 0.77, 0.772, 0.768, 0.768, 0.768, 0.77, 0.77, 0.764, 0.764, 0.766, 0.77, 0.768, 0.768, 0.768, 0.772, 0.77, 0.768, 0.766, 0.768, 0.768, 0.768, 0.764, 0.762, 0.764, 0.764, 0.762, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.762, 0.76, 0.76, 0.76, 0.762, 0.764, 0.766, 0.766, 0.766, 0.764, 0.76, 0.76, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.752, 0.754, 0.752, 0.754, 0.754, 0.752, 0.752, 0.752, 0.752, 0.752, 0.754, 0.754, 0.752, 0.754, 0.752, 0.752, 0.754, 0.752, 0.754, 0.754, 0.754, 0.756, 0.752, 0.754, 0.754, 0.754, 0.754, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.752, 0.754, 0.752, 0.752, 0.752, 0.752, 0.754, 0.752, 0.754, 0.754, 0.754, 0.756, 0.756, 0.756, 0.754, 0.754, 0.756, 0.756, 0.756, 0.756, 0.754, 0.76, 0.754, 0.758, 0.754, 0.76, 0.758, 0.76, 0.76, 0.76, 0.76, 0.758, 0.758, 0.758, 0.758, 0.756, 0.758, 0.754, 0.758, 0.754, 0.756, 0.754, 0.754, 0.754, 0.754, 0.752, 0.754, 0.752, 0.754, 0.752, 0.754, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.744, 0.746, 0.742, 0.746, 0.742, 0.744, 0.744, 0.742, 0.744, 0.742, 0.744, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.74, 0.738, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.74, 0.742, 0.74, 0.742, 0.74, 0.742, 0.74, 0.742, 0.74, 0.742, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74]
test_accuracy_list: [0.319, 0.319, 0.144, 0.149, 0.319, 0.144, 0.116, 0.146, 0.319, 0.319, 0.341, 0.149, 0.172, 0.256, 0.212, 0.35, 0.393, 0.39, 0.393, 0.442, 0.453, 0.459, 0.362, 0.373, 0.461, 0.46, 0.462, 0.476, 0.457, 0.474, 0.494, 0.493, 0.497, 0.5, 0.495, 0.521, 0.535, 0.562, 0.566, 0.566, 0.587, 0.579, 0.57, 0.567, 0.598, 0.593, 0.591, 0.601, 0.594, 0.592, 0.597, 0.607, 0.601, 0.607, 0.619, 0.629, 0.631, 0.628, 0.631, 0.636, 0.635, 0.632, 0.638, 0.635, 0.638, 0.637, 0.642, 0.653, 0.633, 0.655, 0.658, 0.647, 0.669, 0.666, 0.659, 0.677, 0.665, 0.672, 0.69, 0.679, 0.703, 0.698, 0.699, 0.711, 0.706, 0.724, 0.724, 0.735, 0.742, 0.731, 0.746, 0.753, 0.748, 0.762, 0.764, 0.764, 0.769, 0.765, 0.767, 0.765, 0.77, 0.766, 0.768, 0.766, 0.769, 0.767, 0.766, 0.769, 0.768, 0.769, 0.769, 0.766, 0.77, 0.767, 0.771, 0.769, 0.77, 0.772, 0.774, 0.768, 0.769, 0.77, 0.774, 0.775, 0.768, 0.765, 0.773, 0.773, 0.774, 0.771, 0.773, 0.772, 0.77, 0.77, 0.773, 0.77, 0.768, 0.769, 0.769, 0.765, 0.763, 0.764, 0.766, 0.766, 0.765, 0.766, 0.766, 0.767, 0.767, 0.766, 0.766, 0.765, 0.765, 0.764, 0.766, 0.763, 0.766, 0.766, 0.767, 0.768, 0.767, 0.768, 0.767, 0.767, 0.767, 0.765, 0.765, 0.766, 0.767, 0.765, 0.765, 0.764, 0.764, 0.763, 0.764, 0.764, 0.766, 0.765, 0.763, 0.761, 0.762, 0.762, 0.761, 0.762, 0.762, 0.761, 0.761, 0.761, 0.761, 0.76, 0.759, 0.758, 0.758, 0.758, 0.756, 0.757, 0.756, 0.756, 0.756, 0.756, 0.756, 0.755, 0.755, 0.753, 0.753, 0.753, 0.752, 0.753, 0.752, 0.752, 0.752, 0.753, 0.753, 0.752, 0.753, 0.755, 0.755, 0.755, 0.756, 0.756, 0.757, 0.758, 0.759, 0.756, 0.759, 0.759, 0.756, 0.755, 0.755, 0.755, 0.755, 0.756, 0.756, 0.755, 0.756, 0.757, 0.755, 0.757, 0.756, 0.756, 0.756, 0.756, 0.756, 0.754, 0.755, 0.754, 0.754, 0.754, 0.754, 0.754, 0.753, 0.754, 0.753, 0.753, 0.753, 0.756, 0.753, 0.755, 0.754, 0.755, 0.755, 0.755, 0.756, 0.755, 0.755, 0.755, 0.754, 0.753, 0.753, 0.753, 0.753, 0.752, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.751, 0.749, 0.75, 0.749, 0.749, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.748, 0.748, 0.748, 0.748, 0.747, 0.748, 0.748, 0.749, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.745, 0.745, 0.745, 0.745, 0.745, 0.744, 0.745, 0.743, 0.745, 0.743, 0.746, 0.742, 0.745, 0.744, 0.744, 0.743, 0.743, 0.743, 0.742, 0.742, 0.742, 0.742, 0.742, 0.743, 0.743, 0.742, 0.742, 0.741, 0.741, 0.742, 0.741, 0.742, 0.741, 0.742, 0.741, 0.742, 0.741, 0.742, 0.742, 0.741, 0.742, 0.742, 0.742, 0.741, 0.742, 0.741, 0.742, 0.741, 0.741, 0.74, 0.741, 0.74, 0.741, 0.74, 0.741, 0.74, 0.741, 0.74, 0.741, 0.74, 0.741, 0.74, 0.741, 0.74, 0.741, 0.74, 0.741, 0.74, 0.741, 0.74, 0.741, 0.739, 0.74, 0.74, 0.739, 0.739, 0.738, 0.739, 0.739, 0.74, 0.739, 0.74, 0.739, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.739, 0.739, 0.738, 0.739, 0.738, 0.739, 0.738, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.739, 0.738, 0.739, 0.739, 0.738, 0.739, 0.738, 0.738, 0.738, 0.738, 0.739, 0.738, 0.738, 0.738, 0.738, 0.739, 0.739, 0.739, 0.739, 0.74, 0.739, 0.74, 0.739, 0.739, 0.739, 0.74, 0.739, 0.74, 0.739, 0.74, 0.74]
best validation: 0.782
best test: 0.775
Experiment run
dataset: cora
num_mp_layers: 3
mp_hidden_dim: 4000
optimizer_lr: 0.001
loss_func: CrossEntropyLoss
total_epoch: 500
energy_lambda: 1e-07
Norm before input: 0.2552422881126404
Norm after input: 0.9947524070739746
Norm after each mp layer: 0.6566787362098694
Norm after each mp layer: 0.6222918033599854
Norm after each mp layer: 0.6184736490249634
Norm before input: 0.2552422881126404
Norm after input: 1.0018219947814941
Norm after each mp layer: 1.8246620893478394
Norm after each mp layer: 1.6395175457000732
Norm after each mp layer: 1.6298909187316895
Norm before input: 0.2552422881126404
Norm after input: 1.0018219947814941
Norm after each mp layer: 1.8246620893478394
Norm after each mp layer: 1.6395175457000732
Norm after each mp layer: 1.6298909187316895
Norm before input: 0.2552422881126404
Norm after input: 1.054314374923706
Norm after each mp layer: 2.7995729446411133
Norm after each mp layer: 5.275571346282959
Norm after each mp layer: 14.315563201904297
Norm before input: 0.2552422881126404
Norm after input: 1.054314374923706
Norm after each mp layer: 2.7995729446411133
Norm after each mp layer: 5.275571346282959
Norm after each mp layer: 14.315563201904297
Norm before input: 0.2552422881126404
Norm after input: 1.1246603727340698
Norm after each mp layer: 4.174447059631348
Norm after each mp layer: 12.077945709228516
Norm after each mp layer: 44.78059768676758
Norm before input: 0.2552422881126404
Norm after input: 1.1246603727340698
Norm after each mp layer: 4.174447059631348
Norm after each mp layer: 12.077945709228516
Norm after each mp layer: 44.78059768676758
Norm before input: 0.2552422881126404
Norm after input: 1.117196798324585
Norm after each mp layer: 4.33712911605835
Norm after each mp layer: 13.519301414489746
Norm after each mp layer: 51.80256271362305
Norm before input: 0.2552422881126404
Norm after input: 1.117196798324585
Norm after each mp layer: 4.33712911605835
Norm after each mp layer: 13.519301414489746
Norm after each mp layer: 51.80256271362305
Norm before input: 0.2552422881126404
Norm after input: 1.073291301727295
Norm after each mp layer: 3.968568801879883
Norm after each mp layer: 10.409236907958984
Norm after each mp layer: 37.60037612915039
Epoch: 05, Loss: 3.9607, Energy: 406988.7500, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 1.073291301727295
Norm after each mp layer: 3.968568801879883
Norm after each mp layer: 10.409236907958984
Norm after each mp layer: 37.60037612915039
Norm before input: 0.2552422881126404
Norm after input: 1.013195276260376
Norm after each mp layer: 3.5618393421173096
Norm after each mp layer: 7.843693733215332
Norm after each mp layer: 17.09088706970215
Norm before input: 0.2552422881126404
Norm after input: 1.013195276260376
Norm after each mp layer: 3.5618388652801514
Norm after each mp layer: 7.843693733215332
Norm after each mp layer: 17.09088897705078
Norm before input: 0.2552422881126404
Norm after input: 0.9749144315719604
Norm after each mp layer: 3.3826372623443604
Norm after each mp layer: 8.444129943847656
Norm after each mp layer: 24.472938537597656
Norm before input: 0.2552422881126404
Norm after input: 0.9749144315719604
Norm after each mp layer: 3.3826372623443604
Norm after each mp layer: 8.444129943847656
Norm after each mp layer: 24.472938537597656
Norm before input: 0.2552422881126404
Norm after input: 0.9369170069694519
Norm after each mp layer: 3.221400499343872
Norm after each mp layer: 9.060189247131348
Norm after each mp layer: 25.902790069580078
Norm before input: 0.2552422881126404
Norm after input: 0.9369170069694519
Norm after each mp layer: 3.221400499343872
Norm after each mp layer: 9.060189247131348
Norm after each mp layer: 25.902793884277344
Norm before input: 0.2552422881126404
Norm after input: 0.9025794863700867
Norm after each mp layer: 3.0801408290863037
Norm after each mp layer: 9.52523136138916
Norm after each mp layer: 29.360519409179688
Norm before input: 0.2552422881126404
Norm after input: 0.9025794863700867
Norm after each mp layer: 3.0801408290863037
Norm after each mp layer: 9.52523136138916
Norm after each mp layer: 29.360519409179688
Norm before input: 0.2552422881126404
Norm after input: 0.863341212272644
Norm after each mp layer: 2.859043598175049
Norm after each mp layer: 9.127490043640137
Norm after each mp layer: 30.788848876953125
Epoch: 10, Loss: 2.2558, Energy: 118855.8828, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.863341212272644
Norm after each mp layer: 2.859043598175049
Norm after each mp layer: 9.127490043640137
Norm after each mp layer: 30.788848876953125
Norm before input: 0.2552422881126404
Norm after input: 0.8248199820518494
Norm after each mp layer: 2.6228113174438477
Norm after each mp layer: 8.52510929107666
Norm after each mp layer: 30.72942352294922
Norm before input: 0.2552422881126404
Norm after input: 0.8248199820518494
Norm after each mp layer: 2.6228113174438477
Norm after each mp layer: 8.52510929107666
Norm after each mp layer: 30.72942352294922
Norm before input: 0.2552422881126404
Norm after input: 0.7924327850341797
Norm after each mp layer: 2.4531378746032715
Norm after each mp layer: 8.27649211883545
Norm after each mp layer: 29.76219940185547
Norm before input: 0.2552422881126404
Norm after input: 0.7924327850341797
Norm after each mp layer: 2.4531378746032715
Norm after each mp layer: 8.27649211883545
Norm after each mp layer: 29.762203216552734
Norm before input: 0.2552422881126404
Norm after input: 0.7522966265678406
Norm after each mp layer: 2.2112174034118652
Norm after each mp layer: 7.367466449737549
Norm after each mp layer: 27.803186416625977
Norm before input: 0.2552422881126404
Norm after input: 0.7522966265678406
Norm after each mp layer: 2.2112174034118652
Norm after each mp layer: 7.367466449737549
Norm after each mp layer: 27.803186416625977
Norm before input: 0.2552422881126404
Norm after input: 0.7173199653625488
Norm after each mp layer: 2.0034496784210205
Norm after each mp layer: 6.612895965576172
Norm after each mp layer: 25.432750701904297
Norm before input: 0.2552422881126404
Norm after input: 0.7173199653625488
Norm after each mp layer: 2.0034496784210205
Norm after each mp layer: 6.612895965576172
Norm after each mp layer: 25.432750701904297
Norm before input: 0.2552422881126404
Norm after input: 0.6861715316772461
Norm after each mp layer: 1.8178950548171997
Norm after each mp layer: 5.936157703399658
Norm after each mp layer: 23.214393615722656
Epoch: 15, Loss: 1.8496, Energy: 87486.5781, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 36.00%, Best Test: 35.80%
Norm before input: 0.2552422881126404
Norm after input: 0.6861715316772461
Norm after each mp layer: 1.8178950548171997
Norm after each mp layer: 5.936157703399658
Norm after each mp layer: 23.214393615722656
Norm before input: 0.2552422881126404
Norm after input: 0.6560859680175781
Norm after each mp layer: 1.6282798051834106
Norm after each mp layer: 5.214102268218994
Norm after each mp layer: 20.819129943847656
Norm before input: 0.2552422881126404
Norm after input: 0.6560859680175781
Norm after each mp layer: 1.6282798051834106
Norm after each mp layer: 5.214102268218994
Norm after each mp layer: 20.819129943847656
Norm before input: 0.2552422881126404
Norm after input: 0.629737913608551
Norm after each mp layer: 1.4586082696914673
Norm after each mp layer: 4.578031063079834
Norm after each mp layer: 18.645780563354492
Norm before input: 0.2552422881126404
Norm after input: 0.629737913608551
Norm after each mp layer: 1.4586082696914673
Norm after each mp layer: 4.578031063079834
Norm after each mp layer: 18.645780563354492
Norm before input: 0.2552422881126404
Norm after input: 0.6106630563735962
Norm after each mp layer: 1.3501356840133667
Norm after each mp layer: 4.206523418426514
Norm after each mp layer: 17.366350173950195
Norm before input: 0.2552422881126404
Norm after input: 0.6106630563735962
Norm after each mp layer: 1.3501356840133667
Norm after each mp layer: 4.206523418426514
Norm after each mp layer: 17.366350173950195
Norm before input: 0.2552422881126404
Norm after input: 0.5978508591651917
Norm after each mp layer: 1.298109531402588
Norm after each mp layer: 4.087780952453613
Norm after each mp layer: 17.132633209228516
Norm before input: 0.2552422881126404
Norm after input: 0.5978508591651917
Norm after each mp layer: 1.298109531402588
Norm after each mp layer: 4.087780952453613
Norm after each mp layer: 17.132633209228516
Norm before input: 0.2552422881126404
Norm after input: 0.5878015160560608
Norm after each mp layer: 1.2693227529525757
Norm after each mp layer: 4.087679386138916
Norm after each mp layer: 17.518264770507812
Epoch: 20, Loss: 1.5788, Energy: 39912.9023, Train: 31.71%, Valid: 32.20%, Test: 30.30%, Best Valid: 38.20%, Best Test: 38.50%
Norm before input: 0.2552422881126404
Norm after input: 0.5878015160560608
Norm after each mp layer: 1.2693227529525757
Norm after each mp layer: 4.087679386138916
Norm after each mp layer: 17.518264770507812
Norm before input: 0.2552422881126404
Norm after input: 0.580237627029419
Norm after each mp layer: 1.2638593912124634
Norm after each mp layer: 4.1911211013793945
Norm after each mp layer: 18.384817123413086
Norm before input: 0.2552422881126404
Norm after input: 0.580237627029419
Norm after each mp layer: 1.2638593912124634
Norm after each mp layer: 4.1911211013793945
Norm after each mp layer: 18.384817123413086
Norm before input: 0.2552422881126404
Norm after input: 0.5771853923797607
Norm after each mp layer: 1.3041820526123047
Norm after each mp layer: 4.490482330322266
Norm after each mp layer: 20.130502700805664
Norm before input: 0.2552422881126404
Norm after input: 0.5771853923797607
Norm after each mp layer: 1.3041820526123047
Norm after each mp layer: 4.490482330322266
Norm after each mp layer: 20.130502700805664
Norm before input: 0.2552422881126404
Norm after input: 0.5783628225326538
Norm after each mp layer: 1.3806517124176025
Norm after each mp layer: 4.964083671569824
Norm after each mp layer: 22.767921447753906
Norm before input: 0.2552422881126404
Norm after input: 0.5783628225326538
Norm after each mp layer: 1.3806517124176025
Norm after each mp layer: 4.964083671569824
Norm after each mp layer: 22.767921447753906
Norm before input: 0.2552422881126404
Norm after input: 0.5804879665374756
Norm after each mp layer: 1.459894061088562
Norm after each mp layer: 5.469810962677002
Norm after each mp layer: 25.65651512145996
Norm before input: 0.2552422881126404
Norm after input: 0.5804879665374756
Norm after each mp layer: 1.459894061088562
Norm after each mp layer: 5.469810962677002
Norm after each mp layer: 25.65651512145996
Norm before input: 0.2552422881126404
Norm after input: 0.5788801312446594
Norm after each mp layer: 1.502146601676941
Norm after each mp layer: 5.774196624755859
Norm after each mp layer: 27.533546447753906
Epoch: 25, Loss: 1.3053, Energy: 101953.8516, Train: 48.68%, Valid: 48.00%, Test: 49.70%, Best Valid: 50.00%, Best Test: 49.70%
Norm before input: 0.2552422881126404
Norm after input: 0.5788801312446594
Norm after each mp layer: 1.502146601676941
Norm after each mp layer: 5.774196624755859
Norm after each mp layer: 27.533546447753906
Norm before input: 0.2552422881126404
Norm after input: 0.574123740196228
Norm after each mp layer: 1.5188478231430054
Norm after each mp layer: 5.919290542602539
Norm after each mp layer: 28.509361267089844
Norm before input: 0.2552422881126404
Norm after input: 0.574123740196228
Norm after each mp layer: 1.5188478231430054
Norm after each mp layer: 5.919290542602539
Norm after each mp layer: 28.509361267089844
Norm before input: 0.2552422881126404
Norm after input: 0.5703345537185669
Norm after each mp layer: 1.5428400039672852
Norm after each mp layer: 6.104292869567871
Norm after each mp layer: 29.69571304321289
Norm before input: 0.2552422881126404
Norm after input: 0.5703345537185669
Norm after each mp layer: 1.5428400039672852
Norm after each mp layer: 6.104292869567871
Norm after each mp layer: 29.69571304321289
Norm before input: 0.2552422881126404
Norm after input: 0.5687172412872314
Norm after each mp layer: 1.5782192945480347
Norm after each mp layer: 6.356175422668457
Norm after each mp layer: 31.293970108032227
Norm before input: 0.2552422881126404
Norm after input: 0.5687172412872314
Norm after each mp layer: 1.5782192945480347
Norm after each mp layer: 6.356175422668457
Norm after each mp layer: 31.293970108032227
Norm before input: 0.2552422881126404
Norm after input: 0.5678178668022156
Norm after each mp layer: 1.6142956018447876
Norm after each mp layer: 6.598876476287842
Norm after each mp layer: 32.835906982421875
Norm before input: 0.2552422881126404
Norm after input: 0.5678178668022156
Norm after each mp layer: 1.6142956018447876
Norm after each mp layer: 6.598876476287842
Norm after each mp layer: 32.835906982421875
Norm before input: 0.2552422881126404
Norm after input: 0.566813588142395
Norm after each mp layer: 1.646793007850647
Norm after each mp layer: 6.7971978187561035
Norm after each mp layer: 34.090576171875
Epoch: 30, Loss: 1.1318, Energy: 174684.9062, Train: 58.44%, Valid: 55.40%, Test: 55.70%, Best Valid: 57.20%, Best Test: 55.70%
Norm before input: 0.2552422881126404
Norm after input: 0.566813588142395
Norm after each mp layer: 1.646793007850647
Norm after each mp layer: 6.7971978187561035
Norm after each mp layer: 34.090576171875
Norm before input: 0.2552422881126404
Norm after input: 0.5663911700248718
Norm after each mp layer: 1.6797736883163452
Norm after each mp layer: 7.0105791091918945
Norm after each mp layer: 35.49156188964844
Norm before input: 0.2552422881126404
Norm after input: 0.5663911700248718
Norm after each mp layer: 1.6797736883163452
Norm after each mp layer: 7.0105791091918945
Norm after each mp layer: 35.49156188964844
Norm before input: 0.2552422881126404
Norm after input: 0.5650365352630615
Norm after each mp layer: 1.7038804292678833
Norm after each mp layer: 7.2032790184021
Norm after each mp layer: 36.850948333740234
Norm before input: 0.2552422881126404
Norm after input: 0.5650365352630615
Norm after each mp layer: 1.7038804292678833
Norm after each mp layer: 7.2032790184021
Norm after each mp layer: 36.850948333740234
Norm before input: 0.2552422881126404
Norm after input: 0.5604564547538757
Norm after each mp layer: 1.7083561420440674
Norm after each mp layer: 7.277623653411865
Norm after each mp layer: 37.45075225830078
Norm before input: 0.2552422881126404
Norm after input: 0.5604564547538757
Norm after each mp layer: 1.7083561420440674
Norm after each mp layer: 7.277623653411865
Norm after each mp layer: 37.45075225830078
Norm before input: 0.2552422881126404
Norm after input: 0.554121196269989
Norm after each mp layer: 1.7024428844451904
Norm after each mp layer: 7.276309490203857
Norm after each mp layer: 37.524234771728516
Norm before input: 0.2552422881126404
Norm after input: 0.554121196269989
Norm after each mp layer: 1.7024428844451904
Norm after each mp layer: 7.276309490203857
Norm after each mp layer: 37.524234771728516
Norm before input: 0.2552422881126404
Norm after input: 0.5498443245887756
Norm after each mp layer: 1.703291416168213
Norm after each mp layer: 7.34855318069458
Norm after each mp layer: 38.18193054199219
Epoch: 35, Loss: 0.9858, Energy: 234865.7656, Train: 65.23%, Valid: 61.40%, Test: 61.10%, Best Valid: 62.00%, Best Test: 61.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5498443245887756
Norm after each mp layer: 1.703291416168213
Norm after each mp layer: 7.34855318069458
Norm after each mp layer: 38.18193054199219
Norm before input: 0.2552422881126404
Norm after input: 0.5466890931129456
Norm after each mp layer: 1.7088381052017212
Norm after each mp layer: 7.437918663024902
Norm after each mp layer: 38.91909408569336
Norm before input: 0.2552422881126404
Norm after input: 0.5466890931129456
Norm after each mp layer: 1.7088381052017212
Norm after each mp layer: 7.437918186187744
Norm after each mp layer: 38.91909408569336
Norm before input: 0.2552422881126404
Norm after input: 0.5429931879043579
Norm after each mp layer: 1.7132647037506104
Norm after each mp layer: 7.454516887664795
Norm after each mp layer: 38.97316360473633
Norm before input: 0.2552422881126404
Norm after input: 0.5429931879043579
Norm after each mp layer: 1.7132647037506104
Norm after each mp layer: 7.454516887664795
Norm after each mp layer: 38.97316360473633
Norm before input: 0.2552422881126404
Norm after input: 0.5411454439163208
Norm after each mp layer: 1.7258330583572388
Norm after each mp layer: 7.522736072540283
Norm after each mp layer: 39.37522888183594
Norm before input: 0.2552422881126404
Norm after input: 0.5411454439163208
Norm after each mp layer: 1.7258330583572388
Norm after each mp layer: 7.522736072540283
Norm after each mp layer: 39.37522888183594
Norm before input: 0.2552422881126404
Norm after input: 0.5407148599624634
Norm after each mp layer: 1.7413767576217651
Norm after each mp layer: 7.628433704376221
Norm after each mp layer: 40.12716293334961
Norm before input: 0.2552422881126404
Norm after input: 0.5407148599624634
Norm after each mp layer: 1.7413767576217651
Norm after each mp layer: 7.628433704376221
Norm after each mp layer: 40.12716293334961
Norm before input: 0.2552422881126404
Norm after input: 0.5384155511856079
Norm after each mp layer: 1.749133586883545
Norm after each mp layer: 7.667769432067871
Norm after each mp layer: 40.327186584472656
Epoch: 40, Loss: 0.8868, Energy: 260555.6406, Train: 69.95%, Valid: 61.80%, Test: 63.80%, Best Valid: 62.40%, Best Test: 63.80%
Norm before input: 0.2552422881126404
Norm after input: 0.5384155511856079
Norm after each mp layer: 1.749133586883545
Norm after each mp layer: 7.667769432067871
Norm after each mp layer: 40.327186584472656
Norm before input: 0.2552422881126404
Norm after input: 0.5349892973899841
Norm after each mp layer: 1.7465547323226929
Norm after each mp layer: 7.674143314361572
Norm after each mp layer: 40.39089584350586
Norm before input: 0.2552422881126404
Norm after input: 0.5349892973899841
Norm after each mp layer: 1.7465547323226929
Norm after each mp layer: 7.674143314361572
Norm after each mp layer: 40.39089584350586
Norm before input: 0.2552422881126404
Norm after input: 0.5319266319274902
Norm after each mp layer: 1.7379165887832642
Norm after each mp layer: 7.677233695983887
Norm after each mp layer: 40.614253997802734
Norm before input: 0.2552422881126404
Norm after input: 0.5319266319274902
Norm after each mp layer: 1.7379165887832642
Norm after each mp layer: 7.677233695983887
Norm after each mp layer: 40.614253997802734
Norm before input: 0.2552422881126404
Norm after input: 0.5283728837966919
Norm after each mp layer: 1.727303147315979
Norm after each mp layer: 7.6501784324646
Norm after each mp layer: 40.57316589355469
Norm before input: 0.2552422881126404
Norm after input: 0.5283728837966919
Norm after each mp layer: 1.727303147315979
Norm after each mp layer: 7.6501784324646
Norm after each mp layer: 40.57316589355469
Norm before input: 0.2552422881126404
Norm after input: 0.5250343084335327
Norm after each mp layer: 1.7150439023971558
Norm after each mp layer: 7.6144843101501465
Norm after each mp layer: 40.51720428466797
Norm before input: 0.2552422881126404
Norm after input: 0.5250343084335327
Norm after each mp layer: 1.7150439023971558
Norm after each mp layer: 7.6144843101501465
Norm after each mp layer: 40.51720428466797
Norm before input: 0.2552422881126404
Norm after input: 0.5227559804916382
Norm after each mp layer: 1.7039647102355957
Norm after each mp layer: 7.596282958984375
Norm after each mp layer: 40.697078704833984
Epoch: 45, Loss: 0.7745, Energy: 216178.8750, Train: 74.25%, Valid: 68.40%, Test: 67.00%, Best Valid: 68.40%, Best Test: 67.00%
Norm before input: 0.2552422881126404
Norm after input: 0.5227559804916382
Norm after each mp layer: 1.7039647102355957
Norm after each mp layer: 7.596282958984375
Norm after each mp layer: 40.697078704833984
Norm before input: 0.2552422881126404
Norm after input: 0.5198342800140381
Norm after each mp layer: 1.6903672218322754
Norm after each mp layer: 7.5461649894714355
Norm after each mp layer: 40.55687713623047
Norm before input: 0.2552422881126404
Norm after input: 0.5198342800140381
Norm after each mp layer: 1.6903672218322754
Norm after each mp layer: 7.5461649894714355
Norm after each mp layer: 40.55687713623047
Norm before input: 0.2552422881126404
Norm after input: 0.5175424218177795
Norm after each mp layer: 1.6818681955337524
Norm after each mp layer: 7.511007308959961
Norm after each mp layer: 40.448299407958984
Norm before input: 0.2552422881126404
Norm after input: 0.5175424218177795
Norm after each mp layer: 1.6818681955337524
Norm after each mp layer: 7.511007308959961
Norm after each mp layer: 40.448299407958984
Norm before input: 0.2552422881126404
Norm after input: 0.5169763565063477
Norm after each mp layer: 1.682268738746643
Norm after each mp layer: 7.5329060554504395
Norm after each mp layer: 40.80063247680664
Norm before input: 0.2552422881126404
Norm after input: 0.5169763565063477
Norm after each mp layer: 1.682268738746643
Norm after each mp layer: 7.5329060554504395
Norm after each mp layer: 40.80063247680664
Norm before input: 0.2552422881126404
Norm after input: 0.5156412720680237
Norm after each mp layer: 1.6768991947174072
Norm after each mp layer: 7.52766752243042
Norm after each mp layer: 40.91537094116211
Norm before input: 0.2552422881126404
Norm after input: 0.5156412720680237
Norm after each mp layer: 1.6768991947174072
Norm after each mp layer: 7.52766752243042
Norm after each mp layer: 40.91537094116211
Norm before input: 0.2552422881126404
Norm after input: 0.513603687286377
Norm after each mp layer: 1.6626571416854858
Norm after each mp layer: 7.490359783172607
Norm after each mp layer: 40.81785202026367
Epoch: 50, Loss: 0.6615, Energy: 191033.1719, Train: 80.46%, Valid: 72.80%, Test: 69.30%, Best Valid: 72.80%, Best Test: 69.30%
Norm before input: 0.2552422881126404
Norm after input: 0.513603687286377
Norm after each mp layer: 1.6626571416854858
Norm after each mp layer: 7.490359783172607
Norm after each mp layer: 40.81785202026367
Norm before input: 0.2552422881126404
Norm after input: 0.5129461288452148
Norm after each mp layer: 1.6537108421325684
Norm after each mp layer: 7.5080389976501465
Norm after each mp layer: 41.21345138549805
Norm before input: 0.2552422881126404
Norm after input: 0.5129461288452148
Norm after each mp layer: 1.6537108421325684
Norm after each mp layer: 7.5080389976501465
Norm after each mp layer: 41.21345138549805
Norm before input: 0.2552422881126404
Norm after input: 0.5129525065422058
Norm after each mp layer: 1.6567609310150146
Norm after each mp layer: 7.579640865325928
Norm after each mp layer: 41.83615493774414
Norm before input: 0.2552422881126404
Norm after input: 0.5129525065422058
Norm after each mp layer: 1.6567609310150146
Norm after each mp layer: 7.579640865325928
Norm after each mp layer: 41.83615493774414
Norm before input: 0.2552422881126404
Norm after input: 0.5137315392494202
Norm after each mp layer: 1.6758249998092651
Norm after each mp layer: 7.718025207519531
Norm after each mp layer: 42.74882507324219
Norm before input: 0.2552422881126404
Norm after input: 0.5137315392494202
Norm after each mp layer: 1.6758249998092651
Norm after each mp layer: 7.718025207519531
Norm after each mp layer: 42.74882507324219
Norm before input: 0.2552422881126404
Norm after input: 0.515737771987915
Norm after each mp layer: 1.7071001529693604
Norm after each mp layer: 7.934551239013672
Norm after each mp layer: 44.22938919067383
Norm before input: 0.2552422881126404
Norm after input: 0.515737771987915
Norm after each mp layer: 1.7071001529693604
Norm after each mp layer: 7.934551239013672
Norm after each mp layer: 44.22938919067383
Norm before input: 0.2552422881126404
Norm after input: 0.5162334442138672
Norm after each mp layer: 1.7220133543014526
Norm after each mp layer: 8.0599946975708
Norm after each mp layer: 45.08113479614258
Epoch: 55, Loss: 0.5717, Energy: 211943.1875, Train: 81.87%, Valid: 72.20%, Test: 69.10%, Best Valid: 73.00%, Best Test: 69.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5162334442138672
Norm after each mp layer: 1.7220133543014526
Norm after each mp layer: 8.0599946975708
Norm after each mp layer: 45.08113479614258
Norm before input: 0.2552422881126404
Norm after input: 0.5167035460472107
Norm after each mp layer: 1.7311084270477295
Norm after each mp layer: 8.174823760986328
Norm after each mp layer: 45.96860885620117
Norm before input: 0.2552422881126404
Norm after input: 0.5167035460472107
Norm after each mp layer: 1.7311084270477295
Norm after each mp layer: 8.174823760986328
Norm after each mp layer: 45.96860885620117
Norm before input: 0.2552422881126404
Norm after input: 0.5174556970596313
Norm after each mp layer: 1.7453627586364746
Norm after each mp layer: 8.31340217590332
Norm after each mp layer: 46.965980529785156
Norm before input: 0.2552422881126404
Norm after input: 0.5174556970596313
Norm after each mp layer: 1.7453627586364746
Norm after each mp layer: 8.31340217590332
Norm after each mp layer: 46.965980529785156
Norm before input: 0.2552422881126404
Norm after input: 0.5184546709060669
Norm after each mp layer: 1.7702100276947021
Norm after each mp layer: 8.485424995422363
Norm after each mp layer: 48.02256393432617
Norm before input: 0.2552422881126404
Norm after input: 0.5184546709060669
Norm after each mp layer: 1.7702100276947021
Norm after each mp layer: 8.485424995422363
Norm after each mp layer: 48.02256393432617
Norm before input: 0.2552422881126404
Norm after input: 0.52083420753479
Norm after each mp layer: 1.8095115423202515
Norm after each mp layer: 8.754996299743652
Norm after each mp layer: 49.775997161865234
Norm before input: 0.2552422881126404
Norm after input: 0.52083420753479
Norm after each mp layer: 1.8095115423202515
Norm after each mp layer: 8.754996299743652
Norm after each mp layer: 49.775997161865234
Norm before input: 0.2552422881126404
Norm after input: 0.5214841365814209
Norm after each mp layer: 1.8290108442306519
Norm after each mp layer: 8.898370742797852
Norm after each mp layer: 50.61899948120117
Epoch: 60, Loss: 0.4947, Energy: 275915.5938, Train: 83.77%, Valid: 73.20%, Test: 70.10%, Best Valid: 73.60%, Best Test: 71.30%
Norm before input: 0.2552422881126404
Norm after input: 0.5214841365814209
Norm after each mp layer: 1.8290108442306519
Norm after each mp layer: 8.898370742797852
Norm after each mp layer: 50.61899948120117
Norm before input: 0.2552422881126404
Norm after input: 0.5229380130767822
Norm after each mp layer: 1.847262978553772
Norm after each mp layer: 9.075468063354492
Norm after each mp layer: 51.91424560546875
Norm before input: 0.2552422881126404
Norm after input: 0.5229380130767822
Norm after each mp layer: 1.847262978553772
Norm after each mp layer: 9.075468063354492
Norm after each mp layer: 51.91424560546875
Norm before input: 0.2552422881126404
Norm after input: 0.5234102010726929
Norm after each mp layer: 1.8597285747528076
Norm after each mp layer: 9.183858871459961
Norm after each mp layer: 52.598087310791016
Norm before input: 0.2552422881126404
Norm after input: 0.5234102010726929
Norm after each mp layer: 1.8597285747528076
Norm after each mp layer: 9.183858871459961
Norm after each mp layer: 52.598087310791016
Norm before input: 0.2552422881126404
Norm after input: 0.5254936218261719
Norm after each mp layer: 1.8938720226287842
Norm after each mp layer: 9.420088768005371
Norm after each mp layer: 54.13963317871094
Norm before input: 0.2552422881126404
Norm after input: 0.5254936218261719
Norm after each mp layer: 1.8938720226287842
Norm after each mp layer: 9.420088768005371
Norm after each mp layer: 54.13963317871094
Norm before input: 0.2552422881126404
Norm after input: 0.5259109735488892
Norm after each mp layer: 1.9128854274749756
Norm after each mp layer: 9.536187171936035
Norm after each mp layer: 54.73508834838867
Norm before input: 0.2552422881126404
Norm after input: 0.5259109735488892
Norm after each mp layer: 1.9128854274749756
Norm after each mp layer: 9.536187171936035
Norm after each mp layer: 54.73508834838867
Norm before input: 0.2552422881126404
Norm after input: 0.5278196334838867
Norm after each mp layer: 1.9346575736999512
Norm after each mp layer: 9.733011245727539
Norm after each mp layer: 56.22044372558594
Epoch: 65, Loss: 0.4383, Energy: 318962.6562, Train: 85.51%, Valid: 75.20%, Test: 72.60%, Best Valid: 75.20%, Best Test: 72.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5278196334838867
Norm after each mp layer: 1.9346575736999512
Norm after each mp layer: 9.733011245727539
Norm after each mp layer: 56.22044372558594
Norm before input: 0.2552422881126404
Norm after input: 0.5260034799575806
Norm after each mp layer: 1.9254270792007446
Norm after each mp layer: 9.67355728149414
Norm after each mp layer: 55.6304931640625
Norm before input: 0.2552422881126404
Norm after input: 0.5260034799575806
Norm after each mp layer: 1.9254270792007446
Norm after each mp layer: 9.67355728149414
Norm after each mp layer: 55.6304931640625
Norm before input: 0.2552422881126404
Norm after input: 0.5305585265159607
Norm after each mp layer: 1.9784749746322632
Norm after each mp layer: 10.071091651916504
Norm after each mp layer: 58.4867057800293
Norm before input: 0.2552422881126404
Norm after input: 0.5305585265159607
Norm after each mp layer: 1.9784749746322632
Norm after each mp layer: 10.071091651916504
Norm after each mp layer: 58.4867057800293
Norm before input: 0.2552422881126404
Norm after input: 0.5272300243377686
Norm after each mp layer: 1.9746475219726562
Norm after each mp layer: 9.995738983154297
Norm after each mp layer: 57.48470687866211
Norm before input: 0.2552422881126404
Norm after input: 0.5272300243377686
Norm after each mp layer: 1.9746475219726562
Norm after each mp layer: 9.995738983154297
Norm after each mp layer: 57.48470687866211
Norm before input: 0.2552422881126404
Norm after input: 0.5298595428466797
Norm after each mp layer: 2.0001587867736816
Norm after each mp layer: 10.211654663085938
Norm after each mp layer: 59.16702651977539
Norm before input: 0.2552422881126404
Norm after input: 0.5298595428466797
Norm after each mp layer: 2.0001587867736816
Norm after each mp layer: 10.211654663085938
Norm after each mp layer: 59.16702651977539
Norm before input: 0.2552422881126404
Norm after input: 0.5302138328552246
Norm after each mp layer: 2.004587173461914
Norm after each mp layer: 10.28433609008789
Norm after each mp layer: 59.78839874267578
Epoch: 70, Loss: 0.3883, Energy: 385239.8438, Train: 87.42%, Valid: 76.80%, Test: 73.30%, Best Valid: 76.80%, Best Test: 73.30%
Norm before input: 0.2552422881126404
Norm after input: 0.5302138328552246
Norm after each mp layer: 2.004587173461914
Norm after each mp layer: 10.28433609008789
Norm after each mp layer: 59.78839874267578
Norm before input: 0.2552422881126404
Norm after input: 0.5270900130271912
Norm after each mp layer: 1.9970961809158325
Norm after each mp layer: 10.200088500976562
Norm after each mp layer: 58.866920471191406
Norm before input: 0.2552422881126404
Norm after input: 0.5270900130271912
Norm after each mp layer: 1.9970961809158325
Norm after each mp layer: 10.200088500976562
Norm after each mp layer: 58.866920471191406
Norm before input: 0.2552422881126404
Norm after input: 0.531470537185669
Norm after each mp layer: 2.0527877807617188
Norm after each mp layer: 10.567537307739258
Norm after each mp layer: 61.354469299316406
Norm before input: 0.2552422881126404
Norm after input: 0.531470537185669
Norm after each mp layer: 2.0527877807617188
Norm after each mp layer: 10.567537307739258
Norm after each mp layer: 61.354469299316406
Norm before input: 0.2552422881126404
Norm after input: 0.5309492945671082
Norm after each mp layer: 2.0538153648376465
Norm after each mp layer: 10.58719253540039
Norm after each mp layer: 61.5217170715332
Norm before input: 0.2552422881126404
Norm after input: 0.5309492945671082
Norm after each mp layer: 2.0538153648376465
Norm after each mp layer: 10.58719253540039
Norm after each mp layer: 61.5217170715332
Norm before input: 0.2552422881126404
Norm after input: 0.5265063047409058
Norm after each mp layer: 2.019273519515991
Norm after each mp layer: 10.354783058166504
Norm after each mp layer: 59.859493255615234
Norm before input: 0.2552422881126404
Norm after input: 0.5265063047409058
Norm after each mp layer: 2.019273519515991
Norm after each mp layer: 10.354783058166504
Norm after each mp layer: 59.859493255615234
Norm before input: 0.2552422881126404
Norm after input: 0.5295143127441406
Norm after each mp layer: 2.058412551879883
Norm after each mp layer: 10.616388320922852
Norm after each mp layer: 61.640045166015625
Epoch: 75, Loss: 0.3776, Energy: 338369.3750, Train: 89.90%, Valid: 77.40%, Test: 74.20%, Best Valid: 77.40%, Best Test: 74.20%
Norm before input: 0.2552422881126404
Norm after input: 0.5295143127441406
Norm after each mp layer: 2.058412551879883
Norm after each mp layer: 10.616388320922852
Norm after each mp layer: 61.640045166015625
Norm before input: 0.2552422881126404
Norm after input: 0.530707836151123
Norm after each mp layer: 2.084660768508911
Norm after each mp layer: 10.778672218322754
Norm after each mp layer: 62.61075210571289
Norm before input: 0.2552422881126404
Norm after input: 0.530707836151123
Norm after each mp layer: 2.084660768508911
Norm after each mp layer: 10.778672218322754
Norm after each mp layer: 62.61075210571289
Norm before input: 0.2552422881126404
Norm after input: 0.5273431539535522
Norm after each mp layer: 2.0683677196502686
Norm after each mp layer: 10.646265983581543
Norm after each mp layer: 61.49502944946289
Norm before input: 0.2552422881126404
Norm after input: 0.5273431539535522
Norm after each mp layer: 2.0683677196502686
Norm after each mp layer: 10.646265983581543
Norm after each mp layer: 61.49502944946289
Norm before input: 0.2552422881126404
Norm after input: 0.5267603397369385
Norm after each mp layer: 2.0620172023773193
Norm after each mp layer: 10.622893333435059
Norm after each mp layer: 61.47465896606445
Norm before input: 0.2552422881126404
Norm after input: 0.5267603397369385
Norm after each mp layer: 2.0620172023773193
Norm after each mp layer: 10.622893333435059
Norm after each mp layer: 61.47465896606445
Norm before input: 0.2552422881126404
Norm after input: 0.5287087559700012
Norm after each mp layer: 2.0874574184417725
Norm after each mp layer: 10.798554420471191
Norm after each mp layer: 62.678382873535156
Norm before input: 0.2552422881126404
Norm after input: 0.5287087559700012
Norm after each mp layer: 2.0874574184417725
Norm after each mp layer: 10.798554420471191
Norm after each mp layer: 62.678382873535156
Norm before input: 0.2552422881126404
Norm after input: 0.5271493792533875
Norm after each mp layer: 2.096529245376587
Norm after each mp layer: 10.806646347045898
Norm after each mp layer: 62.35007095336914
Epoch: 80, Loss: 0.3256, Energy: 379867.2500, Train: 91.23%, Valid: 77.00%, Test: 74.20%, Best Valid: 78.20%, Best Test: 74.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5271493792533875
Norm after each mp layer: 2.096529245376587
Norm after each mp layer: 10.806646347045898
Norm after each mp layer: 62.35007095336914
Norm before input: 0.2552422881126404
Norm after input: 0.5258508324623108
Norm after each mp layer: 2.0897743701934814
Norm after each mp layer: 10.75765609741211
Norm after each mp layer: 62.004234313964844
Norm before input: 0.2552422881126404
Norm after input: 0.5258508324623108
Norm after each mp layer: 2.0897743701934814
Norm after each mp layer: 10.75765609741211
Norm after each mp layer: 62.004234313964844
Norm before input: 0.2552422881126404
Norm after input: 0.5265545845031738
Norm after each mp layer: 2.09183931350708
Norm after each mp layer: 10.797444343566895
Norm after each mp layer: 62.46575927734375
Norm before input: 0.2552422881126404
Norm after input: 0.5265545845031738
Norm after each mp layer: 2.09183931350708
Norm after each mp layer: 10.797444343566895
Norm after each mp layer: 62.46575927734375
Norm before input: 0.2552422881126404
Norm after input: 0.5257499814033508
Norm after each mp layer: 2.100113868713379
Norm after each mp layer: 10.828645706176758
Norm after each mp layer: 62.452579498291016
Norm before input: 0.2552422881126404
Norm after input: 0.5257499814033508
Norm after each mp layer: 2.100113868713379
Norm after each mp layer: 10.828645706176758
Norm after each mp layer: 62.452579498291016
Norm before input: 0.2552422881126404
Norm after input: 0.5248966813087463
Norm after each mp layer: 2.111732244491577
Norm after each mp layer: 10.88183307647705
Norm after each mp layer: 62.5061149597168
Norm before input: 0.2552422881126404
Norm after input: 0.5248966813087463
Norm after each mp layer: 2.111732244491577
Norm after each mp layer: 10.88183307647705
Norm after each mp layer: 62.5061149597168
Norm before input: 0.2552422881126404
Norm after input: 0.525399923324585
Norm after each mp layer: 2.1178369522094727
Norm after each mp layer: 10.94328498840332
Norm after each mp layer: 62.975521087646484
Epoch: 85, Loss: 0.2809, Energy: 309715.7500, Train: 93.46%, Valid: 78.20%, Test: 76.60%, Best Valid: 78.40%, Best Test: 76.60%
Norm before input: 0.2552422881126404
Norm after input: 0.525399923324585
Norm after each mp layer: 2.1178369522094727
Norm after each mp layer: 10.943284034729004
Norm after each mp layer: 62.975521087646484
Norm before input: 0.2552422881126404
Norm after input: 0.5243403911590576
Norm after each mp layer: 2.111225128173828
Norm after each mp layer: 10.915377616882324
Norm after each mp layer: 62.78074264526367
Norm before input: 0.2552422881126404
Norm after input: 0.5243403911590576
Norm after each mp layer: 2.111225128173828
Norm after each mp layer: 10.915377616882324
Norm after each mp layer: 62.78074264526367
Norm before input: 0.2552422881126404
Norm after input: 0.5229660868644714
Norm after each mp layer: 2.116056203842163
Norm after each mp layer: 10.932197570800781
Norm after each mp layer: 62.609649658203125
Norm before input: 0.2552422881126404
Norm after input: 0.5229660868644714
Norm after each mp layer: 2.116056203842163
Norm after each mp layer: 10.932197570800781
Norm after each mp layer: 62.609649658203125
Norm before input: 0.2552422881126404
Norm after input: 0.5241498351097107
Norm after each mp layer: 2.139082670211792
Norm after each mp layer: 11.091569900512695
Norm after each mp layer: 63.51945495605469
Norm before input: 0.2552422881126404
Norm after input: 0.5241498351097107
Norm after each mp layer: 2.139082670211792
Norm after each mp layer: 11.091569900512695
Norm after each mp layer: 63.51945495605469
Norm before input: 0.2552422881126404
Norm after input: 0.5236976146697998
Norm after each mp layer: 2.138065814971924
Norm after each mp layer: 11.106147766113281
Norm after each mp layer: 63.60612106323242
Norm before input: 0.2552422881126404
Norm after input: 0.5236976146697998
Norm after each mp layer: 2.138065814971924
Norm after each mp layer: 11.106147766113281
Norm after each mp layer: 63.60612106323242
Norm before input: 0.2552422881126404
Norm after input: 0.5215871930122375
Norm after each mp layer: 2.1262271404266357
Norm after each mp layer: 11.034318923950195
Norm after each mp layer: 62.997615814208984
Epoch: 90, Loss: 0.2353, Energy: 311804.0312, Train: 94.45%, Valid: 79.20%, Test: 76.70%, Best Valid: 79.20%, Best Test: 76.80%
Norm before input: 0.2552422881126404
Norm after input: 0.5215871930122375
Norm after each mp layer: 2.1262271404266357
Norm after each mp layer: 11.034318923950195
Norm after each mp layer: 62.997615814208984
Norm before input: 0.2552422881126404
Norm after input: 0.5225679278373718
Norm after each mp layer: 2.147153615951538
Norm after each mp layer: 11.189116477966309
Norm after each mp layer: 63.83850860595703
Norm before input: 0.2552422881126404
Norm after input: 0.5225679278373718
Norm after each mp layer: 2.147153615951538
Norm after each mp layer: 11.189116477966309
Norm after each mp layer: 63.83850860595703
Norm before input: 0.2552422881126404
Norm after input: 0.5224347114562988
Norm after each mp layer: 2.1556146144866943
Norm after each mp layer: 11.26649284362793
Norm after each mp layer: 64.17556762695312
Norm before input: 0.2552422881126404
Norm after input: 0.5224347114562988
Norm after each mp layer: 2.1556146144866943
Norm after each mp layer: 11.26649284362793
Norm after each mp layer: 64.17556762695312
Norm before input: 0.2552422881126404
Norm after input: 0.520261287689209
Norm after each mp layer: 2.1406455039978027
Norm after each mp layer: 11.19517993927002
Norm after each mp layer: 63.58185577392578
Norm before input: 0.2552422881126404
Norm after input: 0.520261287689209
Norm after each mp layer: 2.1406455039978027
Norm after each mp layer: 11.19517993927002
Norm after each mp layer: 63.58185577392578
Norm before input: 0.2552422881126404
Norm after input: 0.5197529196739197
Norm after each mp layer: 2.139263153076172
Norm after each mp layer: 11.232043266296387
Norm after each mp layer: 63.735572814941406
Norm before input: 0.2552422881126404
Norm after input: 0.5197529196739197
Norm after each mp layer: 2.139263153076172
Norm after each mp layer: 11.232043266296387
Norm after each mp layer: 63.735572814941406
Norm before input: 0.2552422881126404
Norm after input: 0.5196188688278198
Norm after each mp layer: 2.1482913494110107
Norm after each mp layer: 11.326197624206543
Norm after each mp layer: 64.11470794677734
Epoch: 95, Loss: 0.2019, Energy: 282120.1250, Train: 95.36%, Valid: 79.40%, Test: 76.90%, Best Valid: 79.40%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5196188688278198
Norm after each mp layer: 2.1482913494110107
Norm after each mp layer: 11.326197624206543
Norm after each mp layer: 64.11470794677734
Norm before input: 0.2552422881126404
Norm after input: 0.5182609558105469
Norm after each mp layer: 2.1441073417663574
Norm after each mp layer: 11.3338041305542
Norm after each mp layer: 63.93212127685547
Norm before input: 0.2552422881126404
Norm after input: 0.5182609558105469
Norm after each mp layer: 2.1441073417663574
Norm after each mp layer: 11.3338041305542
Norm after each mp layer: 63.932125091552734
Norm before input: 0.2552422881126404
Norm after input: 0.5171215534210205
Norm after each mp layer: 2.132570743560791
Norm after each mp layer: 11.315720558166504
Norm after each mp layer: 63.764617919921875
Norm before input: 0.2552422881126404
Norm after input: 0.5171215534210205
Norm after each mp layer: 2.132570743560791
Norm after each mp layer: 11.315720558166504
Norm after each mp layer: 63.764617919921875
Norm before input: 0.2552422881126404
Norm after input: 0.5163649916648865
Norm after each mp layer: 2.1285202503204346
Norm after each mp layer: 11.338493347167969
Norm after each mp layer: 63.77214431762695
Norm before input: 0.2552422881126404
Norm after input: 0.5163649916648865
Norm after each mp layer: 2.1285202503204346
Norm after each mp layer: 11.338493347167969
Norm after each mp layer: 63.77214431762695
Norm before input: 0.2552422881126404
Norm after input: 0.5155674815177917
Norm after each mp layer: 2.1308157444000244
Norm after each mp layer: 11.384413719177246
Norm after each mp layer: 63.76821517944336
Norm before input: 0.2552422881126404
Norm after input: 0.5155674815177917
Norm after each mp layer: 2.1308157444000244
Norm after each mp layer: 11.384413719177246
Norm after each mp layer: 63.76821517944336
Norm before input: 0.2552422881126404
Norm after input: 0.5150263905525208
Norm after each mp layer: 2.128371477127075
Norm after each mp layer: 11.412691116333008
Norm after each mp layer: 63.80290222167969
Epoch: 100, Loss: 0.1733, Energy: 256096.9844, Train: 96.27%, Valid: 78.80%, Test: 76.80%, Best Valid: 79.40%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5150263905525208
Norm after each mp layer: 2.128371477127075
Norm after each mp layer: 11.412691116333008
Norm after each mp layer: 63.80290222167969
Norm before input: 0.2552422881126404
Norm after input: 0.5141918063163757
Norm after each mp layer: 2.1188600063323975
Norm after each mp layer: 11.39963150024414
Norm after each mp layer: 63.67198944091797
Norm before input: 0.2552422881126404
Norm after input: 0.5141918063163757
Norm after each mp layer: 2.1188600063323975
Norm after each mp layer: 11.399630546569824
Norm after each mp layer: 63.67198944091797
Norm before input: 0.2552422881126404
Norm after input: 0.5133119821548462
Norm after each mp layer: 2.1161599159240723
Norm after each mp layer: 11.411149978637695
Norm after each mp layer: 63.53398895263672
Norm before input: 0.2552422881126404
Norm after input: 0.5133119821548462
Norm after each mp layer: 2.1161599159240723
Norm after each mp layer: 11.411149978637695
Norm after each mp layer: 63.53398895263672
Norm before input: 0.2552422881126404
Norm after input: 0.5134748816490173
Norm after each mp layer: 2.124340295791626
Norm after each mp layer: 11.491432189941406
Norm after each mp layer: 63.82412338256836
Norm before input: 0.2552422881126404
Norm after input: 0.5134748816490173
Norm after each mp layer: 2.124340295791626
Norm after each mp layer: 11.491432189941406
Norm after each mp layer: 63.82412338256836
Norm before input: 0.2552422881126404
Norm after input: 0.5132495164871216
Norm after each mp layer: 2.123408079147339
Norm after each mp layer: 11.519383430480957
Norm after each mp layer: 63.91749572753906
Norm before input: 0.2552422881126404
Norm after input: 0.5132495164871216
Norm after each mp layer: 2.123408079147339
Norm after each mp layer: 11.519383430480957
Norm after each mp layer: 63.91749572753906
Norm before input: 0.2552422881126404
Norm after input: 0.51231449842453
Norm after each mp layer: 2.1160027980804443
Norm after each mp layer: 11.500686645507812
Norm after each mp layer: 63.711490631103516
Epoch: 105, Loss: 0.1500, Energy: 238805.9531, Train: 97.02%, Valid: 78.60%, Test: 76.90%, Best Valid: 79.40%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.51231449842453
Norm after each mp layer: 2.1160027980804443
Norm after each mp layer: 11.500686645507812
Norm after each mp layer: 63.711490631103516
Norm before input: 0.2552422881126404
Norm after input: 0.5123997926712036
Norm after each mp layer: 2.1214942932128906
Norm after each mp layer: 11.55986499786377
Norm after each mp layer: 63.91849899291992
Norm before input: 0.2552422881126404
Norm after input: 0.5123997926712036
Norm after each mp layer: 2.1214942932128906
Norm after each mp layer: 11.55986499786377
Norm after each mp layer: 63.91849899291992
Norm before input: 0.2552422881126404
Norm after input: 0.512596845626831
Norm after each mp layer: 2.1281521320343018
Norm after each mp layer: 11.625243186950684
Norm after each mp layer: 64.15467071533203
Norm before input: 0.2552422881126404
Norm after input: 0.512596845626831
Norm after each mp layer: 2.128152370452881
Norm after each mp layer: 11.625243186950684
Norm after each mp layer: 64.15467071533203
Norm before input: 0.2552422881126404
Norm after input: 0.5119145512580872
Norm after each mp layer: 2.1231813430786133
Norm after each mp layer: 11.618364334106445
Norm after each mp layer: 64.02422332763672
Norm before input: 0.2552422881126404
Norm after input: 0.5119145512580872
Norm after each mp layer: 2.1231813430786133
Norm after each mp layer: 11.618364334106445
Norm after each mp layer: 64.02422332763672
Norm before input: 0.2552422881126404
Norm after input: 0.5116089582443237
Norm after each mp layer: 2.1206915378570557
Norm after each mp layer: 11.629276275634766
Norm after each mp layer: 64.03143310546875
Norm before input: 0.2552422881126404
Norm after input: 0.5116089582443237
Norm after each mp layer: 2.1206915378570557
Norm after each mp layer: 11.629276275634766
Norm after each mp layer: 64.03143310546875
Norm before input: 0.2552422881126404
Norm after input: 0.5119008421897888
Norm after each mp layer: 2.127476692199707
Norm after each mp layer: 11.694731712341309
Norm after each mp layer: 64.27693176269531
Epoch: 110, Loss: 0.1310, Energy: 212279.7656, Train: 97.43%, Valid: 78.80%, Test: 76.90%, Best Valid: 79.40%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5119008421897888
Norm after each mp layer: 2.127476692199707
Norm after each mp layer: 11.694731712341309
Norm after each mp layer: 64.27693176269531
Norm before input: 0.2552422881126404
Norm after input: 0.5117742419242859
Norm after each mp layer: 2.130647659301758
Norm after each mp layer: 11.735458374023438
Norm after each mp layer: 64.36192321777344
Norm before input: 0.2552422881126404
Norm after input: 0.5117742419242859
Norm after each mp layer: 2.130647659301758
Norm after each mp layer: 11.735458374023438
Norm after each mp layer: 64.36192321777344
Norm before input: 0.2552422881126404
Norm after input: 0.511462926864624
Norm after each mp layer: 2.1277997493743896
Norm after each mp layer: 11.743437767028809
Norm after each mp layer: 64.36410522460938
Norm before input: 0.2552422881126404
Norm after input: 0.511462926864624
Norm after each mp layer: 2.1277997493743896
Norm after each mp layer: 11.743437767028809
Norm after each mp layer: 64.36410522460938
Norm before input: 0.2552422881126404
Norm after input: 0.5115208029747009
Norm after each mp layer: 2.128803014755249
Norm after each mp layer: 11.776266098022461
Norm after each mp layer: 64.5118637084961
Norm before input: 0.2552422881126404
Norm after input: 0.5115208029747009
Norm after each mp layer: 2.128803014755249
Norm after each mp layer: 11.776266098022461
Norm after each mp layer: 64.5118637084961
Norm before input: 0.2552422881126404
Norm after input: 0.5116050839424133
Norm after each mp layer: 2.1347975730895996
Norm after each mp layer: 11.834325790405273
Norm after each mp layer: 64.69316101074219
Norm before input: 0.2552422881126404
Norm after input: 0.5116050839424133
Norm after each mp layer: 2.1347975730895996
Norm after each mp layer: 11.834325790405273
Norm after each mp layer: 64.69316101074219
Norm before input: 0.2552422881126404
Norm after input: 0.5116269588470459
Norm after each mp layer: 2.1383464336395264
Norm after each mp layer: 11.879100799560547
Norm after each mp layer: 64.85283660888672
Epoch: 115, Loss: 0.1144, Energy: 192418.9375, Train: 97.68%, Valid: 78.80%, Test: 76.40%, Best Valid: 79.40%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5116269588470459
Norm after each mp layer: 2.1383464336395264
Norm after each mp layer: 11.879100799560547
Norm after each mp layer: 64.85283660888672
Norm before input: 0.2552422881126404
Norm after input: 0.5116330981254578
Norm after each mp layer: 2.138056516647339
Norm after each mp layer: 11.903450012207031
Norm after each mp layer: 64.99787139892578
Norm before input: 0.2552422881126404
Norm after input: 0.5116330981254578
Norm after each mp layer: 2.138056516647339
Norm after each mp layer: 11.903450012207031
Norm after each mp layer: 64.99786376953125
Norm before input: 0.2552422881126404
Norm after input: 0.5116444826126099
Norm after each mp layer: 2.140669584274292
Norm after each mp layer: 11.942192077636719
Norm after each mp layer: 65.16300201416016
Norm before input: 0.2552422881126404
Norm after input: 0.5116444826126099
Norm after each mp layer: 2.140669584274292
Norm after each mp layer: 11.942192077636719
Norm after each mp layer: 65.16300201416016
Norm before input: 0.2552422881126404
Norm after input: 0.5118556022644043
Norm after each mp layer: 2.1477739810943604
Norm after each mp layer: 12.00670337677002
Norm after each mp layer: 65.4151611328125
Norm before input: 0.2552422881126404
Norm after input: 0.5118556022644043
Norm after each mp layer: 2.1477739810943604
Norm after each mp layer: 12.00670337677002
Norm after each mp layer: 65.4151611328125
Norm before input: 0.2552422881126404
Norm after input: 0.5120833516120911
Norm after each mp layer: 2.151899576187134
Norm after each mp layer: 12.055355072021484
Norm after each mp layer: 65.65985107421875
Norm before input: 0.2552422881126404
Norm after input: 0.5120833516120911
Norm after each mp layer: 2.151899576187134
Norm after each mp layer: 12.055355072021484
Norm after each mp layer: 65.65985107421875
Norm before input: 0.2552422881126404
Norm after input: 0.5120604038238525
Norm after each mp layer: 2.1521270275115967
Norm after each mp layer: 12.079566955566406
Norm after each mp layer: 65.81084442138672
Epoch: 120, Loss: 0.1009, Energy: 177162.9219, Train: 97.93%, Valid: 80.00%, Test: 76.60%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5120604038238525
Norm after each mp layer: 2.1521270275115967
Norm after each mp layer: 12.079566955566406
Norm after each mp layer: 65.81084442138672
Norm before input: 0.2552422881126404
Norm after input: 0.5121597051620483
Norm after each mp layer: 2.1563916206359863
Norm after each mp layer: 12.126540184020996
Norm after each mp layer: 66.01676940917969
Norm before input: 0.2552422881126404
Norm after input: 0.5121597051620483
Norm after each mp layer: 2.1563916206359863
Norm after each mp layer: 12.126540184020996
Norm after each mp layer: 66.01676940917969
Norm before input: 0.2552422881126404
Norm after input: 0.512536346912384
Norm after each mp layer: 2.1637051105499268
Norm after each mp layer: 12.193429946899414
Norm after each mp layer: 66.31993865966797
Norm before input: 0.2552422881126404
Norm after input: 0.512536346912384
Norm after each mp layer: 2.1637051105499268
Norm after each mp layer: 12.193429946899414
Norm after each mp layer: 66.31993865966797
Norm before input: 0.2552422881126404
Norm after input: 0.5126358270645142
Norm after each mp layer: 2.165922164916992
Norm after each mp layer: 12.22921371459961
Norm after each mp layer: 66.51756286621094
Norm before input: 0.2552422881126404
Norm after input: 0.5126358270645142
Norm after each mp layer: 2.165921926498413
Norm after each mp layer: 12.22921371459961
Norm after each mp layer: 66.51756286621094
Norm before input: 0.2552422881126404
Norm after input: 0.5126346945762634
Norm after each mp layer: 2.1672422885894775
Norm after each mp layer: 12.25893497467041
Norm after each mp layer: 66.68550109863281
Norm before input: 0.2552422881126404
Norm after input: 0.5126346945762634
Norm after each mp layer: 2.1672422885894775
Norm after each mp layer: 12.258935928344727
Norm after each mp layer: 66.68550109863281
Norm before input: 0.2552422881126404
Norm after input: 0.5130025148391724
Norm after each mp layer: 2.1743087768554688
Norm after each mp layer: 12.325000762939453
Norm after each mp layer: 66.99564361572266
Epoch: 125, Loss: 0.0901, Energy: 165910.2188, Train: 98.01%, Valid: 79.00%, Test: 76.50%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5130025148391724
Norm after each mp layer: 2.1743087768554688
Norm after each mp layer: 12.325000762939453
Norm after each mp layer: 66.99564361572266
Norm before input: 0.2552422881126404
Norm after input: 0.5132766962051392
Norm after each mp layer: 2.1795623302459717
Norm after each mp layer: 12.38046932220459
Norm after each mp layer: 67.26847076416016
Norm before input: 0.2552422881126404
Norm after input: 0.5132766962051392
Norm after each mp layer: 2.1795623302459717
Norm after each mp layer: 12.38046932220459
Norm after each mp layer: 67.26847076416016
Norm before input: 0.2552422881126404
Norm after input: 0.5132691860198975
Norm after each mp layer: 2.1802914142608643
Norm after each mp layer: 12.407960891723633
Norm after each mp layer: 67.43336486816406
Norm before input: 0.2552422881126404
Norm after input: 0.5132691860198975
Norm after each mp layer: 2.1802914142608643
Norm after each mp layer: 12.407960891723633
Norm after each mp layer: 67.43336486816406
Norm before input: 0.2552422881126404
Norm after input: 0.5134653449058533
Norm after each mp layer: 2.1839942932128906
Norm after each mp layer: 12.454901695251465
Norm after each mp layer: 67.67001342773438
Norm before input: 0.2552422881126404
Norm after input: 0.5134653449058533
Norm after each mp layer: 2.1839942932128906
Norm after each mp layer: 12.454901695251465
Norm after each mp layer: 67.67001342773438
Norm before input: 0.2552422881126404
Norm after input: 0.5137978196144104
Norm after each mp layer: 2.1902129650115967
Norm after each mp layer: 12.518043518066406
Norm after each mp layer: 67.95830535888672
Norm before input: 0.2552422881126404
Norm after input: 0.5137978196144104
Norm after each mp layer: 2.1902129650115967
Norm after each mp layer: 12.518043518066406
Norm after each mp layer: 67.95830535888672
Norm before input: 0.2552422881126404
Norm after input: 0.5138824582099915
Norm after each mp layer: 2.192444324493408
Norm after each mp layer: 12.556167602539062
Norm after each mp layer: 68.15461730957031
Epoch: 130, Loss: 0.0811, Energy: 161282.3906, Train: 98.10%, Valid: 79.20%, Test: 76.70%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5138824582099915
Norm after each mp layer: 2.192444324493408
Norm after each mp layer: 12.556167602539062
Norm after each mp layer: 68.15461730957031
Norm before input: 0.2552422881126404
Norm after input: 0.513960063457489
Norm after each mp layer: 2.1937549114227295
Norm after each mp layer: 12.589038848876953
Norm after each mp layer: 68.34476470947266
Norm before input: 0.2552422881126404
Norm after input: 0.513960063457489
Norm after each mp layer: 2.1937549114227295
Norm after each mp layer: 12.589038848876953
Norm after each mp layer: 68.34476470947266
Norm before input: 0.2552422881126404
Norm after input: 0.5142600536346436
Norm after each mp layer: 2.199082612991333
Norm after each mp layer: 12.647294998168945
Norm after each mp layer: 68.62570190429688
Norm before input: 0.2552422881126404
Norm after input: 0.5142600536346436
Norm after each mp layer: 2.199082612991333
Norm after each mp layer: 12.647294998168945
Norm after each mp layer: 68.62570190429688
Norm before input: 0.2552422881126404
Norm after input: 0.5144644975662231
Norm after each mp layer: 2.2035107612609863
Norm after each mp layer: 12.699322700500488
Norm after each mp layer: 68.88091278076172
Norm before input: 0.2552422881126404
Norm after input: 0.5144644975662231
Norm after each mp layer: 2.2035107612609863
Norm after each mp layer: 12.699322700500488
Norm after each mp layer: 68.88091278076172
Norm before input: 0.2552422881126404
Norm after input: 0.5145423412322998
Norm after each mp layer: 2.205000877380371
Norm after each mp layer: 12.73288345336914
Norm after each mp layer: 69.08426666259766
Norm before input: 0.2552422881126404
Norm after input: 0.5145423412322998
Norm after each mp layer: 2.205000877380371
Norm after each mp layer: 12.73288345336914
Norm after each mp layer: 69.08426666259766
Norm before input: 0.2552422881126404
Norm after input: 0.5147420763969421
Norm after each mp layer: 2.208116292953491
Norm after each mp layer: 12.777082443237305
Norm after each mp layer: 69.32942962646484
Epoch: 135, Loss: 0.0734, Energy: 157087.7969, Train: 98.26%, Valid: 78.80%, Test: 77.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5147420763969421
Norm after each mp layer: 2.208116292953491
Norm after each mp layer: 12.777082443237305
Norm after each mp layer: 69.32942962646484
Norm before input: 0.2552422881126404
Norm after input: 0.514971137046814
Norm after each mp layer: 2.213003635406494
Norm after each mp layer: 12.831961631774902
Norm after each mp layer: 69.59587860107422
Norm before input: 0.2552422881126404
Norm after input: 0.514971137046814
Norm after each mp layer: 2.213003635406494
Norm after each mp layer: 12.831961631774902
Norm after each mp layer: 69.59587860107422
Norm before input: 0.2552422881126404
Norm after input: 0.5150855779647827
Norm after each mp layer: 2.2155165672302246
Norm after each mp layer: 12.871809959411621
Norm after each mp layer: 69.8128662109375
Norm before input: 0.2552422881126404
Norm after input: 0.5150855779647827
Norm after each mp layer: 2.2155165672302246
Norm after each mp layer: 12.871809959411621
Norm after each mp layer: 69.8128662109375
Norm before input: 0.2552422881126404
Norm after input: 0.5152069330215454
Norm after each mp layer: 2.216888189315796
Norm after each mp layer: 12.905065536499023
Norm after each mp layer: 70.01583099365234
Norm before input: 0.2552422881126404
Norm after input: 0.5152069330215454
Norm after each mp layer: 2.216888189315796
Norm after each mp layer: 12.905065536499023
Norm after each mp layer: 70.01583099365234
Norm before input: 0.2552422881126404
Norm after input: 0.515381932258606
Norm after each mp layer: 2.220252275466919
Norm after each mp layer: 12.950675010681152
Norm after each mp layer: 70.24419403076172
Norm before input: 0.2552422881126404
Norm after input: 0.515381932258606
Norm after each mp layer: 2.220252275466919
Norm after each mp layer: 12.950675010681152
Norm after each mp layer: 70.24419403076172
Norm before input: 0.2552422881126404
Norm after input: 0.5155212879180908
Norm after each mp layer: 2.223306655883789
Norm after each mp layer: 12.994028091430664
Norm after each mp layer: 70.45634460449219
Epoch: 140, Loss: 0.0668, Energy: 155432.8594, Train: 98.18%, Valid: 79.00%, Test: 76.50%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5155212879180908
Norm after each mp layer: 2.223306655883789
Norm after each mp layer: 12.994028091430664
Norm after each mp layer: 70.45634460449219
Norm before input: 0.2552422881126404
Norm after input: 0.5156250596046448
Norm after each mp layer: 2.224485158920288
Norm after each mp layer: 13.025647163391113
Norm after each mp layer: 70.63768005371094
Norm before input: 0.2552422881126404
Norm after input: 0.5156250596046448
Norm after each mp layer: 2.224485158920288
Norm after each mp layer: 13.025647163391113
Norm after each mp layer: 70.63768005371094
Norm before input: 0.2552422881126404
Norm after input: 0.5157317519187927
Norm after each mp layer: 2.226001024246216
Norm after each mp layer: 13.059002876281738
Norm after each mp layer: 70.820068359375
Norm before input: 0.2552422881126404
Norm after input: 0.5157317519187927
Norm after each mp layer: 2.226001024246216
Norm after each mp layer: 13.059002876281738
Norm after each mp layer: 70.820068359375
Norm before input: 0.2552422881126404
Norm after input: 0.5158675312995911
Norm after each mp layer: 2.228968620300293
Norm after each mp layer: 13.101025581359863
Norm after each mp layer: 71.0229263305664
Norm before input: 0.2552422881126404
Norm after input: 0.5158675312995911
Norm after each mp layer: 2.228968620300293
Norm after each mp layer: 13.101025581359863
Norm after each mp layer: 71.0229263305664
Norm before input: 0.2552422881126404
Norm after input: 0.5160049200057983
Norm after each mp layer: 2.2312569618225098
Norm after each mp layer: 13.138729095458984
Norm after each mp layer: 71.22183227539062
Norm before input: 0.2552422881126404
Norm after input: 0.5160049200057983
Norm after each mp layer: 2.2312569618225098
Norm after each mp layer: 13.138729095458984
Norm after each mp layer: 71.22183227539062
Norm before input: 0.2552422881126404
Norm after input: 0.516100287437439
Norm after each mp layer: 2.23233962059021
Norm after each mp layer: 13.168447494506836
Norm after each mp layer: 71.40251922607422
Epoch: 145, Loss: 0.0612, Energy: 152490.7344, Train: 98.59%, Valid: 78.60%, Test: 76.20%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.516100287437439
Norm after each mp layer: 2.23233962059021
Norm after each mp layer: 13.168447494506836
Norm after each mp layer: 71.40251922607422
Norm before input: 0.2552422881126404
Norm after input: 0.5162087678909302
Norm after each mp layer: 2.2344584465026855
Norm after each mp layer: 13.204323768615723
Norm after each mp layer: 71.5970230102539
Norm before input: 0.2552422881126404
Norm after input: 0.5162087678909302
Norm after each mp layer: 2.2344584465026855
Norm after each mp layer: 13.204324722290039
Norm after each mp layer: 71.5970230102539
Norm before input: 0.2552422881126404
Norm after input: 0.5163629055023193
Norm after each mp layer: 2.237389326095581
Norm after each mp layer: 13.245528221130371
Norm after each mp layer: 71.80675506591797
Norm before input: 0.2552422881126404
Norm after input: 0.5163629055023193
Norm after each mp layer: 2.237389326095581
Norm after each mp layer: 13.245528221130371
Norm after each mp layer: 71.80675506591797
Norm before input: 0.2552422881126404
Norm after input: 0.5164693593978882
Norm after each mp layer: 2.2388644218444824
Norm after each mp layer: 13.277242660522461
Norm after each mp layer: 71.9861831665039
Norm before input: 0.2552422881126404
Norm after input: 0.5164693593978882
Norm after each mp layer: 2.2388644218444824
Norm after each mp layer: 13.277242660522461
Norm after each mp layer: 71.9861831665039
Norm before input: 0.2552422881126404
Norm after input: 0.5165352821350098
Norm after each mp layer: 2.239753484725952
Norm after each mp layer: 13.304736137390137
Norm after each mp layer: 72.1432113647461
Norm before input: 0.2552422881126404
Norm after input: 0.5165352821350098
Norm after each mp layer: 2.239753484725952
Norm after each mp layer: 13.304736137390137
Norm after each mp layer: 72.1432113647461
Norm before input: 0.2552422881126404
Norm after input: 0.5166595578193665
Norm after each mp layer: 2.2419373989105225
Norm after each mp layer: 13.340680122375488
Norm after each mp layer: 72.31729888916016
Epoch: 150, Loss: 0.0564, Energy: 150393.5156, Train: 98.51%, Valid: 78.00%, Test: 76.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5166595578193665
Norm after each mp layer: 2.2419373989105225
Norm after each mp layer: 13.340680122375488
Norm after each mp layer: 72.31729888916016
Norm before input: 0.2552422881126404
Norm after input: 0.5167807340621948
Norm after each mp layer: 2.2438089847564697
Norm after each mp layer: 13.374526977539062
Norm after each mp layer: 72.48098754882812
Norm before input: 0.2552422881126404
Norm after input: 0.5167807340621948
Norm after each mp layer: 2.2438089847564697
Norm after each mp layer: 13.374526977539062
Norm after each mp layer: 72.48098754882812
Norm before input: 0.2552422881126404
Norm after input: 0.5168431401252747
Norm after each mp layer: 2.244363784790039
Norm after each mp layer: 13.399310111999512
Norm after each mp layer: 72.61458587646484
Norm before input: 0.2552422881126404
Norm after input: 0.5168431401252747
Norm after each mp layer: 2.244363784790039
Norm after each mp layer: 13.399311065673828
Norm after each mp layer: 72.61458587646484
Norm before input: 0.2552422881126404
Norm after input: 0.5169408321380615
Norm after each mp layer: 2.2456371784210205
Norm after each mp layer: 13.428675651550293
Norm after each mp layer: 72.76080322265625
Norm before input: 0.2552422881126404
Norm after input: 0.5169408321380615
Norm after each mp layer: 2.2456371784210205
Norm after each mp layer: 13.428675651550293
Norm after each mp layer: 72.76080322265625
Norm before input: 0.2552422881126404
Norm after input: 0.5170738101005554
Norm after each mp layer: 2.247725248336792
Norm after each mp layer: 13.463251113891602
Norm after each mp layer: 72.9215087890625
Norm before input: 0.2552422881126404
Norm after input: 0.5170738101005554
Norm after each mp layer: 2.247725248336792
Norm after each mp layer: 13.463251113891602
Norm after each mp layer: 72.9215087890625
Norm before input: 0.2552422881126404
Norm after input: 0.517157793045044
Norm after each mp layer: 2.2487754821777344
Norm after each mp layer: 13.490513801574707
Norm after each mp layer: 73.05892181396484
Epoch: 155, Loss: 0.0521, Energy: 147065.4375, Train: 98.76%, Valid: 77.20%, Test: 75.80%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.517157793045044
Norm after each mp layer: 2.2487754821777344
Norm after each mp layer: 13.490513801574707
Norm after each mp layer: 73.05892181396484
Norm before input: 0.2552422881126404
Norm after input: 0.517238974571228
Norm after each mp layer: 2.2495181560516357
Norm after each mp layer: 13.51562786102295
Norm after each mp layer: 73.18885040283203
Norm before input: 0.2552422881126404
Norm after input: 0.517238974571228
Norm after each mp layer: 2.2495181560516357
Norm after each mp layer: 13.51562786102295
Norm after each mp layer: 73.18885040283203
Norm before input: 0.2552422881126404
Norm after input: 0.5173596143722534
Norm after each mp layer: 2.251187562942505
Norm after each mp layer: 13.546833992004395
Norm after each mp layer: 73.33165740966797
Norm before input: 0.2552422881126404
Norm after input: 0.5173596143722534
Norm after each mp layer: 2.251187562942505
Norm after each mp layer: 13.546833992004395
Norm after each mp layer: 73.33165740966797
Norm before input: 0.2552422881126404
Norm after input: 0.5174580216407776
Norm after each mp layer: 2.252584457397461
Norm after each mp layer: 13.575846672058105
Norm after each mp layer: 73.46092224121094
Norm before input: 0.2552422881126404
Norm after input: 0.5174580216407776
Norm after each mp layer: 2.252584457397461
Norm after each mp layer: 13.575846672058105
Norm after each mp layer: 73.46092224121094
Norm before input: 0.2552422881126404
Norm after input: 0.5175349116325378
Norm after each mp layer: 2.25314998626709
Norm after each mp layer: 13.599260330200195
Norm after each mp layer: 73.57123565673828
Norm before input: 0.2552422881126404
Norm after input: 0.5175349116325378
Norm after each mp layer: 2.25314998626709
Norm after each mp layer: 13.599260330200195
Norm after each mp layer: 73.57123565673828
Norm before input: 0.2552422881126404
Norm after input: 0.5176295638084412
Norm after each mp layer: 2.254040479660034
Norm after each mp layer: 13.62480354309082
Norm after each mp layer: 73.68395233154297
Epoch: 160, Loss: 0.0482, Energy: 143911.9688, Train: 99.09%, Valid: 76.80%, Test: 75.50%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5176295638084412
Norm after each mp layer: 2.254040479660034
Norm after each mp layer: 13.62480354309082
Norm after each mp layer: 73.68395233154297
Norm before input: 0.2552422881126404
Norm after input: 0.5177217125892639
Norm after each mp layer: 2.2552645206451416
Norm after each mp layer: 13.65214729309082
Norm after each mp layer: 73.7956771850586
Norm before input: 0.2552422881126404
Norm after input: 0.5177217125892639
Norm after each mp layer: 2.2552645206451416
Norm after each mp layer: 13.652149200439453
Norm after each mp layer: 73.7956771850586
Norm before input: 0.2552422881126404
Norm after input: 0.5177966356277466
Norm after each mp layer: 2.255870819091797
Norm after each mp layer: 13.67518424987793
Norm after each mp layer: 73.89401245117188
Norm before input: 0.2552422881126404
Norm after input: 0.5177966356277466
Norm after each mp layer: 2.255870819091797
Norm after each mp layer: 13.67518424987793
Norm after each mp layer: 73.89401245117188
Norm before input: 0.2552422881126404
Norm after input: 0.5178680419921875
Norm after each mp layer: 2.256211042404175
Norm after each mp layer: 13.696245193481445
Norm after each mp layer: 73.98627471923828
Norm before input: 0.2552422881126404
Norm after input: 0.5178680419921875
Norm after each mp layer: 2.256211042404175
Norm after each mp layer: 13.696245193481445
Norm after each mp layer: 73.98627471923828
Norm before input: 0.2552422881126404
Norm after input: 0.5179440379142761
Norm after each mp layer: 2.2570276260375977
Norm after each mp layer: 13.719968795776367
Norm after each mp layer: 74.08253479003906
Norm before input: 0.2552422881126404
Norm after input: 0.5179440379142761
Norm after each mp layer: 2.2570276260375977
Norm after each mp layer: 13.719968795776367
Norm after each mp layer: 74.08253479003906
Norm before input: 0.2552422881126404
Norm after input: 0.5180227756500244
Norm after each mp layer: 2.2578561305999756
Norm after each mp layer: 13.743474960327148
Norm after each mp layer: 74.17791748046875
Epoch: 165, Loss: 0.0448, Energy: 140194.4062, Train: 99.25%, Valid: 76.80%, Test: 75.10%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5180227756500244
Norm after each mp layer: 2.2578561305999756
Norm after each mp layer: 13.743474960327148
Norm after each mp layer: 74.17791748046875
Norm before input: 0.2552422881126404
Norm after input: 0.5180898904800415
Norm after each mp layer: 2.258181095123291
Norm after each mp layer: 13.76333999633789
Norm after each mp layer: 74.26363372802734
Norm before input: 0.2552422881126404
Norm after input: 0.5180898904800415
Norm after each mp layer: 2.258181095123291
Norm after each mp layer: 13.76333999633789
Norm after each mp layer: 74.26363372802734
Norm before input: 0.2552422881126404
Norm after input: 0.5181513428688049
Norm after each mp layer: 2.2586164474487305
Norm after each mp layer: 13.783459663391113
Norm after each mp layer: 74.34708404541016
Norm before input: 0.2552422881126404
Norm after input: 0.5181513428688049
Norm after each mp layer: 2.2586164474487305
Norm after each mp layer: 13.783459663391113
Norm after each mp layer: 74.34708404541016
Norm before input: 0.2552422881126404
Norm after input: 0.518227219581604
Norm after each mp layer: 2.2594492435455322
Norm after each mp layer: 13.80607795715332
Norm after each mp layer: 74.43579864501953
Norm before input: 0.2552422881126404
Norm after input: 0.518227219581604
Norm after each mp layer: 2.2594492435455322
Norm after each mp layer: 13.80607795715332
Norm after each mp layer: 74.43579864501953
Norm before input: 0.2552422881126404
Norm after input: 0.51829594373703
Norm after each mp layer: 2.2599093914031982
Norm after each mp layer: 13.82603645324707
Norm after each mp layer: 74.51655578613281
Norm before input: 0.2552422881126404
Norm after input: 0.51829594373703
Norm after each mp layer: 2.2599093914031982
Norm after each mp layer: 13.82603645324707
Norm after each mp layer: 74.51655578613281
Norm before input: 0.2552422881126404
Norm after input: 0.5183494091033936
Norm after each mp layer: 2.2600669860839844
Norm after each mp layer: 13.843570709228516
Norm after each mp layer: 74.58797454833984
Epoch: 170, Loss: 0.0417, Energy: 136260.6250, Train: 99.34%, Valid: 77.00%, Test: 74.90%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5183494091033936
Norm after each mp layer: 2.2600669860839844
Norm after each mp layer: 13.843570709228516
Norm after each mp layer: 74.58797454833984
Norm before input: 0.2552422881126404
Norm after input: 0.5184195637702942
Norm after each mp layer: 2.2606656551361084
Norm after each mp layer: 13.863993644714355
Norm after each mp layer: 74.66532897949219
Norm before input: 0.2552422881126404
Norm after input: 0.5184195637702942
Norm after each mp layer: 2.2606656551361084
Norm after each mp layer: 13.863993644714355
Norm after each mp layer: 74.66532897949219
Norm before input: 0.2552422881126404
Norm after input: 0.5184927582740784
Norm after each mp layer: 2.261261224746704
Norm after each mp layer: 13.884284019470215
Norm after each mp layer: 74.74090576171875
Norm before input: 0.2552422881126404
Norm after input: 0.5184927582740784
Norm after each mp layer: 2.261261224746704
Norm after each mp layer: 13.884284019470215
Norm after each mp layer: 74.74090576171875
Norm before input: 0.2552422881126404
Norm after input: 0.5185456871986389
Norm after each mp layer: 2.2613883018493652
Norm after each mp layer: 13.901006698608398
Norm after each mp layer: 74.80469512939453
Norm before input: 0.2552422881126404
Norm after input: 0.5185456871986389
Norm after each mp layer: 2.2613883018493652
Norm after each mp layer: 13.901007652282715
Norm after each mp layer: 74.80469512939453
Norm before input: 0.2552422881126404
Norm after input: 0.5186067819595337
Norm after each mp layer: 2.2617220878601074
Norm after each mp layer: 13.918984413146973
Norm after each mp layer: 74.87146759033203
Norm before input: 0.2552422881126404
Norm after input: 0.5186067819595337
Norm after each mp layer: 2.2617220878601074
Norm after each mp layer: 13.918984413146973
Norm after each mp layer: 74.87146759033203
Norm before input: 0.2552422881126404
Norm after input: 0.5186792016029358
Norm after each mp layer: 2.2623565196990967
Norm after each mp layer: 13.938876152038574
Norm after each mp layer: 74.94386291503906
Epoch: 175, Loss: 0.0390, Energy: 131875.8906, Train: 99.50%, Valid: 76.40%, Test: 74.40%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5186792016029358
Norm after each mp layer: 2.2623565196990967
Norm after each mp layer: 13.938876152038574
Norm after each mp layer: 74.94386291503906
Norm before input: 0.2552422881126404
Norm after input: 0.5187368988990784
Norm after each mp layer: 2.262657403945923
Norm after each mp layer: 13.956113815307617
Norm after each mp layer: 75.00900268554688
Norm before input: 0.2552422881126404
Norm after input: 0.5187368988990784
Norm after each mp layer: 2.262657403945923
Norm after each mp layer: 13.956113815307617
Norm after each mp layer: 75.00900268554688
Norm before input: 0.2552422881126404
Norm after input: 0.5187936425209045
Norm after each mp layer: 2.262875556945801
Norm after each mp layer: 13.972603797912598
Norm after each mp layer: 75.0724105834961
Norm before input: 0.2552422881126404
Norm after input: 0.5187936425209045
Norm after each mp layer: 2.262875556945801
Norm after each mp layer: 13.972603797912598
Norm after each mp layer: 75.0724105834961
Norm before input: 0.2552422881126404
Norm after input: 0.5188593864440918
Norm after each mp layer: 2.263383626937866
Norm after each mp layer: 13.990899085998535
Norm after each mp layer: 75.1396255493164
Norm before input: 0.2552422881126404
Norm after input: 0.5188593864440918
Norm after each mp layer: 2.263383626937866
Norm after each mp layer: 13.990899085998535
Norm after each mp layer: 75.1396255493164
Norm before input: 0.2552422881126404
Norm after input: 0.5189156532287598
Norm after each mp layer: 2.263744354248047
Norm after each mp layer: 14.007782936096191
Norm after each mp layer: 75.2003402709961
Norm before input: 0.2552422881126404
Norm after input: 0.5189156532287598
Norm after each mp layer: 2.263744354248047
Norm after each mp layer: 14.007782936096191
Norm after each mp layer: 75.2003402709961
Norm before input: 0.2552422881126404
Norm after input: 0.5189655423164368
Norm after each mp layer: 2.2638463973999023
Norm after each mp layer: 14.022653579711914
Norm after each mp layer: 75.25343322753906
Epoch: 180, Loss: 0.0365, Energy: 127966.1797, Train: 99.59%, Valid: 76.40%, Test: 74.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5189655423164368
Norm after each mp layer: 2.2638463973999023
Norm after each mp layer: 14.022653579711914
Norm after each mp layer: 75.25342559814453
Norm before input: 0.2552422881126404
Norm after input: 0.5190199017524719
Norm after each mp layer: 2.264087200164795
Norm after each mp layer: 14.038254737854004
Norm after each mp layer: 75.3056411743164
Norm before input: 0.2552422881126404
Norm after input: 0.5190199017524719
Norm after each mp layer: 2.264087200164795
Norm after each mp layer: 14.038254737854004
Norm after each mp layer: 75.3056411743164
Norm before input: 0.2552422881126404
Norm after input: 0.5190718173980713
Norm after each mp layer: 2.264392852783203
Norm after each mp layer: 14.053936958312988
Norm after each mp layer: 75.35490417480469
Norm before input: 0.2552422881126404
Norm after input: 0.5190718173980713
Norm after each mp layer: 2.264392852783203
Norm after each mp layer: 14.053936958312988
Norm after each mp layer: 75.35490417480469
Norm before input: 0.2552422881126404
Norm after input: 0.5191183090209961
Norm after each mp layer: 2.2644824981689453
Norm after each mp layer: 14.06789779663086
Norm after each mp layer: 75.39833068847656
Norm before input: 0.2552422881126404
Norm after input: 0.5191183090209961
Norm after each mp layer: 2.2644824981689453
Norm after each mp layer: 14.06789779663086
Norm after each mp layer: 75.39833068847656
Norm before input: 0.2552422881126404
Norm after input: 0.5191627740859985
Norm after each mp layer: 2.2645351886749268
Norm after each mp layer: 14.081341743469238
Norm after each mp layer: 75.43956756591797
Norm before input: 0.2552422881126404
Norm after input: 0.5191627740859985
Norm after each mp layer: 2.2645351886749268
Norm after each mp layer: 14.081341743469238
Norm after each mp layer: 75.43956756591797
Norm before input: 0.2552422881126404
Norm after input: 0.5192074179649353
Norm after each mp layer: 2.264747142791748
Norm after each mp layer: 14.095555305480957
Norm after each mp layer: 75.4820556640625
Epoch: 185, Loss: 0.0344, Energy: 123933.0469, Train: 99.59%, Valid: 76.40%, Test: 73.80%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5192074179649353
Norm after each mp layer: 2.264747142791748
Norm after each mp layer: 14.095555305480957
Norm after each mp layer: 75.4820556640625
Norm before input: 0.2552422881126404
Norm after input: 0.5192520022392273
Norm after each mp layer: 2.2649192810058594
Norm after each mp layer: 14.109315872192383
Norm after each mp layer: 75.52397918701172
Norm before input: 0.2552422881126404
Norm after input: 0.5192520022392273
Norm after each mp layer: 2.2649192810058594
Norm after each mp layer: 14.109315872192383
Norm after each mp layer: 75.52397918701172
Norm before input: 0.2552422881126404
Norm after input: 0.519292950630188
Norm after each mp layer: 2.2649755477905273
Norm after each mp layer: 14.122048377990723
Norm after each mp layer: 75.56366729736328
Norm before input: 0.2552422881126404
Norm after input: 0.519292950630188
Norm after each mp layer: 2.2649755477905273
Norm after each mp layer: 14.122048377990723
Norm after each mp layer: 75.56366729736328
Norm before input: 0.2552422881126404
Norm after input: 0.5193343758583069
Norm after each mp layer: 2.265155076980591
Norm after each mp layer: 14.135371208190918
Norm after each mp layer: 75.60458374023438
Norm before input: 0.2552422881126404
Norm after input: 0.5193343758583069
Norm after each mp layer: 2.265155076980591
Norm after each mp layer: 14.135371208190918
Norm after each mp layer: 75.60458374023438
Norm before input: 0.2552422881126404
Norm after input: 0.5193783640861511
Norm after each mp layer: 2.265399932861328
Norm after each mp layer: 14.149025917053223
Norm after each mp layer: 75.64630889892578
Norm before input: 0.2552422881126404
Norm after input: 0.5193783640861511
Norm after each mp layer: 2.265399932861328
Norm after each mp layer: 14.149025917053223
Norm after each mp layer: 75.64630889892578
Norm before input: 0.2552422881126404
Norm after input: 0.5194165706634521
Norm after each mp layer: 2.2654764652252197
Norm after each mp layer: 14.1613130569458
Norm after each mp layer: 75.68392181396484
Epoch: 190, Loss: 0.0324, Energy: 120099.0469, Train: 99.59%, Valid: 75.80%, Test: 73.20%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5194165706634521
Norm after each mp layer: 2.2654764652252197
Norm after each mp layer: 14.1613130569458
Norm after each mp layer: 75.68392181396484
Norm before input: 0.2552422881126404
Norm after input: 0.5194540023803711
Norm after each mp layer: 2.2655718326568604
Norm after each mp layer: 14.17353630065918
Norm after each mp layer: 75.72045135498047
Norm before input: 0.2552422881126404
Norm after input: 0.5194540023803711
Norm after each mp layer: 2.2655718326568604
Norm after each mp layer: 14.17353630065918
Norm after each mp layer: 75.72045135498047
Norm before input: 0.2552422881126404
Norm after input: 0.5194961428642273
Norm after each mp layer: 2.2657954692840576
Norm after each mp layer: 14.186563491821289
Norm after each mp layer: 75.7583236694336
Norm before input: 0.2552422881126404
Norm after input: 0.5194961428642273
Norm after each mp layer: 2.2657954692840576
Norm after each mp layer: 14.186563491821289
Norm after each mp layer: 75.7583236694336
Norm before input: 0.2552422881126404
Norm after input: 0.519533634185791
Norm after each mp layer: 2.2658867835998535
Norm after each mp layer: 14.198431015014648
Norm after each mp layer: 75.79193115234375
Norm before input: 0.2552422881126404
Norm after input: 0.519533634185791
Norm after each mp layer: 2.2658867835998535
Norm after each mp layer: 14.198431015014648
Norm after each mp layer: 75.79193115234375
Norm before input: 0.2552422881126404
Norm after input: 0.5195683836936951
Norm after each mp layer: 2.26590633392334
Norm after each mp layer: 14.209562301635742
Norm after each mp layer: 75.82258605957031
Norm before input: 0.2552422881126404
Norm after input: 0.5195683836936951
Norm after each mp layer: 2.26590633392334
Norm after each mp layer: 14.209562301635742
Norm after each mp layer: 75.82258605957031
Norm before input: 0.2552422881126404
Norm after input: 0.5196071863174438
Norm after each mp layer: 2.2660486698150635
Norm after each mp layer: 14.221386909484863
Norm after each mp layer: 75.85453033447266
Epoch: 195, Loss: 0.0307, Energy: 116636.4766, Train: 99.59%, Valid: 75.40%, Test: 73.10%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5196071863174438
Norm after each mp layer: 2.2660486698150635
Norm after each mp layer: 14.221386909484863
Norm after each mp layer: 75.85453033447266
Norm before input: 0.2552422881126404
Norm after input: 0.5196433663368225
Norm after each mp layer: 2.2661502361297607
Norm after each mp layer: 14.23264217376709
Norm after each mp layer: 75.8845443725586
Norm before input: 0.2552422881126404
Norm after input: 0.5196433663368225
Norm after each mp layer: 2.2661502361297607
Norm after each mp layer: 14.23264217376709
Norm after each mp layer: 75.8845443725586
Norm before input: 0.2552422881126404
Norm after input: 0.5196779370307922
Norm after each mp layer: 2.2661705017089844
Norm after each mp layer: 14.243107795715332
Norm after each mp layer: 75.91242218017578
Norm before input: 0.2552422881126404
Norm after input: 0.5196779370307922
Norm after each mp layer: 2.2661705017089844
Norm after each mp layer: 14.243107795715332
Norm after each mp layer: 75.91242218017578
Norm before input: 0.2552422881126404
Norm after input: 0.519713819026947
Norm after each mp layer: 2.266251564025879
Norm after each mp layer: 14.253774642944336
Norm after each mp layer: 75.94031524658203
Norm before input: 0.2552422881126404
Norm after input: 0.519713819026947
Norm after each mp layer: 2.266251564025879
Norm after each mp layer: 14.253774642944336
Norm after each mp layer: 75.94031524658203
Norm before input: 0.2552422881126404
Norm after input: 0.5197467803955078
Norm after each mp layer: 2.2663159370422363
Norm after each mp layer: 14.263991355895996
Norm after each mp layer: 75.96593475341797
Norm before input: 0.2552422881126404
Norm after input: 0.5197467803955078
Norm after each mp layer: 2.2663159370422363
Norm after each mp layer: 14.263991355895996
Norm after each mp layer: 75.96593475341797
Norm before input: 0.2552422881126404
Norm after input: 0.5197787880897522
Norm after each mp layer: 2.2662909030914307
Norm after each mp layer: 14.273370742797852
Norm after each mp layer: 75.98856353759766
Epoch: 200, Loss: 0.0291, Energy: 113294.9609, Train: 99.59%, Valid: 75.00%, Test: 73.30%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5197787880897522
Norm after each mp layer: 2.2662909030914307
Norm after each mp layer: 14.273370742797852
Norm after each mp layer: 75.98856353759766
Norm before input: 0.2552422881126404
Norm after input: 0.5198109149932861
Norm after each mp layer: 2.266270399093628
Norm after each mp layer: 14.282530784606934
Norm after each mp layer: 76.00931549072266
Norm before input: 0.2552422881126404
Norm after input: 0.5198109149932861
Norm after each mp layer: 2.266270399093628
Norm after each mp layer: 14.282530784606934
Norm after each mp layer: 76.00930786132812
Norm before input: 0.2552422881126404
Norm after input: 0.519842267036438
Norm after each mp layer: 2.266270160675049
Norm after each mp layer: 14.291542053222656
Norm after each mp layer: 76.02800750732422
Norm before input: 0.2552422881126404
Norm after input: 0.519842267036438
Norm after each mp layer: 2.266270160675049
Norm after each mp layer: 14.291542053222656
Norm after each mp layer: 76.02800750732422
Norm before input: 0.2552422881126404
Norm after input: 0.5198733806610107
Norm after each mp layer: 2.266199827194214
Norm after each mp layer: 14.299872398376465
Norm after each mp layer: 76.04376983642578
Norm before input: 0.2552422881126404
Norm after input: 0.5198733806610107
Norm after each mp layer: 2.266199827194214
Norm after each mp layer: 14.299872398376465
Norm after each mp layer: 76.04376983642578
Norm before input: 0.2552422881126404
Norm after input: 0.519902765750885
Norm after each mp layer: 2.2660794258117676
Norm after each mp layer: 14.307600975036621
Norm after each mp layer: 76.0567626953125
Norm before input: 0.2552422881126404
Norm after input: 0.519902765750885
Norm after each mp layer: 2.2660794258117676
Norm after each mp layer: 14.307600975036621
Norm after each mp layer: 76.0567626953125
Norm before input: 0.2552422881126404
Norm after input: 0.5199332237243652
Norm after each mp layer: 2.266005039215088
Norm after each mp layer: 14.315449714660645
Norm after each mp layer: 76.0691146850586
Epoch: 205, Loss: 0.0276, Energy: 109914.4453, Train: 99.59%, Valid: 75.00%, Test: 73.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5199332237243652
Norm after each mp layer: 2.266005039215088
Norm after each mp layer: 14.315449714660645
Norm after each mp layer: 76.0691146850586
Norm before input: 0.2552422881126404
Norm after input: 0.5199642181396484
Norm after each mp layer: 2.265909194946289
Norm after each mp layer: 14.322998046875
Norm after each mp layer: 76.08021545410156
Norm before input: 0.2552422881126404
Norm after input: 0.5199642181396484
Norm after each mp layer: 2.265909194946289
Norm after each mp layer: 14.322998046875
Norm after each mp layer: 76.08021545410156
Norm before input: 0.2552422881126404
Norm after input: 0.5199936628341675
Norm after each mp layer: 2.265760898590088
Norm after each mp layer: 14.329974174499512
Norm after each mp layer: 76.0893325805664
Norm before input: 0.2552422881126404
Norm after input: 0.5199936628341675
Norm after each mp layer: 2.265760898590088
Norm after each mp layer: 14.329974174499512
Norm after each mp layer: 76.0893325805664
Norm before input: 0.2552422881126404
Norm after input: 0.520024836063385
Norm after each mp layer: 2.2656619548797607
Norm after each mp layer: 14.337160110473633
Norm after each mp layer: 76.09859466552734
Norm before input: 0.2552422881126404
Norm after input: 0.520024836063385
Norm after each mp layer: 2.2656619548797607
Norm after each mp layer: 14.337160110473633
Norm after each mp layer: 76.09860229492188
Norm before input: 0.2552422881126404
Norm after input: 0.5200552940368652
Norm after each mp layer: 2.265549659729004
Norm after each mp layer: 14.344069480895996
Norm after each mp layer: 76.10668182373047
Norm before input: 0.2552422881126404
Norm after input: 0.5200552940368652
Norm after each mp layer: 2.265549659729004
Norm after each mp layer: 14.344069480895996
Norm after each mp layer: 76.10668182373047
Norm before input: 0.2552422881126404
Norm after input: 0.5200851559638977
Norm after each mp layer: 2.2654001712799072
Norm after each mp layer: 14.35057544708252
Norm after each mp layer: 76.11361694335938
Epoch: 210, Loss: 0.0263, Energy: 106500.0391, Train: 99.59%, Valid: 75.00%, Test: 72.70%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5200851559638977
Norm after each mp layer: 2.2654001712799072
Norm after each mp layer: 14.35057544708252
Norm after each mp layer: 76.11361694335938
Norm before input: 0.2552422881126404
Norm after input: 0.5201172232627869
Norm after each mp layer: 2.2652995586395264
Norm after each mp layer: 14.357342720031738
Norm after each mp layer: 76.1213150024414
Norm before input: 0.2552422881126404
Norm after input: 0.5201172232627869
Norm after each mp layer: 2.2652995586395264
Norm after each mp layer: 14.357342720031738
Norm after each mp layer: 76.1213150024414
Norm before input: 0.2552422881126404
Norm after input: 0.5201473832130432
Norm after each mp layer: 2.2651872634887695
Norm after each mp layer: 14.363831520080566
Norm after each mp layer: 76.12786865234375
Norm before input: 0.2552422881126404
Norm after input: 0.5201473832130432
Norm after each mp layer: 2.2651872634887695
Norm after each mp layer: 14.363831520080566
Norm after each mp layer: 76.12786865234375
Norm before input: 0.2552422881126404
Norm after input: 0.52017742395401
Norm after each mp layer: 2.2650461196899414
Norm after each mp layer: 14.370017051696777
Norm after each mp layer: 76.13362121582031
Norm before input: 0.2552422881126404
Norm after input: 0.52017742395401
Norm after each mp layer: 2.2650461196899414
Norm after each mp layer: 14.370017051696777
Norm after each mp layer: 76.13362121582031
Norm before input: 0.2552422881126404
Norm after input: 0.5202084183692932
Norm after each mp layer: 2.2649285793304443
Norm after each mp layer: 14.376275062561035
Norm after each mp layer: 76.13975524902344
Norm before input: 0.2552422881126404
Norm after input: 0.5202084183692932
Norm after each mp layer: 2.2649285793304443
Norm after each mp layer: 14.376275062561035
Norm after each mp layer: 76.13975524902344
Norm before input: 0.2552422881126404
Norm after input: 0.5202388763427734
Norm after each mp layer: 2.2648210525512695
Norm after each mp layer: 14.382472038269043
Norm after each mp layer: 76.14567565917969
Epoch: 215, Loss: 0.0250, Energy: 103240.5781, Train: 99.59%, Valid: 75.00%, Test: 72.40%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5202388763427734
Norm after each mp layer: 2.2648210525512695
Norm after each mp layer: 14.382472038269043
Norm after each mp layer: 76.14567565917969
Norm before input: 0.2552422881126404
Norm after input: 0.5202689170837402
Norm after each mp layer: 2.264695882797241
Norm after each mp layer: 14.388450622558594
Norm after each mp layer: 76.15130615234375
Norm before input: 0.2552422881126404
Norm after input: 0.5202689170837402
Norm after each mp layer: 2.264695882797241
Norm after each mp layer: 14.388450622558594
Norm after each mp layer: 76.15130615234375
Norm before input: 0.2552422881126404
Norm after input: 0.5202994346618652
Norm after each mp layer: 2.2645857334136963
Norm after each mp layer: 14.394460678100586
Norm after each mp layer: 76.15762329101562
Norm before input: 0.2552422881126404
Norm after input: 0.5202994346618652
Norm after each mp layer: 2.2645857334136963
Norm after each mp layer: 14.394460678100586
Norm after each mp layer: 76.15762329101562
Norm before input: 0.2552422881126404
Norm after input: 0.5203301906585693
Norm after each mp layer: 2.264493227005005
Norm after each mp layer: 14.400534629821777
Norm after each mp layer: 76.16455841064453
Norm before input: 0.2552422881126404
Norm after input: 0.5203301906585693
Norm after each mp layer: 2.264493227005005
Norm after each mp layer: 14.400534629821777
Norm after each mp layer: 76.16455841064453
Norm before input: 0.2552422881126404
Norm after input: 0.5203593373298645
Norm after each mp layer: 2.264380693435669
Norm after each mp layer: 14.406349182128906
Norm after each mp layer: 76.17115783691406
Norm before input: 0.2552422881126404
Norm after input: 0.5203593373298645
Norm after each mp layer: 2.264380693435669
Norm after each mp layer: 14.406349182128906
Norm after each mp layer: 76.17115783691406
Norm before input: 0.2552422881126404
Norm after input: 0.5203906893730164
Norm after each mp layer: 2.26430344581604
Norm after each mp layer: 14.41244125366211
Norm after each mp layer: 76.17931365966797
Epoch: 220, Loss: 0.0239, Energy: 100186.9844, Train: 99.59%, Valid: 74.40%, Test: 72.30%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5203906893730164
Norm after each mp layer: 2.26430344581604
Norm after each mp layer: 14.41244125366211
Norm after each mp layer: 76.17931365966797
Norm before input: 0.2552422881126404
Norm after input: 0.5204209685325623
Norm after each mp layer: 2.2642173767089844
Norm after each mp layer: 14.41837215423584
Norm after each mp layer: 76.18727111816406
Norm before input: 0.2552422881126404
Norm after input: 0.5204209685325623
Norm after each mp layer: 2.2642173767089844
Norm after each mp layer: 14.41837215423584
Norm after each mp layer: 76.18727111816406
Norm before input: 0.2552422881126404
Norm after input: 0.5204507112503052
Norm after each mp layer: 2.264127731323242
Norm after each mp layer: 14.424208641052246
Norm after each mp layer: 76.19515228271484
Norm before input: 0.2552422881126404
Norm after input: 0.5204507112503052
Norm after each mp layer: 2.264127731323242
Norm after each mp layer: 14.424208641052246
Norm after each mp layer: 76.19515228271484
Norm before input: 0.2552422881126404
Norm after input: 0.5204818844795227
Norm after each mp layer: 2.26405930519104
Norm after each mp layer: 14.43018913269043
Norm after each mp layer: 76.20391082763672
Norm before input: 0.2552422881126404
Norm after input: 0.5204818844795227
Norm after each mp layer: 2.26405930519104
Norm after each mp layer: 14.43018913269043
Norm after each mp layer: 76.20391082763672
Norm before input: 0.2552422881126404
Norm after input: 0.5205129384994507
Norm after each mp layer: 2.2639830112457275
Norm after each mp layer: 14.436064720153809
Norm after each mp layer: 76.21260070800781
Norm before input: 0.2552422881126404
Norm after input: 0.5205129384994507
Norm after each mp layer: 2.2639830112457275
Norm after each mp layer: 14.436064720153809
Norm after each mp layer: 76.21260070800781
Norm before input: 0.2552422881126404
Norm after input: 0.5205433964729309
Norm after each mp layer: 2.263902187347412
Norm after each mp layer: 14.441821098327637
Norm after each mp layer: 76.22110748291016
Epoch: 225, Loss: 0.0228, Energy: 97453.3594, Train: 99.67%, Valid: 74.00%, Test: 72.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5205433964729309
Norm after each mp layer: 2.263902187347412
Norm after each mp layer: 14.441821098327637
Norm after each mp layer: 76.22110748291016
Norm before input: 0.2552422881126404
Norm after input: 0.5205749869346619
Norm after each mp layer: 2.2638375759124756
Norm after each mp layer: 14.44768238067627
Norm after each mp layer: 76.2303695678711
Norm before input: 0.2552422881126404
Norm after input: 0.5205749869346619
Norm after each mp layer: 2.2638375759124756
Norm after each mp layer: 14.44768238067627
Norm after each mp layer: 76.2303695678711
Norm before input: 0.2552422881126404
Norm after input: 0.5206058621406555
Norm after each mp layer: 2.263749599456787
Norm after each mp layer: 14.453300476074219
Norm after each mp layer: 76.23926544189453
Norm before input: 0.2552422881126404
Norm after input: 0.5206058621406555
Norm after each mp layer: 2.263749599456787
Norm after each mp layer: 14.453300476074219
Norm after each mp layer: 76.23926544189453
Norm before input: 0.2552422881126404
Norm after input: 0.5206372737884521
Norm after each mp layer: 2.263674259185791
Norm after each mp layer: 14.45896053314209
Norm after each mp layer: 76.24861145019531
Norm before input: 0.2552422881126404
Norm after input: 0.5206372737884521
Norm after each mp layer: 2.263674259185791
Norm after each mp layer: 14.45896053314209
Norm after each mp layer: 76.24861145019531
Norm before input: 0.2552422881126404
Norm after input: 0.5206682085990906
Norm after each mp layer: 2.2635931968688965
Norm after each mp layer: 14.464497566223145
Norm after each mp layer: 76.2579345703125
Norm before input: 0.2552422881126404
Norm after input: 0.5206682085990906
Norm after each mp layer: 2.2635931968688965
Norm after each mp layer: 14.464497566223145
Norm after each mp layer: 76.2579345703125
Norm before input: 0.2552422881126404
Norm after input: 0.5206988453865051
Norm after each mp layer: 2.2634944915771484
Norm after each mp layer: 14.469840049743652
Norm after each mp layer: 76.26707458496094
Epoch: 230, Loss: 0.0218, Energy: 94925.6016, Train: 99.67%, Valid: 74.00%, Test: 72.10%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5206988453865051
Norm after each mp layer: 2.2634944915771484
Norm after each mp layer: 14.469840049743652
Norm after each mp layer: 76.26707458496094
Norm before input: 0.2552422881126404
Norm after input: 0.5207282304763794
Norm after each mp layer: 2.2633886337280273
Norm after each mp layer: 14.475001335144043
Norm after each mp layer: 76.27593231201172
Norm before input: 0.2552422881126404
Norm after input: 0.5207282304763794
Norm after each mp layer: 2.2633886337280273
Norm after each mp layer: 14.475001335144043
Norm after each mp layer: 76.27593231201172
Norm before input: 0.2552422881126404
Norm after input: 0.5207586884498596
Norm after each mp layer: 2.263291120529175
Norm after each mp layer: 14.480194091796875
Norm after each mp layer: 76.28511047363281
Norm before input: 0.2552422881126404
Norm after input: 0.5207586884498596
Norm after each mp layer: 2.263291120529175
Norm after each mp layer: 14.480194091796875
Norm after each mp layer: 76.28511047363281
Norm before input: 0.2552422881126404
Norm after input: 0.5207844972610474
Norm after each mp layer: 2.263131618499756
Norm after each mp layer: 14.484691619873047
Norm after each mp layer: 76.29228973388672
Norm before input: 0.2552422881126404
Norm after input: 0.5207844972610474
Norm after each mp layer: 2.263131618499756
Norm after each mp layer: 14.484691619873047
Norm after each mp layer: 76.29228973388672
Norm before input: 0.2552422881126404
Norm after input: 0.5208172798156738
Norm after each mp layer: 2.2630536556243896
Norm after each mp layer: 14.489974021911621
Norm after each mp layer: 76.30203247070312
Norm before input: 0.2552422881126404
Norm after input: 0.5208172798156738
Norm after each mp layer: 2.2630536556243896
Norm after each mp layer: 14.489974021911621
Norm after each mp layer: 76.30203247070312
Norm before input: 0.2552422881126404
Norm after input: 0.5208346247673035
Norm after each mp layer: 2.262784957885742
Norm after each mp layer: 14.493182182312012
Norm after each mp layer: 76.3050308227539
Epoch: 235, Loss: 0.0209, Energy: 92583.6484, Train: 99.67%, Valid: 73.80%, Test: 72.20%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5208346247673035
Norm after each mp layer: 2.262784957885742
Norm after each mp layer: 14.493182182312012
Norm after each mp layer: 76.3050308227539
Norm before input: 0.2552422881126404
Norm after input: 0.5208794474601746
Norm after each mp layer: 2.262836217880249
Norm after each mp layer: 14.499749183654785
Norm after each mp layer: 76.31869506835938
Norm before input: 0.2552422881126404
Norm after input: 0.5208794474601746
Norm after each mp layer: 2.262836217880249
Norm after each mp layer: 14.499749183654785
Norm after each mp layer: 76.31869506835938
Norm before input: 0.2552422881126404
Norm after input: 0.5208696126937866
Norm after each mp layer: 2.262235164642334
Norm after each mp layer: 14.499282836914062
Norm after each mp layer: 76.30992126464844
Norm before input: 0.2552422881126404
Norm after input: 0.5208696126937866
Norm after each mp layer: 2.262235164642334
Norm after each mp layer: 14.499282836914062
Norm after each mp layer: 76.30992126464844
Norm before input: 0.2552422881126404
Norm after input: 0.5209695100784302
Norm after each mp layer: 2.262922763824463
Norm after each mp layer: 14.512572288513184
Norm after each mp layer: 76.3446273803711
Norm before input: 0.2552422881126404
Norm after input: 0.5209695100784302
Norm after each mp layer: 2.262922763824463
Norm after each mp layer: 14.512572288513184
Norm after each mp layer: 76.3446273803711
Norm before input: 0.2552422881126404
Norm after input: 0.5208550691604614
Norm after each mp layer: 2.2610538005828857
Norm after each mp layer: 14.498433113098145
Norm after each mp layer: 76.29163360595703
Norm before input: 0.2552422881126404
Norm after input: 0.5208550691604614
Norm after each mp layer: 2.2610538005828857
Norm after each mp layer: 14.498433113098145
Norm after each mp layer: 76.29163360595703
Norm before input: 0.2552422881126404
Norm after input: 0.5211326479911804
Norm after each mp layer: 2.2639710903167725
Norm after each mp layer: 14.535069465637207
Norm after each mp layer: 76.40409851074219
Epoch: 240, Loss: 0.0202, Energy: 89848.5625, Train: 99.75%, Valid: 73.40%, Test: 72.20%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5211326479911804
Norm after each mp layer: 2.2639710903167725
Norm after each mp layer: 14.535069465637207
Norm after each mp layer: 76.40409851074219
Norm before input: 0.2552422881126404
Norm after input: 0.5208340883255005
Norm after each mp layer: 2.2595808506011963
Norm after each mp layer: 14.494613647460938
Norm after each mp layer: 76.25639343261719
Norm before input: 0.2552422881126404
Norm after input: 0.5208340883255005
Norm after each mp layer: 2.2595808506011963
Norm after each mp layer: 14.494613647460938
Norm after each mp layer: 76.25639343261719
Norm before input: 0.2552422881126404
Norm after input: 0.5211990475654602
Norm after each mp layer: 2.26395320892334
Norm after each mp layer: 14.545273780822754
Norm after each mp layer: 76.42466735839844
Norm before input: 0.2552422881126404
Norm after input: 0.5211990475654602
Norm after each mp layer: 2.26395320892334
Norm after each mp layer: 14.545273780822754
Norm after each mp layer: 76.42466735839844
Norm before input: 0.2552422881126404
Norm after input: 0.521016001701355
Norm after each mp layer: 2.2609355449676514
Norm after each mp layer: 14.519182205200195
Norm after each mp layer: 76.32087707519531
Norm before input: 0.2552422881126404
Norm after input: 0.521016001701355
Norm after each mp layer: 2.2609355449676514
Norm after each mp layer: 14.519182205200195
Norm after each mp layer: 76.32087707519531
Norm before input: 0.2552422881126404
Norm after input: 0.5210062861442566
Norm after each mp layer: 2.260019540786743
Norm after each mp layer: 14.51583194732666
Norm after each mp layer: 76.29468536376953
Norm before input: 0.2552422881126404
Norm after input: 0.5210062861442566
Norm after each mp layer: 2.260019540786743
Norm after each mp layer: 14.51583194732666
Norm after each mp layer: 76.29468536376953
Norm before input: 0.2552422881126404
Norm after input: 0.521190345287323
Norm after each mp layer: 2.2620439529418945
Norm after each mp layer: 14.542428970336914
Norm after each mp layer: 76.383056640625
Epoch: 245, Loss: 0.0193, Energy: 87715.1797, Train: 99.75%, Valid: 73.20%, Test: 72.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.521190345287323
Norm after each mp layer: 2.2620439529418945
Norm after each mp layer: 14.542428970336914
Norm after each mp layer: 76.383056640625
Norm before input: 0.2552422881126404
Norm after input: 0.5209904909133911
Norm after each mp layer: 2.258953332901001
Norm after each mp layer: 14.515527725219727
Norm after each mp layer: 76.29051208496094
Norm before input: 0.2552422881126404
Norm after input: 0.5209904909133911
Norm after each mp layer: 2.258953332901001
Norm after each mp layer: 14.515527725219727
Norm after each mp layer: 76.29051208496094
Norm before input: 0.2552422881126404
Norm after input: 0.5212282538414001
Norm after each mp layer: 2.261429786682129
Norm after each mp layer: 14.547443389892578
Norm after each mp layer: 76.3883285522461
Norm before input: 0.2552422881126404
Norm after input: 0.5212282538414001
Norm after each mp layer: 2.261429786682129
Norm after each mp layer: 14.547443389892578
Norm after each mp layer: 76.3883285522461
Norm before input: 0.2552422881126404
Norm after input: 0.5211430191993713
Norm after each mp layer: 2.2600796222686768
Norm after each mp layer: 14.536811828613281
Norm after each mp layer: 76.33251190185547
Norm before input: 0.2552422881126404
Norm after input: 0.5211430191993713
Norm after each mp layer: 2.2600796222686768
Norm after each mp layer: 14.536811828613281
Norm after each mp layer: 76.33251190185547
Norm before input: 0.2552422881126404
Norm after input: 0.5210943222045898
Norm after each mp layer: 2.2586774826049805
Norm after each mp layer: 14.52797794342041
Norm after each mp layer: 76.29415893554688
Norm before input: 0.2552422881126404
Norm after input: 0.5210943222045898
Norm after each mp layer: 2.2586774826049805
Norm after each mp layer: 14.52797794342041
Norm after each mp layer: 76.29415893554688
Norm before input: 0.2552422881126404
Norm after input: 0.5213057398796082
Norm after each mp layer: 2.2604942321777344
Norm after each mp layer: 14.554183959960938
Norm after each mp layer: 76.3798828125
Epoch: 250, Loss: 0.0185, Energy: 85613.2891, Train: 99.83%, Valid: 73.00%, Test: 71.60%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5213057398796082
Norm after each mp layer: 2.2604942321777344
Norm after each mp layer: 14.554183959960938
Norm after each mp layer: 76.3798828125
Norm before input: 0.2552422881126404
Norm after input: 0.5211220979690552
Norm after each mp layer: 2.257993221282959
Norm after each mp layer: 14.530854225158691
Norm after each mp layer: 76.28956604003906
Norm before input: 0.2552422881126404
Norm after input: 0.5211220979690552
Norm after each mp layer: 2.257993221282959
Norm after each mp layer: 14.530854225158691
Norm after each mp layer: 76.28956604003906
Norm before input: 0.2552422881126404
Norm after input: 0.5213194489479065
Norm after each mp layer: 2.260249137878418
Norm after each mp layer: 14.55936336517334
Norm after each mp layer: 76.38336181640625
Norm before input: 0.2552422881126404
Norm after input: 0.5213194489479065
Norm after each mp layer: 2.260249137878418
Norm after each mp layer: 14.55936336517334
Norm after each mp layer: 76.38336181640625
Norm before input: 0.2552422881126404
Norm after input: 0.5212664604187012
Norm after each mp layer: 2.2585086822509766
Norm after each mp layer: 14.547778129577637
Norm after each mp layer: 76.33589935302734
Norm before input: 0.2552422881126404
Norm after input: 0.5212664604187012
Norm after each mp layer: 2.2585086822509766
Norm after each mp layer: 14.547778129577637
Norm after each mp layer: 76.33589935302734
Norm before input: 0.2552422881126404
Norm after input: 0.5212300419807434
Norm after each mp layer: 2.257321834564209
Norm after each mp layer: 14.539806365966797
Norm after each mp layer: 76.28702545166016
Norm before input: 0.2552422881126404
Norm after input: 0.5212300419807434
Norm after each mp layer: 2.257321834564209
Norm after each mp layer: 14.539806365966797
Norm after each mp layer: 76.28702545166016
Norm before input: 0.2552422881126404
Norm after input: 0.5213987827301025
Norm after each mp layer: 2.2590293884277344
Norm after each mp layer: 14.562252044677734
Norm after each mp layer: 76.34825897216797
Epoch: 255, Loss: 0.0178, Energy: 83700.8281, Train: 99.83%, Valid: 72.60%, Test: 71.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5213987827301025
Norm after each mp layer: 2.2590293884277344
Norm after each mp layer: 14.562252044677734
Norm after each mp layer: 76.34825897216797
Norm before input: 0.2552422881126404
Norm after input: 0.521263599395752
Norm after each mp layer: 2.2564239501953125
Norm after each mp layer: 14.540425300598145
Norm after each mp layer: 76.26951599121094
Norm before input: 0.2552422881126404
Norm after input: 0.521263599395752
Norm after each mp layer: 2.2564239501953125
Norm after each mp layer: 14.540425300598145
Norm after each mp layer: 76.26951599121094
Norm before input: 0.2552422881126404
Norm after input: 0.5214347839355469
Norm after each mp layer: 2.2583329677581787
Norm after each mp layer: 14.56476879119873
Norm after each mp layer: 76.34278106689453
Norm before input: 0.2552422881126404
Norm after input: 0.5214347839355469
Norm after each mp layer: 2.2583329677581787
Norm after each mp layer: 14.56476879119873
Norm after each mp layer: 76.34278106689453
Norm before input: 0.2552422881126404
Norm after input: 0.5213829278945923
Norm after each mp layer: 2.257280111312866
Norm after each mp layer: 14.55677318572998
Norm after each mp layer: 76.29673767089844
Norm before input: 0.2552422881126404
Norm after input: 0.5213829278945923
Norm after each mp layer: 2.257280111312866
Norm after each mp layer: 14.55677318572998
Norm after each mp layer: 76.29673767089844
Norm before input: 0.2552422881126404
Norm after input: 0.5213739275932312
Norm after each mp layer: 2.2561657428741455
Norm after each mp layer: 14.550997734069824
Norm after each mp layer: 76.26665496826172
Norm before input: 0.2552422881126404
Norm after input: 0.5213739275932312
Norm after each mp layer: 2.2561657428741455
Norm after each mp layer: 14.550997734069824
Norm after each mp layer: 76.26665496826172
Norm before input: 0.2552422881126404
Norm after input: 0.5214972496032715
Norm after each mp layer: 2.257037401199341
Norm after each mp layer: 14.566045761108398
Norm after each mp layer: 76.3128433227539
Epoch: 260, Loss: 0.0171, Energy: 81782.6406, Train: 99.83%, Valid: 72.20%, Test: 71.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5214972496032715
Norm after each mp layer: 2.257037401199341
Norm after each mp layer: 14.566045761108398
Norm after each mp layer: 76.3128433227539
Norm before input: 0.2552422881126404
Norm after input: 0.5213749408721924
Norm after each mp layer: 2.255152702331543
Norm after each mp layer: 14.549208641052246
Norm after each mp layer: 76.24562072753906
Norm before input: 0.2552422881126404
Norm after input: 0.5213749408721924
Norm after each mp layer: 2.255152702331543
Norm after each mp layer: 14.549208641052246
Norm after each mp layer: 76.24562072753906
Norm before input: 0.2552422881126404
Norm after input: 0.521533727645874
Norm after each mp layer: 2.256844997406006
Norm after each mp layer: 14.5718355178833
Norm after each mp layer: 76.31830596923828
Norm before input: 0.2552422881126404
Norm after input: 0.521533727645874
Norm after each mp layer: 2.256844997406006
Norm after each mp layer: 14.5718355178833
Norm after each mp layer: 76.31830596923828
Norm before input: 0.2552422881126404
Norm after input: 0.5214669704437256
Norm after each mp layer: 2.2551748752593994
Norm after each mp layer: 14.559435844421387
Norm after each mp layer: 76.27102661132812
Norm before input: 0.2552422881126404
Norm after input: 0.5214669704437256
Norm after each mp layer: 2.2551748752593994
Norm after each mp layer: 14.559435844421387
Norm after each mp layer: 76.27102661132812
Norm before input: 0.2552422881126404
Norm after input: 0.5215070843696594
Norm after each mp layer: 2.2550415992736816
Norm after each mp layer: 14.562517166137695
Norm after each mp layer: 76.26808166503906
Norm before input: 0.2552422881126404
Norm after input: 0.5215070843696594
Norm after each mp layer: 2.2550415992736816
Norm after each mp layer: 14.562517166137695
Norm after each mp layer: 76.26808166503906
Norm before input: 0.2552422881126404
Norm after input: 0.5215612053871155
Norm after each mp layer: 2.255000591278076
Norm after each mp layer: 14.566818237304688
Norm after each mp layer: 76.26641082763672
Epoch: 265, Loss: 0.0165, Energy: 80343.8359, Train: 99.92%, Valid: 72.40%, Test: 70.50%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5215612053871155
Norm after each mp layer: 2.255000591278076
Norm after each mp layer: 14.566818237304688
Norm after each mp layer: 76.26641082763672
Norm before input: 0.2552422881126404
Norm after input: 0.5215039849281311
Norm after each mp layer: 2.2535552978515625
Norm after each mp layer: 14.555951118469238
Norm after each mp layer: 76.21863555908203
Norm before input: 0.2552422881126404
Norm after input: 0.5215039849281311
Norm after each mp layer: 2.2535552978515625
Norm after each mp layer: 14.555951118469238
Norm after each mp layer: 76.21863555908203
Norm before input: 0.2552422881126404
Norm after input: 0.5216614007949829
Norm after each mp layer: 2.255307197570801
Norm after each mp layer: 14.57863712310791
Norm after each mp layer: 76.28926849365234
Norm before input: 0.2552422881126404
Norm after input: 0.5216614007949829
Norm after each mp layer: 2.255307197570801
Norm after each mp layer: 14.57863712310791
Norm after each mp layer: 76.28926849365234
Norm before input: 0.2552422881126404
Norm after input: 0.5215709805488586
Norm after each mp layer: 2.2532856464385986
Norm after each mp layer: 14.56151008605957
Norm after each mp layer: 76.2149887084961
Norm before input: 0.2552422881126404
Norm after input: 0.5215709805488586
Norm after each mp layer: 2.2532856464385986
Norm after each mp layer: 14.56151008605957
Norm after each mp layer: 76.2149887084961
Norm before input: 0.2552422881126404
Norm after input: 0.5216801166534424
Norm after each mp layer: 2.253934621810913
Norm after each mp layer: 14.573139190673828
Norm after each mp layer: 76.23609161376953
Norm before input: 0.2552422881126404
Norm after input: 0.5216801166534424
Norm after each mp layer: 2.253934621810913
Norm after each mp layer: 14.573139190673828
Norm after each mp layer: 76.23609161376953
Norm before input: 0.2552422881126404
Norm after input: 0.5216353535652161
Norm after each mp layer: 2.252549171447754
Norm after each mp layer: 14.562705993652344
Norm after each mp layer: 76.18572998046875
Epoch: 270, Loss: 0.0160, Energy: 78718.8828, Train: 99.92%, Valid: 72.00%, Test: 70.30%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5216353535652161
Norm after each mp layer: 2.252549171447754
Norm after each mp layer: 14.562705993652344
Norm after each mp layer: 76.18572998046875
Norm before input: 0.2552422881126404
Norm after input: 0.5216701626777649
Norm after each mp layer: 2.252373218536377
Norm after each mp layer: 14.565252304077148
Norm after each mp layer: 76.1854476928711
Norm before input: 0.2552422881126404
Norm after input: 0.5216701626777649
Norm after each mp layer: 2.252373218536377
Norm after each mp layer: 14.565252304077148
Norm after each mp layer: 76.1854476928711
Norm before input: 0.2552422881126404
Norm after input: 0.5217329263687134
Norm after each mp layer: 2.252701997756958
Norm after each mp layer: 14.572677612304688
Norm after each mp layer: 76.20101928710938
Norm before input: 0.2552422881126404
Norm after input: 0.5217329263687134
Norm after each mp layer: 2.252701997756958
Norm after each mp layer: 14.572677612304688
Norm after each mp layer: 76.20101928710938
Norm before input: 0.2552422881126404
Norm after input: 0.5216867923736572
Norm after each mp layer: 2.251458168029785
Norm after each mp layer: 14.563033103942871
Norm after each mp layer: 76.1548080444336
Norm before input: 0.2552422881126404
Norm after input: 0.5216867923736572
Norm after each mp layer: 2.251458168029785
Norm after each mp layer: 14.563033103942871
Norm after each mp layer: 76.1548080444336
Norm before input: 0.2552422881126404
Norm after input: 0.5217854976654053
Norm after each mp layer: 2.2519733905792236
Norm after each mp layer: 14.573135375976562
Norm after each mp layer: 76.17344665527344
Norm before input: 0.2552422881126404
Norm after input: 0.5217854976654053
Norm after each mp layer: 2.2519733905792236
Norm after each mp layer: 14.573135375976562
Norm after each mp layer: 76.17344665527344
Norm before input: 0.2552422881126404
Norm after input: 0.5217102766036987
Norm after each mp layer: 2.2500014305114746
Norm after each mp layer: 14.556289672851562
Norm after each mp layer: 76.09721374511719
Epoch: 275, Loss: 0.0154, Energy: 77079.3516, Train: 99.92%, Valid: 72.00%, Test: 70.30%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5217102766036987
Norm after each mp layer: 2.2500014305114746
Norm after each mp layer: 14.556291580200195
Norm after each mp layer: 76.09721374511719
Norm before input: 0.2552422881126404
Norm after input: 0.5218199491500854
Norm after each mp layer: 2.251039743423462
Norm after each mp layer: 14.570661544799805
Norm after each mp layer: 76.13288879394531
Norm before input: 0.2552422881126404
Norm after input: 0.5218199491500854
Norm after each mp layer: 2.251039743423462
Norm after each mp layer: 14.570661544799805
Norm after each mp layer: 76.13288879394531
Norm before input: 0.2552422881126404
Norm after input: 0.5217687487602234
Norm after each mp layer: 2.2496166229248047
Norm after each mp layer: 14.559481620788574
Norm after each mp layer: 76.08463287353516
Norm before input: 0.2552422881126404
Norm after input: 0.5217687487602234
Norm after each mp layer: 2.2496166229248047
Norm after each mp layer: 14.559481620788574
Norm after each mp layer: 76.08463287353516
Norm before input: 0.2552422881126404
Norm after input: 0.5218419432640076
Norm after each mp layer: 2.249905586242676
Norm after each mp layer: 14.566590309143066
Norm after each mp layer: 76.09422302246094
Norm before input: 0.2552422881126404
Norm after input: 0.5218419432640076
Norm after each mp layer: 2.249905586242676
Norm after each mp layer: 14.566590309143066
Norm after each mp layer: 76.09422302246094
Norm before input: 0.2552422881126404
Norm after input: 0.5218225121498108
Norm after each mp layer: 2.248871326446533
Norm after each mp layer: 14.55895709991455
Norm after each mp layer: 76.04825592041016
Norm before input: 0.2552422881126404
Norm after input: 0.5218225121498108
Norm after each mp layer: 2.248871326446533
Norm after each mp layer: 14.55895709991455
Norm after each mp layer: 76.04825592041016
Norm before input: 0.2552422881126404
Norm after input: 0.521852433681488
Norm after each mp layer: 2.248408079147339
Norm after each mp layer: 14.558180809020996
Norm after each mp layer: 76.0283432006836
Epoch: 280, Loss: 0.0149, Energy: 75095.0781, Train: 99.92%, Valid: 72.00%, Test: 69.90%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.521852433681488
Norm after each mp layer: 2.248408079147339
Norm after each mp layer: 14.558180809020996
Norm after each mp layer: 76.0283432006836
Norm before input: 0.2552422881126404
Norm after input: 0.5218897461891174
Norm after each mp layer: 2.248112201690674
Norm after each mp layer: 14.55918025970459
Norm after each mp layer: 76.01728820800781
Norm before input: 0.2552422881126404
Norm after input: 0.5218897461891174
Norm after each mp layer: 2.248112201690674
Norm after each mp layer: 14.55918025970459
Norm after each mp layer: 76.01728820800781
Norm before input: 0.2552422881126404
Norm after input: 0.5218822360038757
Norm after each mp layer: 2.247413158416748
Norm after each mp layer: 14.554713249206543
Norm after each mp layer: 75.98627471923828
Norm before input: 0.2552422881126404
Norm after input: 0.5218822360038757
Norm after each mp layer: 2.247413158416748
Norm after each mp layer: 14.554713249206543
Norm after each mp layer: 75.98627471923828
Norm before input: 0.2552422881126404
Norm after input: 0.5219503045082092
Norm after each mp layer: 2.247694253921509
Norm after each mp layer: 14.561192512512207
Norm after each mp layer: 75.99279022216797
Norm before input: 0.2552422881126404
Norm after input: 0.5219503045082092
Norm after each mp layer: 2.247694253921509
Norm after each mp layer: 14.561192512512207
Norm after each mp layer: 75.99279022216797
Norm before input: 0.2552422881126404
Norm after input: 0.521913468837738
Norm after each mp layer: 2.2462174892425537
Norm after each mp layer: 14.549580574035645
Norm after each mp layer: 75.93788146972656
Norm before input: 0.2552422881126404
Norm after input: 0.521913468837738
Norm after each mp layer: 2.2462174892425537
Norm after each mp layer: 14.549580574035645
Norm after each mp layer: 75.93788146972656
Norm before input: 0.2552422881126404
Norm after input: 0.522001326084137
Norm after each mp layer: 2.246742010116577
Norm after each mp layer: 14.558576583862305
Norm after each mp layer: 75.95092010498047
Epoch: 285, Loss: 0.0145, Energy: 73459.8828, Train: 99.92%, Valid: 71.80%, Test: 69.70%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.522001326084137
Norm after each mp layer: 2.246742010116577
Norm after each mp layer: 14.558576583862305
Norm after each mp layer: 75.95092010498047
Norm before input: 0.2552422881126404
Norm after input: 0.5219408273696899
Norm after each mp layer: 2.245074510574341
Norm after each mp layer: 14.544041633605957
Norm after each mp layer: 75.8839111328125
Norm before input: 0.2552422881126404
Norm after input: 0.5219408273696899
Norm after each mp layer: 2.245074510574341
Norm after each mp layer: 14.544041633605957
Norm after each mp layer: 75.8839111328125
Norm before input: 0.2552422881126404
Norm after input: 0.5220553278923035
Norm after each mp layer: 2.246021032333374
Norm after each mp layer: 14.55773639678955
Norm after each mp layer: 75.91622924804688
Norm before input: 0.2552422881126404
Norm after input: 0.5220553278923035
Norm after each mp layer: 2.246021032333374
Norm after each mp layer: 14.55773639678955
Norm after each mp layer: 75.91622924804688
Norm before input: 0.2552422881126404
Norm after input: 0.5219762325286865
Norm after each mp layer: 2.244076728820801
Norm after each mp layer: 14.54030704498291
Norm after each mp layer: 75.8414306640625
Norm before input: 0.2552422881126404
Norm after input: 0.5219762325286865
Norm after each mp layer: 2.244076728820801
Norm after each mp layer: 14.54030704498291
Norm after each mp layer: 75.8414306640625
Norm before input: 0.2552422881126404
Norm after input: 0.5221051573753357
Norm after each mp layer: 2.2451846599578857
Norm after each mp layer: 14.555465698242188
Norm after each mp layer: 75.87432098388672
Norm before input: 0.2552422881126404
Norm after input: 0.5221051573753357
Norm after each mp layer: 2.2451846599578857
Norm after each mp layer: 14.555465698242188
Norm after each mp layer: 75.87432098388672
Norm before input: 0.2552422881126404
Norm after input: 0.5220018029212952
Norm after each mp layer: 2.242729663848877
Norm after each mp layer: 14.532706260681152
Norm after each mp layer: 75.77816009521484
Epoch: 290, Loss: 0.0141, Energy: 72160.3203, Train: 99.92%, Valid: 71.00%, Test: 69.60%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5220018029212952
Norm after each mp layer: 2.242729663848877
Norm after each mp layer: 14.532706260681152
Norm after each mp layer: 75.77816009521484
Norm before input: 0.2552422881126404
Norm after input: 0.5221506357192993
Norm after each mp layer: 2.244231700897217
Norm after each mp layer: 14.551909446716309
Norm after each mp layer: 75.82740020751953
Norm before input: 0.2552422881126404
Norm after input: 0.5221506357192993
Norm after each mp layer: 2.244231700897217
Norm after each mp layer: 14.551909446716309
Norm after each mp layer: 75.82740020751953
Norm before input: 0.2552422881126404
Norm after input: 0.5220345854759216
Norm after each mp layer: 2.241708993911743
Norm after each mp layer: 14.527937889099121
Norm after each mp layer: 75.72865295410156
Norm before input: 0.2552422881126404
Norm after input: 0.5220345854759216
Norm after each mp layer: 2.241708993911743
Norm after each mp layer: 14.527937889099121
Norm after each mp layer: 75.72865295410156
Norm before input: 0.2552422881126404
Norm after input: 0.5222048759460449
Norm after each mp layer: 2.2434918880462646
Norm after each mp layer: 14.550126075744629
Norm after each mp layer: 75.78654479980469
Norm before input: 0.2552422881126404
Norm after input: 0.5222048759460449
Norm after each mp layer: 2.2434918880462646
Norm after each mp layer: 14.550126075744629
Norm after each mp layer: 75.78654479980469
Norm before input: 0.2552422881126404
Norm after input: 0.5220736861228943
Norm after each mp layer: 2.2406625747680664
Norm after each mp layer: 14.522880554199219
Norm after each mp layer: 75.67485046386719
Norm before input: 0.2552422881126404
Norm after input: 0.5220736861228943
Norm after each mp layer: 2.2406625747680664
Norm after each mp layer: 14.522880554199219
Norm after each mp layer: 75.67485046386719
Norm before input: 0.2552422881126404
Norm after input: 0.5222387909889221
Norm after each mp layer: 2.2423226833343506
Norm after each mp layer: 14.54375171661377
Norm after each mp layer: 75.72660827636719
Epoch: 295, Loss: 0.0137, Energy: 70255.0625, Train: 99.92%, Valid: 71.20%, Test: 69.70%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5222387909889221
Norm after each mp layer: 2.2423226833343506
Norm after each mp layer: 14.54375171661377
Norm after each mp layer: 75.72660827636719
Norm before input: 0.2552422881126404
Norm after input: 0.5221181511878967
Norm after each mp layer: 2.239628553390503
Norm after each mp layer: 14.517899513244629
Norm after each mp layer: 75.61940002441406
Norm before input: 0.2552422881126404
Norm after input: 0.5221181511878967
Norm after each mp layer: 2.239628553390503
Norm after each mp layer: 14.517899513244629
Norm after each mp layer: 75.61940002441406
Norm before input: 0.2552422881126404
Norm after input: 0.5222654938697815
Norm after each mp layer: 2.2411348819732666
Norm after each mp layer: 14.536876678466797
Norm after each mp layer: 75.66748809814453
Norm before input: 0.2552422881126404
Norm after input: 0.5222654938697815
Norm after each mp layer: 2.2411348819732666
Norm after each mp layer: 14.536876678466797
Norm after each mp layer: 75.66748809814453
Norm before input: 0.2552422881126404
Norm after input: 0.5221742391586304
Norm after each mp layer: 2.238912582397461
Norm after each mp layer: 14.516036987304688
Norm after each mp layer: 75.5789794921875
Norm before input: 0.2552422881126404
Norm after input: 0.5221742391586304
Norm after each mp layer: 2.238912582397461
Norm after each mp layer: 14.516036987304688
Norm after each mp layer: 75.5789794921875
Norm before input: 0.2552422881126404
Norm after input: 0.5222861766815186
Norm after each mp layer: 2.2397751808166504
Norm after each mp layer: 14.528084754943848
Norm after each mp layer: 75.59951782226562
Norm before input: 0.2552422881126404
Norm after input: 0.5222861766815186
Norm after each mp layer: 2.2397751808166504
Norm after each mp layer: 14.528084754943848
Norm after each mp layer: 75.59951782226562
Norm before input: 0.2552422881126404
Norm after input: 0.5222296118736267
Norm after each mp layer: 2.238039493560791
Norm after each mp layer: 14.512171745300293
Norm after each mp layer: 75.52422332763672
Epoch: 300, Loss: 0.0133, Energy: 69088.3281, Train: 99.92%, Valid: 71.00%, Test: 69.50%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5222296118736267
Norm after each mp layer: 2.238039493560791
Norm after each mp layer: 14.512171745300293
Norm after each mp layer: 75.52422332763672
Norm before input: 0.2552422881126404
Norm after input: 0.5223085880279541
Norm after each mp layer: 2.238405704498291
Norm after each mp layer: 14.518909454345703
Norm after each mp layer: 75.52819061279297
Norm before input: 0.2552422881126404
Norm after input: 0.5223085880279541
Norm after each mp layer: 2.238405704498291
Norm after each mp layer: 14.518909454345703
Norm after each mp layer: 75.52819061279297
Norm before input: 0.2552422881126404
Norm after input: 0.5222872495651245
Norm after each mp layer: 2.2372443675994873
Norm after each mp layer: 14.509194374084473
Norm after each mp layer: 75.47677612304688
Norm before input: 0.2552422881126404
Norm after input: 0.5222872495651245
Norm after each mp layer: 2.2372443675994873
Norm after each mp layer: 14.509194374084473
Norm after each mp layer: 75.47677612304688
Norm before input: 0.2552422881126404
Norm after input: 0.5223392248153687
Norm after each mp layer: 2.2372939586639404
Norm after each mp layer: 14.512145042419434
Norm after each mp layer: 75.46871185302734
Norm before input: 0.2552422881126404
Norm after input: 0.5223392248153687
Norm after each mp layer: 2.2372939586639404
Norm after each mp layer: 14.512145042419434
Norm after each mp layer: 75.46871185302734
Norm before input: 0.2552422881126404
Norm after input: 0.5223338603973389
Norm after each mp layer: 2.2363712787628174
Norm after each mp layer: 14.504968643188477
Norm after each mp layer: 75.42633056640625
Norm before input: 0.2552422881126404
Norm after input: 0.5223338603973389
Norm after each mp layer: 2.2363712787628174
Norm after each mp layer: 14.504968643188477
Norm after each mp layer: 75.42633056640625
Norm before input: 0.2552422881126404
Norm after input: 0.5223703980445862
Norm after each mp layer: 2.2360689640045166
Norm after each mp layer: 14.504488945007324
Norm after each mp layer: 75.40640258789062
Epoch: 305, Loss: 0.0129, Energy: 67503.3047, Train: 99.92%, Valid: 70.80%, Test: 69.70%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5223703980445862
Norm after each mp layer: 2.2360689640045166
Norm after each mp layer: 14.504488945007324
Norm after each mp layer: 75.40640258789062
Norm before input: 0.2552422881126404
Norm after input: 0.5223764181137085
Norm after each mp layer: 2.2353427410125732
Norm after each mp layer: 14.499177932739258
Norm after each mp layer: 75.36958312988281
Norm before input: 0.2552422881126404
Norm after input: 0.5223764181137085
Norm after each mp layer: 2.2353427410125732
Norm after each mp layer: 14.499177932739258
Norm after each mp layer: 75.36958312988281
Norm before input: 0.2552422881126404
Norm after input: 0.5224077105522156
Norm after each mp layer: 2.235016107559204
Norm after each mp layer: 14.498191833496094
Norm after each mp layer: 75.34827423095703
Norm before input: 0.2552422881126404
Norm after input: 0.5224077105522156
Norm after each mp layer: 2.235016107559204
Norm after each mp layer: 14.498191833496094
Norm after each mp layer: 75.34827423095703
Norm before input: 0.2552422881126404
Norm after input: 0.5224194526672363
Norm after each mp layer: 2.2343621253967285
Norm after each mp layer: 14.493708610534668
Norm after each mp layer: 75.31476593017578
Norm before input: 0.2552422881126404
Norm after input: 0.5224194526672363
Norm after each mp layer: 2.2343621253967285
Norm after each mp layer: 14.493708610534668
Norm after each mp layer: 75.31476593017578
Norm before input: 0.2552422881126404
Norm after input: 0.5224502682685852
Norm after each mp layer: 2.2340192794799805
Norm after each mp layer: 14.492335319519043
Norm after each mp layer: 75.2905044555664
Norm before input: 0.2552422881126404
Norm after input: 0.5224502682685852
Norm after each mp layer: 2.2340192794799805
Norm after each mp layer: 14.492335319519043
Norm after each mp layer: 75.2905044555664
Norm before input: 0.2552422881126404
Norm after input: 0.522459089756012
Norm after each mp layer: 2.2332727909088135
Norm after each mp layer: 14.486759185791016
Norm after each mp layer: 75.25128936767578
Epoch: 310, Loss: 0.0126, Energy: 66207.6172, Train: 99.92%, Valid: 70.60%, Test: 69.80%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.522459089756012
Norm after each mp layer: 2.2332727909088135
Norm after each mp layer: 14.486759185791016
Norm after each mp layer: 75.25128936767578
Norm before input: 0.2552422881126404
Norm after input: 0.522494912147522
Norm after each mp layer: 2.232957124710083
Norm after each mp layer: 14.485807418823242
Norm after each mp layer: 75.22830963134766
Norm before input: 0.2552422881126404
Norm after input: 0.522494912147522
Norm after each mp layer: 2.232957124710083
Norm after each mp layer: 14.485807418823242
Norm after each mp layer: 75.22830963134766
Norm before input: 0.2552422881126404
Norm after input: 0.5224940776824951
Norm after each mp layer: 2.2321128845214844
Norm after each mp layer: 14.478960037231445
Norm after each mp layer: 75.18529510498047
Norm before input: 0.2552422881126404
Norm after input: 0.5224940776824951
Norm after each mp layer: 2.2321128845214844
Norm after each mp layer: 14.478960037231445
Norm after each mp layer: 75.18529510498047
Norm before input: 0.2552422881126404
Norm after input: 0.5225453972816467
Norm after each mp layer: 2.232095718383789
Norm after each mp layer: 14.481022834777832
Norm after each mp layer: 75.1733627319336
Norm before input: 0.2552422881126404
Norm after input: 0.5225453972816467
Norm after each mp layer: 2.232095718383789
Norm after each mp layer: 14.481022834777832
Norm after each mp layer: 75.1733627319336
Norm before input: 0.2552422881126404
Norm after input: 0.522523045539856
Norm after each mp layer: 2.230863571166992
Norm after each mp layer: 14.470183372497559
Norm after each mp layer: 75.11735534667969
Norm before input: 0.2552422881126404
Norm after input: 0.522523045539856
Norm after each mp layer: 2.230863571166992
Norm after each mp layer: 14.470183372497559
Norm after each mp layer: 75.11735534667969
Norm before input: 0.2552422881126404
Norm after input: 0.5226054191589355
Norm after each mp layer: 2.2313382625579834
Norm after each mp layer: 14.477459907531738
Norm after each mp layer: 75.12310791015625
Epoch: 315, Loss: 0.0123, Energy: 64848.5312, Train: 99.92%, Valid: 71.00%, Test: 69.80%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5226054191589355
Norm after each mp layer: 2.2313382625579834
Norm after each mp layer: 14.477459907531738
Norm after each mp layer: 75.12310791015625
Norm before input: 0.2552422881126404
Norm after input: 0.5225311517715454
Norm after each mp layer: 2.2292652130126953
Norm after each mp layer: 14.457475662231445
Norm after each mp layer: 75.03523254394531
Norm before input: 0.2552422881126404
Norm after input: 0.5225311517715454
Norm after each mp layer: 2.2292652130126953
Norm after each mp layer: 14.457475662231445
Norm after each mp layer: 75.03523254394531
Norm before input: 0.2552422881126404
Norm after input: 0.5226931571960449
Norm after each mp layer: 2.2310550212860107
Norm after each mp layer: 14.478828430175781
Norm after each mp layer: 75.09136199951172
Norm before input: 0.2552422881126404
Norm after input: 0.5226931571960449
Norm after each mp layer: 2.2310550212860107
Norm after each mp layer: 14.478828430175781
Norm after each mp layer: 75.09136199951172
Norm before input: 0.2552422881126404
Norm after input: 0.5225099921226501
Norm after each mp layer: 2.227280616760254
Norm after each mp layer: 14.440474510192871
Norm after each mp layer: 74.94247436523438
Norm before input: 0.2552422881126404
Norm after input: 0.5225099921226501
Norm after each mp layer: 2.227280616760254
Norm after each mp layer: 14.440474510192871
Norm after each mp layer: 74.94247436523438
Norm before input: 0.2552422881126404
Norm after input: 0.5228292942047119
Norm after each mp layer: 2.231628894805908
Norm after each mp layer: 14.489385604858398
Norm after each mp layer: 75.09751892089844
Norm before input: 0.2552422881126404
Norm after input: 0.5228292942047119
Norm after each mp layer: 2.231628894805908
Norm after each mp layer: 14.489385604858398
Norm after each mp layer: 75.09751892089844
Norm before input: 0.2552422881126404
Norm after input: 0.5224809646606445
Norm after each mp layer: 2.2253143787384033
Norm after each mp layer: 14.423175811767578
Norm after each mp layer: 74.8525390625
Epoch: 320, Loss: 0.0125, Energy: 64226.7383, Train: 99.83%, Valid: 70.20%, Test: 69.20%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5224809646606445
Norm after each mp layer: 2.2253143787384033
Norm after each mp layer: 14.423175811767578
Norm after each mp layer: 74.8525390625
Norm before input: 0.2552422881126404
Norm after input: 0.5229018926620483
Norm after each mp layer: 2.2311484813690186
Norm after each mp layer: 14.48851203918457
Norm after each mp layer: 75.06534576416016
Norm before input: 0.2552422881126404
Norm after input: 0.5229018926620483
Norm after each mp layer: 2.2311487197875977
Norm after each mp layer: 14.48851203918457
Norm after each mp layer: 75.06534576416016
Norm before input: 0.2552422881126404
Norm after input: 0.5225736498832703
Norm after each mp layer: 2.2252461910247803
Norm after each mp layer: 14.425728797912598
Norm after each mp layer: 74.82559967041016
Norm before input: 0.2552422881126404
Norm after input: 0.5225736498832703
Norm after each mp layer: 2.2252461910247803
Norm after each mp layer: 14.425728797912598
Norm after each mp layer: 74.82559967041016
Norm before input: 0.2552422881126404
Norm after input: 0.5227474570274353
Norm after each mp layer: 2.2270376682281494
Norm after each mp layer: 14.446219444274902
Norm after each mp layer: 74.86495208740234
Norm before input: 0.2552422881126404
Norm after input: 0.5227474570274353
Norm after each mp layer: 2.2270376682281494
Norm after each mp layer: 14.446219444274902
Norm after each mp layer: 74.86495208740234
Norm before input: 0.2552422881126404
Norm after input: 0.5227545499801636
Norm after each mp layer: 2.2259140014648438
Norm after each mp layer: 14.436495780944824
Norm after each mp layer: 74.80184173583984
Norm before input: 0.2552422881126404
Norm after input: 0.5227545499801636
Norm after each mp layer: 2.2259140014648438
Norm after each mp layer: 14.436495780944824
Norm after each mp layer: 74.80184173583984
Norm before input: 0.2552422881126404
Norm after input: 0.5226424932479858
Norm after each mp layer: 2.2233223915100098
Norm after each mp layer: 14.409841537475586
Norm after each mp layer: 74.68680572509766
Epoch: 325, Loss: 0.0117, Energy: 62546.6914, Train: 99.92%, Valid: 70.20%, Test: 69.20%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5226424932479858
Norm after each mp layer: 2.2233223915100098
Norm after each mp layer: 14.409841537475586
Norm after each mp layer: 74.68680572509766
Norm before input: 0.2552422881126404
Norm after input: 0.5229284167289734
Norm after each mp layer: 2.2271816730499268
Norm after each mp layer: 14.452771186828613
Norm after each mp layer: 74.81349182128906
Norm before input: 0.2552422881126404
Norm after input: 0.5229284167289734
Norm after each mp layer: 2.2271816730499268
Norm after each mp layer: 14.452771186828613
Norm after each mp layer: 74.81349182128906
Norm before input: 0.2552422881126404
Norm after input: 0.5226759910583496
Norm after each mp layer: 2.222153902053833
Norm after each mp layer: 14.400093078613281
Norm after each mp layer: 74.60287475585938
Norm before input: 0.2552422881126404
Norm after input: 0.5226759910583496
Norm after each mp layer: 2.222153902053833
Norm after each mp layer: 14.400093078613281
Norm after each mp layer: 74.60287475585938
Norm before input: 0.2552422881126404
Norm after input: 0.5229122638702393
Norm after each mp layer: 2.22506046295166
Norm after each mp layer: 14.432418823242188
Norm after each mp layer: 74.68603515625
Norm before input: 0.2552422881126404
Norm after input: 0.5229122638702393
Norm after each mp layer: 2.22506046295166
Norm after each mp layer: 14.432418823242188
Norm after each mp layer: 74.68603515625
Norm before input: 0.2552422881126404
Norm after input: 0.5228197574615479
Norm after each mp layer: 2.222731828689575
Norm after each mp layer: 14.408825874328613
Norm after each mp layer: 74.58438873291016
Norm before input: 0.2552422881126404
Norm after input: 0.5228197574615479
Norm after each mp layer: 2.222731828689575
Norm after each mp layer: 14.408825874328613
Norm after each mp layer: 74.58438873291016
Norm before input: 0.2552422881126404
Norm after input: 0.5227953791618347
Norm after each mp layer: 2.221508026123047
Norm after each mp layer: 14.397905349731445
Norm after each mp layer: 74.53460693359375
Epoch: 330, Loss: 0.0114, Energy: 61123.8633, Train: 99.92%, Valid: 69.80%, Test: 69.30%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5227953791618347
Norm after each mp layer: 2.221508026123047
Norm after each mp layer: 14.397905349731445
Norm after each mp layer: 74.53460693359375
Norm before input: 0.2552422881126404
Norm after input: 0.5229525566101074
Norm after each mp layer: 2.223515272140503
Norm after each mp layer: 14.421000480651855
Norm after each mp layer: 74.6063232421875
Norm before input: 0.2552422881126404
Norm after input: 0.5229525566101074
Norm after each mp layer: 2.223515272140503
Norm after each mp layer: 14.421000480651855
Norm after each mp layer: 74.6063232421875
Norm before input: 0.2552422881126404
Norm after input: 0.5227503180503845
Norm after each mp layer: 2.2197182178497314
Norm after each mp layer: 14.381529808044434
Norm after each mp layer: 74.46097564697266
Norm before input: 0.2552422881126404
Norm after input: 0.5227503180503845
Norm after each mp layer: 2.2197182178497314
Norm after each mp layer: 14.381529808044434
Norm after each mp layer: 74.46097564697266
Norm before input: 0.2552422881126404
Norm after input: 0.523030161857605
Norm after each mp layer: 2.2234113216400146
Norm after each mp layer: 14.423494338989258
Norm after each mp layer: 74.59388732910156
Norm before input: 0.2552422881126404
Norm after input: 0.523030161857605
Norm after each mp layer: 2.2234113216400146
Norm after each mp layer: 14.423494338989258
Norm after each mp layer: 74.59388732910156
Norm before input: 0.2552422881126404
Norm after input: 0.5228362679481506
Norm after each mp layer: 2.219214677810669
Norm after each mp layer: 14.379353523254395
Norm after each mp layer: 74.40902709960938
Norm before input: 0.2552422881126404
Norm after input: 0.5228362679481506
Norm after each mp layer: 2.219214677810669
Norm after each mp layer: 14.379353523254395
Norm after each mp layer: 74.40902709960938
Norm before input: 0.2552422881126404
Norm after input: 0.5229814648628235
Norm after each mp layer: 2.220423698425293
Norm after each mp layer: 14.393033981323242
Norm after each mp layer: 74.4189453125
Epoch: 335, Loss: 0.0113, Energy: 60085.8320, Train: 99.92%, Valid: 70.20%, Test: 69.30%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5229814648628235
Norm after each mp layer: 2.220423698425293
Norm after each mp layer: 14.393033981323242
Norm after each mp layer: 74.4189453125
Norm before input: 0.2552422881126404
Norm after input: 0.5229708552360535
Norm after each mp layer: 2.2191500663757324
Norm after each mp layer: 14.380732536315918
Norm after each mp layer: 74.34748840332031
Norm before input: 0.2552422881126404
Norm after input: 0.5229708552360535
Norm after each mp layer: 2.2191500663757324
Norm after each mp layer: 14.380732536315918
Norm after each mp layer: 74.34748840332031
Norm before input: 0.2552422881126404
Norm after input: 0.5229238271713257
Norm after each mp layer: 2.2176496982574463
Norm after each mp layer: 14.365564346313477
Norm after each mp layer: 74.27640533447266
Norm before input: 0.2552422881126404
Norm after input: 0.5229238271713257
Norm after each mp layer: 2.2176496982574463
Norm after each mp layer: 14.365564346313477
Norm after each mp layer: 74.27640533447266
Norm before input: 0.2552422881126404
Norm after input: 0.5231056809425354
Norm after each mp layer: 2.219923496246338
Norm after each mp layer: 14.391132354736328
Norm after each mp layer: 74.34955596923828
Norm before input: 0.2552422881126404
Norm after input: 0.5231056809425354
Norm after each mp layer: 2.219923496246338
Norm after each mp layer: 14.391132354736328
Norm after each mp layer: 74.34955596923828
Norm before input: 0.2552422881126404
Norm after input: 0.522948682308197
Norm after each mp layer: 2.216398000717163
Norm after each mp layer: 14.354314804077148
Norm after each mp layer: 74.19602966308594
Norm before input: 0.2552422881126404
Norm after input: 0.522948682308197
Norm after each mp layer: 2.216398000717163
Norm after each mp layer: 14.354314804077148
Norm after each mp layer: 74.19602966308594
Norm before input: 0.2552422881126404
Norm after input: 0.5231420397758484
Norm after each mp layer: 2.218674659729004
Norm after each mp layer: 14.379406929016113
Norm after each mp layer: 74.25625610351562
Epoch: 340, Loss: 0.0111, Energy: 58783.0195, Train: 99.92%, Valid: 70.40%, Test: 69.60%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5231420397758484
Norm after each mp layer: 2.218674659729004
Norm after each mp layer: 14.379406929016113
Norm after each mp layer: 74.25625610351562
Norm before input: 0.2552422881126404
Norm after input: 0.5230215191841125
Norm after each mp layer: 2.2157046794891357
Norm after each mp layer: 14.34812068939209
Norm after each mp layer: 74.11812591552734
Norm before input: 0.2552422881126404
Norm after input: 0.5230215191841125
Norm after each mp layer: 2.2157046794891357
Norm after each mp layer: 14.34812068939209
Norm after each mp layer: 74.11812591552734
Norm before input: 0.2552422881126404
Norm after input: 0.5231027603149414
Norm after each mp layer: 2.216116428375244
Norm after each mp layer: 14.354374885559082
Norm after each mp layer: 74.12384796142578
Norm before input: 0.2552422881126404
Norm after input: 0.5231027603149414
Norm after each mp layer: 2.216116428375244
Norm after each mp layer: 14.354374885559082
Norm after each mp layer: 74.12384796142578
Norm before input: 0.2552422881126404
Norm after input: 0.52312833070755
Norm after each mp layer: 2.215920925140381
Norm after each mp layer: 14.353753089904785
Norm after each mp layer: 74.1139144897461
Norm before input: 0.2552422881126404
Norm after input: 0.52312833070755
Norm after each mp layer: 2.215920925140381
Norm after each mp layer: 14.353753089904785
Norm after each mp layer: 74.1139144897461
Norm before input: 0.2552422881126404
Norm after input: 0.5230584144592285
Norm after each mp layer: 2.2142529487609863
Norm after each mp layer: 14.336580276489258
Norm after each mp layer: 74.04395294189453
Norm before input: 0.2552422881126404
Norm after input: 0.5230584144592285
Norm after each mp layer: 2.2142529487609863
Norm after each mp layer: 14.336580276489258
Norm after each mp layer: 74.04395294189453
Norm before input: 0.2552422881126404
Norm after input: 0.5231958031654358
Norm after each mp layer: 2.215667247772217
Norm after each mp layer: 14.353026390075684
Norm after each mp layer: 74.08386993408203
Epoch: 345, Loss: 0.0107, Energy: 57872.6562, Train: 99.92%, Valid: 70.40%, Test: 69.50%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5231958031654358
Norm after each mp layer: 2.215667247772217
Norm after each mp layer: 14.353026390075684
Norm after each mp layer: 74.08386993408203
Norm before input: 0.2552422881126404
Norm after input: 0.52308189868927
Norm after each mp layer: 2.2126305103302
Norm after each mp layer: 14.321528434753418
Norm after each mp layer: 73.94569396972656
Norm before input: 0.2552422881126404
Norm after input: 0.52308189868927
Norm after each mp layer: 2.2126305103302
Norm after each mp layer: 14.321528434753418
Norm after each mp layer: 73.94569396972656
Norm before input: 0.2552422881126404
Norm after input: 0.5232462882995605
Norm after each mp layer: 2.214370012283325
Norm after each mp layer: 14.340928077697754
Norm after each mp layer: 73.98750305175781
Norm before input: 0.2552422881126404
Norm after input: 0.5232462882995605
Norm after each mp layer: 2.214370012283325
Norm after each mp layer: 14.340928077697754
Norm after each mp layer: 73.98750305175781
Norm before input: 0.2552422881126404
Norm after input: 0.5231641530990601
Norm after each mp layer: 2.211981773376465
Norm after each mp layer: 14.31588363647461
Norm after each mp layer: 73.8722152709961
Norm before input: 0.2552422881126404
Norm after input: 0.5231641530990601
Norm after each mp layer: 2.211981773376465
Norm after each mp layer: 14.31588363647461
Norm after each mp layer: 73.8722152709961
Norm before input: 0.2552422881126404
Norm after input: 0.5232692360877991
Norm after each mp layer: 2.2128422260284424
Norm after each mp layer: 14.32569694519043
Norm after each mp layer: 73.88365936279297
Norm before input: 0.2552422881126404
Norm after input: 0.5232692360877991
Norm after each mp layer: 2.2128422260284424
Norm after each mp layer: 14.32569694519043
Norm after each mp layer: 73.88365936279297
Norm before input: 0.2552422881126404
Norm after input: 0.5232563614845276
Norm after each mp layer: 2.211778163909912
Norm after each mp layer: 14.31472396850586
Norm after each mp layer: 73.82367706298828
Epoch: 350, Loss: 0.0105, Energy: 56845.3984, Train: 99.92%, Valid: 70.20%, Test: 69.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5232563614845276
Norm after each mp layer: 2.211778163909912
Norm after each mp layer: 14.31472396850586
Norm after each mp layer: 73.82367706298828
Norm before input: 0.2552422881126404
Norm after input: 0.5232623219490051
Norm after each mp layer: 2.2109525203704834
Norm after each mp layer: 14.306821823120117
Norm after each mp layer: 73.77667999267578
Norm before input: 0.2552422881126404
Norm after input: 0.5232623219490051
Norm after each mp layer: 2.2109525203704834
Norm after each mp layer: 14.306821823120117
Norm after each mp layer: 73.77667999267578
Norm before input: 0.2552422881126404
Norm after input: 0.5233279466629028
Norm after each mp layer: 2.2110698223114014
Norm after each mp layer: 14.309213638305664
Norm after each mp layer: 73.76539611816406
Norm before input: 0.2552422881126404
Norm after input: 0.5233279466629028
Norm after each mp layer: 2.2110698223114014
Norm after each mp layer: 14.309213638305664
Norm after each mp layer: 73.76539611816406
Norm before input: 0.2552422881126404
Norm after input: 0.5232731699943542
Norm after each mp layer: 2.209237575531006
Norm after each mp layer: 14.29007339477539
Norm after each mp layer: 73.67585754394531
Norm before input: 0.2552422881126404
Norm after input: 0.5232731699943542
Norm after each mp layer: 2.209237575531006
Norm after each mp layer: 14.29007339477539
Norm after each mp layer: 73.67585754394531
Norm before input: 0.2552422881126404
Norm after input: 0.5233921408653259
Norm after each mp layer: 2.2104640007019043
Norm after each mp layer: 14.303892135620117
Norm after each mp layer: 73.70753479003906
Norm before input: 0.2552422881126404
Norm after input: 0.5233921408653259
Norm after each mp layer: 2.2104640007019043
Norm after each mp layer: 14.303892135620117
Norm after each mp layer: 73.70753479003906
Norm before input: 0.2552422881126404
Norm after input: 0.5233086347579956
Norm after each mp layer: 2.2081692218780518
Norm after each mp layer: 14.27989673614502
Norm after each mp layer: 73.60282135009766
Epoch: 355, Loss: 0.0103, Energy: 55841.7461, Train: 99.92%, Valid: 70.20%, Test: 68.80%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5233086347579956
Norm after each mp layer: 2.2081692218780518
Norm after each mp layer: 14.27989673614502
Norm after each mp layer: 73.60282135009766
Norm before input: 0.2552422881126404
Norm after input: 0.5234419107437134
Norm after each mp layer: 2.209510326385498
Norm after each mp layer: 14.29492473602295
Norm after each mp layer: 73.63581085205078
Norm before input: 0.2552422881126404
Norm after input: 0.5234419107437134
Norm after each mp layer: 2.209510326385498
Norm after each mp layer: 14.29492473602295
Norm after each mp layer: 73.63581085205078
Norm before input: 0.2552422881126404
Norm after input: 0.5233634114265442
Norm after each mp layer: 2.2071874141693115
Norm after each mp layer: 14.270540237426758
Norm after each mp layer: 73.5265884399414
Norm before input: 0.2552422881126404
Norm after input: 0.5233634114265442
Norm after each mp layer: 2.2071874141693115
Norm after each mp layer: 14.270540237426758
Norm after each mp layer: 73.5265884399414
Norm before input: 0.2552422881126404
Norm after input: 0.5234585404396057
Norm after each mp layer: 2.2079296112060547
Norm after each mp layer: 14.278885841369629
Norm after each mp layer: 73.53579711914062
Norm before input: 0.2552422881126404
Norm after input: 0.5234585404396057
Norm after each mp layer: 2.2079296112060547
Norm after each mp layer: 14.278885841369629
Norm after each mp layer: 73.53579711914062
Norm before input: 0.2552422881126404
Norm after input: 0.5234225392341614
Norm after each mp layer: 2.2064149379730225
Norm after each mp layer: 14.262861251831055
Norm after each mp layer: 73.4569320678711
Norm before input: 0.2552422881126404
Norm after input: 0.5234225392341614
Norm after each mp layer: 2.2064149379730225
Norm after each mp layer: 14.262861251831055
Norm after each mp layer: 73.4569320678711
Norm before input: 0.2552422881126404
Norm after input: 0.523485541343689
Norm after each mp layer: 2.206559419631958
Norm after each mp layer: 14.264927864074707
Norm after each mp layer: 73.44358825683594
Epoch: 360, Loss: 0.0101, Energy: 54734.7461, Train: 99.92%, Valid: 69.80%, Test: 68.90%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.523485541343689
Norm after each mp layer: 2.206559419631958
Norm after each mp layer: 14.264927864074707
Norm after each mp layer: 73.44358825683594
Norm before input: 0.2552422881126404
Norm after input: 0.5235018730163574
Norm after each mp layer: 2.2058606147766113
Norm after each mp layer: 14.257940292358398
Norm after each mp layer: 73.39662170410156
Norm before input: 0.2552422881126404
Norm after input: 0.5235018730163574
Norm after each mp layer: 2.2058606147766113
Norm after each mp layer: 14.257940292358398
Norm after each mp layer: 73.39662170410156
Norm before input: 0.2552422881126404
Norm after input: 0.5235196948051453
Norm after each mp layer: 2.205166816711426
Norm after each mp layer: 14.2507963180542
Norm after each mp layer: 73.34711456298828
Norm before input: 0.2552422881126404
Norm after input: 0.5235196948051453
Norm after each mp layer: 2.205166816711426
Norm after each mp layer: 14.2507963180542
Norm after each mp layer: 73.34711456298828
Norm before input: 0.2552422881126404
Norm after input: 0.5235729217529297
Norm after each mp layer: 2.205077648162842
Norm after each mp layer: 14.250250816345215
Norm after each mp layer: 73.32252502441406
Norm before input: 0.2552422881126404
Norm after input: 0.5235729217529297
Norm after each mp layer: 2.205077648162842
Norm after each mp layer: 14.250250816345215
Norm after each mp layer: 73.32252502441406
Norm before input: 0.2552422881126404
Norm after input: 0.5235529541969299
Norm after each mp layer: 2.203813314437866
Norm after each mp layer: 14.237130165100098
Norm after each mp layer: 73.2568359375
Norm before input: 0.2552422881126404
Norm after input: 0.5235529541969299
Norm after each mp layer: 2.203813314437866
Norm after each mp layer: 14.237130165100098
Norm after each mp layer: 73.2568359375
Norm before input: 0.2552422881126404
Norm after input: 0.5236308574676514
Norm after each mp layer: 2.204380750656128
Norm after each mp layer: 14.243673324584961
Norm after each mp layer: 73.2667465209961
Epoch: 365, Loss: 0.0099, Energy: 53771.1406, Train: 99.92%, Valid: 70.00%, Test: 69.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5236308574676514
Norm after each mp layer: 2.204380750656128
Norm after each mp layer: 14.243673324584961
Norm after each mp layer: 73.2667465209961
Norm before input: 0.2552422881126404
Norm after input: 0.5235821604728699
Norm after each mp layer: 2.202674388885498
Norm after each mp layer: 14.225817680358887
Norm after each mp layer: 73.18717193603516
Norm before input: 0.2552422881126404
Norm after input: 0.5235821604728699
Norm after each mp layer: 2.202674388885498
Norm after each mp layer: 14.225817680358887
Norm after each mp layer: 73.18717193603516
Norm before input: 0.2552422881126404
Norm after input: 0.5236915349960327
Norm after each mp layer: 2.203688621520996
Norm after each mp layer: 14.237092018127441
Norm after each mp layer: 73.21110534667969
Norm before input: 0.2552422881126404
Norm after input: 0.5236915349960327
Norm after each mp layer: 2.203688621520996
Norm after each mp layer: 14.237092018127441
Norm after each mp layer: 73.21110534667969
Norm before input: 0.2552422881126404
Norm after input: 0.5236132144927979
Norm after each mp layer: 2.201387405395508
Norm after each mp layer: 14.212443351745605
Norm after each mp layer: 73.10140228271484
Norm before input: 0.2552422881126404
Norm after input: 0.5236132144927979
Norm after each mp layer: 2.201387405395508
Norm after each mp layer: 14.212443351745605
Norm after each mp layer: 73.10140228271484
Norm before input: 0.2552422881126404
Norm after input: 0.5237557291984558
Norm after each mp layer: 2.202956438064575
Norm after each mp layer: 14.229559898376465
Norm after each mp layer: 73.14517974853516
Norm before input: 0.2552422881126404
Norm after input: 0.5237557291984558
Norm after each mp layer: 2.202956438064575
Norm after each mp layer: 14.229559898376465
Norm after each mp layer: 73.14517974853516
Norm before input: 0.2552422881126404
Norm after input: 0.5236526727676392
Norm after each mp layer: 2.2001609802246094
Norm after each mp layer: 14.19978141784668
Norm after each mp layer: 73.01681518554688
Epoch: 370, Loss: 0.0098, Energy: 52998.2852, Train: 99.92%, Valid: 69.40%, Test: 68.80%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5236526727676392
Norm after each mp layer: 2.2001609802246094
Norm after each mp layer: 14.19978141784668
Norm after each mp layer: 73.01681518554688
Norm before input: 0.2552422881126404
Norm after input: 0.523826003074646
Norm after each mp layer: 2.202399730682373
Norm after each mp layer: 14.223946571350098
Norm after each mp layer: 73.09002685546875
Norm before input: 0.2552422881126404
Norm after input: 0.523826003074646
Norm after each mp layer: 2.202399730682373
Norm after each mp layer: 14.223946571350098
Norm after each mp layer: 73.09002685546875
Norm before input: 0.2552422881126404
Norm after input: 0.5236844420433044
Norm after each mp layer: 2.1989943981170654
Norm after each mp layer: 14.187444686889648
Norm after each mp layer: 72.93838500976562
Norm before input: 0.2552422881126404
Norm after input: 0.5236844420433044
Norm after each mp layer: 2.1989943981170654
Norm after each mp layer: 14.187444686889648
Norm after each mp layer: 72.93838500976562
Norm before input: 0.2552422881126404
Norm after input: 0.5238810777664185
Norm after each mp layer: 2.201517343521118
Norm after each mp layer: 14.214816093444824
Norm after each mp layer: 73.02139282226562
Norm before input: 0.2552422881126404
Norm after input: 0.5238810777664185
Norm after each mp layer: 2.201517343521118
Norm after each mp layer: 14.214816093444824
Norm after each mp layer: 73.02139282226562
Norm before input: 0.2552422881126404
Norm after input: 0.5237293839454651
Norm after each mp layer: 2.197890520095825
Norm after each mp layer: 14.175681114196777
Norm after each mp layer: 72.85735321044922
Norm before input: 0.2552422881126404
Norm after input: 0.5237293839454651
Norm after each mp layer: 2.197890520095825
Norm after each mp layer: 14.175681114196777
Norm after each mp layer: 72.85735321044922
Norm before input: 0.2552422881126404
Norm after input: 0.5239205360412598
Norm after each mp layer: 2.2003366947174072
Norm after each mp layer: 14.202211380004883
Norm after each mp layer: 72.93779754638672
Epoch: 375, Loss: 0.0096, Energy: 52033.9609, Train: 99.92%, Valid: 69.80%, Test: 69.10%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5239205360412598
Norm after each mp layer: 2.2003366947174072
Norm after each mp layer: 14.202213287353516
Norm after each mp layer: 72.93779754638672
Norm before input: 0.2552422881126404
Norm after input: 0.5237969160079956
Norm after each mp layer: 2.197234869003296
Norm after each mp layer: 14.168608665466309
Norm after each mp layer: 72.79412841796875
Norm before input: 0.2552422881126404
Norm after input: 0.5237969160079956
Norm after each mp layer: 2.197234869003296
Norm after each mp layer: 14.168608665466309
Norm after each mp layer: 72.79412841796875
Norm before input: 0.2552422881126404
Norm after input: 0.5239465236663818
Norm after each mp layer: 2.1989219188690186
Norm after each mp layer: 14.186588287353516
Norm after each mp layer: 72.83944702148438
Norm before input: 0.2552422881126404
Norm after input: 0.5239465236663818
Norm after each mp layer: 2.1989219188690186
Norm after each mp layer: 14.186588287353516
Norm after each mp layer: 72.83944702148438
Norm before input: 0.2552422881126404
Norm after input: 0.5238772630691528
Norm after each mp layer: 2.196676254272461
Norm after each mp layer: 14.162349700927734
Norm after each mp layer: 72.7283935546875
Norm before input: 0.2552422881126404
Norm after input: 0.5238772630691528
Norm after each mp layer: 2.196676254272461
Norm after each mp layer: 14.162349700927734
Norm after each mp layer: 72.72838592529297
Norm before input: 0.2552422881126404
Norm after input: 0.5239660739898682
Norm after each mp layer: 2.197263479232788
Norm after each mp layer: 14.16852855682373
Norm after each mp layer: 72.73002624511719
Norm before input: 0.2552422881126404
Norm after input: 0.5239660739898682
Norm after each mp layer: 2.197263479232788
Norm after each mp layer: 14.16852855682373
Norm after each mp layer: 72.73002624511719
Norm before input: 0.2552422881126404
Norm after input: 0.5239542722702026
Norm after each mp layer: 2.1960647106170654
Norm after each mp layer: 14.155485153198242
Norm after each mp layer: 72.66104888916016
Epoch: 380, Loss: 0.0093, Energy: 51218.2461, Train: 99.92%, Valid: 69.40%, Test: 68.90%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5239542722702026
Norm after each mp layer: 2.1960647106170654
Norm after each mp layer: 14.155485153198242
Norm after each mp layer: 72.66104888916016
Norm before input: 0.2552422881126404
Norm after input: 0.523992121219635
Norm after each mp layer: 2.1957688331604004
Norm after each mp layer: 14.152214050292969
Norm after each mp layer: 72.62923431396484
Norm before input: 0.2552422881126404
Norm after input: 0.523992121219635
Norm after each mp layer: 2.1957688331604004
Norm after each mp layer: 14.152214050292969
Norm after each mp layer: 72.62923431396484
Norm before input: 0.2552422881126404
Norm after input: 0.524027943611145
Norm after each mp layer: 2.195467233657837
Norm after each mp layer: 14.148761749267578
Norm after each mp layer: 72.59699249267578
Norm before input: 0.2552422881126404
Norm after input: 0.524027943611145
Norm after each mp layer: 2.195467233657837
Norm after each mp layer: 14.148761749267578
Norm after each mp layer: 72.59699249267578
Norm before input: 0.2552422881126404
Norm after input: 0.5240224003791809
Norm after each mp layer: 2.19439435005188
Norm after each mp layer: 14.13704776763916
Norm after each mp layer: 72.53380584716797
Norm before input: 0.2552422881126404
Norm after input: 0.5240224003791809
Norm after each mp layer: 2.19439435005188
Norm after each mp layer: 14.13704776763916
Norm after each mp layer: 72.53380584716797
Norm before input: 0.2552422881126404
Norm after input: 0.5241028070449829
Norm after each mp layer: 2.194827079772949
Norm after each mp layer: 14.141683578491211
Norm after each mp layer: 72.53195190429688
Norm before input: 0.2552422881126404
Norm after input: 0.5241028070449829
Norm after each mp layer: 2.194827079772949
Norm after each mp layer: 14.141683578491211
Norm after each mp layer: 72.53195190429688
Norm before input: 0.2552422881126404
Norm after input: 0.5240536332130432
Norm after each mp layer: 2.1929807662963867
Norm after each mp layer: 14.121563911437988
Norm after each mp layer: 72.43802642822266
Epoch: 385, Loss: 0.0092, Energy: 50330.9414, Train: 99.92%, Valid: 69.40%, Test: 68.70%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5240536332130432
Norm after each mp layer: 2.1929807662963867
Norm after each mp layer: 14.121563911437988
Norm after each mp layer: 72.43802642822266
Norm before input: 0.2552422881126404
Norm after input: 0.5241793394088745
Norm after each mp layer: 2.1943657398223877
Norm after each mp layer: 14.136252403259277
Norm after each mp layer: 72.47732543945312
Norm before input: 0.2552422881126404
Norm after input: 0.5241793394088745
Norm after each mp layer: 2.1943657398223877
Norm after each mp layer: 14.136252403259277
Norm after each mp layer: 72.47732543945312
Norm before input: 0.2552422881126404
Norm after input: 0.5240734219551086
Norm after each mp layer: 2.191526174545288
Norm after each mp layer: 14.105477333068848
Norm after each mp layer: 72.34539031982422
Norm before input: 0.2552422881126404
Norm after input: 0.5240734219551086
Norm after each mp layer: 2.191526174545288
Norm after each mp layer: 14.105477333068848
Norm after each mp layer: 72.34539031982422
Norm before input: 0.2552422881126404
Norm after input: 0.524273157119751
Norm after each mp layer: 2.194305896759033
Norm after each mp layer: 14.13530445098877
Norm after each mp layer: 72.44562530517578
Norm before input: 0.2552422881126404
Norm after input: 0.524273157119751
Norm after each mp layer: 2.194305896759033
Norm after each mp layer: 14.13530445098877
Norm after each mp layer: 72.44562530517578
Norm before input: 0.2552422881126404
Norm after input: 0.524084210395813
Norm after each mp layer: 2.1899900436401367
Norm after each mp layer: 14.088516235351562
Norm after each mp layer: 72.25373077392578
Norm before input: 0.2552422881126404
Norm after input: 0.524084210395813
Norm after each mp layer: 2.1899900436401367
Norm after each mp layer: 14.088516235351562
Norm after each mp layer: 72.25373077392578
Norm before input: 0.2552422881126404
Norm after input: 0.5243679881095886
Norm after each mp layer: 2.1941721439361572
Norm after each mp layer: 14.133798599243164
Norm after each mp layer: 72.4127426147461
Epoch: 390, Loss: 0.0092, Energy: 49492.7539, Train: 99.92%, Valid: 69.20%, Test: 68.80%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5243679881095886
Norm after each mp layer: 2.1941721439361572
Norm after each mp layer: 14.133798599243164
Norm after each mp layer: 72.4127426147461
Norm before input: 0.2552422881126404
Norm after input: 0.5241205096244812
Norm after each mp layer: 2.1888768672943115
Norm after each mp layer: 14.07597827911377
Norm after each mp layer: 72.17728424072266
Norm before input: 0.2552422881126404
Norm after input: 0.5241205096244812
Norm after each mp layer: 2.1888768672943115
Norm after each mp layer: 14.07597827911377
Norm after each mp layer: 72.17728424072266
Norm before input: 0.2552422881126404
Norm after input: 0.5243887901306152
Norm after each mp layer: 2.1926522254943848
Norm after each mp layer: 14.116436958312988
Norm after each mp layer: 72.310302734375
Norm before input: 0.2552422881126404
Norm after input: 0.5243887901306152
Norm after each mp layer: 2.1926522254943848
Norm after each mp layer: 14.116436958312988
Norm after each mp layer: 72.310302734375
Norm before input: 0.2552422881126404
Norm after input: 0.5242272019386292
Norm after each mp layer: 2.188772201538086
Norm after each mp layer: 14.073721885681152
Norm after each mp layer: 72.12638092041016
Norm before input: 0.2552422881126404
Norm after input: 0.5242272019386292
Norm after each mp layer: 2.188772201538086
Norm after each mp layer: 14.073721885681152
Norm after each mp layer: 72.12638092041016
Norm before input: 0.2552422881126404
Norm after input: 0.5243520140647888
Norm after each mp layer: 2.1900253295898438
Norm after each mp layer: 14.086081504821777
Norm after each mp layer: 72.14733123779297
Norm before input: 0.2552422881126404
Norm after input: 0.5243520140647888
Norm after each mp layer: 2.1900253295898438
Norm after each mp layer: 14.086081504821777
Norm after each mp layer: 72.14733123779297
Norm before input: 0.2552422881126404
Norm after input: 0.5243605971336365
Norm after each mp layer: 2.1890511512756348
Norm after each mp layer: 14.074808120727539
Norm after each mp layer: 72.07845306396484
Epoch: 395, Loss: 0.0089, Energy: 48786.6562, Train: 99.92%, Valid: 69.40%, Test: 68.40%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5243605971336365
Norm after each mp layer: 2.1890511512756348
Norm after each mp layer: 14.074808120727539
Norm after each mp layer: 72.07845306396484
Norm before input: 0.2552422881126404
Norm after input: 0.5243493318557739
Norm after each mp layer: 2.187770366668701
Norm after each mp layer: 14.060220718383789
Norm after each mp layer: 71.99874877929688
Norm before input: 0.2552422881126404
Norm after input: 0.5243493318557739
Norm after each mp layer: 2.187770366668701
Norm after each mp layer: 14.060220718383789
Norm after each mp layer: 71.99874877929688
Norm before input: 0.2552422881126404
Norm after input: 0.5244895219802856
Norm after each mp layer: 2.189347505569458
Norm after each mp layer: 14.07635498046875
Norm after each mp layer: 72.03775787353516
Norm before input: 0.2552422881126404
Norm after input: 0.5244895219802856
Norm after each mp layer: 2.189347505569458
Norm after each mp layer: 14.07635498046875
Norm after each mp layer: 72.03775787353516
Norm before input: 0.2552422881126404
Norm after input: 0.5243629217147827
Norm after each mp layer: 2.1860713958740234
Norm after each mp layer: 14.040239334106445
Norm after each mp layer: 71.87974548339844
Norm before input: 0.2552422881126404
Norm after input: 0.5243629217147827
Norm after each mp layer: 2.1860713958740234
Norm after each mp layer: 14.040239334106445
Norm after each mp layer: 71.87974548339844
Norm before input: 0.2552422881126404
Norm after input: 0.5245831608772278
Norm after each mp layer: 2.1890759468078613
Norm after each mp layer: 14.072637557983398
Norm after each mp layer: 71.98790740966797
Norm before input: 0.2552422881126404
Norm after input: 0.5245831608772278
Norm after each mp layer: 2.1890759468078613
Norm after each mp layer: 14.072637557983398
Norm after each mp layer: 71.98790740966797
Norm before input: 0.2552422881126404
Norm after input: 0.5244031548500061
Norm after each mp layer: 2.1850345134735107
Norm after each mp layer: 14.02832317352295
Norm after each mp layer: 71.80613708496094
Epoch: 400, Loss: 0.0089, Energy: 48031.3086, Train: 99.92%, Valid: 69.20%, Test: 68.50%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5244031548500061
Norm after each mp layer: 2.1850345134735107
Norm after each mp layer: 14.02832317352295
Norm after each mp layer: 71.80613708496094
Norm before input: 0.2552422881126404
Norm after input: 0.524596631526947
Norm after each mp layer: 2.1877028942108154
Norm after each mp layer: 14.056851387023926
Norm after each mp layer: 71.90386199951172
Norm before input: 0.2552422881126404
Norm after input: 0.524596631526947
Norm after each mp layer: 2.1877028942108154
Norm after each mp layer: 14.056851387023926
Norm after each mp layer: 71.90386199951172
Norm before input: 0.2552422881126404
Norm after input: 0.524476170539856
Norm after each mp layer: 2.184596538543701
Norm after each mp layer: 14.022804260253906
Norm after each mp layer: 71.75926208496094
Norm before input: 0.2552422881126404
Norm after input: 0.524476170539856
Norm after each mp layer: 2.184596538543701
Norm after each mp layer: 14.022804260253906
Norm after each mp layer: 71.75926208496094
Norm before input: 0.2552422881126404
Norm after input: 0.5245935320854187
Norm after each mp layer: 2.1857917308807373
Norm after each mp layer: 14.034680366516113
Norm after each mp layer: 71.78559112548828
Norm before input: 0.2552422881126404
Norm after input: 0.5245935320854187
Norm after each mp layer: 2.1857917308807373
Norm after each mp layer: 14.034680366516113
Norm after each mp layer: 71.78559112548828
Norm before input: 0.2552422881126404
Norm after input: 0.5245717167854309
Norm after each mp layer: 2.1843388080596924
Norm after each mp layer: 14.017987251281738
Norm after each mp layer: 71.69910430908203
Norm before input: 0.2552422881126404
Norm after input: 0.5245717167854309
Norm after each mp layer: 2.1843388080596924
Norm after each mp layer: 14.017987251281738
Norm after each mp layer: 71.69910430908203
Norm before input: 0.2552422881126404
Norm after input: 0.5246203541755676
Norm after each mp layer: 2.184051036834717
Norm after each mp layer: 14.013824462890625
Norm after each mp layer: 71.6565170288086
Epoch: 405, Loss: 0.0086, Energy: 47151.7344, Train: 99.92%, Valid: 69.00%, Test: 68.20%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5246203541755676
Norm after each mp layer: 2.184051036834717
Norm after each mp layer: 14.013824462890625
Norm after each mp layer: 71.6565170288086
Norm before input: 0.2552422881126404
Norm after input: 0.5246926546096802
Norm after each mp layer: 2.1842005252838135
Norm after each mp layer: 14.014098167419434
Norm after each mp layer: 71.62874603271484
Norm before input: 0.2552422881126404
Norm after input: 0.5246926546096802
Norm after each mp layer: 2.1842005252838135
Norm after each mp layer: 14.014098167419434
Norm after each mp layer: 71.62874603271484
Norm before input: 0.2552422881126404
Norm after input: 0.5246674418449402
Norm after each mp layer: 2.18259596824646
Norm after each mp layer: 13.995460510253906
Norm after each mp layer: 71.5295639038086
Norm before input: 0.2552422881126404
Norm after input: 0.5246674418449402
Norm after each mp layer: 2.18259596824646
Norm after each mp layer: 13.995460510253906
Norm after each mp layer: 71.5295639038086
Norm before input: 0.2552422881126404
Norm after input: 0.5248019099235535
Norm after each mp layer: 2.1841466426849365
Norm after each mp layer: 14.011187553405762
Norm after each mp layer: 71.57215881347656
Norm before input: 0.2552422881126404
Norm after input: 0.5248019099235535
Norm after each mp layer: 2.1841466426849365
Norm after each mp layer: 14.011187553405762
Norm after each mp layer: 71.57215881347656
Norm before input: 0.2552422881126404
Norm after input: 0.5246972441673279
Norm after each mp layer: 2.18125581741333
Norm after each mp layer: 13.979025840759277
Norm after each mp layer: 71.43035125732422
Norm before input: 0.2552422881126404
Norm after input: 0.5246972441673279
Norm after each mp layer: 2.18125581741333
Norm after each mp layer: 13.979025840759277
Norm after each mp layer: 71.43035125732422
Norm before input: 0.2552422881126404
Norm after input: 0.5248990654945374
Norm after each mp layer: 2.184204578399658
Norm after each mp layer: 14.010700225830078
Norm after each mp layer: 71.54814910888672
Epoch: 410, Loss: 0.0085, Energy: 46256.3047, Train: 99.92%, Valid: 68.80%, Test: 68.30%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5248990654945374
Norm after each mp layer: 2.184204578399658
Norm after each mp layer: 14.010700225830078
Norm after each mp layer: 71.54814910888672
Norm before input: 0.2552422881126404
Norm after input: 0.5247088074684143
Norm after each mp layer: 2.1800003051757812
Norm after each mp layer: 13.964371681213379
Norm after each mp layer: 71.3602066040039
Norm before input: 0.2552422881126404
Norm after input: 0.5247088074684143
Norm after each mp layer: 2.1800003051757812
Norm after each mp layer: 13.964371681213379
Norm after each mp layer: 71.3602066040039
Norm before input: 0.2552422881126404
Norm after input: 0.5249587893486023
Norm after each mp layer: 2.183621883392334
Norm after each mp layer: 14.003467559814453
Norm after each mp layer: 71.50273132324219
Norm before input: 0.2552422881126404
Norm after input: 0.5249587893486023
Norm after each mp layer: 2.183621883392334
Norm after each mp layer: 14.003467559814453
Norm after each mp layer: 71.50273132324219
Norm before input: 0.2552422881126404
Norm after input: 0.5247664451599121
Norm after each mp layer: 2.179297685623169
Norm after each mp layer: 13.955276489257812
Norm after each mp layer: 71.30066680908203
Norm before input: 0.2552422881126404
Norm after input: 0.5247664451599121
Norm after each mp layer: 2.179297685623169
Norm after each mp layer: 13.955276489257812
Norm after each mp layer: 71.30066680908203
Norm before input: 0.2552422881126404
Norm after input: 0.5249664187431335
Norm after each mp layer: 2.1819863319396973
Norm after each mp layer: 13.983306884765625
Norm after each mp layer: 71.39143371582031
Norm before input: 0.2552422881126404
Norm after input: 0.5249664187431335
Norm after each mp layer: 2.1819863319396973
Norm after each mp layer: 13.983306884765625
Norm after each mp layer: 71.39143371582031
Norm before input: 0.2552422881126404
Norm after input: 0.5248832106590271
Norm after each mp layer: 2.1793949604034424
Norm after each mp layer: 13.953977584838867
Norm after each mp layer: 71.25533294677734
Epoch: 415, Loss: 0.0084, Energy: 45837.8477, Train: 99.92%, Valid: 68.80%, Test: 68.20%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5248832106590271
Norm after each mp layer: 2.1793949604034424
Norm after each mp layer: 13.953977584838867
Norm after each mp layer: 71.25533294677734
Norm before input: 0.2552422881126404
Norm after input: 0.5249741673469543
Norm after each mp layer: 2.1799187660217285
Norm after each mp layer: 13.95803451538086
Norm after each mp layer: 71.24417877197266
Norm before input: 0.2552422881126404
Norm after input: 0.5249741673469543
Norm after each mp layer: 2.1799187660217285
Norm after each mp layer: 13.95803451538086
Norm after each mp layer: 71.24417877197266
Norm before input: 0.2552422881126404
Norm after input: 0.5250182747840881
Norm after each mp layer: 2.1794707775115967
Norm after each mp layer: 13.951772689819336
Norm after each mp layer: 71.19121551513672
Norm before input: 0.2552422881126404
Norm after input: 0.5250182747840881
Norm after each mp layer: 2.1794707775115967
Norm after each mp layer: 13.951772689819336
Norm after each mp layer: 71.19121551513672
Norm before input: 0.2552422881126404
Norm after input: 0.5250052213668823
Norm after each mp layer: 2.1780073642730713
Norm after each mp layer: 13.934402465820312
Norm after each mp layer: 71.0955581665039
Norm before input: 0.2552422881126404
Norm after input: 0.5250052213668823
Norm after each mp layer: 2.1780073642730713
Norm after each mp layer: 13.934402465820312
Norm after each mp layer: 71.0955581665039
Norm before input: 0.2552422881126404
Norm after input: 0.5251454710960388
Norm after each mp layer: 2.1795384883880615
Norm after each mp layer: 13.949426651000977
Norm after each mp layer: 71.1307144165039
Norm before input: 0.2552422881126404
Norm after input: 0.5251454710960388
Norm after each mp layer: 2.1795384883880615
Norm after each mp layer: 13.949426651000977
Norm after each mp layer: 71.1307144165039
Norm before input: 0.2552422881126404
Norm after input: 0.5250424146652222
Norm after each mp layer: 2.176661491394043
Norm after each mp layer: 13.916862487792969
Norm after each mp layer: 70.98464965820312
Epoch: 420, Loss: 0.0082, Energy: 44906.2891, Train: 99.92%, Valid: 68.80%, Test: 68.10%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5250424146652222
Norm after each mp layer: 2.176661491394043
Norm after each mp layer: 13.916862487792969
Norm after each mp layer: 70.98464965820312
Norm before input: 0.2552422881126404
Norm after input: 0.5252448320388794
Norm after each mp layer: 2.179547071456909
Norm after each mp layer: 13.947784423828125
Norm after each mp layer: 71.09923553466797
Norm before input: 0.2552422881126404
Norm after input: 0.5252448320388794
Norm after each mp layer: 2.179547071456909
Norm after each mp layer: 13.947783470153809
Norm after each mp layer: 71.09923553466797
Norm before input: 0.2552422881126404
Norm after input: 0.5250809192657471
Norm after each mp layer: 2.1757447719573975
Norm after each mp layer: 13.905427932739258
Norm after each mp layer: 70.92341613769531
Norm before input: 0.2552422881126404
Norm after input: 0.5250809192657471
Norm after each mp layer: 2.1757447719573975
Norm after each mp layer: 13.905427932739258
Norm after each mp layer: 70.92341613769531
Norm before input: 0.2552422881126404
Norm after input: 0.5252833366394043
Norm after each mp layer: 2.1786231994628906
Norm after each mp layer: 13.935741424560547
Norm after each mp layer: 71.03302764892578
Norm before input: 0.2552422881126404
Norm after input: 0.5252833366394043
Norm after each mp layer: 2.1786231994628906
Norm after each mp layer: 13.935741424560547
Norm after each mp layer: 71.03302764892578
Norm before input: 0.2552422881126404
Norm after input: 0.525141179561615
Norm after each mp layer: 2.175065040588379
Norm after each mp layer: 13.896017074584961
Norm after each mp layer: 70.86306762695312
Norm before input: 0.2552422881126404
Norm after input: 0.525141179561615
Norm after each mp layer: 2.175065040588379
Norm after each mp layer: 13.896017074584961
Norm after each mp layer: 70.86306762695312
Norm before input: 0.2552422881126404
Norm after input: 0.5252993106842041
Norm after each mp layer: 2.17704439163208
Norm after each mp layer: 13.915986061096191
Norm after each mp layer: 70.92511749267578
Epoch: 425, Loss: 0.0081, Energy: 44286.0195, Train: 99.92%, Valid: 68.80%, Test: 68.20%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5252993106842041
Norm after each mp layer: 2.17704439163208
Norm after each mp layer: 13.915986061096191
Norm after each mp layer: 70.92511749267578
Norm before input: 0.2552422881126404
Norm after input: 0.5252399444580078
Norm after each mp layer: 2.174896240234375
Norm after each mp layer: 13.891199111938477
Norm after each mp layer: 70.8076171875
Norm before input: 0.2552422881126404
Norm after input: 0.5252399444580078
Norm after each mp layer: 2.174896240234375
Norm after each mp layer: 13.891199111938477
Norm after each mp layer: 70.8076171875
Norm before input: 0.2552422881126404
Norm after input: 0.525341808795929
Norm after each mp layer: 2.1755964756011963
Norm after each mp layer: 13.897205352783203
Norm after each mp layer: 70.80596923828125
Norm before input: 0.2552422881126404
Norm after input: 0.525341808795929
Norm after each mp layer: 2.1755964756011963
Norm after each mp layer: 13.89720344543457
Norm after each mp layer: 70.80596923828125
Norm before input: 0.2552422881126404
Norm after input: 0.5253640413284302
Norm after each mp layer: 2.1747982501983643
Norm after each mp layer: 13.886337280273438
Norm after each mp layer: 70.73250579833984
Norm before input: 0.2552422881126404
Norm after input: 0.5253640413284302
Norm after each mp layer: 2.1747982501983643
Norm after each mp layer: 13.886337280273438
Norm after each mp layer: 70.73250579833984
Norm before input: 0.2552422881126404
Norm after input: 0.5253992080688477
Norm after each mp layer: 2.1741886138916016
Norm after each mp layer: 13.877727508544922
Norm after each mp layer: 70.66795349121094
Norm before input: 0.2552422881126404
Norm after input: 0.5253992080688477
Norm after each mp layer: 2.1741886138916016
Norm after each mp layer: 13.877727508544922
Norm after each mp layer: 70.66795349121094
Norm before input: 0.2552422881126404
Norm after input: 0.5254842638969421
Norm after each mp layer: 2.1745705604553223
Norm after each mp layer: 13.880316734313965
Norm after each mp layer: 70.65308380126953
Epoch: 430, Loss: 0.0079, Energy: 43439.7188, Train: 99.92%, Valid: 68.80%, Test: 68.20%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5254842638969421
Norm after each mp layer: 2.1745705604553223
Norm after each mp layer: 13.880316734313965
Norm after each mp layer: 70.65308380126953
Norm before input: 0.2552422881126404
Norm after input: 0.5254519581794739
Norm after each mp layer: 2.172959566116333
Norm after each mp layer: 13.861078262329102
Norm after each mp layer: 70.55703735351562
Norm before input: 0.2552422881126404
Norm after input: 0.5254519581794739
Norm after each mp layer: 2.172959566116333
Norm after each mp layer: 13.861078262329102
Norm after each mp layer: 70.55703735351562
Norm before input: 0.2552422881126404
Norm after input: 0.5255841016769409
Norm after each mp layer: 2.1746556758880615
Norm after each mp layer: 13.878580093383789
Norm after each mp layer: 70.62049865722656
Norm before input: 0.2552422881126404
Norm after input: 0.5255841016769409
Norm after each mp layer: 2.1746556758880615
Norm after each mp layer: 13.878580093383789
Norm after each mp layer: 70.62049865722656
Norm before input: 0.2552422881126404
Norm after input: 0.5254769921302795
Norm after each mp layer: 2.171884298324585
Norm after each mp layer: 13.847243309020996
Norm after each mp layer: 70.48807525634766
Norm before input: 0.2552422881126404
Norm after input: 0.5254769921302795
Norm after each mp layer: 2.171884298324585
Norm after each mp layer: 13.847243309020996
Norm after each mp layer: 70.48807525634766
Norm before input: 0.2552422881126404
Norm after input: 0.5256682634353638
Norm after each mp layer: 2.174659252166748
Norm after each mp layer: 13.876557350158691
Norm after each mp layer: 70.60047912597656
Norm before input: 0.2552422881126404
Norm after input: 0.5256682634353638
Norm after each mp layer: 2.174659252166748
Norm after each mp layer: 13.876557350158691
Norm after each mp layer: 70.60047912597656
Norm before input: 0.2552422881126404
Norm after input: 0.5254971981048584
Norm after each mp layer: 2.1706414222717285
Norm after each mp layer: 13.83151626586914
Norm after each mp layer: 70.41145324707031
Epoch: 435, Loss: 0.0079, Energy: 42973.5898, Train: 99.92%, Valid: 68.80%, Test: 68.10%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5254971981048584
Norm after each mp layer: 2.1706414222717285
Norm after each mp layer: 13.83151626586914
Norm after each mp layer: 70.41145324707031
Norm before input: 0.2552422881126404
Norm after input: 0.5257495045661926
Norm after each mp layer: 2.1743874549865723
Norm after each mp layer: 13.871797561645508
Norm after each mp layer: 70.56719207763672
Norm before input: 0.2552422881126404
Norm after input: 0.5257495045661926
Norm after each mp layer: 2.1743874549865723
Norm after each mp layer: 13.871797561645508
Norm after each mp layer: 70.56719207763672
Norm before input: 0.2552422881126404
Norm after input: 0.5255621671676636
Norm after each mp layer: 2.1700491905212402
Norm after each mp layer: 13.82278060913086
Norm after each mp layer: 70.35784149169922
Norm before input: 0.2552422881126404
Norm after input: 0.5255621671676636
Norm after each mp layer: 2.1700491905212402
Norm after each mp layer: 13.822778701782227
Norm after each mp layer: 70.35784149169922
Norm before input: 0.2552422881126404
Norm after input: 0.525791585445404
Norm after each mp layer: 2.1731812953948975
Norm after each mp layer: 13.854692459106445
Norm after each mp layer: 70.46144104003906
Norm before input: 0.2552422881126404
Norm after input: 0.525791585445404
Norm after each mp layer: 2.1731812953948975
Norm after each mp layer: 13.854692459106445
Norm after each mp layer: 70.46144104003906
Norm before input: 0.2552422881126404
Norm after input: 0.5256903171539307
Norm after each mp layer: 2.1700170040130615
Norm after each mp layer: 13.81815242767334
Norm after each mp layer: 70.28712463378906
Norm before input: 0.2552422881126404
Norm after input: 0.5256903171539307
Norm after each mp layer: 2.1700170040130615
Norm after each mp layer: 13.81815242767334
Norm after each mp layer: 70.28712463378906
Norm before input: 0.2552422881126404
Norm after input: 0.5258222818374634
Norm after each mp layer: 2.171189308166504
Norm after each mp layer: 13.828384399414062
Norm after each mp layer: 70.29678344726562
Epoch: 440, Loss: 0.0077, Energy: 42301.5000, Train: 99.92%, Valid: 68.80%, Test: 68.30%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5258222818374634
Norm after each mp layer: 2.171189308166504
Norm after each mp layer: 13.828384399414062
Norm after each mp layer: 70.29678344726562
Norm before input: 0.2552422881126404
Norm after input: 0.5258352756500244
Norm after each mp layer: 2.170170307159424
Norm after each mp layer: 13.81490421295166
Norm after each mp layer: 70.2138442993164
Norm before input: 0.2552422881126404
Norm after input: 0.5258352756500244
Norm after each mp layer: 2.170170307159424
Norm after each mp layer: 13.81490421295166
Norm after each mp layer: 70.2138442993164
Norm before input: 0.2552422881126404
Norm after input: 0.5258668065071106
Norm after each mp layer: 2.1695380210876465
Norm after each mp layer: 13.806095123291016
Norm after each mp layer: 70.1540298461914
Norm before input: 0.2552422881126404
Norm after input: 0.5258668065071106
Norm after each mp layer: 2.1695380210876465
Norm after each mp layer: 13.806095123291016
Norm after each mp layer: 70.1540298461914
Norm before input: 0.2552422881126404
Norm after input: 0.5259717702865601
Norm after each mp layer: 2.1704790592193604
Norm after each mp layer: 13.814106941223145
Norm after each mp layer: 70.16517639160156
Norm before input: 0.2552422881126404
Norm after input: 0.5259717702865601
Norm after each mp layer: 2.1704790592193604
Norm after each mp layer: 13.814106941223145
Norm after each mp layer: 70.16517639160156
Norm before input: 0.2552422881126404
Norm after input: 0.5259009599685669
Norm after each mp layer: 2.1682233810424805
Norm after each mp layer: 13.787343978881836
Norm after each mp layer: 70.04045867919922
Norm before input: 0.2552422881126404
Norm after input: 0.5259009599685669
Norm after each mp layer: 2.1682233810424805
Norm after each mp layer: 13.787343978881836
Norm after each mp layer: 70.04045867919922
Norm before input: 0.2552422881126404
Norm after input: 0.5260794758796692
Norm after each mp layer: 2.1706769466400146
Norm after each mp layer: 13.813538551330566
Norm after each mp layer: 70.14285278320312
Epoch: 445, Loss: 0.0076, Energy: 41458.2500, Train: 99.92%, Valid: 68.60%, Test: 68.10%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5260794758796692
Norm after each mp layer: 2.1706769466400146
Norm after each mp layer: 13.813538551330566
Norm after each mp layer: 70.14285278320312
Norm before input: 0.2552422881126404
Norm after input: 0.5259272456169128
Norm after each mp layer: 2.1670784950256348
Norm after each mp layer: 13.772747993469238
Norm after each mp layer: 69.97260284423828
Norm before input: 0.2552422881126404
Norm after input: 0.5259272456169128
Norm after each mp layer: 2.1670784950256348
Norm after each mp layer: 13.772747993469238
Norm after each mp layer: 69.97260284423828
Norm before input: 0.2552422881126404
Norm after input: 0.526142954826355
Norm after each mp layer: 2.1705474853515625
Norm after each mp layer: 13.80941104888916
Norm after each mp layer: 70.12444305419922
Norm before input: 0.2552422881126404
Norm after input: 0.526142954826355
Norm after each mp layer: 2.1705474853515625
Norm after each mp layer: 13.80941104888916
Norm after each mp layer: 70.12444305419922
Norm before input: 0.2552422881126404
Norm after input: 0.5259668827056885
Norm after each mp layer: 2.166447162628174
Norm after each mp layer: 13.763547897338867
Norm after each mp layer: 69.93477630615234
Norm before input: 0.2552422881126404
Norm after input: 0.5259668827056885
Norm after each mp layer: 2.166447162628174
Norm after each mp layer: 13.763547897338867
Norm after each mp layer: 69.93477630615234
Norm before input: 0.2552422881126404
Norm after input: 0.5261867046356201
Norm after each mp layer: 2.1694374084472656
Norm after each mp layer: 13.794114112854004
Norm after each mp layer: 70.03955841064453
Norm before input: 0.2552422881126404
Norm after input: 0.5261867046356201
Norm after each mp layer: 2.1694374084472656
Norm after each mp layer: 13.794114112854004
Norm after each mp layer: 70.03955841064453
Norm before input: 0.2552422881126404
Norm after input: 0.5260778665542603
Norm after each mp layer: 2.16623854637146
Norm after each mp layer: 13.756793975830078
Norm after each mp layer: 69.86458587646484
Epoch: 450, Loss: 0.0076, Energy: 41140.2266, Train: 99.92%, Valid: 68.80%, Test: 67.90%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5260778665542603
Norm after each mp layer: 2.16623854637146
Norm after each mp layer: 13.756793975830078
Norm after each mp layer: 69.86458587646484
Norm before input: 0.2552422881126404
Norm after input: 0.5262359380722046
Norm after each mp layer: 2.167869806289673
Norm after each mp layer: 13.77245044708252
Norm after each mp layer: 69.90059661865234
Norm before input: 0.2552422881126404
Norm after input: 0.5262359380722046
Norm after each mp layer: 2.167869806289673
Norm after each mp layer: 13.77245044708252
Norm after each mp layer: 69.90059661865234
Norm before input: 0.2552422881126404
Norm after input: 0.5262334942817688
Norm after each mp layer: 2.1665759086608887
Norm after each mp layer: 13.755282402038574
Norm after each mp layer: 69.7999038696289
Norm before input: 0.2552422881126404
Norm after input: 0.5262334942817688
Norm after each mp layer: 2.1665759086608887
Norm after each mp layer: 13.755282402038574
Norm after each mp layer: 69.7999038696289
Norm before input: 0.2552422881126404
Norm after input: 0.5263016223907471
Norm after each mp layer: 2.1665897369384766
Norm after each mp layer: 13.752555847167969
Norm after each mp layer: 69.75823974609375
Norm before input: 0.2552422881126404
Norm after input: 0.5263016223907471
Norm after each mp layer: 2.1665897369384766
Norm after each mp layer: 13.752555847167969
Norm after each mp layer: 69.75823974609375
Norm before input: 0.2552422881126404
Norm after input: 0.5263763666152954
Norm after each mp layer: 2.166703939437866
Norm after each mp layer: 13.751615524291992
Norm after each mp layer: 69.72828674316406
Norm before input: 0.2552422881126404
Norm after input: 0.5263763666152954
Norm after each mp layer: 2.166703939437866
Norm after each mp layer: 13.751615524291992
Norm after each mp layer: 69.72828674316406
Norm before input: 0.2552422881126404
Norm after input: 0.5263571739196777
Norm after each mp layer: 2.1651947498321533
Norm after each mp layer: 13.732721328735352
Norm after each mp layer: 69.6292495727539
Epoch: 455, Loss: 0.0074, Energy: 40242.6055, Train: 99.92%, Valid: 68.60%, Test: 67.70%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5263571739196777
Norm after each mp layer: 2.1651947498321533
Norm after each mp layer: 13.732721328735352
Norm after each mp layer: 69.6292495727539
Norm before input: 0.2552422881126404
Norm after input: 0.5264908671379089
Norm after each mp layer: 2.1667017936706543
Norm after each mp layer: 13.74765396118164
Norm after each mp layer: 69.67835235595703
Norm before input: 0.2552422881126404
Norm after input: 0.5264908671379089
Norm after each mp layer: 2.1667017936706543
Norm after each mp layer: 13.747655868530273
Norm after each mp layer: 69.67835235595703
Norm before input: 0.2552422881126404
Norm after input: 0.5263914465904236
Norm after each mp layer: 2.1640281677246094
Norm after each mp layer: 13.716614723205566
Norm after each mp layer: 69.54472351074219
Norm before input: 0.2552422881126404
Norm after input: 0.5263914465904236
Norm after each mp layer: 2.1640281677246094
Norm after each mp layer: 13.716614723205566
Norm after each mp layer: 69.54472351074219
Norm before input: 0.2552422881126404
Norm after input: 0.5265800356864929
Norm after each mp layer: 2.1672019958496094
Norm after each mp layer: 13.750138282775879
Norm after each mp layer: 69.69303894042969
Norm before input: 0.2552422881126404
Norm after input: 0.5265800356864929
Norm after each mp layer: 2.1672019958496094
Norm after each mp layer: 13.750138282775879
Norm after each mp layer: 69.69303894042969
Norm before input: 0.2552422881126404
Norm after input: 0.5263998508453369
Norm after each mp layer: 2.1632189750671387
Norm after each mp layer: 13.70498275756836
Norm after each mp layer: 69.51049041748047
Norm before input: 0.2552422881126404
Norm after input: 0.5263998508453369
Norm after each mp layer: 2.1632189750671387
Norm after each mp layer: 13.70498275756836
Norm after each mp layer: 69.51049041748047
Norm before input: 0.2552422881126404
Norm after input: 0.5266644358634949
Norm after each mp layer: 2.166919708251953
Norm after each mp layer: 13.74464225769043
Norm after each mp layer: 69.6626205444336
Epoch: 460, Loss: 0.0075, Energy: 39848.5078, Train: 99.92%, Valid: 68.60%, Test: 68.30%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5266644358634949
Norm after each mp layer: 2.166919708251953
Norm after each mp layer: 13.74464225769043
Norm after each mp layer: 69.6626205444336
Norm before input: 0.2552422881126404
Norm after input: 0.5264847278594971
Norm after each mp layer: 2.162497043609619
Norm after each mp layer: 13.693611145019531
Norm after each mp layer: 69.43433380126953
Norm before input: 0.2552422881126404
Norm after input: 0.5264847278594971
Norm after each mp layer: 2.162497043609619
Norm after each mp layer: 13.693611145019531
Norm after each mp layer: 69.43433380126953
Norm before input: 0.2552422881126404
Norm after input: 0.5267015099525452
Norm after each mp layer: 2.165499448776245
Norm after each mp layer: 13.72358226776123
Norm after each mp layer: 69.53532409667969
Norm before input: 0.2552422881126404
Norm after input: 0.5267015099525452
Norm after each mp layer: 2.165499448776245
Norm after each mp layer: 13.72358226776123
Norm after each mp layer: 69.53532409667969
Norm before input: 0.2552422881126404
Norm after input: 0.5266287326812744
Norm after each mp layer: 2.162909984588623
Norm after each mp layer: 13.692548751831055
Norm after each mp layer: 69.38341522216797
Norm before input: 0.2552422881126404
Norm after input: 0.5266287326812744
Norm after each mp layer: 2.162909984588623
Norm after each mp layer: 13.692548751831055
Norm after each mp layer: 69.38341522216797
Norm before input: 0.2552422881126404
Norm after input: 0.5267640948295593
Norm after each mp layer: 2.164158344268799
Norm after each mp layer: 13.703120231628418
Norm after each mp layer: 69.39701843261719
Norm before input: 0.2552422881126404
Norm after input: 0.5267640948295593
Norm after each mp layer: 2.164158344268799
Norm after each mp layer: 13.703120231628418
Norm after each mp layer: 69.39701843261719
Norm before input: 0.2552422881126404
Norm after input: 0.5267876386642456
Norm after each mp layer: 2.16328501701355
Norm after each mp layer: 13.690444946289062
Norm after each mp layer: 69.31472778320312
Epoch: 465, Loss: 0.0072, Energy: 39165.5430, Train: 99.92%, Valid: 68.60%, Test: 67.70%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5267876386642456
Norm after each mp layer: 2.16328501701355
Norm after each mp layer: 13.690444946289062
Norm after each mp layer: 69.31472778320312
Norm before input: 0.2552422881126404
Norm after input: 0.5268245339393616
Norm after each mp layer: 2.162551164627075
Norm after each mp layer: 13.679924011230469
Norm after each mp layer: 69.2426528930664
Norm before input: 0.2552422881126404
Norm after input: 0.5268245339393616
Norm after each mp layer: 2.162551164627075
Norm after each mp layer: 13.679924011230469
Norm after each mp layer: 69.2426528930664
Norm before input: 0.2552422881126404
Norm after input: 0.5269386172294617
Norm after each mp layer: 2.163496494293213
Norm after each mp layer: 13.687603950500488
Norm after each mp layer: 69.2503433227539
Norm before input: 0.2552422881126404
Norm after input: 0.5269386172294617
Norm after each mp layer: 2.163496494293213
Norm after each mp layer: 13.687603950500488
Norm after each mp layer: 69.25035095214844
Norm before input: 0.2552422881126404
Norm after input: 0.5268728733062744
Norm after each mp layer: 2.161393642425537
Norm after each mp layer: 13.661674499511719
Norm after each mp layer: 69.12907409667969
Norm before input: 0.2552422881126404
Norm after input: 0.5268728733062744
Norm after each mp layer: 2.161393642425537
Norm after each mp layer: 13.661674499511719
Norm after each mp layer: 69.12907409667969
Norm before input: 0.2552422881126404
Norm after input: 0.5270482301712036
Norm after each mp layer: 2.1643171310424805
Norm after each mp layer: 13.692906379699707
Norm after each mp layer: 69.27314758300781
Norm before input: 0.2552422881126404
Norm after input: 0.5270482301712036
Norm after each mp layer: 2.1643171310424805
Norm after each mp layer: 13.692907333374023
Norm after each mp layer: 69.27314758300781
Norm before input: 0.2552422881126404
Norm after input: 0.5268815755844116
Norm after each mp layer: 2.1606621742248535
Norm after each mp layer: 13.651252746582031
Norm after each mp layer: 69.10823059082031
Epoch: 470, Loss: 0.0072, Energy: 38620.5859, Train: 99.92%, Valid: 68.60%, Test: 67.90%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5268815755844116
Norm after each mp layer: 2.1606621742248535
Norm after each mp layer: 13.651252746582031
Norm after each mp layer: 69.10823059082031
Norm before input: 0.2552422881126404
Norm after input: 0.5271233916282654
Norm after each mp layer: 2.1640870571136475
Norm after each mp layer: 13.687370300292969
Norm after each mp layer: 69.2488784790039
Norm before input: 0.2552422881126404
Norm after input: 0.5271233916282654
Norm after each mp layer: 2.1640870571136475
Norm after each mp layer: 13.687370300292969
Norm after each mp layer: 69.2488784790039
Norm before input: 0.2552422881126404
Norm after input: 0.5269535183906555
Norm after each mp layer: 2.159808874130249
Norm after each mp layer: 13.638110160827637
Norm after each mp layer: 69.02954864501953
Norm before input: 0.2552422881126404
Norm after input: 0.5269535183906555
Norm after each mp layer: 2.159808874130249
Norm after each mp layer: 13.638110160827637
Norm after each mp layer: 69.02955627441406
Norm before input: 0.2552422881126404
Norm after input: 0.5271623730659485
Norm after each mp layer: 2.1627540588378906
Norm after each mp layer: 13.667625427246094
Norm after each mp layer: 69.13550567626953
Norm before input: 0.2552422881126404
Norm after input: 0.5271623730659485
Norm after each mp layer: 2.1627540588378906
Norm after each mp layer: 13.667625427246094
Norm after each mp layer: 69.13550567626953
Norm before input: 0.2552422881126404
Norm after input: 0.5270795822143555
Norm after each mp layer: 2.1601693630218506
Norm after each mp layer: 13.636092185974121
Norm after each mp layer: 68.98570251464844
Norm before input: 0.2552422881126404
Norm after input: 0.5270795822143555
Norm after each mp layer: 2.1601693630218506
Norm after each mp layer: 13.636092185974121
Norm after each mp layer: 68.98570251464844
Norm before input: 0.2552422881126404
Norm after input: 0.5272396206855774
Norm after each mp layer: 2.1618220806121826
Norm after each mp layer: 13.651639938354492
Norm after each mp layer: 69.02483367919922
Epoch: 475, Loss: 0.0070, Energy: 38128.3008, Train: 99.92%, Valid: 68.60%, Test: 68.00%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5272396206855774
Norm after each mp layer: 2.1618220806121826
Norm after each mp layer: 13.651639938354492
Norm after each mp layer: 69.02483367919922
Norm before input: 0.2552422881126404
Norm after input: 0.5272521376609802
Norm after each mp layer: 2.1606805324554443
Norm after each mp layer: 13.635629653930664
Norm after each mp layer: 68.92633819580078
Norm before input: 0.2552422881126404
Norm after input: 0.5272521376609802
Norm after each mp layer: 2.1606805324554443
Norm after each mp layer: 13.635629653930664
Norm after each mp layer: 68.92633819580078
Norm before input: 0.2552422881126404
Norm after input: 0.5273151993751526
Norm after each mp layer: 2.1604180335998535
Norm after each mp layer: 13.629168510437012
Norm after each mp layer: 68.86408996582031
Norm before input: 0.2552422881126404
Norm after input: 0.5273151993751526
Norm after each mp layer: 2.1604180335998535
Norm after each mp layer: 13.629168510437012
Norm after each mp layer: 68.86408996582031
Norm before input: 0.2552422881126404
Norm after input: 0.5274040699005127
Norm after each mp layer: 2.160682439804077
Norm after each mp layer: 13.629196166992188
Norm after each mp layer: 68.83494567871094
Norm before input: 0.2552422881126404
Norm after input: 0.5274040699005127
Norm after each mp layer: 2.160682439804077
Norm after each mp layer: 13.629196166992188
Norm after each mp layer: 68.83494567871094
Norm before input: 0.2552422881126404
Norm after input: 0.527385413646698
Norm after each mp layer: 2.1592659950256348
Norm after each mp layer: 13.610751152038574
Norm after each mp layer: 68.74050903320312
Norm before input: 0.2552422881126404
Norm after input: 0.527385413646698
Norm after each mp layer: 2.1592659950256348
Norm after each mp layer: 13.610751152038574
Norm after each mp layer: 68.74050903320312
Norm before input: 0.2552422881126404
Norm after input: 0.5275238752365112
Norm after each mp layer: 2.161367893218994
Norm after each mp layer: 13.632208824157715
Norm after each mp layer: 68.8370590209961
Epoch: 480, Loss: 0.0069, Energy: 37349.8125, Train: 99.92%, Valid: 68.60%, Test: 67.80%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5275238752365112
Norm after each mp layer: 2.161367893218994
Norm after each mp layer: 13.632208824157715
Norm after each mp layer: 68.8370590209961
Norm before input: 0.2552422881126404
Norm after input: 0.5274040699005127
Norm after each mp layer: 2.1585192680358887
Norm after each mp layer: 13.599020957946777
Norm after each mp layer: 68.70339965820312
Norm before input: 0.2552422881126404
Norm after input: 0.5274040699005127
Norm after each mp layer: 2.1585192680358887
Norm after each mp layer: 13.599020957946777
Norm after each mp layer: 68.70339965820312
Norm before input: 0.2552422881126404
Norm after input: 0.5276225209236145
Norm after each mp layer: 2.1616344451904297
Norm after each mp layer: 13.631717681884766
Norm after each mp layer: 68.8350830078125
Norm before input: 0.2552422881126404
Norm after input: 0.5276225209236145
Norm after each mp layer: 2.1616344451904297
Norm after each mp layer: 13.631717681884766
Norm after each mp layer: 68.8350830078125
Norm before input: 0.2552422881126404
Norm after input: 0.5274493098258972
Norm after each mp layer: 2.1574058532714844
Norm after each mp layer: 13.582316398620605
Norm after each mp layer: 68.61575317382812
Norm before input: 0.2552422881126404
Norm after input: 0.5274493098258972
Norm after each mp layer: 2.1574058532714844
Norm after each mp layer: 13.582316398620605
Norm after each mp layer: 68.61575317382812
Norm before input: 0.2552422881126404
Norm after input: 0.5277056097984314
Norm after each mp layer: 2.1614091396331787
Norm after each mp layer: 13.624637603759766
Norm after each mp layer: 68.79257202148438
Norm before input: 0.2552422881126404
Norm after input: 0.5277056097984314
Norm after each mp layer: 2.1614091396331787
Norm after each mp layer: 13.624637603759766
Norm after each mp layer: 68.79257202148438
Norm before input: 0.2552422881126404
Norm after input: 0.5275329947471619
Norm after each mp layer: 2.157371759414673
Norm after each mp layer: 13.577219009399414
Norm after each mp layer: 68.58582305908203
Epoch: 485, Loss: 0.0070, Energy: 37146.2773, Train: 99.92%, Valid: 68.60%, Test: 67.70%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5275329947471619
Norm after each mp layer: 2.157371759414673
Norm after each mp layer: 13.577219009399414
Norm after each mp layer: 68.58582305908203
Norm before input: 0.2552422881126404
Norm after input: 0.5277737975120544
Norm after each mp layer: 2.160639524459839
Norm after each mp layer: 13.610047340393066
Norm after each mp layer: 68.69966888427734
Norm before input: 0.2552422881126404
Norm after input: 0.5277737975120544
Norm after each mp layer: 2.160639524459839
Norm after each mp layer: 13.610047340393066
Norm after each mp layer: 68.69966888427734
Norm before input: 0.2552422881126404
Norm after input: 0.5276978015899658
Norm after each mp layer: 2.1578140258789062
Norm after each mp layer: 13.575457572937012
Norm after each mp layer: 68.52440643310547
Norm before input: 0.2552422881126404
Norm after input: 0.5276978015899658
Norm after each mp layer: 2.1578140258789062
Norm after each mp layer: 13.575457572937012
Norm after each mp layer: 68.52440643310547
Norm before input: 0.2552422881126404
Norm after input: 0.5278357267379761
Norm after each mp layer: 2.1589746475219727
Norm after each mp layer: 13.584624290466309
Norm after each mp layer: 68.52888488769531
Norm before input: 0.2552422881126404
Norm after input: 0.5278357267379761
Norm after each mp layer: 2.1589746475219727
Norm after each mp layer: 13.584624290466309
Norm after each mp layer: 68.52888488769531
Norm before input: 0.2552422881126404
Norm after input: 0.5278657674789429
Norm after each mp layer: 2.158224105834961
Norm after each mp layer: 13.572530746459961
Norm after each mp layer: 68.44734954833984
Norm before input: 0.2552422881126404
Norm after input: 0.5278657674789429
Norm after each mp layer: 2.158224105834961
Norm after each mp layer: 13.572530746459961
Norm after each mp layer: 68.44734954833984
Norm before input: 0.2552422881126404
Norm after input: 0.5279057025909424
Norm after each mp layer: 2.1577210426330566
Norm after each mp layer: 13.564135551452637
Norm after each mp layer: 68.39032745361328
Epoch: 490, Loss: 0.0067, Energy: 36381.7148, Train: 99.92%, Valid: 68.60%, Test: 67.60%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5279057025909424
Norm after each mp layer: 2.1577210426330566
Norm after each mp layer: 13.564135551452637
Norm after each mp layer: 68.39032745361328
Norm before input: 0.2552422881126404
Norm after input: 0.5280265808105469
Norm after each mp layer: 2.1589152812957764
Norm after each mp layer: 13.574792861938477
Norm after each mp layer: 68.42000579833984
Norm before input: 0.2552422881126404
Norm after input: 0.5280265808105469
Norm after each mp layer: 2.1589152812957764
Norm after each mp layer: 13.574792861938477
Norm after each mp layer: 68.42000579833984
Norm before input: 0.2552422881126404
Norm after input: 0.5279603004455566
Norm after each mp layer: 2.1567931175231934
Norm after each mp layer: 13.548080444335938
Norm after each mp layer: 68.29437255859375
Norm before input: 0.2552422881126404
Norm after input: 0.5279603004455566
Norm after each mp layer: 2.1567931175231934
Norm after each mp layer: 13.548080444335938
Norm after each mp layer: 68.29437255859375
Norm before input: 0.2552422881126404
Norm after input: 0.5281403660774231
Norm after each mp layer: 2.1591906547546387
Norm after each mp layer: 13.572643280029297
Norm after each mp layer: 68.39020538330078
Norm before input: 0.2552422881126404
Norm after input: 0.5281403660774231
Norm after each mp layer: 2.1591906547546387
Norm after each mp layer: 13.572643280029297
Norm after each mp layer: 68.39020538330078
Norm before input: 0.2552422881126404
Norm after input: 0.5280135869979858
Norm after each mp layer: 2.1559150218963623
Norm after each mp layer: 13.533719062805176
Norm after each mp layer: 68.21785736083984
Norm before input: 0.2552422881126404
Norm after input: 0.5280135869979858
Norm after each mp layer: 2.1559150218963623
Norm after each mp layer: 13.533719062805176
Norm after each mp layer: 68.21786499023438
Norm before input: 0.2552422881126404
Norm after input: 0.5282282829284668
Norm after each mp layer: 2.1595442295074463
Norm after each mp layer: 13.571953773498535
Norm after each mp layer: 68.39215850830078
Epoch: 495, Loss: 0.0067, Energy: 35942.8633, Train: 99.92%, Valid: 68.40%, Test: 68.10%, Best Valid: 80.00%, Best Test: 77.10%
Norm before input: 0.2552422881126404
Norm after input: 0.5282282829284668
Norm after each mp layer: 2.1595442295074463
Norm after each mp layer: 13.571953773498535
Norm after each mp layer: 68.39215850830078
Norm before input: 0.2552422881126404
Norm after input: 0.5280531644821167
Norm after each mp layer: 2.1556248664855957
Norm after each mp layer: 13.526385307312012
Norm after each mp layer: 68.20366668701172
Norm before input: 0.2552422881126404
Norm after input: 0.5280531644821167
Norm after each mp layer: 2.1556248664855957
Norm after each mp layer: 13.526385307312012
Norm after each mp layer: 68.20366668701172
Norm before input: 0.2552422881126404
Norm after input: 0.5283039212226868
Norm after each mp layer: 2.1590938568115234
Norm after each mp layer: 13.562247276306152
Norm after each mp layer: 68.33968353271484
Norm before input: 0.2552422881126404
Norm after input: 0.5283039212226868
Norm after each mp layer: 2.1590938568115234
Norm after each mp layer: 13.562247276306152
Norm after each mp layer: 68.33968353271484
Norm before input: 0.2552422881126404
Norm after input: 0.5281795859336853
Norm after each mp layer: 2.1555135250091553
Norm after each mp layer: 13.518900871276855
Norm after each mp layer: 68.13047790527344
Norm before input: 0.2552422881126404
Norm after input: 0.5281795859336853
Norm after each mp layer: 2.1555135250091553
Norm after each mp layer: 13.518900871276855
Norm after each mp layer: 68.13047790527344
Norm before input: 0.2552422881126404
Norm after input: 0.5283722281455994
Norm after each mp layer: 2.1577749252319336
Norm after each mp layer: 13.540542602539062
Norm after each mp layer: 68.19403839111328
train_accuracy_list: [0.28228476821192056, 0.28228476821192056, 0.16225165562913907, 0.16225165562913907, 0.28228476821192056, 0.16225165562913907, 0.11423841059602649, 0.17135761589403972, 0.28228476821192056, 0.28228476821192056, 0.13245033112582782, 0.16225165562913907, 0.173841059602649, 0.3170529801324503, 0.28228476821192056, 0.28394039735099336, 0.3576158940397351, 0.3543046357615894, 0.31125827814569534, 0.3170529801324503, 0.4105960264900662, 0.4793046357615894, 0.4908940397350993, 0.4793046357615894, 0.4867549668874172, 0.5264900662251656, 0.5811258278145696, 0.5538079470198676, 0.5397350993377483, 0.5844370860927153, 0.6200331125827815, 0.6225165562913907, 0.5918874172185431, 0.6423841059602649, 0.652317880794702, 0.6134105960264901, 0.6465231788079471, 0.6746688741721855, 0.6423841059602649, 0.6995033112582781, 0.7152317880794702, 0.6697019867549668, 0.7251655629139073, 0.7367549668874173, 0.7425496688741722, 0.7392384105960265, 0.7632450331125827, 0.7814569536423841, 0.793046357615894, 0.804635761589404, 0.8071192052980133, 0.8120860927152318, 0.8153973509933775, 0.8170529801324503, 0.8187086092715232, 0.8311258278145696, 0.8394039735099338, 0.8402317880794702, 0.8336092715231788, 0.8377483443708609, 0.8451986754966887, 0.8534768211920529, 0.8526490066225165, 0.8625827814569537, 0.8551324503311258, 0.8667218543046358, 0.8443708609271523, 0.8683774834437086, 0.8758278145695364, 0.8741721854304636, 0.890728476821192, 0.8733443708609272, 0.8824503311258278, 0.9048013245033113, 0.8990066225165563, 0.8882450331125827, 0.9122516556291391, 0.9197019867549668, 0.9014900662251656, 0.9122516556291391, 0.9238410596026491, 0.9221854304635762, 0.9329470198675497, 0.929635761589404, 0.9346026490066225, 0.9354304635761589, 0.9379139072847682, 0.9387417218543046, 0.9412251655629139, 0.9445364238410596, 0.9470198675496688, 0.9495033112582781, 0.9478476821192053, 0.9503311258278145, 0.9536423841059603, 0.9528145695364238, 0.9561258278145696, 0.9586092715231788, 0.9610927152317881, 0.9627483443708609, 0.9644039735099338, 0.9652317880794702, 0.9668874172185431, 0.9660596026490066, 0.9701986754966887, 0.9685430463576159, 0.9693708609271523, 0.9718543046357616, 0.9718543046357616, 0.9743377483443708, 0.972682119205298, 0.9759933774834437, 0.9768211920529801, 0.9784768211920529, 0.9768211920529801, 0.9776490066225165, 0.9776490066225165, 0.9776490066225165, 0.9784768211920529, 0.9793046357615894, 0.9793046357615894, 0.9801324503311258, 0.9801324503311258, 0.9801324503311258, 0.9801324503311258, 0.9801324503311258, 0.9801324503311258, 0.9801324503311258, 0.9809602649006622, 0.9809602649006622, 0.9801324503311258, 0.9809602649006622, 0.9817880794701986, 0.9817880794701986, 0.9826158940397351, 0.9817880794701986, 0.9817880794701986, 0.9817880794701986, 0.9817880794701986, 0.9817880794701986, 0.9826158940397351, 0.9834437086092715, 0.9834437086092715, 0.984271523178808, 0.9859271523178808, 0.9850993377483444, 0.9850993377483444, 0.9850993377483444, 0.9859271523178808, 0.9850993377483444, 0.9859271523178808, 0.9867549668874173, 0.9867549668874173, 0.9859271523178808, 0.9875827814569537, 0.9892384105960265, 0.9892384105960265, 0.9892384105960265, 0.9900662251655629, 0.9908940397350994, 0.9917218543046358, 0.9917218543046358, 0.9917218543046358, 0.9917218543046358, 0.9925496688741722, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9950331125827815, 0.9950331125827815, 0.9950331125827815, 0.9950331125827815, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9966887417218543, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9983443708609272, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9991721854304636, 0.9991721854304636, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9983443708609272, 0.9983443708609272, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636]
valid_accuracy_list: [0.316, 0.316, 0.156, 0.162, 0.316, 0.156, 0.114, 0.166, 0.316, 0.316, 0.122, 0.156, 0.164, 0.36, 0.316, 0.318, 0.382, 0.364, 0.318, 0.322, 0.378, 0.494, 0.5, 0.482, 0.48, 0.524, 0.572, 0.528, 0.51, 0.554, 0.606, 0.598, 0.578, 0.62, 0.614, 0.602, 0.6, 0.624, 0.604, 0.618, 0.632, 0.632, 0.664, 0.668, 0.684, 0.69, 0.696, 0.71, 0.714, 0.728, 0.716, 0.712, 0.722, 0.73, 0.722, 0.732, 0.736, 0.728, 0.734, 0.732, 0.736, 0.742, 0.742, 0.746, 0.752, 0.752, 0.742, 0.742, 0.766, 0.768, 0.758, 0.754, 0.764, 0.77, 0.774, 0.766, 0.772, 0.782, 0.77, 0.77, 0.784, 0.78, 0.784, 0.784, 0.782, 0.788, 0.788, 0.786, 0.788, 0.792, 0.794, 0.794, 0.788, 0.794, 0.794, 0.786, 0.792, 0.79, 0.788, 0.788, 0.792, 0.782, 0.784, 0.784, 0.786, 0.786, 0.786, 0.788, 0.788, 0.788, 0.79, 0.788, 0.792, 0.79, 0.788, 0.794, 0.794, 0.792, 0.794, 0.8, 0.794, 0.794, 0.796, 0.792, 0.79, 0.788, 0.79, 0.79, 0.79, 0.792, 0.79, 0.79, 0.788, 0.786, 0.788, 0.788, 0.788, 0.788, 0.788, 0.79, 0.79, 0.79, 0.79, 0.788, 0.786, 0.784, 0.784, 0.782, 0.782, 0.78, 0.778, 0.778, 0.776, 0.772, 0.772, 0.772, 0.77, 0.768, 0.768, 0.768, 0.768, 0.766, 0.766, 0.766, 0.768, 0.77, 0.77, 0.77, 0.77, 0.77, 0.766, 0.766, 0.764, 0.764, 0.764, 0.764, 0.762, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.762, 0.76, 0.76, 0.758, 0.758, 0.756, 0.754, 0.754, 0.754, 0.752, 0.752, 0.752, 0.75, 0.75, 0.748, 0.748, 0.75, 0.75, 0.75, 0.752, 0.752, 0.752, 0.752, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.748, 0.748, 0.748, 0.744, 0.746, 0.744, 0.742, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.74, 0.742, 0.742, 0.742, 0.738, 0.738, 0.732, 0.732, 0.732, 0.734, 0.732, 0.736, 0.734, 0.734, 0.732, 0.73, 0.732, 0.73, 0.726, 0.73, 0.724, 0.726, 0.726, 0.724, 0.726, 0.724, 0.724, 0.724, 0.724, 0.722, 0.722, 0.722, 0.722, 0.722, 0.724, 0.718, 0.722, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.718, 0.718, 0.712, 0.716, 0.708, 0.712, 0.71, 0.712, 0.71, 0.712, 0.71, 0.712, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.708, 0.708, 0.708, 0.708, 0.708, 0.708, 0.708, 0.706, 0.706, 0.708, 0.708, 0.708, 0.708, 0.71, 0.704, 0.704, 0.704, 0.708, 0.702, 0.706, 0.702, 0.704, 0.702, 0.702, 0.704, 0.7, 0.702, 0.7, 0.698, 0.704, 0.7, 0.704, 0.7, 0.702, 0.702, 0.7, 0.702, 0.7, 0.704, 0.7, 0.702, 0.702, 0.7, 0.704, 0.7, 0.704, 0.702, 0.704, 0.702, 0.702, 0.704, 0.702, 0.702, 0.702, 0.7, 0.702, 0.7, 0.702, 0.698, 0.698, 0.698, 0.7, 0.698, 0.7, 0.698, 0.698, 0.694, 0.698, 0.694, 0.698, 0.696, 0.698, 0.698, 0.698, 0.696, 0.696, 0.696, 0.694, 0.694, 0.694, 0.692, 0.694, 0.692, 0.694, 0.692, 0.694, 0.694, 0.694, 0.692, 0.694, 0.692, 0.694, 0.694, 0.694, 0.692, 0.69, 0.692, 0.69, 0.692, 0.688, 0.69, 0.69, 0.69, 0.69, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.69, 0.686, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.688, 0.686, 0.688, 0.686, 0.688, 0.686, 0.688, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.686, 0.684, 0.686, 0.684, 0.686, 0.684, 0.686, 0.684]
test_accuracy_list: [0.319, 0.319, 0.144, 0.149, 0.319, 0.144, 0.103, 0.147, 0.319, 0.319, 0.13, 0.144, 0.15, 0.358, 0.319, 0.32, 0.385, 0.362, 0.298, 0.303, 0.399, 0.472, 0.487, 0.49, 0.497, 0.519, 0.551, 0.526, 0.509, 0.557, 0.589, 0.59, 0.566, 0.602, 0.611, 0.594, 0.614, 0.627, 0.604, 0.638, 0.656, 0.634, 0.661, 0.667, 0.67, 0.666, 0.67, 0.69, 0.692, 0.693, 0.694, 0.699, 0.689, 0.692, 0.691, 0.707, 0.713, 0.712, 0.701, 0.701, 0.709, 0.716, 0.721, 0.721, 0.726, 0.723, 0.719, 0.715, 0.729, 0.733, 0.729, 0.731, 0.736, 0.742, 0.742, 0.733, 0.739, 0.749, 0.747, 0.742, 0.754, 0.764, 0.76, 0.756, 0.766, 0.761, 0.766, 0.764, 0.768, 0.767, 0.771, 0.771, 0.761, 0.767, 0.769, 0.762, 0.767, 0.77, 0.764, 0.768, 0.771, 0.768, 0.769, 0.767, 0.769, 0.77, 0.769, 0.767, 0.769, 0.769, 0.769, 0.766, 0.764, 0.764, 0.764, 0.764, 0.766, 0.767, 0.766, 0.766, 0.766, 0.766, 0.767, 0.766, 0.765, 0.765, 0.765, 0.766, 0.767, 0.767, 0.768, 0.766, 0.768, 0.769, 0.77, 0.767, 0.765, 0.767, 0.765, 0.765, 0.765, 0.765, 0.765, 0.763, 0.762, 0.762, 0.76, 0.762, 0.761, 0.76, 0.762, 0.761, 0.759, 0.758, 0.758, 0.758, 0.758, 0.755, 0.755, 0.755, 0.755, 0.755, 0.753, 0.753, 0.751, 0.75, 0.75, 0.75, 0.75, 0.749, 0.749, 0.747, 0.745, 0.744, 0.744, 0.744, 0.741, 0.741, 0.74, 0.74, 0.739, 0.739, 0.738, 0.738, 0.738, 0.737, 0.735, 0.733, 0.733, 0.732, 0.733, 0.733, 0.731, 0.731, 0.731, 0.731, 0.731, 0.733, 0.733, 0.733, 0.731, 0.73, 0.73, 0.73, 0.73, 0.728, 0.727, 0.727, 0.728, 0.727, 0.727, 0.727, 0.725, 0.725, 0.724, 0.724, 0.723, 0.723, 0.723, 0.723, 0.722, 0.722, 0.722, 0.722, 0.72, 0.72, 0.721, 0.721, 0.721, 0.721, 0.721, 0.721, 0.721, 0.722, 0.722, 0.722, 0.721, 0.721, 0.721, 0.722, 0.715, 0.723, 0.717, 0.715, 0.72, 0.712, 0.718, 0.714, 0.712, 0.716, 0.709, 0.712, 0.712, 0.711, 0.71, 0.711, 0.71, 0.711, 0.71, 0.71, 0.708, 0.71, 0.704, 0.704, 0.705, 0.703, 0.705, 0.703, 0.704, 0.703, 0.703, 0.703, 0.702, 0.702, 0.703, 0.702, 0.702, 0.701, 0.699, 0.699, 0.697, 0.699, 0.697, 0.698, 0.697, 0.698, 0.696, 0.697, 0.696, 0.696, 0.697, 0.696, 0.697, 0.697, 0.697, 0.695, 0.698, 0.697, 0.696, 0.695, 0.698, 0.696, 0.697, 0.697, 0.697, 0.697, 0.697, 0.697, 0.698, 0.698, 0.698, 0.698, 0.698, 0.698, 0.698, 0.697, 0.697, 0.695, 0.698, 0.692, 0.698, 0.693, 0.695, 0.696, 0.692, 0.698, 0.69, 0.694, 0.693, 0.693, 0.695, 0.69, 0.697, 0.692, 0.693, 0.693, 0.692, 0.696, 0.693, 0.696, 0.693, 0.694, 0.695, 0.692, 0.695, 0.691, 0.695, 0.691, 0.693, 0.69, 0.689, 0.691, 0.688, 0.692, 0.688, 0.691, 0.688, 0.691, 0.689, 0.689, 0.689, 0.689, 0.69, 0.689, 0.69, 0.689, 0.691, 0.689, 0.692, 0.688, 0.692, 0.688, 0.692, 0.688, 0.691, 0.689, 0.69, 0.689, 0.689, 0.689, 0.689, 0.689, 0.688, 0.689, 0.687, 0.687, 0.688, 0.688, 0.686, 0.688, 0.686, 0.688, 0.686, 0.684, 0.684, 0.686, 0.684, 0.684, 0.686, 0.685, 0.682, 0.685, 0.683, 0.683, 0.682, 0.683, 0.682, 0.684, 0.683, 0.683, 0.683, 0.683, 0.683, 0.682, 0.682, 0.682, 0.682, 0.68, 0.683, 0.681, 0.683, 0.681, 0.684, 0.681, 0.682, 0.682, 0.682, 0.682, 0.683, 0.682, 0.683, 0.684, 0.682, 0.684, 0.681, 0.684, 0.682, 0.684, 0.682, 0.683, 0.683, 0.682, 0.683, 0.681, 0.681, 0.68, 0.681, 0.679, 0.679, 0.679, 0.679, 0.679, 0.678, 0.677, 0.677, 0.679, 0.678, 0.682, 0.679, 0.683, 0.678, 0.679, 0.678, 0.677, 0.677, 0.677, 0.677, 0.677, 0.679, 0.679, 0.68, 0.678, 0.678, 0.679, 0.68, 0.68, 0.68, 0.68, 0.681, 0.678, 0.68, 0.68, 0.679, 0.681, 0.677, 0.68, 0.676, 0.677, 0.676, 0.676, 0.678, 0.676, 0.679, 0.675, 0.681, 0.676, 0.681, 0.676, 0.677]
best validation: 0.8
best test: 0.771
Experiment run
dataset: cora
num_mp_layers: 3
mp_hidden_dim: 4000
optimizer_lr: 0.001
loss_func: CrossEntropyLoss
total_epoch: 500
energy_lambda: 1e-06
Norm before input: 0.2552422881126404
Norm after input: 0.9947524070739746
Norm after each mp layer: 0.6566787362098694
Norm after each mp layer: 0.6222918033599854
Norm after each mp layer: 0.6184736490249634
Norm before input: 0.2552422881126404
Norm after input: 1.0018209218978882
Norm after each mp layer: 1.8245917558670044
Norm after each mp layer: 1.6392133235931396
Norm after each mp layer: 1.6293678283691406
Norm before input: 0.2552422881126404
Norm after input: 1.0018209218978882
Norm after each mp layer: 1.8245917558670044
Norm after each mp layer: 1.6392133235931396
Norm after each mp layer: 1.6293678283691406
Norm before input: 0.2552422881126404
Norm after input: 1.054268717765808
Norm after each mp layer: 2.7995364665985107
Norm after each mp layer: 5.274531364440918
Norm after each mp layer: 14.310269355773926
Norm before input: 0.2552422881126404
Norm after input: 1.054268717765808
Norm after each mp layer: 2.7995364665985107
Norm after each mp layer: 5.274531364440918
Norm after each mp layer: 14.310269355773926
Norm before input: 0.2552422881126404
Norm after input: 1.124648928642273
Norm after each mp layer: 4.177539348602295
Norm after each mp layer: 12.072419166564941
Norm after each mp layer: 44.66135787963867
Norm before input: 0.2552422881126404
Norm after input: 1.124648928642273
Norm after each mp layer: 4.177539348602295
Norm after each mp layer: 12.072419166564941
Norm after each mp layer: 44.66135787963867
Norm before input: 0.2552422881126404
Norm after input: 1.106275200843811
Norm after each mp layer: 4.153521537780762
Norm after each mp layer: 12.342529296875
Norm after each mp layer: 46.63563537597656
Norm before input: 0.2552422881126404
Norm after input: 1.106275200843811
Norm after each mp layer: 4.153521537780762
Norm after each mp layer: 12.342529296875
Norm after each mp layer: 46.63563537597656
Norm before input: 0.2552422881126404
Norm after input: 1.0569497346878052
Norm after each mp layer: 3.708193063735962
Norm after each mp layer: 8.770230293273926
Norm after each mp layer: 30.115787506103516
Epoch: 05, Loss: 3.9720, Energy: 325089.5625, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 1.0569497346878052
Norm after each mp layer: 3.708193063735962
Norm after each mp layer: 8.770230293273926
Norm after each mp layer: 30.115787506103516
Norm before input: 0.2552422881126404
Norm after input: 0.9982120394706726
Norm after each mp layer: 3.3733346462249756
Norm after each mp layer: 7.340482711791992
Norm after each mp layer: 13.673360824584961
Norm before input: 0.2552422881126404
Norm after input: 0.9982120394706726
Norm after each mp layer: 3.3733346462249756
Norm after each mp layer: 7.340482711791992
Norm after each mp layer: 13.673360824584961
Norm before input: 0.2552422881126404
Norm after input: 0.9518348574638367
Norm after each mp layer: 3.205564260482788
Norm after each mp layer: 7.7682061195373535
Norm after each mp layer: 23.94074821472168
Norm before input: 0.2552422881126404
Norm after input: 0.9518348574638367
Norm after each mp layer: 3.205564260482788
Norm after each mp layer: 7.7682061195373535
Norm after each mp layer: 23.94074821472168
Norm before input: 0.2552422881126404
Norm after input: 0.9034187197685242
Norm after each mp layer: 3.0172009468078613
Norm after each mp layer: 8.400260925292969
Norm after each mp layer: 29.71882438659668
Norm before input: 0.2552422881126404
Norm after input: 0.9034187197685242
Norm after each mp layer: 3.0172009468078613
Norm after each mp layer: 8.400260925292969
Norm after each mp layer: 29.71882438659668
Norm before input: 0.2552422881126404
Norm after input: 0.8499447107315063
Norm after each mp layer: 2.766845226287842
Norm after each mp layer: 8.628183364868164
Norm after each mp layer: 31.385517120361328
Norm before input: 0.2552422881126404
Norm after input: 0.8499447107315063
Norm after each mp layer: 2.766845226287842
Norm after each mp layer: 8.62818431854248
Norm after each mp layer: 31.385517120361328
Norm before input: 0.2552422881126404
Norm after input: 0.7970099449157715
Norm after each mp layer: 2.503007650375366
Norm after each mp layer: 8.655406951904297
Norm after each mp layer: 32.45066452026367
Epoch: 10, Loss: 2.3780, Energy: 131541.9062, Train: 20.70%, Valid: 25.00%, Test: 23.80%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.7970099449157715
Norm after each mp layer: 2.503007650375366
Norm after each mp layer: 8.655406951904297
Norm after each mp layer: 32.45066452026367
Norm before input: 0.2552422881126404
Norm after input: 0.7402989864349365
Norm after each mp layer: 2.174290180206299
Norm after each mp layer: 7.760529041290283
Norm after each mp layer: 30.35009002685547
Norm before input: 0.2552422881126404
Norm after input: 0.7402989864349365
Norm after each mp layer: 2.174290180206299
Norm after each mp layer: 7.760529041290283
Norm after each mp layer: 30.35009002685547
Norm before input: 0.2552422881126404
Norm after input: 0.6840488314628601
Norm after each mp layer: 1.8190430402755737
Norm after each mp layer: 6.359930038452148
Norm after each mp layer: 25.99152374267578
Norm before input: 0.2552422881126404
Norm after input: 0.6840488314628601
Norm after each mp layer: 1.8190430402755737
Norm after each mp layer: 6.359930038452148
Norm after each mp layer: 25.99152374267578
Norm before input: 0.2552422881126404
Norm after input: 0.6350456476211548
Norm after each mp layer: 1.486826777458191
Norm after each mp layer: 5.028185844421387
Norm after each mp layer: 21.27998161315918
Norm before input: 0.2552422881126404
Norm after input: 0.6350456476211548
Norm after each mp layer: 1.486826777458191
Norm after each mp layer: 5.028185844421387
Norm after each mp layer: 21.27998161315918
Norm before input: 0.2552422881126404
Norm after input: 0.5957518219947815
Norm after each mp layer: 1.2085098028182983
Norm after each mp layer: 4.0079264640808105
Norm after each mp layer: 17.272903442382812
Norm before input: 0.2552422881126404
Norm after input: 0.5957518219947815
Norm after each mp layer: 1.2085098028182983
Norm after each mp layer: 4.0079264640808105
Norm after each mp layer: 17.272903442382812
Norm before input: 0.2552422881126404
Norm after input: 0.566348135471344
Norm after each mp layer: 1.0073186159133911
Norm after each mp layer: 3.318927764892578
Norm after each mp layer: 14.544018745422363
Epoch: 15, Loss: 2.1792, Energy: 34952.5000, Train: 28.64%, Valid: 32.20%, Test: 32.10%, Best Valid: 32.20%, Best Test: 32.10%
Norm before input: 0.2552422881126404
Norm after input: 0.566348135471344
Norm after each mp layer: 1.0073186159133911
Norm after each mp layer: 3.318927764892578
Norm after each mp layer: 14.544018745422363
Norm before input: 0.2552422881126404
Norm after input: 0.5473098158836365
Norm after each mp layer: 0.9243642687797546
Norm after each mp layer: 3.11234974861145
Norm after each mp layer: 13.960733413696289
Norm before input: 0.2552422881126404
Norm after input: 0.5473098158836365
Norm after each mp layer: 0.9243643283843994
Norm after each mp layer: 3.11234974861145
Norm after each mp layer: 13.960732460021973
Norm before input: 0.2552422881126404
Norm after input: 0.5353495478630066
Norm after each mp layer: 0.9391099810600281
Norm after each mp layer: 3.3298392295837402
Norm after each mp layer: 15.310105323791504
Norm before input: 0.2552422881126404
Norm after input: 0.5353495478630066
Norm after each mp layer: 0.9391099810600281
Norm after each mp layer: 3.3298392295837402
Norm after each mp layer: 15.310105323791504
Norm before input: 0.2552422881126404
Norm after input: 0.5251621603965759
Norm after each mp layer: 0.9749142527580261
Norm after each mp layer: 3.634553909301758
Norm after each mp layer: 17.112394332885742
Norm before input: 0.2552422881126404
Norm after input: 0.5251621603965759
Norm after each mp layer: 0.9749142527580261
Norm after each mp layer: 3.634553909301758
Norm after each mp layer: 17.112394332885742
Norm before input: 0.2552422881126404
Norm after input: 0.5133647918701172
Norm after each mp layer: 0.9842309355735779
Norm after each mp layer: 3.7900006771087646
Norm after each mp layer: 18.236614227294922
Norm before input: 0.2552422881126404
Norm after input: 0.5133647918701172
Norm after each mp layer: 0.9842309355735779
Norm after each mp layer: 3.7900006771087646
Norm after each mp layer: 18.236614227294922
Norm before input: 0.2552422881126404
Norm after input: 0.5001885890960693
Norm after each mp layer: 0.9628300070762634
Norm after each mp layer: 3.774672269821167
Norm after each mp layer: 18.49925422668457
Epoch: 20, Loss: 1.9810, Energy: 39673.3281, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 34.80%, Best Test: 35.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5001885890960693
Norm after each mp layer: 0.9628300070762634
Norm after each mp layer: 3.774672269821167
Norm after each mp layer: 18.49925422668457
Norm before input: 0.2552422881126404
Norm after input: 0.48709091544151306
Norm after each mp layer: 0.9214997291564941
Norm after each mp layer: 3.6454005241394043
Norm after each mp layer: 18.124614715576172
Norm before input: 0.2552422881126404
Norm after input: 0.48709091544151306
Norm after each mp layer: 0.9214997291564941
Norm after each mp layer: 3.6454005241394043
Norm after each mp layer: 18.124614715576172
Norm before input: 0.2552422881126404
Norm after input: 0.47550955414772034
Norm after each mp layer: 0.8751514554023743
Norm after each mp layer: 3.4747517108917236
Norm after each mp layer: 17.45659637451172
Norm before input: 0.2552422881126404
Norm after input: 0.47550955414772034
Norm after each mp layer: 0.8751514554023743
Norm after each mp layer: 3.4747517108917236
Norm after each mp layer: 17.45659637451172
Norm before input: 0.2552422881126404
Norm after input: 0.46616795659065247
Norm after each mp layer: 0.8364367485046387
Norm after each mp layer: 3.319366455078125
Norm after each mp layer: 16.79269790649414
Norm before input: 0.2552422881126404
Norm after input: 0.46616795659065247
Norm after each mp layer: 0.8364367485046387
Norm after each mp layer: 3.319366455078125
Norm after each mp layer: 16.79269790649414
Norm before input: 0.2552422881126404
Norm after input: 0.45914122462272644
Norm after each mp layer: 0.8140941262245178
Norm after each mp layer: 3.224849224090576
Norm after each mp layer: 16.41231346130371
Norm before input: 0.2552422881126404
Norm after input: 0.45914122462272644
Norm after each mp layer: 0.8140941262245178
Norm after each mp layer: 3.224849224090576
Norm after each mp layer: 16.41231346130371
Norm before input: 0.2552422881126404
Norm after input: 0.45408573746681213
Norm after each mp layer: 0.8126300573348999
Norm after each mp layer: 3.2267322540283203
Norm after each mp layer: 16.583614349365234
Epoch: 25, Loss: 1.7775, Energy: 33054.0391, Train: 40.48%, Valid: 44.40%, Test: 42.10%, Best Valid: 44.40%, Best Test: 42.10%
Norm before input: 0.2552422881126404
Norm after input: 0.45408573746681213
Norm after each mp layer: 0.8126300573348999
Norm after each mp layer: 3.2267322540283203
Norm after each mp layer: 16.583614349365234
Norm before input: 0.2552422881126404
Norm after input: 0.45064666867256165
Norm after each mp layer: 0.8310095071792603
Norm after each mp layer: 3.334287643432617
Norm after each mp layer: 17.418569564819336
Norm before input: 0.2552422881126404
Norm after input: 0.45064666867256165
Norm after each mp layer: 0.8310095071792603
Norm after each mp layer: 3.334287643432617
Norm after each mp layer: 17.418569564819336
Norm before input: 0.2552422881126404
Norm after input: 0.44852760434150696
Norm after each mp layer: 0.8610958456993103
Norm after each mp layer: 3.5112996101379395
Norm after each mp layer: 18.703937530517578
Norm before input: 0.2552422881126404
Norm after input: 0.44852760434150696
Norm after each mp layer: 0.8610958456993103
Norm after each mp layer: 3.5112996101379395
Norm after each mp layer: 18.703937530517578
Norm before input: 0.2552422881126404
Norm after input: 0.4470384120941162
Norm after each mp layer: 0.8885256052017212
Norm after each mp layer: 3.671743392944336
Norm after each mp layer: 19.854902267456055
Norm before input: 0.2552422881126404
Norm after input: 0.4470384120941162
Norm after each mp layer: 0.8885256052017212
Norm after each mp layer: 3.671743392944336
Norm after each mp layer: 19.854902267456055
Norm before input: 0.2552422881126404
Norm after input: 0.44512176513671875
Norm after each mp layer: 0.8990442156791687
Norm after each mp layer: 3.71797776222229
Norm after each mp layer: 20.196556091308594
Norm before input: 0.2552422881126404
Norm after input: 0.44512176513671875
Norm after each mp layer: 0.8990442156791687
Norm after each mp layer: 3.71797776222229
Norm after each mp layer: 20.196556091308594
Norm before input: 0.2552422881126404
Norm after input: 0.4428013861179352
Norm after each mp layer: 0.8938458561897278
Norm after each mp layer: 3.64048171043396
Norm after each mp layer: 19.577669143676758
Epoch: 30, Loss: 1.5481, Energy: 54069.0820, Train: 50.50%, Valid: 51.20%, Test: 50.70%, Best Valid: 51.20%, Best Test: 50.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4428013861179352
Norm after each mp layer: 0.8938458561897278
Norm after each mp layer: 3.64048171043396
Norm after each mp layer: 19.577669143676758
Norm before input: 0.2552422881126404
Norm after input: 0.4412994384765625
Norm after each mp layer: 0.8928534388542175
Norm after each mp layer: 3.5585272312164307
Norm after each mp layer: 18.73342514038086
Norm before input: 0.2552422881126404
Norm after input: 0.4412994384765625
Norm after each mp layer: 0.8928534388542175
Norm after each mp layer: 3.5585272312164307
Norm after each mp layer: 18.73342514038086
Norm before input: 0.2552422881126404
Norm after input: 0.4406653344631195
Norm after each mp layer: 0.9153159260749817
Norm after each mp layer: 3.6155264377593994
Norm after each mp layer: 18.778114318847656
Norm before input: 0.2552422881126404
Norm after input: 0.4406653344631195
Norm after each mp layer: 0.9153159260749817
Norm after each mp layer: 3.6155264377593994
Norm after each mp layer: 18.778114318847656
Norm before input: 0.2552422881126404
Norm after input: 0.44010376930236816
Norm after each mp layer: 0.956108570098877
Norm after each mp layer: 3.8070826530456543
Norm after each mp layer: 19.85834312438965
Norm before input: 0.2552422881126404
Norm after input: 0.44010376930236816
Norm after each mp layer: 0.956108570098877
Norm after each mp layer: 3.807082414627075
Norm after each mp layer: 19.85834312438965
Norm before input: 0.2552422881126404
Norm after input: 0.43983545899391174
Norm after each mp layer: 0.9991117715835571
Norm after each mp layer: 4.028965473175049
Norm after each mp layer: 21.2199764251709
Norm before input: 0.2552422881126404
Norm after input: 0.43983545899391174
Norm after each mp layer: 0.9991117715835571
Norm after each mp layer: 4.028965473175049
Norm after each mp layer: 21.2199764251709
Norm before input: 0.2552422881126404
Norm after input: 0.4399130046367645
Norm after each mp layer: 1.034383773803711
Norm after each mp layer: 4.189802646636963
Norm after each mp layer: 22.109798431396484
Epoch: 35, Loss: 1.2462, Energy: 53218.4453, Train: 54.30%, Valid: 53.20%, Test: 53.50%, Best Valid: 54.00%, Best Test: 53.80%
Norm before input: 0.2552422881126404
Norm after input: 0.4399130046367645
Norm after each mp layer: 1.034383773803711
Norm after each mp layer: 4.189802646636963
Norm after each mp layer: 22.109798431396484
Norm before input: 0.2552422881126404
Norm after input: 0.4402274191379547
Norm after each mp layer: 1.058936357498169
Norm after each mp layer: 4.253086090087891
Norm after each mp layer: 22.195728302001953
Norm before input: 0.2552422881126404
Norm after input: 0.4402274191379547
Norm after each mp layer: 1.058936357498169
Norm after each mp layer: 4.253086090087891
Norm after each mp layer: 22.195728302001953
Norm before input: 0.2552422881126404
Norm after input: 0.44122573733329773
Norm after each mp layer: 1.0798122882843018
Norm after each mp layer: 4.277092933654785
Norm after each mp layer: 21.891952514648438
Norm before input: 0.2552422881126404
Norm after input: 0.44122573733329773
Norm after each mp layer: 1.0798122882843018
Norm after each mp layer: 4.277092933654785
Norm after each mp layer: 21.891952514648438
Norm before input: 0.2552422881126404
Norm after input: 0.4426305592060089
Norm after each mp layer: 1.103513240814209
Norm after each mp layer: 4.337125778198242
Norm after each mp layer: 21.88559913635254
Norm before input: 0.2552422881126404
Norm after input: 0.4426305592060089
Norm after each mp layer: 1.103513240814209
Norm after each mp layer: 4.337125778198242
Norm after each mp layer: 21.88559913635254
Norm before input: 0.2552422881126404
Norm after input: 0.4433019459247589
Norm after each mp layer: 1.1365876197814941
Norm after each mp layer: 4.490772724151611
Norm after each mp layer: 22.68310546875
Norm before input: 0.2552422881126404
Norm after input: 0.4433019459247589
Norm after each mp layer: 1.1365876197814941
Norm after each mp layer: 4.490772724151611
Norm after each mp layer: 22.68310546875
Norm before input: 0.2552422881126404
Norm after input: 0.4433424174785614
Norm after each mp layer: 1.1868460178375244
Norm after each mp layer: 4.773024082183838
Norm after each mp layer: 24.404691696166992
Epoch: 40, Loss: 1.0661, Energy: 50717.9062, Train: 59.69%, Valid: 59.60%, Test: 59.30%, Best Valid: 59.60%, Best Test: 59.90%
Norm before input: 0.2552422881126404
Norm after input: 0.4433424174785614
Norm after each mp layer: 1.1868460178375244
Norm after each mp layer: 4.773024082183838
Norm after each mp layer: 24.404691696166992
Norm before input: 0.2552422881126404
Norm after input: 0.44392073154449463
Norm after each mp layer: 1.244849443435669
Norm after each mp layer: 5.0950798988342285
Norm after each mp layer: 26.26780891418457
Norm before input: 0.2552422881126404
Norm after input: 0.44392073154449463
Norm after each mp layer: 1.244849443435669
Norm after each mp layer: 5.095080375671387
Norm after each mp layer: 26.26780891418457
Norm before input: 0.2552422881126404
Norm after input: 0.44520434737205505
Norm after each mp layer: 1.2960957288742065
Norm after each mp layer: 5.35265588760376
Norm after each mp layer: 27.505414962768555
Norm before input: 0.2552422881126404
Norm after input: 0.44520434737205505
Norm after each mp layer: 1.2960957288742065
Norm after each mp layer: 5.35265588760376
Norm after each mp layer: 27.505414962768555
Norm before input: 0.2552422881126404
Norm after input: 0.44674596190452576
Norm after each mp layer: 1.3330966234207153
Norm after each mp layer: 5.534058570861816
Norm after each mp layer: 28.264921188354492
Norm before input: 0.2552422881126404
Norm after input: 0.44674596190452576
Norm after each mp layer: 1.3330966234207153
Norm after each mp layer: 5.534058570861816
Norm after each mp layer: 28.264921188354492
Norm before input: 0.2552422881126404
Norm after input: 0.44765233993530273
Norm after each mp layer: 1.3511818647384644
Norm after each mp layer: 5.650999546051025
Norm after each mp layer: 28.957324981689453
Norm before input: 0.2552422881126404
Norm after input: 0.44765233993530273
Norm after each mp layer: 1.3511818647384644
Norm after each mp layer: 5.650999546051025
Norm after each mp layer: 28.957324981689453
Norm before input: 0.2552422881126404
Norm after input: 0.44813376665115356
Norm after each mp layer: 1.3612890243530273
Norm after each mp layer: 5.748356342315674
Norm after each mp layer: 29.79916763305664
Epoch: 45, Loss: 0.9294, Energy: 85764.2500, Train: 71.85%, Valid: 68.40%, Test: 68.30%, Best Valid: 68.40%, Best Test: 68.30%
Norm before input: 0.2552422881126404
Norm after input: 0.44813376665115356
Norm after each mp layer: 1.3612890243530273
Norm after each mp layer: 5.748356342315674
Norm after each mp layer: 29.79916763305664
Norm before input: 0.2552422881126404
Norm after input: 0.4491674304008484
Norm after each mp layer: 1.376973032951355
Norm after each mp layer: 5.852419853210449
Norm after each mp layer: 30.563648223876953
Norm before input: 0.2552422881126404
Norm after input: 0.4491674304008484
Norm after each mp layer: 1.376973032951355
Norm after each mp layer: 5.852419853210449
Norm after each mp layer: 30.563648223876953
Norm before input: 0.2552422881126404
Norm after input: 0.4508771598339081
Norm after each mp layer: 1.403788685798645
Norm after each mp layer: 5.974948883056641
Norm after each mp layer: 31.144588470458984
Norm before input: 0.2552422881126404
Norm after input: 0.4508771598339081
Norm after each mp layer: 1.4037889242172241
Norm after each mp layer: 5.974948883056641
Norm after each mp layer: 31.144588470458984
Norm before input: 0.2552422881126404
Norm after input: 0.45292651653289795
Norm after each mp layer: 1.4464805126190186
Norm after each mp layer: 6.172905445098877
Norm after each mp layer: 32.11223220825195
Norm before input: 0.2552422881126404
Norm after input: 0.45292651653289795
Norm after each mp layer: 1.4464805126190186
Norm after each mp layer: 6.172904968261719
Norm after each mp layer: 32.11223220825195
Norm before input: 0.2552422881126404
Norm after input: 0.4546343684196472
Norm after each mp layer: 1.490868330001831
Norm after each mp layer: 6.402773380279541
Norm after each mp layer: 33.502323150634766
Norm before input: 0.2552422881126404
Norm after input: 0.4546343684196472
Norm after each mp layer: 1.490868330001831
Norm after each mp layer: 6.402773380279541
Norm after each mp layer: 33.502323150634766
Norm before input: 0.2552422881126404
Norm after input: 0.45607781410217285
Norm after each mp layer: 1.5144684314727783
Norm after each mp layer: 6.534068584442139
Norm after each mp layer: 34.437374114990234
Epoch: 50, Loss: 0.7747, Energy: 88554.6094, Train: 76.82%, Valid: 70.80%, Test: 72.30%, Best Valid: 72.40%, Best Test: 72.30%
Norm before input: 0.2552422881126404
Norm after input: 0.45607781410217285
Norm after each mp layer: 1.5144684314727783
Norm after each mp layer: 6.534068584442139
Norm after each mp layer: 34.437374114990234
Norm before input: 0.2552422881126404
Norm after input: 0.4574272036552429
Norm after each mp layer: 1.513210654258728
Norm after each mp layer: 6.514535903930664
Norm after each mp layer: 34.36355209350586
Norm before input: 0.2552422881126404
Norm after input: 0.4574272036552429
Norm after each mp layer: 1.513210654258728
Norm after each mp layer: 6.514535903930664
Norm after each mp layer: 34.36355209350586
Norm before input: 0.2552422881126404
Norm after input: 0.45877835154533386
Norm after each mp layer: 1.5105727910995483
Norm after each mp layer: 6.483161926269531
Norm after each mp layer: 34.21957015991211
Norm before input: 0.2552422881126404
Norm after input: 0.45877835154533386
Norm after each mp layer: 1.5105727910995483
Norm after each mp layer: 6.483161926269531
Norm after each mp layer: 34.21957015991211
Norm before input: 0.2552422881126404
Norm after input: 0.4597535729408264
Norm after each mp layer: 1.5362149477005005
Norm after each mp layer: 6.6362528800964355
Norm after each mp layer: 35.389251708984375
Norm before input: 0.2552422881126404
Norm after input: 0.4597535729408264
Norm after each mp layer: 1.5362149477005005
Norm after each mp layer: 6.6362528800964355
Norm after each mp layer: 35.389251708984375
Norm before input: 0.2552422881126404
Norm after input: 0.46172040700912476
Norm after each mp layer: 1.5585100650787354
Norm after each mp layer: 6.711248874664307
Norm after each mp layer: 35.65999984741211
Norm before input: 0.2552422881126404
Norm after input: 0.46172040700912476
Norm after each mp layer: 1.5585100650787354
Norm after each mp layer: 6.711248874664307
Norm after each mp layer: 35.65999984741211
Norm before input: 0.2552422881126404
Norm after input: 0.4640007019042969
Norm after each mp layer: 1.580277919769287
Norm after each mp layer: 6.770143508911133
Norm after each mp layer: 35.75336837768555
Epoch: 55, Loss: 0.6608, Energy: 75547.2266, Train: 81.29%, Valid: 75.60%, Test: 75.00%, Best Valid: 75.60%, Best Test: 75.00%
Norm before input: 0.2552422881126404
Norm after input: 0.4640007019042969
Norm after each mp layer: 1.580277919769287
Norm after each mp layer: 6.770143508911133
Norm after each mp layer: 35.75336837768555
Norm before input: 0.2552422881126404
Norm after input: 0.4652213156223297
Norm after each mp layer: 1.607098937034607
Norm after each mp layer: 6.917021751403809
Norm after each mp layer: 36.88372802734375
Norm before input: 0.2552422881126404
Norm after input: 0.4652213156223297
Norm after each mp layer: 1.607098937034607
Norm after each mp layer: 6.917021751403809
Norm after each mp layer: 36.88372802734375
Norm before input: 0.2552422881126404
Norm after input: 0.4666838049888611
Norm after each mp layer: 1.6149213314056396
Norm after each mp layer: 6.933510780334473
Norm after each mp layer: 37.05302047729492
Norm before input: 0.2552422881126404
Norm after input: 0.4666838049888611
Norm after each mp layer: 1.6149213314056396
Norm after each mp layer: 6.933510780334473
Norm after each mp layer: 37.05302047729492
Norm before input: 0.2552422881126404
Norm after input: 0.46861979365348816
Norm after each mp layer: 1.6149511337280273
Norm after each mp layer: 6.874212265014648
Norm after each mp layer: 36.505741119384766
Norm before input: 0.2552422881126404
Norm after input: 0.46861979365348816
Norm after each mp layer: 1.6149511337280273
Norm after each mp layer: 6.874212265014648
Norm after each mp layer: 36.505741119384766
Norm before input: 0.2552422881126404
Norm after input: 0.4698854088783264
Norm after each mp layer: 1.6349576711654663
Norm after each mp layer: 6.981081008911133
Norm after each mp layer: 37.348358154296875
Norm before input: 0.2552422881126404
Norm after input: 0.4698854088783264
Norm after each mp layer: 1.6349576711654663
Norm after each mp layer: 6.981081008911133
Norm after each mp layer: 37.348358154296875
Norm before input: 0.2552422881126404
Norm after input: 0.471818208694458
Norm after each mp layer: 1.6627800464630127
Norm after each mp layer: 7.098620891571045
Norm after each mp layer: 37.94581604003906
Epoch: 60, Loss: 0.5544, Energy: 71694.7188, Train: 85.76%, Valid: 79.40%, Test: 78.20%, Best Valid: 79.40%, Best Test: 78.20%
Norm before input: 0.2552422881126404
Norm after input: 0.471818208694458
Norm after each mp layer: 1.6627800464630127
Norm after each mp layer: 7.098620891571045
Norm after each mp layer: 37.94581604003906
Norm before input: 0.2552422881126404
Norm after input: 0.47423824667930603
Norm after each mp layer: 1.6889392137527466
Norm after each mp layer: 7.180920600891113
Norm after each mp layer: 38.090675354003906
Norm before input: 0.2552422881126404
Norm after input: 0.47423824667930603
Norm after each mp layer: 1.6889392137527466
Norm after each mp layer: 7.180920600891113
Norm after each mp layer: 38.090675354003906
Norm before input: 0.2552422881126404
Norm after input: 0.47552815079689026
Norm after each mp layer: 1.7092511653900146
Norm after each mp layer: 7.310973644256592
Norm after each mp layer: 39.16609191894531
Norm before input: 0.2552422881126404
Norm after input: 0.47552815079689026
Norm after each mp layer: 1.7092511653900146
Norm after each mp layer: 7.310973644256592
Norm after each mp layer: 39.16609191894531
Norm before input: 0.2552422881126404
Norm after input: 0.47730258107185364
Norm after each mp layer: 1.7227463722229004
Norm after each mp layer: 7.362250328063965
Norm after each mp layer: 39.424949645996094
Norm before input: 0.2552422881126404
Norm after input: 0.47730258107185364
Norm after each mp layer: 1.7227463722229004
Norm after each mp layer: 7.362250328063965
Norm after each mp layer: 39.424949645996094
Norm before input: 0.2552422881126404
Norm after input: 0.4792134165763855
Norm after each mp layer: 1.738764762878418
Norm after each mp layer: 7.411985397338867
Norm after each mp layer: 39.515201568603516
Norm before input: 0.2552422881126404
Norm after input: 0.4792134165763855
Norm after each mp layer: 1.738764762878418
Norm after each mp layer: 7.411985397338867
Norm after each mp layer: 39.515201568603516
Norm before input: 0.2552422881126404
Norm after input: 0.4802444279193878
Norm after each mp layer: 1.7619174718856812
Norm after each mp layer: 7.559632301330566
Norm after each mp layer: 40.62596130371094
Epoch: 65, Loss: 0.4629, Energy: 66935.9219, Train: 88.08%, Valid: 81.20%, Test: 78.40%, Best Valid: 81.20%, Best Test: 78.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4802444279193878
Norm after each mp layer: 1.7619174718856812
Norm after each mp layer: 7.559632301330566
Norm after each mp layer: 40.62596130371094
Norm before input: 0.2552422881126404
Norm after input: 0.4817394018173218
Norm after each mp layer: 1.779746174812317
Norm after each mp layer: 7.614604473114014
Norm after each mp layer: 40.623287200927734
Norm before input: 0.2552422881126404
Norm after input: 0.4817394018173218
Norm after each mp layer: 1.779746174812317
Norm after each mp layer: 7.614604473114014
Norm after each mp layer: 40.623287200927734
Norm before input: 0.2552422881126404
Norm after input: 0.48249560594558716
Norm after each mp layer: 1.790903091430664
Norm after each mp layer: 7.66517448425293
Norm after each mp layer: 40.806549072265625
Norm before input: 0.2552422881126404
Norm after input: 0.48249560594558716
Norm after each mp layer: 1.790903091430664
Norm after each mp layer: 7.66517448425293
Norm after each mp layer: 40.806549072265625
Norm before input: 0.2552422881126404
Norm after input: 0.48225486278533936
Norm after each mp layer: 1.797285556793213
Norm after each mp layer: 7.742711544036865
Norm after each mp layer: 41.542484283447266
Norm before input: 0.2552422881126404
Norm after input: 0.48225486278533936
Norm after each mp layer: 1.797285556793213
Norm after each mp layer: 7.742711544036865
Norm after each mp layer: 41.542484283447266
Norm before input: 0.2552422881126404
Norm after input: 0.4826229214668274
Norm after each mp layer: 1.8029515743255615
Norm after each mp layer: 7.74294900894165
Norm after each mp layer: 41.18804168701172
Norm before input: 0.2552422881126404
Norm after input: 0.4826229214668274
Norm after each mp layer: 1.8029515743255615
Norm after each mp layer: 7.74294900894165
Norm after each mp layer: 41.18804168701172
Norm before input: 0.2552422881126404
Norm after input: 0.4819888174533844
Norm after each mp layer: 1.8122172355651855
Norm after each mp layer: 7.824439525604248
Norm after each mp layer: 41.80725860595703
Epoch: 70, Loss: 0.3987, Energy: 71476.2266, Train: 90.65%, Valid: 80.60%, Test: 78.80%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4819888174533844
Norm after each mp layer: 1.8122172355651855
Norm after each mp layer: 7.824439525604248
Norm after each mp layer: 41.80725860595703
Norm before input: 0.2552422881126404
Norm after input: 0.4813746511936188
Norm after each mp layer: 1.8199530839920044
Norm after each mp layer: 7.878676891326904
Norm after each mp layer: 42.106788635253906
Norm before input: 0.2552422881126404
Norm after input: 0.4813746511936188
Norm after each mp layer: 1.8199530839920044
Norm after each mp layer: 7.878676891326904
Norm after each mp layer: 42.10679244995117
Norm before input: 0.2552422881126404
Norm after input: 0.480787068605423
Norm after each mp layer: 1.8213027715682983
Norm after each mp layer: 7.876304626464844
Norm after each mp layer: 41.887847900390625
Norm before input: 0.2552422881126404
Norm after input: 0.480787068605423
Norm after each mp layer: 1.8213027715682983
Norm after each mp layer: 7.876304626464844
Norm after each mp layer: 41.887847900390625
Norm before input: 0.2552422881126404
Norm after input: 0.47928863763809204
Norm after each mp layer: 1.816311240196228
Norm after each mp layer: 7.895173072814941
Norm after each mp layer: 42.25564956665039
Norm before input: 0.2552422881126404
Norm after input: 0.47928863763809204
Norm after each mp layer: 1.816311240196228
Norm after each mp layer: 7.895173072814941
Norm after each mp layer: 42.25564956665039
Norm before input: 0.2552422881126404
Norm after input: 0.4784846901893616
Norm after each mp layer: 1.8125096559524536
Norm after each mp layer: 7.858238220214844
Norm after each mp layer: 41.79698181152344
Norm before input: 0.2552422881126404
Norm after input: 0.4784846901893616
Norm after each mp layer: 1.8125096559524536
Norm after each mp layer: 7.858238220214844
Norm after each mp layer: 41.79698181152344
Norm before input: 0.2552422881126404
Norm after input: 0.47727954387664795
Norm after each mp layer: 1.8114458322525024
Norm after each mp layer: 7.867854595184326
Norm after each mp layer: 41.88353729248047
Epoch: 75, Loss: 0.3511, Energy: 67992.9297, Train: 92.22%, Valid: 80.40%, Test: 78.50%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.47727954387664795
Norm after each mp layer: 1.8114458322525024
Norm after each mp layer: 7.867854595184326
Norm after each mp layer: 41.88353729248047
Norm before input: 0.2552422881126404
Norm after input: 0.4760914742946625
Norm after each mp layer: 1.810575008392334
Norm after each mp layer: 7.8784284591674805
Norm after each mp layer: 41.98146057128906
Norm before input: 0.2552422881126404
Norm after input: 0.4760914742946625
Norm after each mp layer: 1.810575008392334
Norm after each mp layer: 7.8784284591674805
Norm after each mp layer: 41.98146057128906
Norm before input: 0.2552422881126404
Norm after input: 0.4751843810081482
Norm after each mp layer: 1.806948184967041
Norm after each mp layer: 7.851804733276367
Norm after each mp layer: 41.67878723144531
Norm before input: 0.2552422881126404
Norm after input: 0.4751843810081482
Norm after each mp layer: 1.806948184967041
Norm after each mp layer: 7.851804733276367
Norm after each mp layer: 41.67878723144531
Norm before input: 0.2552422881126404
Norm after input: 0.47355422377586365
Norm after each mp layer: 1.7993111610412598
Norm after each mp layer: 7.862588882446289
Norm after each mp layer: 42.055118560791016
Norm before input: 0.2552422881126404
Norm after input: 0.47355422377586365
Norm after each mp layer: 1.7993111610412598
Norm after each mp layer: 7.862588882446289
Norm after each mp layer: 42.055118560791016
Norm before input: 0.2552422881126404
Norm after input: 0.4729171097278595
Norm after each mp layer: 1.7957981824874878
Norm after each mp layer: 7.818269729614258
Norm after each mp layer: 41.49314498901367
Norm before input: 0.2552422881126404
Norm after input: 0.4729171097278595
Norm after each mp layer: 1.7957981824874878
Norm after each mp layer: 7.818269729614258
Norm after each mp layer: 41.49314498901367
Norm before input: 0.2552422881126404
Norm after input: 0.4714978337287903
Norm after each mp layer: 1.7958861589431763
Norm after each mp layer: 7.857774257659912
Norm after each mp layer: 41.94403839111328
Epoch: 80, Loss: 0.3165, Energy: 61393.3516, Train: 93.46%, Valid: 80.00%, Test: 78.10%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4714978337287903
Norm after each mp layer: 1.7958861589431763
Norm after each mp layer: 7.857773780822754
Norm after each mp layer: 41.94403839111328
Norm before input: 0.2552422881126404
Norm after input: 0.47057104110717773
Norm after each mp layer: 1.7923293113708496
Norm after each mp layer: 7.833182334899902
Norm after each mp layer: 41.65478515625
Norm before input: 0.2552422881126404
Norm after input: 0.47057104110717773
Norm after each mp layer: 1.7923293113708496
Norm after each mp layer: 7.833182334899902
Norm after each mp layer: 41.65478515625
Norm before input: 0.2552422881126404
Norm after input: 0.4694652855396271
Norm after each mp layer: 1.7839142084121704
Norm after each mp layer: 7.792404651641846
Norm after each mp layer: 41.34111404418945
Norm before input: 0.2552422881126404
Norm after input: 0.4694652855396271
Norm after each mp layer: 1.7839142084121704
Norm after each mp layer: 7.792404651641846
Norm after each mp layer: 41.34111404418945
Norm before input: 0.2552422881126404
Norm after input: 0.4678569436073303
Norm after each mp layer: 1.7764079570770264
Norm after each mp layer: 7.793786525726318
Norm after each mp layer: 41.5678825378418
Norm before input: 0.2552422881126404
Norm after input: 0.4678569436073303
Norm after each mp layer: 1.7764079570770264
Norm after each mp layer: 7.793786525726318
Norm after each mp layer: 41.5678825378418
Norm before input: 0.2552422881126404
Norm after input: 0.4670936167240143
Norm after each mp layer: 1.7728846073150635
Norm after each mp layer: 7.753121376037598
Norm after each mp layer: 41.026187896728516
Norm before input: 0.2552422881126404
Norm after input: 0.4670936167240143
Norm after each mp layer: 1.7728846073150635
Norm after each mp layer: 7.753121376037598
Norm after each mp layer: 41.026187896728516
Norm before input: 0.2552422881126404
Norm after input: 0.46562886238098145
Norm after each mp layer: 1.7697663307189941
Norm after each mp layer: 7.769284248352051
Norm after each mp layer: 41.239688873291016
Epoch: 85, Loss: 0.2824, Energy: 56039.6992, Train: 94.04%, Valid: 80.00%, Test: 77.80%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.46562886238098145
Norm after each mp layer: 1.7697663307189941
Norm after each mp layer: 7.769284248352051
Norm after each mp layer: 41.239688873291016
Norm before input: 0.2552422881126404
Norm after input: 0.46445444226264954
Norm after each mp layer: 1.7641528844833374
Norm after each mp layer: 7.755939960479736
Norm after each mp layer: 41.135684967041016
Norm before input: 0.2552422881126404
Norm after input: 0.46445444226264954
Norm after each mp layer: 1.7641528844833374
Norm after each mp layer: 7.755939960479736
Norm after each mp layer: 41.135684967041016
Norm before input: 0.2552422881126404
Norm after input: 0.46345382928848267
Norm after each mp layer: 1.7565513849258423
Norm after each mp layer: 7.725247383117676
Norm after each mp layer: 40.86858367919922
Norm before input: 0.2552422881126404
Norm after input: 0.46345382928848267
Norm after each mp layer: 1.7565513849258423
Norm after each mp layer: 7.725247383117676
Norm after each mp layer: 40.86858367919922
Norm before input: 0.2552422881126404
Norm after input: 0.4620093107223511
Norm after each mp layer: 1.7503011226654053
Norm after each mp layer: 7.742558002471924
Norm after each mp layer: 41.201866149902344
Norm before input: 0.2552422881126404
Norm after input: 0.4620093107223511
Norm after each mp layer: 1.7503011226654053
Norm after each mp layer: 7.742558002471924
Norm after each mp layer: 41.201866149902344
Norm before input: 0.2552422881126404
Norm after input: 0.46151623129844666
Norm after each mp layer: 1.7484276294708252
Norm after each mp layer: 7.718236446380615
Norm after each mp layer: 40.76984405517578
Norm before input: 0.2552422881126404
Norm after input: 0.46151623129844666
Norm after each mp layer: 1.7484276294708252
Norm after each mp layer: 7.718236446380615
Norm after each mp layer: 40.76984405517578
Norm before input: 0.2552422881126404
Norm after input: 0.46011364459991455
Norm after each mp layer: 1.7438535690307617
Norm after each mp layer: 7.768035411834717
Norm after each mp layer: 41.46923828125
Epoch: 90, Loss: 0.2580, Energy: 49165.9531, Train: 93.71%, Valid: 80.60%, Test: 78.20%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.46011364459991455
Norm after each mp layer: 1.7438535690307617
Norm after each mp layer: 7.768035411834717
Norm after each mp layer: 41.46923828125
Norm before input: 0.2552422881126404
Norm after input: 0.46000608801841736
Norm after each mp layer: 1.741479516029358
Norm after each mp layer: 7.719564914703369
Norm after each mp layer: 40.7137451171875
Norm before input: 0.2552422881126404
Norm after input: 0.46000608801841736
Norm after each mp layer: 1.741479516029358
Norm after each mp layer: 7.719564914703369
Norm after each mp layer: 40.7137451171875
Norm before input: 0.2552422881126404
Norm after input: 0.4588353931903839
Norm after each mp layer: 1.738327145576477
Norm after each mp layer: 7.760364055633545
Norm after each mp layer: 41.23381042480469
Norm before input: 0.2552422881126404
Norm after input: 0.4588353931903839
Norm after each mp layer: 1.738327145576477
Norm after each mp layer: 7.760364055633545
Norm after each mp layer: 41.23381042480469
Norm before input: 0.2552422881126404
Norm after input: 0.4582503139972687
Norm after each mp layer: 1.734845519065857
Norm after each mp layer: 7.754771709442139
Norm after each mp layer: 41.12272644042969
Norm before input: 0.2552422881126404
Norm after input: 0.4582503139972687
Norm after each mp layer: 1.734845519065857
Norm after each mp layer: 7.754771709442139
Norm after each mp layer: 41.12272644042969
Norm before input: 0.2552422881126404
Norm after input: 0.45795688033103943
Norm after each mp layer: 1.7318713665008545
Norm after each mp layer: 7.7387166023254395
Norm after each mp layer: 40.835540771484375
Norm before input: 0.2552422881126404
Norm after input: 0.45795688033103943
Norm after each mp layer: 1.7318713665008545
Norm after each mp layer: 7.7387166023254395
Norm after each mp layer: 40.835540771484375
Norm before input: 0.2552422881126404
Norm after input: 0.4571066200733185
Norm after each mp layer: 1.7309184074401855
Norm after each mp layer: 7.785496711730957
Norm after each mp layer: 41.31257247924805
Epoch: 95, Loss: 0.2369, Energy: 46817.6758, Train: 95.03%, Valid: 79.40%, Test: 77.60%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4571066200733185
Norm after each mp layer: 1.7309184074401855
Norm after each mp layer: 7.785496711730957
Norm after each mp layer: 41.31257247924805
Norm before input: 0.2552422881126404
Norm after input: 0.4567064940929413
Norm after each mp layer: 1.728717565536499
Norm after each mp layer: 7.790219306945801
Norm after each mp layer: 41.25829315185547
Norm before input: 0.2552422881126404
Norm after input: 0.4567064940929413
Norm after each mp layer: 1.728717565536499
Norm after each mp layer: 7.790219306945801
Norm after each mp layer: 41.25829315185547
Norm before input: 0.2552422881126404
Norm after input: 0.45645496249198914
Norm after each mp layer: 1.725091814994812
Norm after each mp layer: 7.780698299407959
Norm after each mp layer: 41.07322692871094
Norm before input: 0.2552422881126404
Norm after input: 0.45645496249198914
Norm after each mp layer: 1.725091814994812
Norm after each mp layer: 7.780698299407959
Norm after each mp layer: 41.07322692871094
Norm before input: 0.2552422881126404
Norm after input: 0.4557172656059265
Norm after each mp layer: 1.7221546173095703
Norm after each mp layer: 7.8185553550720215
Norm after each mp layer: 41.499481201171875
Norm before input: 0.2552422881126404
Norm after input: 0.4557172656059265
Norm after each mp layer: 1.7221546173095703
Norm after each mp layer: 7.8185553550720215
Norm after each mp layer: 41.499481201171875
Norm before input: 0.2552422881126404
Norm after input: 0.45548874139785767
Norm after each mp layer: 1.719976782798767
Norm after each mp layer: 7.810545444488525
Norm after each mp layer: 41.273048400878906
Norm before input: 0.2552422881126404
Norm after input: 0.45548874139785767
Norm after each mp layer: 1.719976782798767
Norm after each mp layer: 7.810545444488525
Norm after each mp layer: 41.273048400878906
Norm before input: 0.2552422881126404
Norm after input: 0.4551481306552887
Norm after each mp layer: 1.7179315090179443
Norm after each mp layer: 7.819990634918213
Norm after each mp layer: 41.28022384643555
Epoch: 100, Loss: 0.2094, Energy: 43572.5898, Train: 96.11%, Valid: 78.20%, Test: 76.30%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4551481306552887
Norm after each mp layer: 1.7179315090179443
Norm after each mp layer: 7.819990634918213
Norm after each mp layer: 41.28022384643555
Norm before input: 0.2552422881126404
Norm after input: 0.4546443223953247
Norm after each mp layer: 1.715195894241333
Norm after each mp layer: 7.845897674560547
Norm after each mp layer: 41.546592712402344
Norm before input: 0.2552422881126404
Norm after input: 0.4546443223953247
Norm after each mp layer: 1.715195894241333
Norm after each mp layer: 7.845897674560547
Norm after each mp layer: 41.546592712402344
Norm before input: 0.2552422881126404
Norm after input: 0.45458078384399414
Norm after each mp layer: 1.7114248275756836
Norm after each mp layer: 7.828924655914307
Norm after each mp layer: 41.28458023071289
Norm before input: 0.2552422881126404
Norm after input: 0.45458078384399414
Norm after each mp layer: 1.7114248275756836
Norm after each mp layer: 7.828924655914307
Norm after each mp layer: 41.28458023071289
Norm before input: 0.2552422881126404
Norm after input: 0.4543931186199188
Norm after each mp layer: 1.710233211517334
Norm after each mp layer: 7.849505424499512
Norm after each mp layer: 41.4097900390625
Norm before input: 0.2552422881126404
Norm after input: 0.4543931186199188
Norm after each mp layer: 1.710233211517334
Norm after each mp layer: 7.849505424499512
Norm after each mp layer: 41.4097900390625
Norm before input: 0.2552422881126404
Norm after input: 0.45427757501602173
Norm after each mp layer: 1.7110484838485718
Norm after each mp layer: 7.888776779174805
Norm after each mp layer: 41.69143295288086
Norm before input: 0.2552422881126404
Norm after input: 0.45427757501602173
Norm after each mp layer: 1.7110484838485718
Norm after each mp layer: 7.888776779174805
Norm after each mp layer: 41.69143295288086
Norm before input: 0.2552422881126404
Norm after input: 0.4545133709907532
Norm after each mp layer: 1.7104052305221558
Norm after each mp layer: 7.894076824188232
Norm after each mp layer: 41.592533111572266
Epoch: 105, Loss: 0.1929, Energy: 41765.8203, Train: 96.69%, Valid: 78.00%, Test: 75.50%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4545133709907532
Norm after each mp layer: 1.7104052305221558
Norm after each mp layer: 7.894077301025391
Norm after each mp layer: 41.592533111572266
Norm before input: 0.2552422881126404
Norm after input: 0.454475998878479
Norm after each mp layer: 1.710472822189331
Norm after each mp layer: 7.958089351654053
Norm after each mp layer: 42.227508544921875
Norm before input: 0.2552422881126404
Norm after input: 0.454475998878479
Norm after each mp layer: 1.710472822189331
Norm after each mp layer: 7.958089351654053
Norm after each mp layer: 42.227508544921875
Norm before input: 0.2552422881126404
Norm after input: 0.4549793004989624
Norm after each mp layer: 1.7107793092727661
Norm after each mp layer: 7.951178073883057
Norm after each mp layer: 41.93815612792969
Norm before input: 0.2552422881126404
Norm after input: 0.4549793004989624
Norm after each mp layer: 1.7107793092727661
Norm after each mp layer: 7.951178073883057
Norm after each mp layer: 41.93815612792969
Norm before input: 0.2552422881126404
Norm after input: 0.45490363240242004
Norm after each mp layer: 1.7185076475143433
Norm after each mp layer: 8.046669960021973
Norm after each mp layer: 42.646785736083984
Norm before input: 0.2552422881126404
Norm after input: 0.45490363240242004
Norm after each mp layer: 1.7185076475143433
Norm after each mp layer: 8.046669960021973
Norm after each mp layer: 42.646785736083984
Norm before input: 0.2552422881126404
Norm after input: 0.4549904465675354
Norm after each mp layer: 1.7185699939727783
Norm after each mp layer: 8.0527982711792
Norm after each mp layer: 42.5679817199707
Norm before input: 0.2552422881126404
Norm after input: 0.4549904465675354
Norm after each mp layer: 1.7185699939727783
Norm after each mp layer: 8.0527982711792
Norm after each mp layer: 42.5679817199707
Norm before input: 0.2552422881126404
Norm after input: 0.45510876178741455
Norm after each mp layer: 1.7160838842391968
Norm after each mp layer: 8.047995567321777
Norm after each mp layer: 42.4803352355957
Epoch: 110, Loss: 0.1758, Energy: 41530.1719, Train: 96.52%, Valid: 77.60%, Test: 75.50%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.45510876178741455
Norm after each mp layer: 1.7160838842391968
Norm after each mp layer: 8.047995567321777
Norm after each mp layer: 42.4803352355957
Norm before input: 0.2552422881126404
Norm after input: 0.4550483524799347
Norm after each mp layer: 1.7185277938842773
Norm after each mp layer: 8.10295295715332
Norm after each mp layer: 42.91687774658203
Norm before input: 0.2552422881126404
Norm after input: 0.4550483524799347
Norm after each mp layer: 1.7185277938842773
Norm after each mp layer: 8.10295295715332
Norm after each mp layer: 42.91687774658203
Norm before input: 0.2552422881126404
Norm after input: 0.4552549421787262
Norm after each mp layer: 1.7186360359191895
Norm after each mp layer: 8.111858367919922
Norm after each mp layer: 42.85846710205078
Norm before input: 0.2552422881126404
Norm after input: 0.4552549421787262
Norm after each mp layer: 1.7186360359191895
Norm after each mp layer: 8.111858367919922
Norm after each mp layer: 42.85846710205078
Norm before input: 0.2552422881126404
Norm after input: 0.45564010739326477
Norm after each mp layer: 1.7202905416488647
Norm after each mp layer: 8.127379417419434
Norm after each mp layer: 42.81523895263672
Norm before input: 0.2552422881126404
Norm after input: 0.45564010739326477
Norm after each mp layer: 1.7202905416488647
Norm after each mp layer: 8.127379417419434
Norm after each mp layer: 42.81523895263672
Norm before input: 0.2552422881126404
Norm after input: 0.4557649493217468
Norm after each mp layer: 1.7241899967193604
Norm after each mp layer: 8.190531730651855
Norm after each mp layer: 43.252960205078125
Norm before input: 0.2552422881126404
Norm after input: 0.4557649493217468
Norm after each mp layer: 1.7241899967193604
Norm after each mp layer: 8.190532684326172
Norm after each mp layer: 43.252960205078125
Norm before input: 0.2552422881126404
Norm after input: 0.4559501111507416
Norm after each mp layer: 1.7208267450332642
Norm after each mp layer: 8.178345680236816
Norm after each mp layer: 43.1046028137207
Epoch: 115, Loss: 0.1680, Energy: 41921.4805, Train: 97.19%, Valid: 78.40%, Test: 75.20%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4559501111507416
Norm after each mp layer: 1.7208267450332642
Norm after each mp layer: 8.178345680236816
Norm after each mp layer: 43.1046028137207
Norm before input: 0.2552422881126404
Norm after input: 0.45623528957366943
Norm after each mp layer: 1.7198283672332764
Norm after each mp layer: 8.178654670715332
Norm after each mp layer: 43.019309997558594
Norm before input: 0.2552422881126404
Norm after input: 0.45623528957366943
Norm after each mp layer: 1.7198283672332764
Norm after each mp layer: 8.178654670715332
Norm after each mp layer: 43.019309997558594
Norm before input: 0.2552422881126404
Norm after input: 0.4563915431499481
Norm after each mp layer: 1.7260938882827759
Norm after each mp layer: 8.248577117919922
Norm after each mp layer: 43.44404220581055
Norm before input: 0.2552422881126404
Norm after input: 0.4563915431499481
Norm after each mp layer: 1.7260938882827759
Norm after each mp layer: 8.248577117919922
Norm after each mp layer: 43.44404220581055
Norm before input: 0.2552422881126404
Norm after input: 0.45653700828552246
Norm after each mp layer: 1.7243958711624146
Norm after each mp layer: 8.234976768493652
Norm after each mp layer: 43.23869323730469
Norm before input: 0.2552422881126404
Norm after input: 0.45653700828552246
Norm after each mp layer: 1.7243958711624146
Norm after each mp layer: 8.234976768493652
Norm after each mp layer: 43.23869323730469
Norm before input: 0.2552422881126404
Norm after input: 0.4566706717014313
Norm after each mp layer: 1.7226052284240723
Norm after each mp layer: 8.229568481445312
Norm after each mp layer: 43.15174102783203
Norm before input: 0.2552422881126404
Norm after input: 0.4566706717014313
Norm after each mp layer: 1.7226052284240723
Norm after each mp layer: 8.229568481445312
Norm after each mp layer: 43.15174102783203
Norm before input: 0.2552422881126404
Norm after input: 0.45673707127571106
Norm after each mp layer: 1.7253961563110352
Norm after each mp layer: 8.273849487304688
Norm after each mp layer: 43.44700241088867
Epoch: 120, Loss: 0.1526, Energy: 38153.1211, Train: 97.43%, Valid: 78.00%, Test: 74.90%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.45673707127571106
Norm after each mp layer: 1.7253961563110352
Norm after each mp layer: 8.273849487304688
Norm after each mp layer: 43.44700241088867
Norm before input: 0.2552422881126404
Norm after input: 0.45693740248680115
Norm after each mp layer: 1.7237697839736938
Norm after each mp layer: 8.261153221130371
Norm after each mp layer: 43.266868591308594
Norm before input: 0.2552422881126404
Norm after input: 0.45693740248680115
Norm after each mp layer: 1.7237697839736938
Norm after each mp layer: 8.261153221130371
Norm after each mp layer: 43.266868591308594
Norm before input: 0.2552422881126404
Norm after input: 0.4572370648384094
Norm after each mp layer: 1.7257120609283447
Norm after each mp layer: 8.27958869934082
Norm after each mp layer: 43.298973083496094
Norm before input: 0.2552422881126404
Norm after input: 0.4572370648384094
Norm after each mp layer: 1.7257120609283447
Norm after each mp layer: 8.27958869934082
Norm after each mp layer: 43.298973083496094
Norm before input: 0.2552422881126404
Norm after input: 0.4576664865016937
Norm after each mp layer: 1.7341139316558838
Norm after each mp layer: 8.390437126159668
Norm after each mp layer: 44.13234329223633
Norm before input: 0.2552422881126404
Norm after input: 0.4576664865016937
Norm after each mp layer: 1.7341139316558838
Norm after each mp layer: 8.390437126159668
Norm after each mp layer: 44.13234329223633
Norm before input: 0.2552422881126404
Norm after input: 0.4578247368335724
Norm after each mp layer: 1.7293181419372559
Norm after each mp layer: 8.332125663757324
Norm after each mp layer: 43.61473846435547
Norm before input: 0.2552422881126404
Norm after input: 0.4578247368335724
Norm after each mp layer: 1.7293181419372559
Norm after each mp layer: 8.332125663757324
Norm after each mp layer: 43.61473846435547
Norm before input: 0.2552422881126404
Norm after input: 0.45785635709762573
Norm after each mp layer: 1.7404308319091797
Norm after each mp layer: 8.448284149169922
Norm after each mp layer: 44.370174407958984
Epoch: 125, Loss: 0.1715, Energy: 37961.2344, Train: 97.02%, Valid: 76.60%, Test: 75.30%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.45785635709762573
Norm after each mp layer: 1.7404308319091797
Norm after each mp layer: 8.448284149169922
Norm after each mp layer: 44.370174407958984
Norm before input: 0.2552422881126404
Norm after input: 0.45761507749557495
Norm after each mp layer: 1.739668846130371
Norm after each mp layer: 8.445290565490723
Norm after each mp layer: 44.30781555175781
Norm before input: 0.2552422881126404
Norm after input: 0.45761507749557495
Norm after each mp layer: 1.739668846130371
Norm after each mp layer: 8.445290565490723
Norm after each mp layer: 44.30781555175781
Norm before input: 0.2552422881126404
Norm after input: 0.45750537514686584
Norm after each mp layer: 1.735379695892334
Norm after each mp layer: 8.406182289123535
Norm after each mp layer: 43.99872970581055
Norm before input: 0.2552422881126404
Norm after input: 0.45750537514686584
Norm after each mp layer: 1.735379695892334
Norm after each mp layer: 8.406182289123535
Norm after each mp layer: 43.99872970581055
Norm before input: 0.2552422881126404
Norm after input: 0.4577333629131317
Norm after each mp layer: 1.744303822517395
Norm after each mp layer: 8.480842590332031
Norm after each mp layer: 44.323524475097656
Norm before input: 0.2552422881126404
Norm after input: 0.4577333629131317
Norm after each mp layer: 1.744303822517395
Norm after each mp layer: 8.480842590332031
Norm after each mp layer: 44.323524475097656
Norm before input: 0.2552422881126404
Norm after input: 0.45804721117019653
Norm after each mp layer: 1.7483681440353394
Norm after each mp layer: 8.521875381469727
Norm after each mp layer: 44.49628829956055
Norm before input: 0.2552422881126404
Norm after input: 0.45804721117019653
Norm after each mp layer: 1.7483681440353394
Norm after each mp layer: 8.521875381469727
Norm after each mp layer: 44.49628829956055
Norm before input: 0.2552422881126404
Norm after input: 0.4579814374446869
Norm after each mp layer: 1.739119291305542
Norm after each mp layer: 8.454951286315918
Norm after each mp layer: 44.06502151489258
Epoch: 130, Loss: 0.1425, Energy: 39897.4766, Train: 97.43%, Valid: 75.40%, Test: 75.20%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4579814374446869
Norm after each mp layer: 1.739119291305542
Norm after each mp layer: 8.454951286315918
Norm after each mp layer: 44.06502151489258
Norm before input: 0.2552422881126404
Norm after input: 0.4582241177558899
Norm after each mp layer: 1.7405773401260376
Norm after each mp layer: 8.469135284423828
Norm after each mp layer: 44.07695388793945
Norm before input: 0.2552422881126404
Norm after input: 0.4582241177558899
Norm after each mp layer: 1.7405773401260376
Norm after each mp layer: 8.469135284423828
Norm after each mp layer: 44.07695388793945
Norm before input: 0.2552422881126404
Norm after input: 0.4586239159107208
Norm after each mp layer: 1.7525416612625122
Norm after each mp layer: 8.569753646850586
Norm after each mp layer: 44.569759368896484
Norm before input: 0.2552422881126404
Norm after input: 0.4586239159107208
Norm after each mp layer: 1.7525416612625122
Norm after each mp layer: 8.569753646850586
Norm after each mp layer: 44.569759368896484
Norm before input: 0.2552422881126404
Norm after input: 0.45852988958358765
Norm after each mp layer: 1.7513738870620728
Norm after each mp layer: 8.562722206115723
Norm after each mp layer: 44.49169158935547
Norm before input: 0.2552422881126404
Norm after input: 0.45852988958358765
Norm after each mp layer: 1.7513738870620728
Norm after each mp layer: 8.562722206115723
Norm after each mp layer: 44.49169158935547
Norm before input: 0.2552422881126404
Norm after input: 0.4582847058773041
Norm after each mp layer: 1.7447608709335327
Norm after each mp layer: 8.51729965209961
Norm after each mp layer: 44.25727081298828
Norm before input: 0.2552422881126404
Norm after input: 0.4582847058773041
Norm after each mp layer: 1.7447608709335327
Norm after each mp layer: 8.51729965209961
Norm after each mp layer: 44.25727081298828
Norm before input: 0.2552422881126404
Norm after input: 0.458219051361084
Norm after each mp layer: 1.7473785877227783
Norm after each mp layer: 8.551721572875977
Norm after each mp layer: 44.44635009765625
Epoch: 135, Loss: 0.1303, Energy: 35787.2383, Train: 97.85%, Valid: 75.60%, Test: 75.10%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.458219051361084
Norm after each mp layer: 1.7473785877227783
Norm after each mp layer: 8.551721572875977
Norm after each mp layer: 44.44635009765625
Norm before input: 0.2552422881126404
Norm after input: 0.4583023488521576
Norm after each mp layer: 1.7506585121154785
Norm after each mp layer: 8.577553749084473
Norm after each mp layer: 44.514549255371094
Norm before input: 0.2552422881126404
Norm after input: 0.4583023488521576
Norm after each mp layer: 1.7506585121154785
Norm after each mp layer: 8.577553749084473
Norm after each mp layer: 44.514549255371094
Norm before input: 0.2552422881126404
Norm after input: 0.458315908908844
Norm after each mp layer: 1.7486158609390259
Norm after each mp layer: 8.55258560180664
Norm after each mp layer: 44.26030349731445
Norm before input: 0.2552422881126404
Norm after input: 0.458315908908844
Norm after each mp layer: 1.7486158609390259
Norm after each mp layer: 8.55258560180664
Norm after each mp layer: 44.26030349731445
Norm before input: 0.2552422881126404
Norm after input: 0.4584239721298218
Norm after each mp layer: 1.7487787008285522
Norm after each mp layer: 8.558900833129883
Norm after each mp layer: 44.23883819580078
Norm before input: 0.2552422881126404
Norm after input: 0.4584239721298218
Norm after each mp layer: 1.7487787008285522
Norm after each mp layer: 8.558899879455566
Norm after each mp layer: 44.23883819580078
Norm before input: 0.2552422881126404
Norm after input: 0.458618700504303
Norm after each mp layer: 1.7499419450759888
Norm after each mp layer: 8.579887390136719
Norm after each mp layer: 44.33905029296875
Norm before input: 0.2552422881126404
Norm after input: 0.458618700504303
Norm after each mp layer: 1.7499419450759888
Norm after each mp layer: 8.579887390136719
Norm after each mp layer: 44.33905029296875
Norm before input: 0.2552422881126404
Norm after input: 0.4587814211845398
Norm after each mp layer: 1.7479441165924072
Norm after each mp layer: 8.565596580505371
Norm after each mp layer: 44.20189666748047
Epoch: 140, Loss: 0.1178, Energy: 34093.8945, Train: 98.01%, Valid: 75.20%, Test: 74.40%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4587814211845398
Norm after each mp layer: 1.7479441165924072
Norm after each mp layer: 8.565596580505371
Norm after each mp layer: 44.20189666748047
Norm before input: 0.2552422881126404
Norm after input: 0.4590868651866913
Norm after each mp layer: 1.750162959098816
Norm after each mp layer: 8.582048416137695
Norm after each mp layer: 44.23109817504883
Norm before input: 0.2552422881126404
Norm after input: 0.4590868651866913
Norm after each mp layer: 1.750162959098816
Norm after each mp layer: 8.582048416137695
Norm after each mp layer: 44.23109817504883
Norm before input: 0.2552422881126404
Norm after input: 0.45936307311058044
Norm after each mp layer: 1.757408618927002
Norm after each mp layer: 8.650944709777832
Norm after each mp layer: 44.608123779296875
Norm before input: 0.2552422881126404
Norm after input: 0.45936307311058044
Norm after each mp layer: 1.757408618927002
Norm after each mp layer: 8.650944709777832
Norm after each mp layer: 44.608123779296875
Norm before input: 0.2552422881126404
Norm after input: 0.4593164622783661
Norm after each mp layer: 1.7558221817016602
Norm after each mp layer: 8.64976978302002
Norm after each mp layer: 44.64501953125
Norm before input: 0.2552422881126404
Norm after input: 0.4593164622783661
Norm after each mp layer: 1.7558221817016602
Norm after each mp layer: 8.64976978302002
Norm after each mp layer: 44.64501953125
Norm before input: 0.2552422881126404
Norm after input: 0.4592452645301819
Norm after each mp layer: 1.753220796585083
Norm after each mp layer: 8.636980056762695
Norm after each mp layer: 44.61532211303711
Norm before input: 0.2552422881126404
Norm after input: 0.4592452645301819
Norm after each mp layer: 1.753220796585083
Norm after each mp layer: 8.636980056762695
Norm after each mp layer: 44.61532211303711
Norm before input: 0.2552422881126404
Norm after input: 0.4594116508960724
Norm after each mp layer: 1.7608675956726074
Norm after each mp layer: 8.713879585266113
Norm after each mp layer: 45.0706787109375
Epoch: 145, Loss: 0.1102, Energy: 32277.0625, Train: 97.93%, Valid: 75.20%, Test: 74.40%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4594116508960724
Norm after each mp layer: 1.7608675956726074
Norm after each mp layer: 8.713879585266113
Norm after each mp layer: 45.0706787109375
Norm before input: 0.2552422881126404
Norm after input: 0.4594004452228546
Norm after each mp layer: 1.7618613243103027
Norm after each mp layer: 8.721535682678223
Norm after each mp layer: 45.08966827392578
Norm before input: 0.2552422881126404
Norm after input: 0.4594004452228546
Norm after each mp layer: 1.7618613243103027
Norm after each mp layer: 8.721535682678223
Norm after each mp layer: 45.08966827392578
Norm before input: 0.2552422881126404
Norm after input: 0.45935964584350586
Norm after each mp layer: 1.759211540222168
Norm after each mp layer: 8.708280563354492
Norm after each mp layer: 45.03972625732422
Norm before input: 0.2552422881126404
Norm after input: 0.45935964584350586
Norm after each mp layer: 1.759211540222168
Norm after each mp layer: 8.708280563354492
Norm after each mp layer: 45.03972625732422
Norm before input: 0.2552422881126404
Norm after input: 0.4594799876213074
Norm after each mp layer: 1.7610905170440674
Norm after each mp layer: 8.740662574768066
Norm after each mp layer: 45.2423095703125
Norm before input: 0.2552422881126404
Norm after input: 0.4594799876213074
Norm after each mp layer: 1.7610905170440674
Norm after each mp layer: 8.740662574768066
Norm after each mp layer: 45.2423095703125
Norm before input: 0.2552422881126404
Norm after input: 0.4596864879131317
Norm after each mp layer: 1.7608832120895386
Norm after each mp layer: 8.744510650634766
Norm after each mp layer: 45.2331657409668
Norm before input: 0.2552422881126404
Norm after input: 0.4596864879131317
Norm after each mp layer: 1.7608832120895386
Norm after each mp layer: 8.744510650634766
Norm after each mp layer: 45.2331657409668
Norm before input: 0.2552422881126404
Norm after input: 0.46000006794929504
Norm after each mp layer: 1.7602895498275757
Norm after each mp layer: 8.736648559570312
Norm after each mp layer: 45.12343978881836
Epoch: 150, Loss: 0.1015, Energy: 32182.6113, Train: 97.85%, Valid: 74.00%, Test: 73.60%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.46000006794929504
Norm after each mp layer: 1.7602895498275757
Norm after each mp layer: 8.736648559570312
Norm after each mp layer: 45.12343978881836
Norm before input: 0.2552422881126404
Norm after input: 0.4603692293167114
Norm after each mp layer: 1.7659426927566528
Norm after each mp layer: 8.789769172668457
Norm after each mp layer: 45.373130798339844
Norm before input: 0.2552422881126404
Norm after input: 0.4603692293167114
Norm after each mp layer: 1.7659426927566528
Norm after each mp layer: 8.789769172668457
Norm after each mp layer: 45.373130798339844
Norm before input: 0.2552422881126404
Norm after input: 0.4605046510696411
Norm after each mp layer: 1.765564203262329
Norm after each mp layer: 8.799619674682617
Norm after each mp layer: 45.453033447265625
Norm before input: 0.2552422881126404
Norm after input: 0.4605046510696411
Norm after each mp layer: 1.765564203262329
Norm after each mp layer: 8.799619674682617
Norm after each mp layer: 45.453033447265625
Norm before input: 0.2552422881126404
Norm after input: 0.4604979455471039
Norm after each mp layer: 1.7616846561431885
Norm after each mp layer: 8.773980140686035
Norm after each mp layer: 45.336978912353516
Norm before input: 0.2552422881126404
Norm after input: 0.4604979455471039
Norm after each mp layer: 1.7616846561431885
Norm after each mp layer: 8.773980140686035
Norm after each mp layer: 45.336978912353516
Norm before input: 0.2552422881126404
Norm after input: 0.46064332127571106
Norm after each mp layer: 1.7678498029708862
Norm after each mp layer: 8.82883071899414
Norm after each mp layer: 45.63225173950195
Norm before input: 0.2552422881126404
Norm after input: 0.46064332127571106
Norm after each mp layer: 1.7678498029708862
Norm after each mp layer: 8.82883071899414
Norm after each mp layer: 45.63225173950195
Norm before input: 0.2552422881126404
Norm after input: 0.46069803833961487
Norm after each mp layer: 1.7717775106430054
Norm after each mp layer: 8.868718147277832
Norm after each mp layer: 45.8751220703125
Epoch: 155, Loss: 0.0939, Energy: 29987.5508, Train: 98.26%, Valid: 74.40%, Test: 73.50%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.46069803833961487
Norm after each mp layer: 1.7717775106430054
Norm after each mp layer: 8.868718147277832
Norm after each mp layer: 45.8751220703125
Norm before input: 0.2552422881126404
Norm after input: 0.4605007767677307
Norm after each mp layer: 1.7672189474105835
Norm after each mp layer: 8.841571807861328
Norm after each mp layer: 45.79018020629883
Norm before input: 0.2552422881126404
Norm after input: 0.4605007767677307
Norm after each mp layer: 1.7672189474105835
Norm after each mp layer: 8.841571807861328
Norm after each mp layer: 45.79018020629883
Norm before input: 0.2552422881126404
Norm after input: 0.4605480134487152
Norm after each mp layer: 1.7674453258514404
Norm after each mp layer: 8.85159683227539
Norm after each mp layer: 45.85927200317383
Norm before input: 0.2552422881126404
Norm after input: 0.4605480134487152
Norm after each mp layer: 1.7674453258514404
Norm after each mp layer: 8.85159683227539
Norm after each mp layer: 45.85927200317383
Norm before input: 0.2552422881126404
Norm after input: 0.46087998151779175
Norm after each mp layer: 1.7721437215805054
Norm after each mp layer: 8.893360137939453
Norm after each mp layer: 46.038543701171875
Norm before input: 0.2552422881126404
Norm after input: 0.46087998151779175
Norm after each mp layer: 1.7721437215805054
Norm after each mp layer: 8.893360137939453
Norm after each mp layer: 46.038543701171875
Norm before input: 0.2552422881126404
Norm after input: 0.46112051606178284
Norm after each mp layer: 1.7708969116210938
Norm after each mp layer: 8.885509490966797
Norm after each mp layer: 45.95077896118164
Norm before input: 0.2552422881126404
Norm after input: 0.46112051606178284
Norm after each mp layer: 1.7708969116210938
Norm after each mp layer: 8.885509490966797
Norm after each mp layer: 45.95077896118164
Norm before input: 0.2552422881126404
Norm after input: 0.46132898330688477
Norm after each mp layer: 1.7677735090255737
Norm after each mp layer: 8.86825942993164
Norm after each mp layer: 45.84165954589844
Epoch: 160, Loss: 0.0880, Energy: 28999.9727, Train: 98.51%, Valid: 74.20%, Test: 72.60%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.46132898330688477
Norm after each mp layer: 1.7677735090255737
Norm after each mp layer: 8.86825942993164
Norm after each mp layer: 45.84165954589844
Norm before input: 0.2552422881126404
Norm after input: 0.46168822050094604
Norm after each mp layer: 1.7704046964645386
Norm after each mp layer: 8.899062156677246
Norm after each mp layer: 45.98564529418945
Norm before input: 0.2552422881126404
Norm after input: 0.46168822050094604
Norm after each mp layer: 1.7704046964645386
Norm after each mp layer: 8.899062156677246
Norm after each mp layer: 45.98564529418945
Norm before input: 0.2552422881126404
Norm after input: 0.4620048403739929
Norm after each mp layer: 1.771971344947815
Norm after each mp layer: 8.911796569824219
Norm after each mp layer: 46.00664138793945
Norm before input: 0.2552422881126404
Norm after input: 0.4620048403739929
Norm after each mp layer: 1.771971344947815
Norm after each mp layer: 8.911796569824219
Norm after each mp layer: 46.00664138793945
Norm before input: 0.2552422881126404
Norm after input: 0.4620961546897888
Norm after each mp layer: 1.7721251249313354
Norm after each mp layer: 8.920564651489258
Norm after each mp layer: 46.07699966430664
Norm before input: 0.2552422881126404
Norm after input: 0.4620961546897888
Norm after each mp layer: 1.7721251249313354
Norm after each mp layer: 8.920564651489258
Norm after each mp layer: 46.07699966430664
Norm before input: 0.2552422881126404
Norm after input: 0.4620212912559509
Norm after each mp layer: 1.7737313508987427
Norm after each mp layer: 8.951902389526367
Norm after each mp layer: 46.343814849853516
Norm before input: 0.2552422881126404
Norm after input: 0.4620212912559509
Norm after each mp layer: 1.7737313508987427
Norm after each mp layer: 8.951902389526367
Norm after each mp layer: 46.343814849853516
Norm before input: 0.2552422881126404
Norm after input: 0.46196308732032776
Norm after each mp layer: 1.7730213403701782
Norm after each mp layer: 8.950859069824219
Norm after each mp layer: 46.37459945678711
Epoch: 165, Loss: 0.0838, Energy: 28280.4102, Train: 98.68%, Valid: 74.20%, Test: 72.80%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.46196308732032776
Norm after each mp layer: 1.7730213403701782
Norm after each mp layer: 8.950859069824219
Norm after each mp layer: 46.37459945678711
Norm before input: 0.2552422881126404
Norm after input: 0.46210211515426636
Norm after each mp layer: 1.7757079601287842
Norm after each mp layer: 8.975961685180664
Norm after each mp layer: 46.51105880737305
Norm before input: 0.2552422881126404
Norm after input: 0.46210211515426636
Norm after each mp layer: 1.7757079601287842
Norm after each mp layer: 8.975961685180664
Norm after each mp layer: 46.51105880737305
Norm before input: 0.2552422881126404
Norm after input: 0.4622598886489868
Norm after each mp layer: 1.7779470682144165
Norm after each mp layer: 9.006171226501465
Norm after each mp layer: 46.69718551635742
Norm before input: 0.2552422881126404
Norm after input: 0.4622598886489868
Norm after each mp layer: 1.7779470682144165
Norm after each mp layer: 9.006171226501465
Norm after each mp layer: 46.69718551635742
Norm before input: 0.2552422881126404
Norm after input: 0.46232637763023376
Norm after each mp layer: 1.7738420963287354
Norm after each mp layer: 8.978894233703613
Norm after each mp layer: 46.55185317993164
Norm before input: 0.2552422881126404
Norm after input: 0.46232637763023376
Norm after each mp layer: 1.7738420963287354
Norm after each mp layer: 8.978894233703613
Norm after each mp layer: 46.55185317993164
Norm before input: 0.2552422881126404
Norm after input: 0.46267008781433105
Norm after each mp layer: 1.7746213674545288
Norm after each mp layer: 8.984661102294922
Norm after each mp layer: 46.52235412597656
Norm before input: 0.2552422881126404
Norm after input: 0.46267008781433105
Norm after each mp layer: 1.7746213674545288
Norm after each mp layer: 8.984661102294922
Norm after each mp layer: 46.52235412597656
Norm before input: 0.2552422881126404
Norm after input: 0.4631023406982422
Norm after each mp layer: 1.7805233001708984
Norm after each mp layer: 9.038531303405762
Norm after each mp layer: 46.767093658447266
Epoch: 170, Loss: 0.0779, Energy: 27043.5625, Train: 98.51%, Valid: 73.80%, Test: 72.50%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4631023406982422
Norm after each mp layer: 1.7805233001708984
Norm after each mp layer: 9.038531303405762
Norm after each mp layer: 46.767093658447266
Norm before input: 0.2552422881126404
Norm after input: 0.4631425440311432
Norm after each mp layer: 1.776340126991272
Norm after each mp layer: 9.011417388916016
Norm after each mp layer: 46.632720947265625
Norm before input: 0.2552422881126404
Norm after input: 0.4631425440311432
Norm after each mp layer: 1.7763398885726929
Norm after each mp layer: 9.011417388916016
Norm after each mp layer: 46.632720947265625
Norm before input: 0.2552422881126404
Norm after input: 0.46324798464775085
Norm after each mp layer: 1.7744605541229248
Norm after each mp layer: 9.000946044921875
Norm after each mp layer: 46.58017349243164
Norm before input: 0.2552422881126404
Norm after input: 0.46324798464775085
Norm after each mp layer: 1.7744605541229248
Norm after each mp layer: 9.000946044921875
Norm after each mp layer: 46.580169677734375
Norm before input: 0.2552422881126404
Norm after input: 0.4635082185268402
Norm after each mp layer: 1.781963586807251
Norm after each mp layer: 9.072206497192383
Norm after each mp layer: 46.97737121582031
Norm before input: 0.2552422881126404
Norm after input: 0.4635082185268402
Norm after each mp layer: 1.781963586807251
Norm after each mp layer: 9.072206497192383
Norm after each mp layer: 46.97737121582031
Norm before input: 0.2552422881126404
Norm after input: 0.4634750783443451
Norm after each mp layer: 1.779849648475647
Norm after each mp layer: 9.059085845947266
Norm after each mp layer: 46.92606735229492
Norm before input: 0.2552422881126404
Norm after input: 0.4634750783443451
Norm after each mp layer: 1.779849648475647
Norm after each mp layer: 9.059085845947266
Norm after each mp layer: 46.92606735229492
Norm before input: 0.2552422881126404
Norm after input: 0.46340519189834595
Norm after each mp layer: 1.775528907775879
Norm after each mp layer: 9.032035827636719
Norm after each mp layer: 46.820011138916016
Epoch: 175, Loss: 0.0725, Energy: 25935.1016, Train: 99.01%, Valid: 73.40%, Test: 71.50%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.46340519189834595
Norm after each mp layer: 1.775528907775879
Norm after each mp layer: 9.032035827636719
Norm after each mp layer: 46.82001495361328
Norm before input: 0.2552422881126404
Norm after input: 0.4636783003807068
Norm after each mp layer: 1.7817541360855103
Norm after each mp layer: 9.100985527038574
Norm after each mp layer: 47.24040222167969
Norm before input: 0.2552422881126404
Norm after input: 0.4636783003807068
Norm after each mp layer: 1.7817541360855103
Norm after each mp layer: 9.100985527038574
Norm after each mp layer: 47.24040222167969
Norm before input: 0.2552422881126404
Norm after input: 0.46383675932884216
Norm after each mp layer: 1.7803101539611816
Norm after each mp layer: 9.080055236816406
Norm after each mp layer: 47.05019760131836
Norm before input: 0.2552422881126404
Norm after input: 0.46383675932884216
Norm after each mp layer: 1.7803101539611816
Norm after each mp layer: 9.080055236816406
Norm after each mp layer: 47.05019760131836
Norm before input: 0.2552422881126404
Norm after input: 0.46385514736175537
Norm after each mp layer: 1.7778233289718628
Norm after each mp layer: 9.06867504119873
Norm after each mp layer: 47.00749969482422
Norm before input: 0.2552422881126404
Norm after input: 0.46385514736175537
Norm after each mp layer: 1.7778233289718628
Norm after each mp layer: 9.06867504119873
Norm after each mp layer: 47.00749969482422
Norm before input: 0.2552422881126404
Norm after input: 0.46400755643844604
Norm after each mp layer: 1.7797447443008423
Norm after each mp layer: 9.101405143737793
Norm after each mp layer: 47.22703552246094
Norm before input: 0.2552422881126404
Norm after input: 0.46400755643844604
Norm after each mp layer: 1.7797447443008423
Norm after each mp layer: 9.101405143737793
Norm after each mp layer: 47.22703552246094
Norm before input: 0.2552422881126404
Norm after input: 0.46424466371536255
Norm after each mp layer: 1.7782694101333618
Norm after each mp layer: 9.085428237915039
Norm after each mp layer: 47.069129943847656
Epoch: 180, Loss: 0.0712, Energy: 26236.2891, Train: 99.17%, Valid: 73.40%, Test: 71.70%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.46424466371536255
Norm after each mp layer: 1.7782694101333618
Norm after each mp layer: 9.085428237915039
Norm after each mp layer: 47.069129943847656
Norm before input: 0.2552422881126404
Norm after input: 0.4645814299583435
Norm after each mp layer: 1.7783772945404053
Norm after each mp layer: 9.082167625427246
Norm after each mp layer: 46.98386001586914
Norm before input: 0.2552422881126404
Norm after input: 0.4645814299583435
Norm after each mp layer: 1.7783772945404053
Norm after each mp layer: 9.082167625427246
Norm after each mp layer: 46.98386001586914
Norm before input: 0.2552422881126404
Norm after input: 0.46454668045043945
Norm after each mp layer: 1.7847967147827148
Norm after each mp layer: 9.160362243652344
Norm after each mp layer: 47.54243087768555
Norm before input: 0.2552422881126404
Norm after input: 0.46454668045043945
Norm after each mp layer: 1.7847967147827148
Norm after each mp layer: 9.160362243652344
Norm after each mp layer: 47.54243087768555
Norm before input: 0.2552422881126404
Norm after input: 0.4641041159629822
Norm after each mp layer: 1.7761330604553223
Norm after each mp layer: 9.093742370605469
Norm after each mp layer: 47.250064849853516
Norm before input: 0.2552422881126404
Norm after input: 0.4641041159629822
Norm after each mp layer: 1.7761330604553223
Norm after each mp layer: 9.093742370605469
Norm after each mp layer: 47.250064849853516
Norm before input: 0.2552422881126404
Norm after input: 0.4643387794494629
Norm after each mp layer: 1.777230978012085
Norm after each mp layer: 9.097015380859375
Norm after each mp layer: 47.21560287475586
Norm before input: 0.2552422881126404
Norm after input: 0.4643387794494629
Norm after each mp layer: 1.777230978012085
Norm after each mp layer: 9.097015380859375
Norm after each mp layer: 47.21560287475586
Norm before input: 0.2552422881126404
Norm after input: 0.4651116728782654
Norm after each mp layer: 1.796079397201538
Norm after each mp layer: 9.293546676635742
Norm after each mp layer: 48.46430206298828
Epoch: 185, Loss: 0.0696, Energy: 24773.0508, Train: 97.60%, Valid: 72.20%, Test: 71.70%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4651116728782654
Norm after each mp layer: 1.796079397201538
Norm after each mp layer: 9.293546676635742
Norm after each mp layer: 48.46430206298828
Norm before input: 0.2552422881126404
Norm after input: 0.46379753947257996
Norm after each mp layer: 1.7737627029418945
Norm after each mp layer: 9.082749366760254
Norm after each mp layer: 47.433536529541016
Norm before input: 0.2552422881126404
Norm after input: 0.46379753947257996
Norm after each mp layer: 1.7737627029418945
Norm after each mp layer: 9.082749366760254
Norm after each mp layer: 47.433536529541016
Norm before input: 0.2552422881126404
Norm after input: 0.4707089364528656
Norm after each mp layer: 1.9153547286987305
Norm after each mp layer: 10.492899894714355
Norm after each mp layer: 57.66227340698242
Norm before input: 0.2552422881126404
Norm after input: 0.4707089364528656
Norm after each mp layer: 1.9153547286987305
Norm after each mp layer: 10.492899894714355
Norm after each mp layer: 57.66227340698242
Norm before input: 0.2552422881126404
Norm after input: 0.45602506399154663
Norm after each mp layer: 1.8068052530288696
Norm after each mp layer: 9.475393295288086
Norm after each mp layer: 50.86960220336914
Norm before input: 0.2552422881126404
Norm after input: 0.45602506399154663
Norm after each mp layer: 1.8068052530288696
Norm after each mp layer: 9.475394248962402
Norm after each mp layer: 50.86960220336914
Norm before input: 0.2552422881126404
Norm after input: 0.4530586302280426
Norm after each mp layer: 1.8361603021621704
Norm after each mp layer: 9.556028366088867
Norm after each mp layer: 50.33992385864258
Norm before input: 0.2552422881126404
Norm after input: 0.4530586302280426
Norm after each mp layer: 1.8361603021621704
Norm after each mp layer: 9.556028366088867
Norm after each mp layer: 50.33992385864258
Norm before input: 0.2552422881126404
Norm after input: 0.4538748562335968
Norm after each mp layer: 1.8564672470092773
Norm after each mp layer: 9.410715103149414
Norm after each mp layer: 47.554073333740234
Epoch: 190, Loss: 0.9911, Energy: 51387.9570, Train: 90.40%, Valid: 73.60%, Test: 72.00%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4538748562335968
Norm after each mp layer: 1.8564672470092773
Norm after each mp layer: 9.410714149475098
Norm after each mp layer: 47.554073333740234
Norm before input: 0.2552422881126404
Norm after input: 0.45755472779273987
Norm after each mp layer: 1.902767300605774
Norm after each mp layer: 9.42662525177002
Norm after each mp layer: 45.99799728393555
Norm before input: 0.2552422881126404
Norm after input: 0.45755472779273987
Norm after each mp layer: 1.902767300605774
Norm after each mp layer: 9.42662525177002
Norm after each mp layer: 45.99799728393555
Norm before input: 0.2552422881126404
Norm after input: 0.4559538960456848
Norm after each mp layer: 1.8639074563980103
Norm after each mp layer: 8.910633087158203
Norm after each mp layer: 41.58788299560547
Norm before input: 0.2552422881126404
Norm after input: 0.4559538960456848
Norm after each mp layer: 1.8639074563980103
Norm after each mp layer: 8.910633087158203
Norm after each mp layer: 41.58788299560547
Norm before input: 0.2552422881126404
Norm after input: 0.450811505317688
Norm after each mp layer: 1.8214260339736938
Norm after each mp layer: 8.59593677520752
Norm after each mp layer: 39.63938522338867
Norm before input: 0.2552422881126404
Norm after input: 0.450811505317688
Norm after each mp layer: 1.8214260339736938
Norm after each mp layer: 8.59593677520752
Norm after each mp layer: 39.63938522338867
Norm before input: 0.2552422881126404
Norm after input: 0.44837668538093567
Norm after each mp layer: 1.8344638347625732
Norm after each mp layer: 8.644830703735352
Norm after each mp layer: 39.58192825317383
Norm before input: 0.2552422881126404
Norm after input: 0.44837668538093567
Norm after each mp layer: 1.8344638347625732
Norm after each mp layer: 8.644830703735352
Norm after each mp layer: 39.58192825317383
Norm before input: 0.2552422881126404
Norm after input: 0.4479430317878723
Norm after each mp layer: 1.8660290241241455
Norm after each mp layer: 8.721860885620117
Norm after each mp layer: 39.43630599975586
Epoch: 195, Loss: 0.3212, Energy: 45807.9453, Train: 81.71%, Valid: 67.20%, Test: 68.20%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4479430317878723
Norm after each mp layer: 1.8660290241241455
Norm after each mp layer: 8.721860885620117
Norm after each mp layer: 39.43630599975586
Norm before input: 0.2552422881126404
Norm after input: 0.446306049823761
Norm after each mp layer: 1.8571982383728027
Norm after each mp layer: 8.490190505981445
Norm after each mp layer: 37.624080657958984
Norm before input: 0.2552422881126404
Norm after input: 0.446306049823761
Norm after each mp layer: 1.8571982383728027
Norm after each mp layer: 8.490190505981445
Norm after each mp layer: 37.624080657958984
Norm before input: 0.2552422881126404
Norm after input: 0.44494009017944336
Norm after each mp layer: 1.8346679210662842
Norm after each mp layer: 8.173885345458984
Norm after each mp layer: 35.17121505737305
Norm before input: 0.2552422881126404
Norm after input: 0.44494009017944336
Norm after each mp layer: 1.8346679210662842
Norm after each mp layer: 8.173885345458984
Norm after each mp layer: 35.17121505737305
Norm before input: 0.2552422881126404
Norm after input: 0.44530996680259705
Norm after each mp layer: 1.8294737339019775
Norm after each mp layer: 7.985042095184326
Norm after each mp layer: 33.34138488769531
Norm before input: 0.2552422881126404
Norm after input: 0.44530996680259705
Norm after each mp layer: 1.8294737339019775
Norm after each mp layer: 7.985042095184326
Norm after each mp layer: 33.34138488769531
Norm before input: 0.2552422881126404
Norm after input: 0.44612917304039
Norm after each mp layer: 1.8418866395950317
Norm after each mp layer: 7.950288772583008
Norm after each mp layer: 32.72442626953125
Norm before input: 0.2552422881126404
Norm after input: 0.44612917304039
Norm after each mp layer: 1.8418866395950317
Norm after each mp layer: 7.950288772583008
Norm after each mp layer: 32.72442626953125
Norm before input: 0.2552422881126404
Norm after input: 0.44668298959732056
Norm after each mp layer: 1.857567548751831
Norm after each mp layer: 7.967292785644531
Norm after each mp layer: 32.862300872802734
Epoch: 200, Loss: 0.3601, Energy: 44074.5234, Train: 91.97%, Valid: 76.60%, Test: 76.20%, Best Valid: 81.40%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44668298959732056
Norm after each mp layer: 1.857567548751831
Norm after each mp layer: 7.967292785644531
Norm after each mp layer: 32.862300872802734
Norm before input: 0.2552422881126404
Norm after input: 0.44735801219940186
Norm after each mp layer: 1.8675051927566528
Norm after each mp layer: 7.968811988830566
Norm after each mp layer: 33.1430549621582
Norm before input: 0.2552422881126404
Norm after input: 0.44735801219940186
Norm after each mp layer: 1.8675051927566528
Norm after each mp layer: 7.968811988830566
Norm after each mp layer: 33.1430549621582
Norm before input: 0.2552422881126404
Norm after input: 0.44732439517974854
Norm after each mp layer: 1.8624542951583862
Norm after each mp layer: 7.8865180015563965
Norm after each mp layer: 33.22021484375
Norm before input: 0.2552422881126404
Norm after input: 0.44732439517974854
Norm after each mp layer: 1.8624542951583862
Norm after each mp layer: 7.8865180015563965
Norm after each mp layer: 33.22021484375
Norm before input: 0.2552422881126404
Norm after input: 0.4467478394508362
Norm after each mp layer: 1.8449732065200806
Norm after each mp layer: 7.725615978240967
Norm after each mp layer: 32.892398834228516
Norm before input: 0.2552422881126404
Norm after input: 0.4467478394508362
Norm after each mp layer: 1.8449732065200806
Norm after each mp layer: 7.725615978240967
Norm after each mp layer: 32.89239501953125
Norm before input: 0.2552422881126404
Norm after input: 0.4463071823120117
Norm after each mp layer: 1.8244112730026245
Norm after each mp layer: 7.5358476638793945
Norm after each mp layer: 32.16204833984375
Norm before input: 0.2552422881126404
Norm after input: 0.4463071823120117
Norm after each mp layer: 1.8244112730026245
Norm after each mp layer: 7.5358476638793945
Norm after each mp layer: 32.16204833984375
Norm before input: 0.2552422881126404
Norm after input: 0.4461503028869629
Norm after each mp layer: 1.8051490783691406
Norm after each mp layer: 7.349761962890625
Norm after each mp layer: 31.239233016967773
Epoch: 205, Loss: 0.2260, Energy: 50463.3672, Train: 95.70%, Valid: 82.00%, Test: 79.50%, Best Valid: 82.00%, Best Test: 79.50%
Norm before input: 0.2552422881126404
Norm after input: 0.4461503028869629
Norm after each mp layer: 1.8051490783691406
Norm after each mp layer: 7.349761962890625
Norm after each mp layer: 31.239233016967773
Norm before input: 0.2552422881126404
Norm after input: 0.4462873339653015
Norm after each mp layer: 1.78780996799469
Norm after each mp layer: 7.188435077667236
Norm after each mp layer: 30.394620895385742
Norm before input: 0.2552422881126404
Norm after input: 0.4462873339653015
Norm after each mp layer: 1.78780996799469
Norm after each mp layer: 7.188435077667236
Norm after each mp layer: 30.394620895385742
Norm before input: 0.2552422881126404
Norm after input: 0.4466932713985443
Norm after each mp layer: 1.772958517074585
Norm after each mp layer: 7.055001258850098
Norm after each mp layer: 29.695194244384766
Norm before input: 0.2552422881126404
Norm after input: 0.4466932713985443
Norm after each mp layer: 1.772958517074585
Norm after each mp layer: 7.055001258850098
Norm after each mp layer: 29.695194244384766
Norm before input: 0.2552422881126404
Norm after input: 0.44736677408218384
Norm after each mp layer: 1.7614668607711792
Norm after each mp layer: 6.949694633483887
Norm after each mp layer: 29.15205192565918
Norm before input: 0.2552422881126404
Norm after input: 0.44736677408218384
Norm after each mp layer: 1.7614668607711792
Norm after each mp layer: 6.949694633483887
Norm after each mp layer: 29.15205192565918
Norm before input: 0.2552422881126404
Norm after input: 0.4483073651790619
Norm after each mp layer: 1.754595398902893
Norm after each mp layer: 6.872755527496338
Norm after each mp layer: 28.746726989746094
Norm before input: 0.2552422881126404
Norm after input: 0.4483073651790619
Norm after each mp layer: 1.754595398902893
Norm after each mp layer: 6.872755527496338
Norm after each mp layer: 28.746726989746094
Norm before input: 0.2552422881126404
Norm after input: 0.44943469762802124
Norm after each mp layer: 1.7527683973312378
Norm after each mp layer: 6.81873083114624
Norm after each mp layer: 28.41811752319336
Epoch: 210, Loss: 0.1806, Energy: 36719.4453, Train: 96.36%, Valid: 79.60%, Test: 79.30%, Best Valid: 82.00%, Best Test: 79.50%
Norm before input: 0.2552422881126404
Norm after input: 0.44943469762802124
Norm after each mp layer: 1.7527683973312378
Norm after each mp layer: 6.81873083114624
Norm after each mp layer: 28.41811752319336
Norm before input: 0.2552422881126404
Norm after input: 0.45061150193214417
Norm after each mp layer: 1.7549635171890259
Norm after each mp layer: 6.777985572814941
Norm after each mp layer: 28.12209129333496
Norm before input: 0.2552422881126404
Norm after input: 0.45061150193214417
Norm after each mp layer: 1.7549635171890259
Norm after each mp layer: 6.777985572814941
Norm after each mp layer: 28.12209129333496
Norm before input: 0.2552422881126404
Norm after input: 0.4516809582710266
Norm after each mp layer: 1.7591561079025269
Norm after each mp layer: 6.740961074829102
Norm after each mp layer: 27.857582092285156
Norm before input: 0.2552422881126404
Norm after input: 0.4516809582710266
Norm after each mp layer: 1.7591561079025269
Norm after each mp layer: 6.740961074829102
Norm after each mp layer: 27.857582092285156
Norm before input: 0.2552422881126404
Norm after input: 0.4525548815727234
Norm after each mp layer: 1.763318419456482
Norm after each mp layer: 6.703380107879639
Norm after each mp layer: 27.644397735595703
Norm before input: 0.2552422881126404
Norm after input: 0.4525548815727234
Norm after each mp layer: 1.763318419456482
Norm after each mp layer: 6.703380107879639
Norm after each mp layer: 27.644397735595703
Norm before input: 0.2552422881126404
Norm after input: 0.4531923234462738
Norm after each mp layer: 1.7660876512527466
Norm after each mp layer: 6.666696548461914
Norm after each mp layer: 27.496580123901367
Norm before input: 0.2552422881126404
Norm after input: 0.4531923234462738
Norm after each mp layer: 1.7660876512527466
Norm after each mp layer: 6.666696548461914
Norm after each mp layer: 27.496580123901367
Norm before input: 0.2552422881126404
Norm after input: 0.45353466272354126
Norm after each mp layer: 1.766385555267334
Norm after each mp layer: 6.6324143409729
Norm after each mp layer: 27.423858642578125
Epoch: 215, Loss: 0.1453, Energy: 28454.8848, Train: 96.69%, Valid: 81.60%, Test: 81.70%, Best Valid: 82.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45353466272354126
Norm after each mp layer: 1.766385555267334
Norm after each mp layer: 6.6324143409729
Norm after each mp layer: 27.423858642578125
Norm before input: 0.2552422881126404
Norm after input: 0.453573614358902
Norm after each mp layer: 1.7640122175216675
Norm after each mp layer: 6.603216171264648
Norm after each mp layer: 27.43294334411621
Norm before input: 0.2552422881126404
Norm after input: 0.453573614358902
Norm after each mp layer: 1.7640122175216675
Norm after each mp layer: 6.603216171264648
Norm after each mp layer: 27.43294334411621
Norm before input: 0.2552422881126404
Norm after input: 0.45337286591529846
Norm after each mp layer: 1.759910225868225
Norm after each mp layer: 6.584013938903809
Norm after each mp layer: 27.52609634399414
Norm before input: 0.2552422881126404
Norm after input: 0.45337286591529846
Norm after each mp layer: 1.759910225868225
Norm after each mp layer: 6.584013938903809
Norm after each mp layer: 27.52609634399414
Norm before input: 0.2552422881126404
Norm after input: 0.45300760865211487
Norm after each mp layer: 1.755483627319336
Norm after each mp layer: 6.579261779785156
Norm after each mp layer: 27.693496704101562
Norm before input: 0.2552422881126404
Norm after input: 0.45300760865211487
Norm after each mp layer: 1.755483627319336
Norm after each mp layer: 6.579261779785156
Norm after each mp layer: 27.693496704101562
Norm before input: 0.2552422881126404
Norm after input: 0.4525761306285858
Norm after each mp layer: 1.7524724006652832
Norm after each mp layer: 6.5927734375
Norm after each mp layer: 27.919185638427734
Norm before input: 0.2552422881126404
Norm after input: 0.4525761306285858
Norm after each mp layer: 1.7524724006652832
Norm after each mp layer: 6.5927734375
Norm after each mp layer: 27.919185638427734
Norm before input: 0.2552422881126404
Norm after input: 0.45212453603744507
Norm after each mp layer: 1.7513052225112915
Norm after each mp layer: 6.622437953948975
Norm after each mp layer: 28.176355361938477
Epoch: 220, Loss: 0.1223, Energy: 27467.2910, Train: 97.76%, Valid: 82.20%, Test: 81.20%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45212453603744507
Norm after each mp layer: 1.7513052225112915
Norm after each mp layer: 6.622437477111816
Norm after each mp layer: 28.176355361938477
Norm before input: 0.2552422881126404
Norm after input: 0.4517213702201843
Norm after each mp layer: 1.7519451379776
Norm after each mp layer: 6.666086196899414
Norm after each mp layer: 28.462932586669922
Norm before input: 0.2552422881126404
Norm after input: 0.4517213702201843
Norm after each mp layer: 1.7519451379776
Norm after each mp layer: 6.666086196899414
Norm after each mp layer: 28.462932586669922
Norm before input: 0.2552422881126404
Norm after input: 0.45143240690231323
Norm after each mp layer: 1.7542073726654053
Norm after each mp layer: 6.722127437591553
Norm after each mp layer: 28.794208526611328
Norm before input: 0.2552422881126404
Norm after input: 0.45143240690231323
Norm after each mp layer: 1.7542073726654053
Norm after each mp layer: 6.722127437591553
Norm after each mp layer: 28.794208526611328
Norm before input: 0.2552422881126404
Norm after input: 0.4512750804424286
Norm after each mp layer: 1.7576916217803955
Norm after each mp layer: 6.787707805633545
Norm after each mp layer: 29.180362701416016
Norm before input: 0.2552422881126404
Norm after input: 0.4512750804424286
Norm after each mp layer: 1.7576916217803955
Norm after each mp layer: 6.787707805633545
Norm after each mp layer: 29.180362701416016
Norm before input: 0.2552422881126404
Norm after input: 0.4512181580066681
Norm after each mp layer: 1.7618532180786133
Norm after each mp layer: 6.858125686645508
Norm after each mp layer: 29.614194869995117
Norm before input: 0.2552422881126404
Norm after input: 0.4512181580066681
Norm after each mp layer: 1.7618532180786133
Norm after each mp layer: 6.858125686645508
Norm after each mp layer: 29.614194869995117
Norm before input: 0.2552422881126404
Norm after input: 0.45121172070503235
Norm after each mp layer: 1.7661116123199463
Norm after each mp layer: 6.927616119384766
Norm after each mp layer: 30.069141387939453
Epoch: 225, Loss: 0.1000, Energy: 25845.5820, Train: 97.76%, Valid: 81.60%, Test: 80.20%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45121172070503235
Norm after each mp layer: 1.7661116123199463
Norm after each mp layer: 6.927616119384766
Norm after each mp layer: 30.069141387939453
Norm before input: 0.2552422881126404
Norm after input: 0.45121243596076965
Norm after each mp layer: 1.769910454750061
Norm after each mp layer: 6.990482330322266
Norm after each mp layer: 30.50550079345703
Norm before input: 0.2552422881126404
Norm after input: 0.45121243596076965
Norm after each mp layer: 1.769910216331482
Norm after each mp layer: 6.990482330322266
Norm after each mp layer: 30.50550079345703
Norm before input: 0.2552422881126404
Norm after input: 0.4511910378932953
Norm after each mp layer: 1.7727875709533691
Norm after each mp layer: 7.042082786560059
Norm after each mp layer: 30.88165283203125
Norm before input: 0.2552422881126404
Norm after input: 0.4511910378932953
Norm after each mp layer: 1.7727875709533691
Norm after each mp layer: 7.042082786560059
Norm after each mp layer: 30.88165283203125
Norm before input: 0.2552422881126404
Norm after input: 0.45113202929496765
Norm after each mp layer: 1.7744868993759155
Norm after each mp layer: 7.079689025878906
Norm after each mp layer: 31.166074752807617
Norm before input: 0.2552422881126404
Norm after input: 0.45113202929496765
Norm after each mp layer: 1.7744868993759155
Norm after each mp layer: 7.079689025878906
Norm after each mp layer: 31.166074752807617
Norm before input: 0.2552422881126404
Norm after input: 0.4510308504104614
Norm after each mp layer: 1.7749727964401245
Norm after each mp layer: 7.102854251861572
Norm after each mp layer: 31.34528350830078
Norm before input: 0.2552422881126404
Norm after input: 0.4510308504104614
Norm after each mp layer: 1.7749727964401245
Norm after each mp layer: 7.102854251861572
Norm after each mp layer: 31.34528350830078
Norm before input: 0.2552422881126404
Norm after input: 0.45089444518089294
Norm after each mp layer: 1.7743704319000244
Norm after each mp layer: 7.113348007202148
Norm after each mp layer: 31.42731285095215
Epoch: 230, Loss: 0.0896, Energy: 27298.5762, Train: 98.68%, Valid: 80.80%, Test: 80.50%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45089444518089294
Norm after each mp layer: 1.7743704319000244
Norm after each mp layer: 7.113348007202148
Norm after each mp layer: 31.42731285095215
Norm before input: 0.2552422881126404
Norm after input: 0.45074087381362915
Norm after each mp layer: 1.772909164428711
Norm after each mp layer: 7.114521503448486
Norm after each mp layer: 31.437591552734375
Norm before input: 0.2552422881126404
Norm after input: 0.45074087381362915
Norm after each mp layer: 1.772909164428711
Norm after each mp layer: 7.114521503448486
Norm after each mp layer: 31.437591552734375
Norm before input: 0.2552422881126404
Norm after input: 0.4505941867828369
Norm after each mp layer: 1.7708882093429565
Norm after each mp layer: 7.110313892364502
Norm after each mp layer: 31.40748405456543
Norm before input: 0.2552422881126404
Norm after input: 0.4505941867828369
Norm after each mp layer: 1.7708882093429565
Norm after each mp layer: 7.110313892364502
Norm after each mp layer: 31.40748405456543
Norm before input: 0.2552422881126404
Norm after input: 0.4504808783531189
Norm after each mp layer: 1.7686651945114136
Norm after each mp layer: 7.1044111251831055
Norm after each mp layer: 31.36273765563965
Norm before input: 0.2552422881126404
Norm after input: 0.4504808783531189
Norm after each mp layer: 1.7686651945114136
Norm after each mp layer: 7.1044111251831055
Norm after each mp layer: 31.36273765563965
Norm before input: 0.2552422881126404
Norm after input: 0.4504256546497345
Norm after each mp layer: 1.7666141986846924
Norm after each mp layer: 7.099724769592285
Norm after each mp layer: 31.318580627441406
Norm before input: 0.2552422881126404
Norm after input: 0.4504256546497345
Norm after each mp layer: 1.7666141986846924
Norm after each mp layer: 7.099724769592285
Norm after each mp layer: 31.318580627441406
Norm before input: 0.2552422881126404
Norm after input: 0.45044299960136414
Norm after each mp layer: 1.76503586769104
Norm after each mp layer: 7.09813117980957
Norm after each mp layer: 31.281639099121094
Epoch: 235, Loss: 0.0781, Energy: 24359.5742, Train: 98.84%, Valid: 81.20%, Test: 80.20%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45044299960136414
Norm after each mp layer: 1.76503586769104
Norm after each mp layer: 7.09813117980957
Norm after each mp layer: 31.281639099121094
Norm before input: 0.2552422881126404
Norm after input: 0.4505322277545929
Norm after each mp layer: 1.7640984058380127
Norm after each mp layer: 7.1005859375
Norm after each mp layer: 31.255420684814453
Norm before input: 0.2552422881126404
Norm after input: 0.4505322277545929
Norm after each mp layer: 1.7640984058380127
Norm after each mp layer: 7.1005859375
Norm after each mp layer: 31.255420684814453
Norm before input: 0.2552422881126404
Norm after input: 0.45067930221557617
Norm after each mp layer: 1.7638404369354248
Norm after each mp layer: 7.107356071472168
Norm after each mp layer: 31.244400024414062
Norm before input: 0.2552422881126404
Norm after input: 0.45067930221557617
Norm after each mp layer: 1.7638404369354248
Norm after each mp layer: 7.107356071472168
Norm after each mp layer: 31.244400024414062
Norm before input: 0.2552422881126404
Norm after input: 0.4508608877658844
Norm after each mp layer: 1.764183521270752
Norm after each mp layer: 7.1181640625
Norm after each mp layer: 31.25461769104004
Norm before input: 0.2552422881126404
Norm after input: 0.4508608877658844
Norm after each mp layer: 1.764183521270752
Norm after each mp layer: 7.1181640625
Norm after each mp layer: 31.254615783691406
Norm before input: 0.2552422881126404
Norm after input: 0.4510480761528015
Norm after each mp layer: 1.7649534940719604
Norm after each mp layer: 7.13224983215332
Norm after each mp layer: 31.29161262512207
Norm before input: 0.2552422881126404
Norm after input: 0.4510480761528015
Norm after each mp layer: 1.7649534940719604
Norm after each mp layer: 7.13224983215332
Norm after each mp layer: 31.29161262512207
Norm before input: 0.2552422881126404
Norm after input: 0.4512127935886383
Norm after each mp layer: 1.7659300565719604
Norm after each mp layer: 7.148544788360596
Norm after each mp layer: 31.35770034790039
Epoch: 240, Loss: 0.0708, Energy: 21828.8457, Train: 99.01%, Valid: 80.40%, Test: 79.70%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4512127935886383
Norm after each mp layer: 1.7659300565719604
Norm after each mp layer: 7.148544788360596
Norm after each mp layer: 31.35770034790039
Norm before input: 0.2552422881126404
Norm after input: 0.45133522152900696
Norm after each mp layer: 1.766910433769226
Norm after each mp layer: 7.165925979614258
Norm after each mp layer: 31.450279235839844
Norm before input: 0.2552422881126404
Norm after input: 0.45133522152900696
Norm after each mp layer: 1.766910433769226
Norm after each mp layer: 7.165925979614258
Norm after each mp layer: 31.450279235839844
Norm before input: 0.2552422881126404
Norm after input: 0.45140838623046875
Norm after each mp layer: 1.7677509784698486
Norm after each mp layer: 7.1834235191345215
Norm after each mp layer: 31.561737060546875
Norm before input: 0.2552422881126404
Norm after input: 0.45140838623046875
Norm after each mp layer: 1.7677509784698486
Norm after each mp layer: 7.1834235191345215
Norm after each mp layer: 31.561737060546875
Norm before input: 0.2552422881126404
Norm after input: 0.45143747329711914
Norm after each mp layer: 1.7683745622634888
Norm after each mp layer: 7.20032262802124
Norm after each mp layer: 31.680763244628906
Norm before input: 0.2552422881126404
Norm after input: 0.45143747329711914
Norm after each mp layer: 1.7683745622634888
Norm after each mp layer: 7.20032262802124
Norm after each mp layer: 31.680763244628906
Norm before input: 0.2552422881126404
Norm after input: 0.45143431425094604
Norm after each mp layer: 1.768755316734314
Norm after each mp layer: 7.216203689575195
Norm after each mp layer: 31.79486083984375
Norm before input: 0.2552422881126404
Norm after input: 0.45143431425094604
Norm after each mp layer: 1.768755316734314
Norm after each mp layer: 7.216203689575195
Norm after each mp layer: 31.79486083984375
Norm before input: 0.2552422881126404
Norm after input: 0.4514125883579254
Norm after each mp layer: 1.7689114809036255
Norm after each mp layer: 7.230985164642334
Norm after each mp layer: 31.893497467041016
Epoch: 245, Loss: 0.0654, Energy: 22088.0215, Train: 99.09%, Valid: 80.40%, Test: 79.40%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4514125883579254
Norm after each mp layer: 1.7689114809036255
Norm after each mp layer: 7.230985164642334
Norm after each mp layer: 31.893497467041016
Norm before input: 0.2552422881126404
Norm after input: 0.4513852596282959
Norm after each mp layer: 1.768897294998169
Norm after each mp layer: 7.244930744171143
Norm after each mp layer: 31.970985412597656
Norm before input: 0.2552422881126404
Norm after input: 0.4513852596282959
Norm after each mp layer: 1.768897294998169
Norm after each mp layer: 7.244930744171143
Norm after each mp layer: 31.970985412597656
Norm before input: 0.2552422881126404
Norm after input: 0.45136281847953796
Norm after each mp layer: 1.7687854766845703
Norm after each mp layer: 7.2585062980651855
Norm after each mp layer: 32.0278205871582
Norm before input: 0.2552422881126404
Norm after input: 0.45136281847953796
Norm after each mp layer: 1.7687854766845703
Norm after each mp layer: 7.2585062980651855
Norm after each mp layer: 32.0278205871582
Norm before input: 0.2552422881126404
Norm after input: 0.451351135969162
Norm after each mp layer: 1.7686371803283691
Norm after each mp layer: 7.272128105163574
Norm after each mp layer: 32.069759368896484
Norm before input: 0.2552422881126404
Norm after input: 0.451351135969162
Norm after each mp layer: 1.7686371803283691
Norm after each mp layer: 7.272128105163574
Norm after each mp layer: 32.069759368896484
Norm before input: 0.2552422881126404
Norm after input: 0.451351135969162
Norm after each mp layer: 1.768485426902771
Norm after each mp layer: 7.285954475402832
Norm after each mp layer: 32.10485076904297
Norm before input: 0.2552422881126404
Norm after input: 0.451351135969162
Norm after each mp layer: 1.768485426902771
Norm after each mp layer: 7.285954475402832
Norm after each mp layer: 32.10485076904297
Norm before input: 0.2552422881126404
Norm after input: 0.4513600170612335
Norm after each mp layer: 1.7683460712432861
Norm after each mp layer: 7.299829959869385
Norm after each mp layer: 32.140342712402344
Epoch: 250, Loss: 0.0603, Energy: 21300.6113, Train: 99.25%, Valid: 80.00%, Test: 79.30%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4513600170612335
Norm after each mp layer: 1.7683460712432861
Norm after each mp layer: 7.299829959869385
Norm after each mp layer: 32.140342712402344
Norm before input: 0.2552422881126404
Norm after input: 0.4513741433620453
Norm after each mp layer: 1.7682377099990845
Norm after each mp layer: 7.313399791717529
Norm after each mp layer: 32.18077850341797
Norm before input: 0.2552422881126404
Norm after input: 0.4513741433620453
Norm after each mp layer: 1.7682377099990845
Norm after each mp layer: 7.313399314880371
Norm after each mp layer: 32.18077850341797
Norm before input: 0.2552422881126404
Norm after input: 0.45139116048812866
Norm after each mp layer: 1.7681955099105835
Norm after each mp layer: 7.326297760009766
Norm after each mp layer: 32.227806091308594
Norm before input: 0.2552422881126404
Norm after input: 0.45139116048812866
Norm after each mp layer: 1.7681955099105835
Norm after each mp layer: 7.326297760009766
Norm after each mp layer: 32.227806091308594
Norm before input: 0.2552422881126404
Norm after input: 0.45141085982322693
Norm after each mp layer: 1.7682628631591797
Norm after each mp layer: 7.338283538818359
Norm after each mp layer: 32.28099060058594
Norm before input: 0.2552422881126404
Norm after input: 0.45141085982322693
Norm after each mp layer: 1.7682628631591797
Norm after each mp layer: 7.338283538818359
Norm after each mp layer: 32.28099060058594
Norm before input: 0.2552422881126404
Norm after input: 0.4514339566230774
Norm after each mp layer: 1.7684767246246338
Norm after each mp layer: 7.349300384521484
Norm after each mp layer: 32.33891677856445
Norm before input: 0.2552422881126404
Norm after input: 0.4514339566230774
Norm after each mp layer: 1.7684767246246338
Norm after each mp layer: 7.349300384521484
Norm after each mp layer: 32.33891677856445
Norm before input: 0.2552422881126404
Norm after input: 0.4514616131782532
Norm after each mp layer: 1.7688524723052979
Norm after each mp layer: 7.359457015991211
Norm after each mp layer: 32.400001525878906
Epoch: 255, Loss: 0.0568, Energy: 20225.7246, Train: 99.34%, Valid: 80.20%, Test: 79.00%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4514616131782532
Norm after each mp layer: 1.7688524723052979
Norm after each mp layer: 7.359457015991211
Norm after each mp layer: 32.400001525878906
Norm before input: 0.2552422881126404
Norm after input: 0.451494038105011
Norm after each mp layer: 1.7693747282028198
Norm after each mp layer: 7.368959426879883
Norm after each mp layer: 32.46284484863281
Norm before input: 0.2552422881126404
Norm after input: 0.451494038105011
Norm after each mp layer: 1.7693747282028198
Norm after each mp layer: 7.368959426879883
Norm after each mp layer: 32.46284484863281
Norm before input: 0.2552422881126404
Norm after input: 0.45152968168258667
Norm after each mp layer: 1.7699925899505615
Norm after each mp layer: 7.378008842468262
Norm after each mp layer: 32.52599334716797
Norm before input: 0.2552422881126404
Norm after input: 0.45152968168258667
Norm after each mp layer: 1.7699925899505615
Norm after each mp layer: 7.3780083656311035
Norm after each mp layer: 32.52599334716797
Norm before input: 0.2552422881126404
Norm after input: 0.4515650272369385
Norm after each mp layer: 1.7706257104873657
Norm after each mp layer: 7.386716365814209
Norm after each mp layer: 32.58740997314453
Norm before input: 0.2552422881126404
Norm after input: 0.4515650272369385
Norm after each mp layer: 1.7706257104873657
Norm after each mp layer: 7.386716365814209
Norm after each mp layer: 32.58740997314453
Norm before input: 0.2552422881126404
Norm after input: 0.4515962302684784
Norm after each mp layer: 1.7711818218231201
Norm after each mp layer: 7.395077228546143
Norm after each mp layer: 32.64411544799805
Norm before input: 0.2552422881126404
Norm after input: 0.4515962302684784
Norm after each mp layer: 1.7711818218231201
Norm after each mp layer: 7.395077228546143
Norm after each mp layer: 32.64411544799805
Norm before input: 0.2552422881126404
Norm after input: 0.45162084698677063
Norm after each mp layer: 1.7715846300125122
Norm after each mp layer: 7.403001308441162
Norm after each mp layer: 32.692588806152344
Epoch: 260, Loss: 0.0535, Energy: 19715.9785, Train: 99.42%, Valid: 79.40%, Test: 78.80%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45162084698677063
Norm after each mp layer: 1.7715846300125122
Norm after each mp layer: 7.403001308441162
Norm after each mp layer: 32.692588806152344
Norm before input: 0.2552422881126404
Norm after input: 0.451638787984848
Norm after each mp layer: 1.7717965841293335
Norm after each mp layer: 7.410399913787842
Norm after each mp layer: 32.730098724365234
Norm before input: 0.2552422881126404
Norm after input: 0.451638787984848
Norm after each mp layer: 1.7717965841293335
Norm after each mp layer: 7.410399913787842
Norm after each mp layer: 32.730098724365234
Norm before input: 0.2552422881126404
Norm after input: 0.451651930809021
Norm after each mp layer: 1.7718263864517212
Norm after each mp layer: 7.41726541519165
Norm after each mp layer: 32.75627136230469
Norm before input: 0.2552422881126404
Norm after input: 0.451651930809021
Norm after each mp layer: 1.7718263864517212
Norm after each mp layer: 7.41726541519165
Norm after each mp layer: 32.75627136230469
Norm before input: 0.2552422881126404
Norm after input: 0.4516623914241791
Norm after each mp layer: 1.7717177867889404
Norm after each mp layer: 7.423686504364014
Norm after each mp layer: 32.773712158203125
Norm before input: 0.2552422881126404
Norm after input: 0.4516623914241791
Norm after each mp layer: 1.7717177867889404
Norm after each mp layer: 7.423686504364014
Norm after each mp layer: 32.773712158203125
Norm before input: 0.2552422881126404
Norm after input: 0.4516712427139282
Norm after each mp layer: 1.7715284824371338
Norm after each mp layer: 7.42980432510376
Norm after each mp layer: 32.787288665771484
Norm before input: 0.2552422881126404
Norm after input: 0.4516712427139282
Norm after each mp layer: 1.7715284824371338
Norm after each mp layer: 7.42980432510376
Norm after each mp layer: 32.787288665771484
Norm before input: 0.2552422881126404
Norm after input: 0.4516781270503998
Norm after each mp layer: 1.7713121175765991
Norm after each mp layer: 7.4357805252075195
Norm after each mp layer: 32.80268478393555
Epoch: 265, Loss: 0.0508, Energy: 18855.6992, Train: 99.42%, Valid: 79.20%, Test: 78.80%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4516781270503998
Norm after each mp layer: 1.7713121175765991
Norm after each mp layer: 7.4357805252075195
Norm after each mp layer: 32.80268478393555
Norm before input: 0.2552422881126404
Norm after input: 0.45168259739875793
Norm after each mp layer: 1.771113395690918
Norm after each mp layer: 7.441793918609619
Norm after each mp layer: 32.82500457763672
Norm before input: 0.2552422881126404
Norm after input: 0.45168259739875793
Norm after each mp layer: 1.771113395690918
Norm after each mp layer: 7.441793918609619
Norm after each mp layer: 32.82500457763672
Norm before input: 0.2552422881126404
Norm after input: 0.4516848921775818
Norm after each mp layer: 1.7709709405899048
Norm after each mp layer: 7.448069095611572
Norm after each mp layer: 32.85765075683594
Norm before input: 0.2552422881126404
Norm after input: 0.4516848921775818
Norm after each mp layer: 1.7709709405899048
Norm after each mp layer: 7.448069095611572
Norm after each mp layer: 32.85765075683594
Norm before input: 0.2552422881126404
Norm after input: 0.45168745517730713
Norm after each mp layer: 1.7709206342697144
Norm after each mp layer: 7.454873085021973
Norm after each mp layer: 32.901756286621094
Norm before input: 0.2552422881126404
Norm after input: 0.45168745517730713
Norm after each mp layer: 1.7709206342697144
Norm after each mp layer: 7.454873085021973
Norm after each mp layer: 32.901756286621094
Norm before input: 0.2552422881126404
Norm after input: 0.45169368386268616
Norm after each mp layer: 1.770991325378418
Norm after each mp layer: 7.462461948394775
Norm after each mp layer: 32.95612335205078
Norm before input: 0.2552422881126404
Norm after input: 0.45169368386268616
Norm after each mp layer: 1.770991325378418
Norm after each mp layer: 7.462461948394775
Norm after each mp layer: 32.95612335205078
Norm before input: 0.2552422881126404
Norm after input: 0.4517071545124054
Norm after each mp layer: 1.7711944580078125
Norm after each mp layer: 7.470990180969238
Norm after each mp layer: 33.01766586303711
Epoch: 270, Loss: 0.0485, Energy: 18262.3652, Train: 99.42%, Valid: 78.80%, Test: 78.50%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4517071545124054
Norm after each mp layer: 1.7711944580078125
Norm after each mp layer: 7.470990180969238
Norm after each mp layer: 33.01766586303711
Norm before input: 0.2552422881126404
Norm after input: 0.45172932744026184
Norm after each mp layer: 1.7715137004852295
Norm after each mp layer: 7.480415344238281
Norm after each mp layer: 33.08222198486328
Norm before input: 0.2552422881126404
Norm after input: 0.45172932744026184
Norm after each mp layer: 1.7715137004852295
Norm after each mp layer: 7.480415344238281
Norm after each mp layer: 33.08222198486328
Norm before input: 0.2552422881126404
Norm after input: 0.4517587721347809
Norm after each mp layer: 1.771902322769165
Norm after each mp layer: 7.490463733673096
Norm after each mp layer: 33.14548110961914
Norm before input: 0.2552422881126404
Norm after input: 0.4517587721347809
Norm after each mp layer: 1.771902322769165
Norm after each mp layer: 7.490463733673096
Norm after each mp layer: 33.14548110961914
Norm before input: 0.2552422881126404
Norm after input: 0.45179131627082825
Norm after each mp layer: 1.7722907066345215
Norm after each mp layer: 7.500670433044434
Norm after each mp layer: 33.203861236572266
Norm before input: 0.2552422881126404
Norm after input: 0.45179131627082825
Norm after each mp layer: 1.7722907066345215
Norm after each mp layer: 7.500670433044434
Norm after each mp layer: 33.203861236572266
Norm before input: 0.2552422881126404
Norm after input: 0.45182135701179504
Norm after each mp layer: 1.7726044654846191
Norm after each mp layer: 7.510492324829102
Norm after each mp layer: 33.255043029785156
Norm before input: 0.2552422881126404
Norm after input: 0.45182135701179504
Norm after each mp layer: 1.7726044654846191
Norm after each mp layer: 7.510492324829102
Norm after each mp layer: 33.255043029785156
Norm before input: 0.2552422881126404
Norm after input: 0.45184433460235596
Norm after each mp layer: 1.7727861404418945
Norm after each mp layer: 7.519454479217529
Norm after each mp layer: 33.29819869995117
Epoch: 275, Loss: 0.0465, Energy: 17836.5391, Train: 99.42%, Valid: 78.20%, Test: 78.60%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45184433460235596
Norm after each mp layer: 1.7727861404418945
Norm after each mp layer: 7.519454479217529
Norm after each mp layer: 33.29819869995117
Norm before input: 0.2552422881126404
Norm after input: 0.4518579840660095
Norm after each mp layer: 1.7728170156478882
Norm after each mp layer: 7.527290344238281
Norm after each mp layer: 33.333953857421875
Norm before input: 0.2552422881126404
Norm after input: 0.4518579840660095
Norm after each mp layer: 1.7728170156478882
Norm after each mp layer: 7.527290344238281
Norm after each mp layer: 33.333953857421875
Norm before input: 0.2552422881126404
Norm after input: 0.4518636167049408
Norm after each mp layer: 1.772721290588379
Norm after each mp layer: 7.534003734588623
Norm after each mp layer: 33.363983154296875
Norm before input: 0.2552422881126404
Norm after input: 0.4518636167049408
Norm after each mp layer: 1.772721290588379
Norm after each mp layer: 7.534003734588623
Norm after each mp layer: 33.363983154296875
Norm before input: 0.2552422881126404
Norm after input: 0.4518650770187378
Norm after each mp layer: 1.7725564241409302
Norm after each mp layer: 7.5398383140563965
Norm after each mp layer: 33.39048385620117
Norm before input: 0.2552422881126404
Norm after input: 0.4518650770187378
Norm after each mp layer: 1.7725564241409302
Norm after each mp layer: 7.5398383140563965
Norm after each mp layer: 33.39048385620117
Norm before input: 0.2552422881126404
Norm after input: 0.4518672823905945
Norm after each mp layer: 1.772389531135559
Norm after each mp layer: 7.545169830322266
Norm after each mp layer: 33.415626525878906
Norm before input: 0.2552422881126404
Norm after input: 0.4518672823905945
Norm after each mp layer: 1.772389531135559
Norm after each mp layer: 7.545169830322266
Norm after each mp layer: 33.415626525878906
Norm before input: 0.2552422881126404
Norm after input: 0.4518742263317108
Norm after each mp layer: 1.7722773551940918
Norm after each mp layer: 7.550367832183838
Norm after each mp layer: 33.441131591796875
Epoch: 280, Loss: 0.0447, Energy: 17190.7090, Train: 99.42%, Valid: 78.00%, Test: 78.00%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4518742263317108
Norm after each mp layer: 1.7722773551940918
Norm after each mp layer: 7.550367832183838
Norm after each mp layer: 33.441131591796875
Norm before input: 0.2552422881126404
Norm after input: 0.4518875181674957
Norm after each mp layer: 1.7722485065460205
Norm after each mp layer: 7.555689811706543
Norm after each mp layer: 33.46809005737305
Norm before input: 0.2552422881126404
Norm after input: 0.4518875181674957
Norm after each mp layer: 1.7722485065460205
Norm after each mp layer: 7.555689811706543
Norm after each mp layer: 33.46809005737305
Norm before input: 0.2552422881126404
Norm after input: 0.45190635323524475
Norm after each mp layer: 1.7722989320755005
Norm after each mp layer: 7.561227798461914
Norm after each mp layer: 33.49684524536133
Norm before input: 0.2552422881126404
Norm after input: 0.45190635323524475
Norm after each mp layer: 1.7722989320755005
Norm after each mp layer: 7.561227798461914
Norm after each mp layer: 33.49684524536133
Norm before input: 0.2552422881126404
Norm after input: 0.45192790031433105
Norm after each mp layer: 1.7724002599716187
Norm after each mp layer: 7.5669121742248535
Norm after each mp layer: 33.52701950073242
Norm before input: 0.2552422881126404
Norm after input: 0.45192790031433105
Norm after each mp layer: 1.7724002599716187
Norm after each mp layer: 7.5669121742248535
Norm after each mp layer: 33.52701950073242
Norm before input: 0.2552422881126404
Norm after input: 0.45194917917251587
Norm after each mp layer: 1.7725120782852173
Norm after each mp layer: 7.572582244873047
Norm after each mp layer: 33.5576057434082
Norm before input: 0.2552422881126404
Norm after input: 0.45194917917251587
Norm after each mp layer: 1.7725120782852173
Norm after each mp layer: 7.572582244873047
Norm after each mp layer: 33.5576057434082
Norm before input: 0.2552422881126404
Norm after input: 0.45196840167045593
Norm after each mp layer: 1.7725999355316162
Norm after each mp layer: 7.5780744552612305
Norm after each mp layer: 33.587257385253906
Epoch: 285, Loss: 0.0430, Energy: 16710.5215, Train: 99.42%, Valid: 78.20%, Test: 77.70%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45196840167045593
Norm after each mp layer: 1.7725999355316162
Norm after each mp layer: 7.5780744552612305
Norm after each mp layer: 33.587257385253906
Norm before input: 0.2552422881126404
Norm after input: 0.45198577642440796
Norm after each mp layer: 1.772645354270935
Norm after each mp layer: 7.583292007446289
Norm after each mp layer: 33.61463165283203
Norm before input: 0.2552422881126404
Norm after input: 0.45198577642440796
Norm after each mp layer: 1.772645354270935
Norm after each mp layer: 7.583292007446289
Norm after each mp layer: 33.61463165283203
Norm before input: 0.2552422881126404
Norm after input: 0.4520035982131958
Norm after each mp layer: 1.7726492881774902
Norm after each mp layer: 7.5882368087768555
Norm after each mp layer: 33.638771057128906
Norm before input: 0.2552422881126404
Norm after input: 0.4520035982131958
Norm after each mp layer: 1.7726492881774902
Norm after each mp layer: 7.5882368087768555
Norm after each mp layer: 33.638771057128906
Norm before input: 0.2552422881126404
Norm after input: 0.4520246982574463
Norm after each mp layer: 1.772627353668213
Norm after each mp layer: 7.592992305755615
Norm after each mp layer: 33.659393310546875
Norm before input: 0.2552422881126404
Norm after input: 0.4520246982574463
Norm after each mp layer: 1.772627353668213
Norm after each mp layer: 7.592992305755615
Norm after each mp layer: 33.659393310546875
Norm before input: 0.2552422881126404
Norm after input: 0.4520513415336609
Norm after each mp layer: 1.7725986242294312
Norm after each mp layer: 7.5976691246032715
Norm after each mp layer: 33.676979064941406
Norm before input: 0.2552422881126404
Norm after input: 0.4520513415336609
Norm after each mp layer: 1.7725986242294312
Norm after each mp layer: 7.5976691246032715
Norm after each mp layer: 33.676979064941406
Norm before input: 0.2552422881126404
Norm after input: 0.4520839750766754
Norm after each mp layer: 1.7725788354873657
Norm after each mp layer: 7.602358818054199
Norm after each mp layer: 33.69264602661133
Epoch: 290, Loss: 0.0415, Energy: 16326.1885, Train: 99.42%, Valid: 77.80%, Test: 77.60%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4520839750766754
Norm after each mp layer: 1.7725788354873657
Norm after each mp layer: 7.602358818054199
Norm after each mp layer: 33.69264602661133
Norm before input: 0.2552422881126404
Norm after input: 0.4521211087703705
Norm after each mp layer: 1.7725738286972046
Norm after each mp layer: 7.607098579406738
Norm after each mp layer: 33.707828521728516
Norm before input: 0.2552422881126404
Norm after input: 0.4521211087703705
Norm after each mp layer: 1.7725738286972046
Norm after each mp layer: 7.607098579406738
Norm after each mp layer: 33.707828521728516
Norm before input: 0.2552422881126404
Norm after input: 0.45215943455696106
Norm after each mp layer: 1.772580862045288
Norm after each mp layer: 7.611878871917725
Norm after each mp layer: 33.72390365600586
Norm before input: 0.2552422881126404
Norm after input: 0.45215943455696106
Norm after each mp layer: 1.772580862045288
Norm after each mp layer: 7.611878871917725
Norm after each mp layer: 33.72390365600586
Norm before input: 0.2552422881126404
Norm after input: 0.45219525694847107
Norm after each mp layer: 1.772592306137085
Norm after each mp layer: 7.616663932800293
Norm after each mp layer: 33.74184036254883
Norm before input: 0.2552422881126404
Norm after input: 0.45219525694847107
Norm after each mp layer: 1.772592306137085
Norm after each mp layer: 7.616663932800293
Norm after each mp layer: 33.74184036254883
Norm before input: 0.2552422881126404
Norm after input: 0.45222586393356323
Norm after each mp layer: 1.7726035118103027
Norm after each mp layer: 7.621429920196533
Norm after each mp layer: 33.76201629638672
Norm before input: 0.2552422881126404
Norm after input: 0.45222586393356323
Norm after each mp layer: 1.7726035118103027
Norm after each mp layer: 7.621429920196533
Norm after each mp layer: 33.76201629638672
Norm before input: 0.2552422881126404
Norm after input: 0.4522501230239868
Norm after each mp layer: 1.7726134061813354
Norm after each mp layer: 7.6261820793151855
Norm after each mp layer: 33.784217834472656
Epoch: 295, Loss: 0.0401, Energy: 15831.7812, Train: 99.42%, Valid: 78.00%, Test: 77.30%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4522501230239868
Norm after each mp layer: 1.7726134061813354
Norm after each mp layer: 7.6261820793151855
Norm after each mp layer: 33.784217834472656
Norm before input: 0.2552422881126404
Norm after input: 0.4522683322429657
Norm after each mp layer: 1.7726253271102905
Norm after each mp layer: 7.630948543548584
Norm after each mp layer: 33.80781555175781
Norm before input: 0.2552422881126404
Norm after input: 0.4522683322429657
Norm after each mp layer: 1.7726253271102905
Norm after each mp layer: 7.630948543548584
Norm after each mp layer: 33.80781555175781
Norm before input: 0.2552422881126404
Norm after input: 0.4522823095321655
Norm after each mp layer: 1.7726421356201172
Norm after each mp layer: 7.635756492614746
Norm after each mp layer: 33.832035064697266
Norm before input: 0.2552422881126404
Norm after input: 0.4522823095321655
Norm after each mp layer: 1.7726421356201172
Norm after each mp layer: 7.635756492614746
Norm after each mp layer: 33.832035064697266
Norm before input: 0.2552422881126404
Norm after input: 0.4522940218448639
Norm after each mp layer: 1.7726635932922363
Norm after each mp layer: 7.640598773956299
Norm after each mp layer: 33.85618209838867
Norm before input: 0.2552422881126404
Norm after input: 0.4522940218448639
Norm after each mp layer: 1.7726635932922363
Norm after each mp layer: 7.640598773956299
Norm after each mp layer: 33.85618209838867
Norm before input: 0.2552422881126404
Norm after input: 0.4523051679134369
Norm after each mp layer: 1.772683024406433
Norm after each mp layer: 7.645422458648682
Norm after each mp layer: 33.87975311279297
Norm before input: 0.2552422881126404
Norm after input: 0.4523051679134369
Norm after each mp layer: 1.772683024406433
Norm after each mp layer: 7.645422458648682
Norm after each mp layer: 33.87975311279297
Norm before input: 0.2552422881126404
Norm after input: 0.45231693983078003
Norm after each mp layer: 1.7726917266845703
Norm after each mp layer: 7.650136470794678
Norm after each mp layer: 33.9024658203125
Epoch: 300, Loss: 0.0389, Energy: 15475.8047, Train: 99.42%, Valid: 77.60%, Test: 77.30%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45231693983078003
Norm after each mp layer: 1.7726917266845703
Norm after each mp layer: 7.650136470794678
Norm after each mp layer: 33.9024658203125
Norm before input: 0.2552422881126404
Norm after input: 0.4523300528526306
Norm after each mp layer: 1.7726794481277466
Norm after each mp layer: 7.654638767242432
Norm after each mp layer: 33.92412185668945
Norm before input: 0.2552422881126404
Norm after input: 0.4523300528526306
Norm after each mp layer: 1.7726794481277466
Norm after each mp layer: 7.654638767242432
Norm after each mp layer: 33.92411804199219
Norm before input: 0.2552422881126404
Norm after input: 0.4523450434207916
Norm after each mp layer: 1.772641897201538
Norm after each mp layer: 7.658849239349365
Norm after each mp layer: 33.94457244873047
Norm before input: 0.2552422881126404
Norm after input: 0.4523450434207916
Norm after each mp layer: 1.772641897201538
Norm after each mp layer: 7.658849239349365
Norm after each mp layer: 33.94457244873047
Norm before input: 0.2552422881126404
Norm after input: 0.4523623585700989
Norm after each mp layer: 1.7725813388824463
Norm after each mp layer: 7.6627397537231445
Norm after each mp layer: 33.96363830566406
Norm before input: 0.2552422881126404
Norm after input: 0.4523623585700989
Norm after each mp layer: 1.7725813388824463
Norm after each mp layer: 7.6627397537231445
Norm after each mp layer: 33.96363830566406
Norm before input: 0.2552422881126404
Norm after input: 0.45238247513771057
Norm after each mp layer: 1.7725054025650024
Norm after each mp layer: 7.666330814361572
Norm after each mp layer: 33.98111343383789
Norm before input: 0.2552422881126404
Norm after input: 0.45238247513771057
Norm after each mp layer: 1.7725054025650024
Norm after each mp layer: 7.666330814361572
Norm after each mp layer: 33.98111343383789
Norm before input: 0.2552422881126404
Norm after input: 0.45240527391433716
Norm after each mp layer: 1.7724230289459229
Norm after each mp layer: 7.6696858406066895
Norm after each mp layer: 33.996883392333984
Epoch: 305, Loss: 0.0377, Energy: 15145.3340, Train: 99.42%, Valid: 77.60%, Test: 77.40%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45240527391433716
Norm after each mp layer: 1.7724230289459229
Norm after each mp layer: 7.6696858406066895
Norm after each mp layer: 33.996883392333984
Norm before input: 0.2552422881126404
Norm after input: 0.4524301588535309
Norm after each mp layer: 1.7723406553268433
Norm after each mp layer: 7.672874450683594
Norm after each mp layer: 34.01094055175781
Norm before input: 0.2552422881126404
Norm after input: 0.4524301588535309
Norm after each mp layer: 1.7723406553268433
Norm after each mp layer: 7.672874450683594
Norm after each mp layer: 34.01094055175781
Norm before input: 0.2552422881126404
Norm after input: 0.4524558186531067
Norm after each mp layer: 1.7722593545913696
Norm after each mp layer: 7.675955772399902
Norm after each mp layer: 34.02348709106445
Norm before input: 0.2552422881126404
Norm after input: 0.4524558186531067
Norm after each mp layer: 1.7722593545913696
Norm after each mp layer: 7.675955772399902
Norm after each mp layer: 34.02348709106445
Norm before input: 0.2552422881126404
Norm after input: 0.4524805247783661
Norm after each mp layer: 1.7721760272979736
Norm after each mp layer: 7.67896032333374
Norm after each mp layer: 34.03487777709961
Norm before input: 0.2552422881126404
Norm after input: 0.4524805247783661
Norm after each mp layer: 1.7721760272979736
Norm after each mp layer: 7.67896032333374
Norm after each mp layer: 34.03487777709961
Norm before input: 0.2552422881126404
Norm after input: 0.4525027275085449
Norm after each mp layer: 1.7720860242843628
Norm after each mp layer: 7.681901454925537
Norm after each mp layer: 34.045589447021484
Norm before input: 0.2552422881126404
Norm after input: 0.4525027275085449
Norm after each mp layer: 1.7720860242843628
Norm after each mp layer: 7.681901454925537
Norm after each mp layer: 34.045589447021484
Norm before input: 0.2552422881126404
Norm after input: 0.45252180099487305
Norm after each mp layer: 1.7719870805740356
Norm after each mp layer: 7.684781551361084
Norm after each mp layer: 34.056060791015625
Epoch: 310, Loss: 0.0366, Energy: 14780.5137, Train: 99.50%, Valid: 77.80%, Test: 77.60%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45252180099487305
Norm after each mp layer: 1.7719870805740356
Norm after each mp layer: 7.684781551361084
Norm after each mp layer: 34.056060791015625
Norm before input: 0.2552422881126404
Norm after input: 0.4525379538536072
Norm after each mp layer: 1.7718801498413086
Norm after each mp layer: 7.6876020431518555
Norm after each mp layer: 34.06663513183594
Norm before input: 0.2552422881126404
Norm after input: 0.4525379538536072
Norm after each mp layer: 1.7718801498413086
Norm after each mp layer: 7.6876020431518555
Norm after each mp layer: 34.06663513183594
Norm before input: 0.2552422881126404
Norm after input: 0.4525521993637085
Norm after each mp layer: 1.7717703580856323
Norm after each mp layer: 7.690367698669434
Norm after each mp layer: 34.0775032043457
Norm before input: 0.2552422881126404
Norm after input: 0.4525521993637085
Norm after each mp layer: 1.7717703580856323
Norm after each mp layer: 7.690367698669434
Norm after each mp layer: 34.0775032043457
Norm before input: 0.2552422881126404
Norm after input: 0.45256584882736206
Norm after each mp layer: 1.7716624736785889
Norm after each mp layer: 7.693072319030762
Norm after each mp layer: 34.08867645263672
Norm before input: 0.2552422881126404
Norm after input: 0.45256584882736206
Norm after each mp layer: 1.7716624736785889
Norm after each mp layer: 7.693072319030762
Norm after each mp layer: 34.08867645263672
Norm before input: 0.2552422881126404
Norm after input: 0.4525800049304962
Norm after each mp layer: 1.7715590000152588
Norm after each mp layer: 7.695703029632568
Norm after each mp layer: 34.10003662109375
Norm before input: 0.2552422881126404
Norm after input: 0.4525800049304962
Norm after each mp layer: 1.7715590000152588
Norm after each mp layer: 7.695703029632568
Norm after each mp layer: 34.10003662109375
Norm before input: 0.2552422881126404
Norm after input: 0.452595055103302
Norm after each mp layer: 1.7714576721191406
Norm after each mp layer: 7.698237895965576
Norm after each mp layer: 34.111392974853516
Epoch: 315, Loss: 0.0355, Energy: 14455.5225, Train: 99.59%, Valid: 77.80%, Test: 77.70%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.452595055103302
Norm after each mp layer: 1.7714576721191406
Norm after each mp layer: 7.698237895965576
Norm after each mp layer: 34.111392974853516
Norm before input: 0.2552422881126404
Norm after input: 0.4526110291481018
Norm after each mp layer: 1.7713534832000732
Norm after each mp layer: 7.700655460357666
Norm after each mp layer: 34.122528076171875
Norm before input: 0.2552422881126404
Norm after input: 0.4526110291481018
Norm after each mp layer: 1.7713534832000732
Norm after each mp layer: 7.700655460357666
Norm after each mp layer: 34.122528076171875
Norm before input: 0.2552422881126404
Norm after input: 0.45262739062309265
Norm after each mp layer: 1.7712409496307373
Norm after each mp layer: 7.702950954437256
Norm after each mp layer: 34.13322067260742
Norm before input: 0.2552422881126404
Norm after input: 0.45262739062309265
Norm after each mp layer: 1.7712409496307373
Norm after each mp layer: 7.702950954437256
Norm after each mp layer: 34.13322067260742
Norm before input: 0.2552422881126404
Norm after input: 0.45264384150505066
Norm after each mp layer: 1.771117925643921
Norm after each mp layer: 7.7051472663879395
Norm after each mp layer: 34.143310546875
Norm before input: 0.2552422881126404
Norm after input: 0.45264384150505066
Norm after each mp layer: 1.771117925643921
Norm after each mp layer: 7.7051472663879395
Norm after each mp layer: 34.143310546875
Norm before input: 0.2552422881126404
Norm after input: 0.4526602327823639
Norm after each mp layer: 1.7709859609603882
Norm after each mp layer: 7.707284450531006
Norm after each mp layer: 34.15272521972656
Norm before input: 0.2552422881126404
Norm after input: 0.4526602327823639
Norm after each mp layer: 1.7709859609603882
Norm after each mp layer: 7.707284450531006
Norm after each mp layer: 34.15272521972656
Norm before input: 0.2552422881126404
Norm after input: 0.4526767432689667
Norm after each mp layer: 1.7708497047424316
Norm after each mp layer: 7.709414005279541
Norm after each mp layer: 34.161495208740234
Epoch: 320, Loss: 0.0345, Energy: 14155.3818, Train: 99.67%, Valid: 77.20%, Test: 77.60%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4526767432689667
Norm after each mp layer: 1.7708497047424316
Norm after each mp layer: 7.709414005279541
Norm after each mp layer: 34.161495208740234
Norm before input: 0.2552422881126404
Norm after input: 0.45269322395324707
Norm after each mp layer: 1.7707149982452393
Norm after each mp layer: 7.711572647094727
Norm after each mp layer: 34.16978454589844
Norm before input: 0.2552422881126404
Norm after input: 0.45269322395324707
Norm after each mp layer: 1.7707149982452393
Norm after each mp layer: 7.711572647094727
Norm after each mp layer: 34.16978454589844
Norm before input: 0.2552422881126404
Norm after input: 0.45270979404449463
Norm after each mp layer: 1.7705856561660767
Norm after each mp layer: 7.7137675285339355
Norm after each mp layer: 34.1778564453125
Norm before input: 0.2552422881126404
Norm after input: 0.45270979404449463
Norm after each mp layer: 1.7705856561660767
Norm after each mp layer: 7.7137675285339355
Norm after each mp layer: 34.1778564453125
Norm before input: 0.2552422881126404
Norm after input: 0.45272597670555115
Norm after each mp layer: 1.770462155342102
Norm after each mp layer: 7.715978145599365
Norm after each mp layer: 34.185977935791016
Norm before input: 0.2552422881126404
Norm after input: 0.45272597670555115
Norm after each mp layer: 1.770462155342102
Norm after each mp layer: 7.715978145599365
Norm after each mp layer: 34.185977935791016
Norm before input: 0.2552422881126404
Norm after input: 0.4527413249015808
Norm after each mp layer: 1.7703428268432617
Norm after each mp layer: 7.718165874481201
Norm after each mp layer: 34.194393157958984
Norm before input: 0.2552422881126404
Norm after input: 0.4527413249015808
Norm after each mp layer: 1.7703428268432617
Norm after each mp layer: 7.718165874481201
Norm after each mp layer: 34.194393157958984
Norm before input: 0.2552422881126404
Norm after input: 0.45275571942329407
Norm after each mp layer: 1.7702239751815796
Norm after each mp layer: 7.720293998718262
Norm after each mp layer: 34.20321273803711
Epoch: 325, Loss: 0.0336, Energy: 13862.4160, Train: 99.67%, Valid: 77.00%, Test: 77.50%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45275571942329407
Norm after each mp layer: 1.7702239751815796
Norm after each mp layer: 7.720293998718262
Norm after each mp layer: 34.20321273803711
Norm before input: 0.2552422881126404
Norm after input: 0.4527692496776581
Norm after each mp layer: 1.770103931427002
Norm after each mp layer: 7.722343921661377
Norm after each mp layer: 34.21236801147461
Norm before input: 0.2552422881126404
Norm after input: 0.4527692496776581
Norm after each mp layer: 1.770103931427002
Norm after each mp layer: 7.722343921661377
Norm after each mp layer: 34.21236801147461
Norm before input: 0.2552422881126404
Norm after input: 0.4527823030948639
Norm after each mp layer: 1.7699809074401855
Norm after each mp layer: 7.724318504333496
Norm after each mp layer: 34.22165298461914
Norm before input: 0.2552422881126404
Norm after input: 0.4527823030948639
Norm after each mp layer: 1.7699809074401855
Norm after each mp layer: 7.724318504333496
Norm after each mp layer: 34.22165298461914
Norm before input: 0.2552422881126404
Norm after input: 0.4527955651283264
Norm after each mp layer: 1.7698553800582886
Norm after each mp layer: 7.726235389709473
Norm after each mp layer: 34.230770111083984
Norm before input: 0.2552422881126404
Norm after input: 0.4527955651283264
Norm after each mp layer: 1.7698553800582886
Norm after each mp layer: 7.726235389709473
Norm after each mp layer: 34.230770111083984
Norm before input: 0.2552422881126404
Norm after input: 0.4528094530105591
Norm after each mp layer: 1.7697277069091797
Norm after each mp layer: 7.72811222076416
Norm after each mp layer: 34.23942947387695
Norm before input: 0.2552422881126404
Norm after input: 0.4528094530105591
Norm after each mp layer: 1.7697277069091797
Norm after each mp layer: 7.728111743927002
Norm after each mp layer: 34.23942947387695
Norm before input: 0.2552422881126404
Norm after input: 0.45282405614852905
Norm after each mp layer: 1.769596815109253
Norm after each mp layer: 7.7299580574035645
Norm after each mp layer: 34.24742126464844
Epoch: 330, Loss: 0.0327, Energy: 13610.8623, Train: 99.67%, Valid: 77.00%, Test: 77.30%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45282405614852905
Norm after each mp layer: 1.769596815109253
Norm after each mp layer: 7.7299580574035645
Norm after each mp layer: 34.24742126464844
Norm before input: 0.2552422881126404
Norm after input: 0.4528389871120453
Norm after each mp layer: 1.7694613933563232
Norm after each mp layer: 7.731767654418945
Norm after each mp layer: 34.25468063354492
Norm before input: 0.2552422881126404
Norm after input: 0.4528389871120453
Norm after each mp layer: 1.7694613933563232
Norm after each mp layer: 7.731766700744629
Norm after each mp layer: 34.25468063354492
Norm before input: 0.2552422881126404
Norm after input: 0.45285385847091675
Norm after each mp layer: 1.7693206071853638
Norm after each mp layer: 7.733532428741455
Norm after each mp layer: 34.26130294799805
Norm before input: 0.2552422881126404
Norm after input: 0.45285385847091675
Norm after each mp layer: 1.7693206071853638
Norm after each mp layer: 7.733532428741455
Norm after each mp layer: 34.26130294799805
Norm before input: 0.2552422881126404
Norm after input: 0.4528680443763733
Norm after each mp layer: 1.769174575805664
Norm after each mp layer: 7.735246181488037
Norm after each mp layer: 34.267494201660156
Norm before input: 0.2552422881126404
Norm after input: 0.4528680443763733
Norm after each mp layer: 1.769174575805664
Norm after each mp layer: 7.735246181488037
Norm after each mp layer: 34.267494201660156
Norm before input: 0.2552422881126404
Norm after input: 0.452881395816803
Norm after each mp layer: 1.769025206565857
Norm after each mp layer: 7.736915111541748
Norm after each mp layer: 34.27351379394531
Norm before input: 0.2552422881126404
Norm after input: 0.452881395816803
Norm after each mp layer: 1.769025206565857
Norm after each mp layer: 7.736915111541748
Norm after each mp layer: 34.27351379394531
Norm before input: 0.2552422881126404
Norm after input: 0.45289382338523865
Norm after each mp layer: 1.7688759565353394
Norm after each mp layer: 7.73854923248291
Norm after each mp layer: 34.279605865478516
Epoch: 335, Loss: 0.0319, Energy: 13359.5459, Train: 99.67%, Valid: 77.00%, Test: 77.10%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45289382338523865
Norm after each mp layer: 1.7688759565353394
Norm after each mp layer: 7.73854923248291
Norm after each mp layer: 34.279605865478516
Norm before input: 0.2552422881126404
Norm after input: 0.4529055655002594
Norm after each mp layer: 1.768728256225586
Norm after each mp layer: 7.740157604217529
Norm after each mp layer: 34.28593444824219
Norm before input: 0.2552422881126404
Norm after input: 0.4529055655002594
Norm after each mp layer: 1.768728256225586
Norm after each mp layer: 7.740157604217529
Norm after each mp layer: 34.28593444824219
Norm before input: 0.2552422881126404
Norm after input: 0.4529166519641876
Norm after each mp layer: 1.7685824632644653
Norm after each mp layer: 7.741743564605713
Norm after each mp layer: 34.29255676269531
Norm before input: 0.2552422881126404
Norm after input: 0.4529166519641876
Norm after each mp layer: 1.7685824632644653
Norm after each mp layer: 7.741743087768555
Norm after each mp layer: 34.29255676269531
Norm before input: 0.2552422881126404
Norm after input: 0.45292723178863525
Norm after each mp layer: 1.768437147140503
Norm after each mp layer: 7.743298053741455
Norm after each mp layer: 34.29941940307617
Norm before input: 0.2552422881126404
Norm after input: 0.45292723178863525
Norm after each mp layer: 1.768437147140503
Norm after each mp layer: 7.743298053741455
Norm after each mp layer: 34.29941940307617
Norm before input: 0.2552422881126404
Norm after input: 0.4529372453689575
Norm after each mp layer: 1.7682888507843018
Norm after each mp layer: 7.744808197021484
Norm after each mp layer: 34.3063850402832
Norm before input: 0.2552422881126404
Norm after input: 0.4529372453689575
Norm after each mp layer: 1.7682888507843018
Norm after each mp layer: 7.744808197021484
Norm after each mp layer: 34.3063850402832
Norm before input: 0.2552422881126404
Norm after input: 0.45294690132141113
Norm after each mp layer: 1.76813542842865
Norm after each mp layer: 7.7462639808654785
Norm after each mp layer: 34.313270568847656
Epoch: 340, Loss: 0.0311, Energy: 13127.0117, Train: 99.67%, Valid: 77.00%, Test: 77.00%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45294690132141113
Norm after each mp layer: 1.76813542842865
Norm after each mp layer: 7.7462639808654785
Norm after each mp layer: 34.313270568847656
Norm before input: 0.2552422881126404
Norm after input: 0.45295634865760803
Norm after each mp layer: 1.7679758071899414
Norm after each mp layer: 7.747665882110596
Norm after each mp layer: 34.31991958618164
Norm before input: 0.2552422881126404
Norm after input: 0.45295634865760803
Norm after each mp layer: 1.7679758071899414
Norm after each mp layer: 7.747665882110596
Norm after each mp layer: 34.31991958618164
Norm before input: 0.2552422881126404
Norm after input: 0.4529658555984497
Norm after each mp layer: 1.7678114175796509
Norm after each mp layer: 7.749029159545898
Norm after each mp layer: 34.32622528076172
Norm before input: 0.2552422881126404
Norm after input: 0.4529658555984497
Norm after each mp layer: 1.7678114175796509
Norm after each mp layer: 7.749029159545898
Norm after each mp layer: 34.32622528076172
Norm before input: 0.2552422881126404
Norm after input: 0.4529757797718048
Norm after each mp layer: 1.7676446437835693
Norm after each mp layer: 7.750370502471924
Norm after each mp layer: 34.332191467285156
Norm before input: 0.2552422881126404
Norm after input: 0.4529757797718048
Norm after each mp layer: 1.7676446437835693
Norm after each mp layer: 7.750370502471924
Norm after each mp layer: 34.332191467285156
Norm before input: 0.2552422881126404
Norm after input: 0.4529859721660614
Norm after each mp layer: 1.7674775123596191
Norm after each mp layer: 7.751704692840576
Norm after each mp layer: 34.33788299560547
Norm before input: 0.2552422881126404
Norm after input: 0.4529859721660614
Norm after each mp layer: 1.7674775123596191
Norm after each mp layer: 7.751705646514893
Norm after each mp layer: 34.33788299560547
Norm before input: 0.2552422881126404
Norm after input: 0.4529963433742523
Norm after each mp layer: 1.7673112154006958
Norm after each mp layer: 7.753037929534912
Norm after each mp layer: 34.34343719482422
Epoch: 345, Loss: 0.0303, Energy: 12911.3291, Train: 99.67%, Valid: 76.80%, Test: 76.90%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4529963433742523
Norm after each mp layer: 1.7673112154006958
Norm after each mp layer: 7.753037929534912
Norm after each mp layer: 34.34343719482422
Norm before input: 0.2552422881126404
Norm after input: 0.4530065357685089
Norm after each mp layer: 1.767146110534668
Norm after each mp layer: 7.754366397857666
Norm after each mp layer: 34.349021911621094
Norm before input: 0.2552422881126404
Norm after input: 0.4530065357685089
Norm after each mp layer: 1.767146110534668
Norm after each mp layer: 7.754366397857666
Norm after each mp layer: 34.349021911621094
Norm before input: 0.2552422881126404
Norm after input: 0.4530162513256073
Norm after each mp layer: 1.7669812440872192
Norm after each mp layer: 7.755681991577148
Norm after each mp layer: 34.3547477722168
Norm before input: 0.2552422881126404
Norm after input: 0.4530162513256073
Norm after each mp layer: 1.7669812440872192
Norm after each mp layer: 7.755681991577148
Norm after each mp layer: 34.3547477722168
Norm before input: 0.2552422881126404
Norm after input: 0.45302528142929077
Norm after each mp layer: 1.7668166160583496
Norm after each mp layer: 7.75697660446167
Norm after each mp layer: 34.36067581176758
Norm before input: 0.2552422881126404
Norm after input: 0.45302528142929077
Norm after each mp layer: 1.7668166160583496
Norm after each mp layer: 7.756977081298828
Norm after each mp layer: 34.36067581176758
Norm before input: 0.2552422881126404
Norm after input: 0.4530336558818817
Norm after each mp layer: 1.7666512727737427
Norm after each mp layer: 7.758249282836914
Norm after each mp layer: 34.366783142089844
Norm before input: 0.2552422881126404
Norm after input: 0.4530336558818817
Norm after each mp layer: 1.7666512727737427
Norm after each mp layer: 7.758249282836914
Norm after each mp layer: 34.366783142089844
Norm before input: 0.2552422881126404
Norm after input: 0.453041672706604
Norm after each mp layer: 1.766485571861267
Norm after each mp layer: 7.7595014572143555
Norm after each mp layer: 34.37300109863281
Epoch: 350, Loss: 0.0296, Energy: 12702.4971, Train: 99.75%, Valid: 76.40%, Test: 77.00%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.453041672706604
Norm after each mp layer: 1.766485571861267
Norm after each mp layer: 7.7595014572143555
Norm after each mp layer: 34.37300109863281
Norm before input: 0.2552422881126404
Norm after input: 0.4530494511127472
Norm after each mp layer: 1.7663187980651855
Norm after each mp layer: 7.7607316970825195
Norm after each mp layer: 34.37921142578125
Norm before input: 0.2552422881126404
Norm after input: 0.4530494511127472
Norm after each mp layer: 1.7663187980651855
Norm after each mp layer: 7.7607316970825195
Norm after each mp layer: 34.37921142578125
Norm before input: 0.2552422881126404
Norm after input: 0.45305711030960083
Norm after each mp layer: 1.7661492824554443
Norm after each mp layer: 7.761937141418457
Norm after each mp layer: 34.385318756103516
Norm before input: 0.2552422881126404
Norm after input: 0.45305711030960083
Norm after each mp layer: 1.7661492824554443
Norm after each mp layer: 7.761937141418457
Norm after each mp layer: 34.385318756103516
Norm before input: 0.2552422881126404
Norm after input: 0.4530646800994873
Norm after each mp layer: 1.7659761905670166
Norm after each mp layer: 7.763110637664795
Norm after each mp layer: 34.39125442504883
Norm before input: 0.2552422881126404
Norm after input: 0.4530646800994873
Norm after each mp layer: 1.7659761905670166
Norm after each mp layer: 7.763110637664795
Norm after each mp layer: 34.39125442504883
Norm before input: 0.2552422881126404
Norm after input: 0.4530721604824066
Norm after each mp layer: 1.765798807144165
Norm after each mp layer: 7.76424503326416
Norm after each mp layer: 34.39700698852539
Norm before input: 0.2552422881126404
Norm after input: 0.4530721604824066
Norm after each mp layer: 1.765798807144165
Norm after each mp layer: 7.76424503326416
Norm after each mp layer: 34.39700698852539
Norm before input: 0.2552422881126404
Norm after input: 0.4530794024467468
Norm after each mp layer: 1.7656166553497314
Norm after each mp layer: 7.7653398513793945
Norm after each mp layer: 34.40259552001953
Epoch: 355, Loss: 0.0289, Energy: 12508.8330, Train: 99.75%, Valid: 76.00%, Test: 77.10%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4530794024467468
Norm after each mp layer: 1.7656166553497314
Norm after each mp layer: 7.7653398513793945
Norm after each mp layer: 34.40259552001953
Norm before input: 0.2552422881126404
Norm after input: 0.4530864953994751
Norm after each mp layer: 1.7654318809509277
Norm after each mp layer: 7.766399383544922
Norm after each mp layer: 34.4080810546875
Norm before input: 0.2552422881126404
Norm after input: 0.4530864953994751
Norm after each mp layer: 1.7654318809509277
Norm after each mp layer: 7.766399383544922
Norm after each mp layer: 34.4080810546875
Norm before input: 0.2552422881126404
Norm after input: 0.45309343934059143
Norm after each mp layer: 1.7652454376220703
Norm after each mp layer: 7.767428874969482
Norm after each mp layer: 34.41352081298828
Norm before input: 0.2552422881126404
Norm after input: 0.45309343934059143
Norm after each mp layer: 1.7652454376220703
Norm after each mp layer: 7.767428874969482
Norm after each mp layer: 34.41352081298828
Norm before input: 0.2552422881126404
Norm after input: 0.4531002342700958
Norm after each mp layer: 1.7650586366653442
Norm after each mp layer: 7.768436431884766
Norm after each mp layer: 34.41895294189453
Norm before input: 0.2552422881126404
Norm after input: 0.4531002342700958
Norm after each mp layer: 1.7650586366653442
Norm after each mp layer: 7.768436431884766
Norm after each mp layer: 34.41895294189453
Norm before input: 0.2552422881126404
Norm after input: 0.45310670137405396
Norm after each mp layer: 1.764871597290039
Norm after each mp layer: 7.769423007965088
Norm after each mp layer: 34.42439270019531
Norm before input: 0.2552422881126404
Norm after input: 0.45310670137405396
Norm after each mp layer: 1.764871597290039
Norm after each mp layer: 7.769423007965088
Norm after each mp layer: 34.42439270019531
Norm before input: 0.2552422881126404
Norm after input: 0.4531128704547882
Norm after each mp layer: 1.764683723449707
Norm after each mp layer: 7.770388126373291
Norm after each mp layer: 34.4298210144043
Epoch: 360, Loss: 0.0282, Energy: 12322.3467, Train: 99.75%, Valid: 75.20%, Test: 77.00%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4531128704547882
Norm after each mp layer: 1.764683723449707
Norm after each mp layer: 7.770388126373291
Norm after each mp layer: 34.4298210144043
Norm before input: 0.2552422881126404
Norm after input: 0.45311859250068665
Norm after each mp layer: 1.764493703842163
Norm after each mp layer: 7.771328449249268
Norm after each mp layer: 34.435203552246094
Norm before input: 0.2552422881126404
Norm after input: 0.45311859250068665
Norm after each mp layer: 1.764493703842163
Norm after each mp layer: 7.771328449249268
Norm after each mp layer: 34.435203552246094
Norm before input: 0.2552422881126404
Norm after input: 0.45312389731407166
Norm after each mp layer: 1.7643011808395386
Norm after each mp layer: 7.772241592407227
Norm after each mp layer: 34.440494537353516
Norm before input: 0.2552422881126404
Norm after input: 0.45312389731407166
Norm after each mp layer: 1.7643011808395386
Norm after each mp layer: 7.772241592407227
Norm after each mp layer: 34.440494537353516
Norm before input: 0.2552422881126404
Norm after input: 0.45312896370887756
Norm after each mp layer: 1.7641061544418335
Norm after each mp layer: 7.773129940032959
Norm after each mp layer: 34.44567108154297
Norm before input: 0.2552422881126404
Norm after input: 0.45312896370887756
Norm after each mp layer: 1.7641061544418335
Norm after each mp layer: 7.773129940032959
Norm after each mp layer: 34.44567108154297
Norm before input: 0.2552422881126404
Norm after input: 0.45313388109207153
Norm after each mp layer: 1.7639089822769165
Norm after each mp layer: 7.773993492126465
Norm after each mp layer: 34.450714111328125
Norm before input: 0.2552422881126404
Norm after input: 0.45313388109207153
Norm after each mp layer: 1.7639089822769165
Norm after each mp layer: 7.773993492126465
Norm after each mp layer: 34.450714111328125
Norm before input: 0.2552422881126404
Norm after input: 0.45313888788223267
Norm after each mp layer: 1.7637099027633667
Norm after each mp layer: 7.774829864501953
Norm after each mp layer: 34.45563507080078
Epoch: 365, Loss: 0.0276, Energy: 12144.7822, Train: 99.75%, Valid: 75.20%, Test: 76.80%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45313888788223267
Norm after each mp layer: 1.7637099027633667
Norm after each mp layer: 7.774829864501953
Norm after each mp layer: 34.45563507080078
Norm before input: 0.2552422881126404
Norm after input: 0.4531438648700714
Norm after each mp layer: 1.7635091543197632
Norm after each mp layer: 7.775638580322266
Norm after each mp layer: 34.46044158935547
Norm before input: 0.2552422881126404
Norm after input: 0.4531438648700714
Norm after each mp layer: 1.7635091543197632
Norm after each mp layer: 7.775637626647949
Norm after each mp layer: 34.46044158935547
Norm before input: 0.2552422881126404
Norm after input: 0.4531489312648773
Norm after each mp layer: 1.7633064985275269
Norm after each mp layer: 7.776413917541504
Norm after each mp layer: 34.465145111083984
Norm before input: 0.2552422881126404
Norm after input: 0.4531489312648773
Norm after each mp layer: 1.7633064985275269
Norm after each mp layer: 7.776413917541504
Norm after each mp layer: 34.465145111083984
Norm before input: 0.2552422881126404
Norm after input: 0.45315393805503845
Norm after each mp layer: 1.7631016969680786
Norm after each mp layer: 7.777155876159668
Norm after each mp layer: 34.46975326538086
Norm before input: 0.2552422881126404
Norm after input: 0.45315393805503845
Norm after each mp layer: 1.7631016969680786
Norm after each mp layer: 7.777155876159668
Norm after each mp layer: 34.46975326538086
Norm before input: 0.2552422881126404
Norm after input: 0.45315879583358765
Norm after each mp layer: 1.7628940343856812
Norm after each mp layer: 7.777867317199707
Norm after each mp layer: 34.474246978759766
Norm before input: 0.2552422881126404
Norm after input: 0.45315879583358765
Norm after each mp layer: 1.7628940343856812
Norm after each mp layer: 7.777867794036865
Norm after each mp layer: 34.474246978759766
Norm before input: 0.2552422881126404
Norm after input: 0.4531635642051697
Norm after each mp layer: 1.7626844644546509
Norm after each mp layer: 7.778552532196045
Norm after each mp layer: 34.4786376953125
Epoch: 370, Loss: 0.0270, Energy: 11973.4121, Train: 99.75%, Valid: 75.00%, Test: 76.80%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4531635642051697
Norm after each mp layer: 1.7626844644546509
Norm after each mp layer: 7.778552532196045
Norm after each mp layer: 34.4786376953125
Norm before input: 0.2552422881126404
Norm after input: 0.45316818356513977
Norm after each mp layer: 1.7624727487564087
Norm after each mp layer: 7.77921724319458
Norm after each mp layer: 34.48291015625
Norm before input: 0.2552422881126404
Norm after input: 0.45316818356513977
Norm after each mp layer: 1.7624727487564087
Norm after each mp layer: 7.77921724319458
Norm after each mp layer: 34.48291015625
Norm before input: 0.2552422881126404
Norm after input: 0.45317259430885315
Norm after each mp layer: 1.762259602546692
Norm after each mp layer: 7.779862403869629
Norm after each mp layer: 34.48708724975586
Norm before input: 0.2552422881126404
Norm after input: 0.45317259430885315
Norm after each mp layer: 1.762259602546692
Norm after each mp layer: 7.779862403869629
Norm after each mp layer: 34.48708724975586
Norm before input: 0.2552422881126404
Norm after input: 0.45317673683166504
Norm after each mp layer: 1.7620450258255005
Norm after each mp layer: 7.780488014221191
Norm after each mp layer: 34.491188049316406
Norm before input: 0.2552422881126404
Norm after input: 0.45317673683166504
Norm after each mp layer: 1.7620450258255005
Norm after each mp layer: 7.780488014221191
Norm after each mp layer: 34.491188049316406
Norm before input: 0.2552422881126404
Norm after input: 0.45318058133125305
Norm after each mp layer: 1.7618285417556763
Norm after each mp layer: 7.781091213226318
Norm after each mp layer: 34.495235443115234
Norm before input: 0.2552422881126404
Norm after input: 0.45318058133125305
Norm after each mp layer: 1.7618285417556763
Norm after each mp layer: 7.781091213226318
Norm after each mp layer: 34.495235443115234
Norm before input: 0.2552422881126404
Norm after input: 0.4531840980052948
Norm after each mp layer: 1.7616097927093506
Norm after each mp layer: 7.781665325164795
Norm after each mp layer: 34.49924087524414
Epoch: 375, Loss: 0.0264, Energy: 11807.4727, Train: 99.75%, Valid: 74.80%, Test: 76.70%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4531840980052948
Norm after each mp layer: 1.7616097927093506
Norm after each mp layer: 7.781665325164795
Norm after each mp layer: 34.49924087524414
Norm before input: 0.2552422881126404
Norm after input: 0.4531872570514679
Norm after each mp layer: 1.7613892555236816
Norm after each mp layer: 7.782209873199463
Norm after each mp layer: 34.50319290161133
Norm before input: 0.2552422881126404
Norm after input: 0.4531872570514679
Norm after each mp layer: 1.7613892555236816
Norm after each mp layer: 7.782209873199463
Norm after each mp layer: 34.50319290161133
Norm before input: 0.2552422881126404
Norm after input: 0.45319026708602905
Norm after each mp layer: 1.7611665725708008
Norm after each mp layer: 7.782726764678955
Norm after each mp layer: 34.5070686340332
Norm before input: 0.2552422881126404
Norm after input: 0.45319026708602905
Norm after each mp layer: 1.7611664533615112
Norm after each mp layer: 7.782726764678955
Norm after each mp layer: 34.5070686340332
Norm before input: 0.2552422881126404
Norm after input: 0.4531932473182678
Norm after each mp layer: 1.760941982269287
Norm after each mp layer: 7.783217906951904
Norm after each mp layer: 34.51082229614258
Norm before input: 0.2552422881126404
Norm after input: 0.4531932473182678
Norm after each mp layer: 1.760941982269287
Norm after each mp layer: 7.783217906951904
Norm after each mp layer: 34.51082229614258
Norm before input: 0.2552422881126404
Norm after input: 0.4531962275505066
Norm after each mp layer: 1.7607154846191406
Norm after each mp layer: 7.7836833000183105
Norm after each mp layer: 34.51441955566406
Norm before input: 0.2552422881126404
Norm after input: 0.4531962275505066
Norm after each mp layer: 1.7607154846191406
Norm after each mp layer: 7.7836833000183105
Norm after each mp layer: 34.51441955566406
Norm before input: 0.2552422881126404
Norm after input: 0.453199177980423
Norm after each mp layer: 1.7604866027832031
Norm after each mp layer: 7.784121990203857
Norm after each mp layer: 34.5178337097168
Epoch: 380, Loss: 0.0258, Energy: 11647.9482, Train: 99.75%, Valid: 74.80%, Test: 76.70%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.453199177980423
Norm after each mp layer: 1.7604866027832031
Norm after each mp layer: 7.784121990203857
Norm after each mp layer: 34.5178337097168
Norm before input: 0.2552422881126404
Norm after input: 0.45320212841033936
Norm after each mp layer: 1.7602553367614746
Norm after each mp layer: 7.784531593322754
Norm after each mp layer: 34.521060943603516
Norm before input: 0.2552422881126404
Norm after input: 0.45320212841033936
Norm after each mp layer: 1.7602553367614746
Norm after each mp layer: 7.784531593322754
Norm after each mp layer: 34.521060943603516
Norm before input: 0.2552422881126404
Norm after input: 0.4532049596309662
Norm after each mp layer: 1.760021448135376
Norm after each mp layer: 7.784912109375
Norm after each mp layer: 34.524112701416016
Norm before input: 0.2552422881126404
Norm after input: 0.4532049596309662
Norm after each mp layer: 1.760021448135376
Norm after each mp layer: 7.784912109375
Norm after each mp layer: 34.524112701416016
Norm before input: 0.2552422881126404
Norm after input: 0.453207790851593
Norm after each mp layer: 1.7597854137420654
Norm after each mp layer: 7.785265922546387
Norm after each mp layer: 34.52702331542969
Norm before input: 0.2552422881126404
Norm after input: 0.453207790851593
Norm after each mp layer: 1.7597854137420654
Norm after each mp layer: 7.785265922546387
Norm after each mp layer: 34.52702331542969
Norm before input: 0.2552422881126404
Norm after input: 0.4532104432582855
Norm after each mp layer: 1.7595478296279907
Norm after each mp layer: 7.785593032836914
Norm after each mp layer: 34.52981948852539
Norm before input: 0.2552422881126404
Norm after input: 0.4532104432582855
Norm after each mp layer: 1.7595478296279907
Norm after each mp layer: 7.785593032836914
Norm after each mp layer: 34.52981948852539
Norm before input: 0.2552422881126404
Norm after input: 0.45321306586265564
Norm after each mp layer: 1.7593086957931519
Norm after each mp layer: 7.785895347595215
Norm after each mp layer: 34.53252029418945
Epoch: 385, Loss: 0.0253, Energy: 11491.5967, Train: 99.75%, Valid: 74.40%, Test: 76.70%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45321306586265564
Norm after each mp layer: 1.7593086957931519
Norm after each mp layer: 7.785896301269531
Norm after each mp layer: 34.53252029418945
Norm before input: 0.2552422881126404
Norm after input: 0.45321550965309143
Norm after each mp layer: 1.7590681314468384
Norm after each mp layer: 7.786174297332764
Norm after each mp layer: 34.53513717651367
Norm before input: 0.2552422881126404
Norm after input: 0.45321550965309143
Norm after each mp layer: 1.7590681314468384
Norm after each mp layer: 7.786174297332764
Norm after each mp layer: 34.53513717651367
Norm before input: 0.2552422881126404
Norm after input: 0.4532177746295929
Norm after each mp layer: 1.7588257789611816
Norm after each mp layer: 7.786428451538086
Norm after each mp layer: 34.53765869140625
Norm before input: 0.2552422881126404
Norm after input: 0.4532177746295929
Norm after each mp layer: 1.7588257789611816
Norm after each mp layer: 7.786428451538086
Norm after each mp layer: 34.53765869140625
Norm before input: 0.2552422881126404
Norm after input: 0.4532198905944824
Norm after each mp layer: 1.7585816383361816
Norm after each mp layer: 7.786656856536865
Norm after each mp layer: 34.54005813598633
Norm before input: 0.2552422881126404
Norm after input: 0.4532198905944824
Norm after each mp layer: 1.7585816383361816
Norm after each mp layer: 7.786656856536865
Norm after each mp layer: 34.54005813598633
Norm before input: 0.2552422881126404
Norm after input: 0.4532219171524048
Norm after each mp layer: 1.7583353519439697
Norm after each mp layer: 7.786860466003418
Norm after each mp layer: 34.54233169555664
Norm before input: 0.2552422881126404
Norm after input: 0.4532219171524048
Norm after each mp layer: 1.7583353519439697
Norm after each mp layer: 7.786860466003418
Norm after each mp layer: 34.54233169555664
Norm before input: 0.2552422881126404
Norm after input: 0.45322370529174805
Norm after each mp layer: 1.758086919784546
Norm after each mp layer: 7.787038326263428
Norm after each mp layer: 34.544464111328125
Epoch: 390, Loss: 0.0247, Energy: 11339.2734, Train: 99.75%, Valid: 74.40%, Test: 76.40%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45322370529174805
Norm after each mp layer: 1.758086919784546
Norm after each mp layer: 7.787038326263428
Norm after each mp layer: 34.544464111328125
Norm before input: 0.2552422881126404
Norm after input: 0.4532255530357361
Norm after each mp layer: 1.7578362226486206
Norm after each mp layer: 7.787193775177002
Norm after each mp layer: 34.546443939208984
Norm before input: 0.2552422881126404
Norm after input: 0.4532255530357361
Norm after each mp layer: 1.7578362226486206
Norm after each mp layer: 7.787193775177002
Norm after each mp layer: 34.546443939208984
Norm before input: 0.2552422881126404
Norm after input: 0.45322728157043457
Norm after each mp layer: 1.7575842142105103
Norm after each mp layer: 7.787323951721191
Norm after each mp layer: 34.54829025268555
Norm before input: 0.2552422881126404
Norm after input: 0.45322728157043457
Norm after each mp layer: 1.7575842142105103
Norm after each mp layer: 7.787323951721191
Norm after each mp layer: 34.54829025268555
Norm before input: 0.2552422881126404
Norm after input: 0.4532289505004883
Norm after each mp layer: 1.7573299407958984
Norm after each mp layer: 7.787430763244629
Norm after each mp layer: 34.550010681152344
Norm before input: 0.2552422881126404
Norm after input: 0.4532289505004883
Norm after each mp layer: 1.7573299407958984
Norm after each mp layer: 7.787430763244629
Norm after each mp layer: 34.550010681152344
Norm before input: 0.2552422881126404
Norm after input: 0.45323047041893005
Norm after each mp layer: 1.7570738792419434
Norm after each mp layer: 7.787511825561523
Norm after each mp layer: 34.55162048339844
Norm before input: 0.2552422881126404
Norm after input: 0.45323047041893005
Norm after each mp layer: 1.7570738792419434
Norm after each mp layer: 7.787511825561523
Norm after each mp layer: 34.55162048339844
Norm before input: 0.2552422881126404
Norm after input: 0.45323190093040466
Norm after each mp layer: 1.7568163871765137
Norm after each mp layer: 7.787569522857666
Norm after each mp layer: 34.55311965942383
Epoch: 395, Loss: 0.0242, Energy: 11190.0312, Train: 99.83%, Valid: 74.40%, Test: 76.40%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45323190093040466
Norm after each mp layer: 1.7568163871765137
Norm after each mp layer: 7.787569522857666
Norm after each mp layer: 34.55311965942383
Norm before input: 0.2552422881126404
Norm after input: 0.4532332718372345
Norm after each mp layer: 1.7565569877624512
Norm after each mp layer: 7.787602424621582
Norm after each mp layer: 34.55451965332031
Norm before input: 0.2552422881126404
Norm after input: 0.4532332718372345
Norm after each mp layer: 1.7565569877624512
Norm after each mp layer: 7.787602424621582
Norm after each mp layer: 34.55451965332031
Norm before input: 0.2552422881126404
Norm after input: 0.4532344341278076
Norm after each mp layer: 1.756295919418335
Norm after each mp layer: 7.787612438201904
Norm after each mp layer: 34.55580520629883
Norm before input: 0.2552422881126404
Norm after input: 0.4532344341278076
Norm after each mp layer: 1.756295919418335
Norm after each mp layer: 7.787612438201904
Norm after each mp layer: 34.55580520629883
Norm before input: 0.2552422881126404
Norm after input: 0.4532356262207031
Norm after each mp layer: 1.7560333013534546
Norm after each mp layer: 7.787602424621582
Norm after each mp layer: 34.556983947753906
Norm before input: 0.2552422881126404
Norm after input: 0.4532356262207031
Norm after each mp layer: 1.7560333013534546
Norm after each mp layer: 7.787602424621582
Norm after each mp layer: 34.556983947753906
Norm before input: 0.2552422881126404
Norm after input: 0.45323678851127625
Norm after each mp layer: 1.7557693719863892
Norm after each mp layer: 7.787569999694824
Norm after each mp layer: 34.55802917480469
Norm before input: 0.2552422881126404
Norm after input: 0.45323678851127625
Norm after each mp layer: 1.7557693719863892
Norm after each mp layer: 7.787569999694824
Norm after each mp layer: 34.55802917480469
Norm before input: 0.2552422881126404
Norm after input: 0.45323777198791504
Norm after each mp layer: 1.75550377368927
Norm after each mp layer: 7.787517547607422
Norm after each mp layer: 34.5589485168457
Epoch: 400, Loss: 0.0237, Energy: 11044.1221, Train: 99.83%, Valid: 74.20%, Test: 76.40%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45323777198791504
Norm after each mp layer: 1.75550377368927
Norm after each mp layer: 7.787517547607422
Norm after each mp layer: 34.5589485168457
Norm before input: 0.2552422881126404
Norm after input: 0.4532388150691986
Norm after each mp layer: 1.7552365064620972
Norm after each mp layer: 7.7874436378479
Norm after each mp layer: 34.55973815917969
Norm before input: 0.2552422881126404
Norm after input: 0.4532388150691986
Norm after each mp layer: 1.7552365064620972
Norm after each mp layer: 7.7874436378479
Norm after each mp layer: 34.55973815917969
Norm before input: 0.2552422881126404
Norm after input: 0.453239768743515
Norm after each mp layer: 1.7549679279327393
Norm after each mp layer: 7.787348747253418
Norm after each mp layer: 34.56040954589844
Norm before input: 0.2552422881126404
Norm after input: 0.453239768743515
Norm after each mp layer: 1.7549679279327393
Norm after each mp layer: 7.787348747253418
Norm after each mp layer: 34.56040954589844
Norm before input: 0.2552422881126404
Norm after input: 0.45324066281318665
Norm after each mp layer: 1.7546977996826172
Norm after each mp layer: 7.787233352661133
Norm after each mp layer: 34.56096267700195
Norm before input: 0.2552422881126404
Norm after input: 0.45324066281318665
Norm after each mp layer: 1.7546977996826172
Norm after each mp layer: 7.787233352661133
Norm after each mp layer: 34.56096649169922
Norm before input: 0.2552422881126404
Norm after input: 0.4532414674758911
Norm after each mp layer: 1.7544262409210205
Norm after each mp layer: 7.78709602355957
Norm after each mp layer: 34.5614128112793
Norm before input: 0.2552422881126404
Norm after input: 0.4532414674758911
Norm after each mp layer: 1.7544262409210205
Norm after each mp layer: 7.78709602355957
Norm after each mp layer: 34.5614128112793
Norm before input: 0.2552422881126404
Norm after input: 0.4532422721385956
Norm after each mp layer: 1.7541532516479492
Norm after each mp layer: 7.7869391441345215
Norm after each mp layer: 34.5617561340332
Epoch: 405, Loss: 0.0232, Energy: 10900.8848, Train: 99.83%, Valid: 74.20%, Test: 76.30%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532422721385956
Norm after each mp layer: 1.7541532516479492
Norm after each mp layer: 7.7869391441345215
Norm after each mp layer: 34.5617561340332
Norm before input: 0.2552422881126404
Norm after input: 0.4532430171966553
Norm after each mp layer: 1.7538784742355347
Norm after each mp layer: 7.786760330200195
Norm after each mp layer: 34.5620002746582
Norm before input: 0.2552422881126404
Norm after input: 0.4532430171966553
Norm after each mp layer: 1.7538784742355347
Norm after each mp layer: 7.786760330200195
Norm after each mp layer: 34.5620002746582
Norm before input: 0.2552422881126404
Norm after input: 0.4532436430454254
Norm after each mp layer: 1.7536022663116455
Norm after each mp layer: 7.786560535430908
Norm after each mp layer: 34.562137603759766
Norm before input: 0.2552422881126404
Norm after input: 0.4532436430454254
Norm after each mp layer: 1.7536022663116455
Norm after each mp layer: 7.786560535430908
Norm after each mp layer: 34.562137603759766
Norm before input: 0.2552422881126404
Norm after input: 0.4532442092895508
Norm after each mp layer: 1.7533245086669922
Norm after each mp layer: 7.786340236663818
Norm after each mp layer: 34.56216812133789
Norm before input: 0.2552422881126404
Norm after input: 0.4532442092895508
Norm after each mp layer: 1.7533245086669922
Norm after each mp layer: 7.786340236663818
Norm after each mp layer: 34.56216812133789
Norm before input: 0.2552422881126404
Norm after input: 0.45324471592903137
Norm after each mp layer: 1.7530452013015747
Norm after each mp layer: 7.7861008644104
Norm after each mp layer: 34.56209945678711
Norm before input: 0.2552422881126404
Norm after input: 0.45324471592903137
Norm after each mp layer: 1.7530452013015747
Norm after each mp layer: 7.7861008644104
Norm after each mp layer: 34.56209945678711
Norm before input: 0.2552422881126404
Norm after input: 0.4532451331615448
Norm after each mp layer: 1.7527648210525513
Norm after each mp layer: 7.7858428955078125
Norm after each mp layer: 34.561927795410156
Epoch: 410, Loss: 0.0228, Energy: 10760.5586, Train: 99.92%, Valid: 74.20%, Test: 76.10%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532451331615448
Norm after each mp layer: 1.7527648210525513
Norm after each mp layer: 7.7858428955078125
Norm after each mp layer: 34.561927795410156
Norm before input: 0.2552422881126404
Norm after input: 0.45324552059173584
Norm after each mp layer: 1.7524830102920532
Norm after each mp layer: 7.7855658531188965
Norm after each mp layer: 34.56166458129883
Norm before input: 0.2552422881126404
Norm after input: 0.45324552059173584
Norm after each mp layer: 1.7524830102920532
Norm after each mp layer: 7.7855658531188965
Norm after each mp layer: 34.56166076660156
Norm before input: 0.2552422881126404
Norm after input: 0.4532458484172821
Norm after each mp layer: 1.7522001266479492
Norm after each mp layer: 7.7852702140808105
Norm after each mp layer: 34.56130599975586
Norm before input: 0.2552422881126404
Norm after input: 0.4532458484172821
Norm after each mp layer: 1.7522001266479492
Norm after each mp layer: 7.7852702140808105
Norm after each mp layer: 34.56130599975586
Norm before input: 0.2552422881126404
Norm after input: 0.4532461166381836
Norm after each mp layer: 1.7519159317016602
Norm after each mp layer: 7.7849555015563965
Norm after each mp layer: 34.560855865478516
Norm before input: 0.2552422881126404
Norm after input: 0.4532461166381836
Norm after each mp layer: 1.7519159317016602
Norm after each mp layer: 7.7849555015563965
Norm after each mp layer: 34.560855865478516
Norm before input: 0.2552422881126404
Norm after input: 0.4532463848590851
Norm after each mp layer: 1.7516303062438965
Norm after each mp layer: 7.7846221923828125
Norm after each mp layer: 34.560302734375
Norm before input: 0.2552422881126404
Norm after input: 0.4532463848590851
Norm after each mp layer: 1.7516303062438965
Norm after each mp layer: 7.7846221923828125
Norm after each mp layer: 34.560302734375
Norm before input: 0.2552422881126404
Norm after input: 0.4532466232776642
Norm after each mp layer: 1.7513436079025269
Norm after each mp layer: 7.784270286560059
Norm after each mp layer: 34.559654235839844
Epoch: 415, Loss: 0.0223, Energy: 10623.1270, Train: 99.92%, Valid: 74.20%, Test: 76.10%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532466232776642
Norm after each mp layer: 1.7513436079025269
Norm after each mp layer: 7.784270286560059
Norm after each mp layer: 34.559654235839844
Norm before input: 0.2552422881126404
Norm after input: 0.4532468318939209
Norm after each mp layer: 1.7510557174682617
Norm after each mp layer: 7.783901691436768
Norm after each mp layer: 34.558902740478516
Norm before input: 0.2552422881126404
Norm after input: 0.4532468318939209
Norm after each mp layer: 1.7510557174682617
Norm after each mp layer: 7.783901691436768
Norm after each mp layer: 34.558902740478516
Norm before input: 0.2552422881126404
Norm after input: 0.4532470107078552
Norm after each mp layer: 1.750766634941101
Norm after each mp layer: 7.783514499664307
Norm after each mp layer: 34.55805206298828
Norm before input: 0.2552422881126404
Norm after input: 0.4532470107078552
Norm after each mp layer: 1.750766634941101
Norm after each mp layer: 7.783514499664307
Norm after each mp layer: 34.55805206298828
Norm before input: 0.2552422881126404
Norm after input: 0.45324718952178955
Norm after each mp layer: 1.7504760026931763
Norm after each mp layer: 7.78311014175415
Norm after each mp layer: 34.55710220336914
Norm before input: 0.2552422881126404
Norm after input: 0.45324718952178955
Norm after each mp layer: 1.7504760026931763
Norm after each mp layer: 7.78311014175415
Norm after each mp layer: 34.55710220336914
Norm before input: 0.2552422881126404
Norm after input: 0.4532473385334015
Norm after each mp layer: 1.7501846551895142
Norm after each mp layer: 7.782686710357666
Norm after each mp layer: 34.55606460571289
Norm before input: 0.2552422881126404
Norm after input: 0.4532473385334015
Norm after each mp layer: 1.7501846551895142
Norm after each mp layer: 7.782686710357666
Norm after each mp layer: 34.55606460571289
Norm before input: 0.2552422881126404
Norm after input: 0.45324742794036865
Norm after each mp layer: 1.749891757965088
Norm after each mp layer: 7.78224515914917
Norm after each mp layer: 34.55494689941406
Epoch: 420, Loss: 0.0219, Energy: 10488.3242, Train: 99.92%, Valid: 74.20%, Test: 76.00%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45324742794036865
Norm after each mp layer: 1.749891757965088
Norm after each mp layer: 7.78224515914917
Norm after each mp layer: 34.55494689941406
Norm before input: 0.2552422881126404
Norm after input: 0.45324742794036865
Norm after each mp layer: 1.7495975494384766
Norm after each mp layer: 7.781785011291504
Norm after each mp layer: 34.55374526977539
Norm before input: 0.2552422881126404
Norm after input: 0.45324742794036865
Norm after each mp layer: 1.7495975494384766
Norm after each mp layer: 7.781785011291504
Norm after each mp layer: 34.55374526977539
Norm before input: 0.2552422881126404
Norm after input: 0.45324742794036865
Norm after each mp layer: 1.7493023872375488
Norm after each mp layer: 7.781308174133301
Norm after each mp layer: 34.552459716796875
Norm before input: 0.2552422881126404
Norm after input: 0.45324742794036865
Norm after each mp layer: 1.7493023872375488
Norm after each mp layer: 7.781308174133301
Norm after each mp layer: 34.552459716796875
Norm before input: 0.2552422881126404
Norm after input: 0.45324742794036865
Norm after each mp layer: 1.7490060329437256
Norm after each mp layer: 7.780814170837402
Norm after each mp layer: 34.551090240478516
Norm before input: 0.2552422881126404
Norm after input: 0.45324742794036865
Norm after each mp layer: 1.7490060329437256
Norm after each mp layer: 7.780814170837402
Norm after each mp layer: 34.551090240478516
Norm before input: 0.2552422881126404
Norm after input: 0.45324742794036865
Norm after each mp layer: 1.748708963394165
Norm after each mp layer: 7.780304431915283
Norm after each mp layer: 34.54963684082031
Norm before input: 0.2552422881126404
Norm after input: 0.45324742794036865
Norm after each mp layer: 1.748708963394165
Norm after each mp layer: 7.780304431915283
Norm after each mp layer: 34.54963684082031
Norm before input: 0.2552422881126404
Norm after input: 0.4532473385334015
Norm after each mp layer: 1.7484103441238403
Norm after each mp layer: 7.779778003692627
Norm after each mp layer: 34.548091888427734
Epoch: 425, Loss: 0.0215, Energy: 10356.3096, Train: 99.92%, Valid: 74.20%, Test: 76.00%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532473385334015
Norm after each mp layer: 1.7484103441238403
Norm after each mp layer: 7.779778003692627
Norm after each mp layer: 34.548091888427734
Norm before input: 0.2552422881126404
Norm after input: 0.4532472491264343
Norm after each mp layer: 1.7481106519699097
Norm after each mp layer: 7.779234886169434
Norm after each mp layer: 34.54646301269531
Norm before input: 0.2552422881126404
Norm after input: 0.4532472491264343
Norm after each mp layer: 1.7481106519699097
Norm after each mp layer: 7.779234886169434
Norm after each mp layer: 34.54646301269531
Norm before input: 0.2552422881126404
Norm after input: 0.45324715971946716
Norm after each mp layer: 1.747809886932373
Norm after each mp layer: 7.778675556182861
Norm after each mp layer: 34.54475021362305
Norm before input: 0.2552422881126404
Norm after input: 0.45324715971946716
Norm after each mp layer: 1.747809886932373
Norm after each mp layer: 7.778675556182861
Norm after each mp layer: 34.54475021362305
Norm before input: 0.2552422881126404
Norm after input: 0.45324698090553284
Norm after each mp layer: 1.7475084066390991
Norm after each mp layer: 7.77810001373291
Norm after each mp layer: 34.54296112060547
Norm before input: 0.2552422881126404
Norm after input: 0.45324698090553284
Norm after each mp layer: 1.7475084066390991
Norm after each mp layer: 7.77810001373291
Norm after each mp layer: 34.54296112060547
Norm before input: 0.2552422881126404
Norm after input: 0.4532468020915985
Norm after each mp layer: 1.7472057342529297
Norm after each mp layer: 7.77750825881958
Norm after each mp layer: 34.541099548339844
Norm before input: 0.2552422881126404
Norm after input: 0.4532468020915985
Norm after each mp layer: 1.7472057342529297
Norm after each mp layer: 7.77750825881958
Norm after each mp layer: 34.541099548339844
Norm before input: 0.2552422881126404
Norm after input: 0.4532466232776642
Norm after each mp layer: 1.7469016313552856
Norm after each mp layer: 7.776899337768555
Norm after each mp layer: 34.539154052734375
Epoch: 430, Loss: 0.0210, Energy: 10226.8457, Train: 99.92%, Valid: 74.20%, Test: 76.00%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532466232776642
Norm after each mp layer: 1.7469016313552856
Norm after each mp layer: 7.776899337768555
Norm after each mp layer: 34.53915786743164
Norm before input: 0.2552422881126404
Norm after input: 0.45324644446372986
Norm after each mp layer: 1.7465968132019043
Norm after each mp layer: 7.776274681091309
Norm after each mp layer: 34.53713607788086
Norm before input: 0.2552422881126404
Norm after input: 0.45324644446372986
Norm after each mp layer: 1.7465968132019043
Norm after each mp layer: 7.776274681091309
Norm after each mp layer: 34.53713607788086
Norm before input: 0.2552422881126404
Norm after input: 0.453246146440506
Norm after each mp layer: 1.746290922164917
Norm after each mp layer: 7.775634288787842
Norm after each mp layer: 34.53504180908203
Norm before input: 0.2552422881126404
Norm after input: 0.453246146440506
Norm after each mp layer: 1.746290922164917
Norm after each mp layer: 7.775634288787842
Norm after each mp layer: 34.53504180908203
Norm before input: 0.2552422881126404
Norm after input: 0.45324593782424927
Norm after each mp layer: 1.7459840774536133
Norm after each mp layer: 7.7749786376953125
Norm after each mp layer: 34.53286361694336
Norm before input: 0.2552422881126404
Norm after input: 0.45324593782424927
Norm after each mp layer: 1.7459840774536133
Norm after each mp layer: 7.7749786376953125
Norm after each mp layer: 34.53286361694336
Norm before input: 0.2552422881126404
Norm after input: 0.45324569940567017
Norm after each mp layer: 1.7456761598587036
Norm after each mp layer: 7.7743072509765625
Norm after each mp layer: 34.530609130859375
Norm before input: 0.2552422881126404
Norm after input: 0.45324569940567017
Norm after each mp layer: 1.7456761598587036
Norm after each mp layer: 7.7743072509765625
Norm after each mp layer: 34.530609130859375
Norm before input: 0.2552422881126404
Norm after input: 0.45324549078941345
Norm after each mp layer: 1.7453672885894775
Norm after each mp layer: 7.77362060546875
Norm after each mp layer: 34.52827072143555
Epoch: 435, Loss: 0.0206, Energy: 10100.0059, Train: 99.92%, Valid: 74.20%, Test: 75.90%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45324549078941345
Norm after each mp layer: 1.7453672885894775
Norm after each mp layer: 7.77362060546875
Norm after each mp layer: 34.52827072143555
Norm before input: 0.2552422881126404
Norm after input: 0.4532451629638672
Norm after each mp layer: 1.745057463645935
Norm after each mp layer: 7.772919654846191
Norm after each mp layer: 34.5258674621582
Norm before input: 0.2552422881126404
Norm after input: 0.4532451629638672
Norm after each mp layer: 1.745057463645935
Norm after each mp layer: 7.772919654846191
Norm after each mp layer: 34.5258674621582
Norm before input: 0.2552422881126404
Norm after input: 0.4532448947429657
Norm after each mp layer: 1.7447468042373657
Norm after each mp layer: 7.7722039222717285
Norm after each mp layer: 34.52338409423828
Norm before input: 0.2552422881126404
Norm after input: 0.4532448947429657
Norm after each mp layer: 1.7447468042373657
Norm after each mp layer: 7.7722039222717285
Norm after each mp layer: 34.52338409423828
Norm before input: 0.2552422881126404
Norm after input: 0.453244686126709
Norm after each mp layer: 1.74443519115448
Norm after each mp layer: 7.771472454071045
Norm after each mp layer: 34.520835876464844
Norm before input: 0.2552422881126404
Norm after input: 0.453244686126709
Norm after each mp layer: 1.74443519115448
Norm after each mp layer: 7.771472454071045
Norm after each mp layer: 34.520835876464844
Norm before input: 0.2552422881126404
Norm after input: 0.4532443881034851
Norm after each mp layer: 1.744122862815857
Norm after each mp layer: 7.770728588104248
Norm after each mp layer: 34.518211364746094
Norm before input: 0.2552422881126404
Norm after input: 0.4532443881034851
Norm after each mp layer: 1.744122862815857
Norm after each mp layer: 7.770728588104248
Norm after each mp layer: 34.518211364746094
Norm before input: 0.2552422881126404
Norm after input: 0.4532441198825836
Norm after each mp layer: 1.743809700012207
Norm after each mp layer: 7.769970417022705
Norm after each mp layer: 34.51551818847656
Epoch: 440, Loss: 0.0203, Energy: 9975.6602, Train: 99.92%, Valid: 74.40%, Test: 75.90%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532441198825836
Norm after each mp layer: 1.743809700012207
Norm after each mp layer: 7.769970417022705
Norm after each mp layer: 34.51551818847656
Norm before input: 0.2552422881126404
Norm after input: 0.45324382185935974
Norm after each mp layer: 1.7434955835342407
Norm after each mp layer: 7.769197940826416
Norm after each mp layer: 34.51275634765625
Norm before input: 0.2552422881126404
Norm after input: 0.45324382185935974
Norm after each mp layer: 1.7434955835342407
Norm after each mp layer: 7.769197940826416
Norm after each mp layer: 34.51275634765625
Norm before input: 0.2552422881126404
Norm after input: 0.4532434940338135
Norm after each mp layer: 1.7431806325912476
Norm after each mp layer: 7.768411636352539
Norm after each mp layer: 34.50992202758789
Norm before input: 0.2552422881126404
Norm after input: 0.4532434940338135
Norm after each mp layer: 1.7431806325912476
Norm after each mp layer: 7.768411636352539
Norm after each mp layer: 34.50992202758789
Norm before input: 0.2552422881126404
Norm after input: 0.4532431960105896
Norm after each mp layer: 1.7428650856018066
Norm after each mp layer: 7.767611026763916
Norm after each mp layer: 34.507022857666016
Norm before input: 0.2552422881126404
Norm after input: 0.4532431960105896
Norm after each mp layer: 1.7428650856018066
Norm after each mp layer: 7.767611026763916
Norm after each mp layer: 34.507022857666016
Norm before input: 0.2552422881126404
Norm after input: 0.4532429277896881
Norm after each mp layer: 1.7425484657287598
Norm after each mp layer: 7.766798496246338
Norm after each mp layer: 34.504051208496094
Norm before input: 0.2552422881126404
Norm after input: 0.4532429277896881
Norm after each mp layer: 1.7425484657287598
Norm after each mp layer: 7.766798496246338
Norm after each mp layer: 34.504051208496094
Norm before input: 0.2552422881126404
Norm after input: 0.45324262976646423
Norm after each mp layer: 1.7422311305999756
Norm after each mp layer: 7.76597261428833
Norm after each mp layer: 34.501014709472656
Epoch: 445, Loss: 0.0199, Energy: 9853.8223, Train: 99.92%, Valid: 74.40%, Test: 75.80%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45324262976646423
Norm after each mp layer: 1.7422311305999756
Norm after each mp layer: 7.76597261428833
Norm after each mp layer: 34.501014709472656
Norm before input: 0.2552422881126404
Norm after input: 0.4532422721385956
Norm after each mp layer: 1.7419134378433228
Norm after each mp layer: 7.765132904052734
Norm after each mp layer: 34.4979133605957
Norm before input: 0.2552422881126404
Norm after input: 0.4532422721385956
Norm after each mp layer: 1.7419134378433228
Norm after each mp layer: 7.765132904052734
Norm after each mp layer: 34.49791717529297
Norm before input: 0.2552422881126404
Norm after input: 0.4532420039176941
Norm after each mp layer: 1.7415945529937744
Norm after each mp layer: 7.764280319213867
Norm after each mp layer: 34.494747161865234
Norm before input: 0.2552422881126404
Norm after input: 0.4532420039176941
Norm after each mp layer: 1.7415945529937744
Norm after each mp layer: 7.764281272888184
Norm after each mp layer: 34.494747161865234
Norm before input: 0.2552422881126404
Norm after input: 0.4532417356967926
Norm after each mp layer: 1.7412753105163574
Norm after each mp layer: 7.763416290283203
Norm after each mp layer: 34.491512298583984
Norm before input: 0.2552422881126404
Norm after input: 0.4532417356967926
Norm after each mp layer: 1.7412753105163574
Norm after each mp layer: 7.763416290283203
Norm after each mp layer: 34.491512298583984
Norm before input: 0.2552422881126404
Norm after input: 0.4532414674758911
Norm after each mp layer: 1.7409552335739136
Norm after each mp layer: 7.762540817260742
Norm after each mp layer: 34.48820877075195
Norm before input: 0.2552422881126404
Norm after input: 0.4532414674758911
Norm after each mp layer: 1.7409552335739136
Norm after each mp layer: 7.762540817260742
Norm after each mp layer: 34.48820495605469
Norm before input: 0.2552422881126404
Norm after input: 0.4532411992549896
Norm after each mp layer: 1.7406346797943115
Norm after each mp layer: 7.76165246963501
Norm after each mp layer: 34.48483657836914
Epoch: 450, Loss: 0.0195, Energy: 9734.4717, Train: 99.92%, Valid: 74.40%, Test: 75.70%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532411992549896
Norm after each mp layer: 1.7406346797943115
Norm after each mp layer: 7.76165246963501
Norm after each mp layer: 34.48483657836914
Norm before input: 0.2552422881126404
Norm after input: 0.45324093103408813
Norm after each mp layer: 1.7403132915496826
Norm after each mp layer: 7.760750770568848
Norm after each mp layer: 34.48140335083008
Norm before input: 0.2552422881126404
Norm after input: 0.45324093103408813
Norm after each mp layer: 1.7403132915496826
Norm after each mp layer: 7.760750770568848
Norm after each mp layer: 34.48140335083008
Norm before input: 0.2552422881126404
Norm after input: 0.45324069261550903
Norm after each mp layer: 1.739991307258606
Norm after each mp layer: 7.759839057922363
Norm after each mp layer: 34.4779052734375
Norm before input: 0.2552422881126404
Norm after input: 0.45324069261550903
Norm after each mp layer: 1.739991307258606
Norm after each mp layer: 7.759839057922363
Norm after each mp layer: 34.4779052734375
Norm before input: 0.2552422881126404
Norm after input: 0.4532404839992523
Norm after each mp layer: 1.739668846130371
Norm after each mp layer: 7.758914470672607
Norm after each mp layer: 34.47434997558594
Norm before input: 0.2552422881126404
Norm after input: 0.4532404839992523
Norm after each mp layer: 1.739668846130371
Norm after each mp layer: 7.758914470672607
Norm after each mp layer: 34.47434997558594
Norm before input: 0.2552422881126404
Norm after input: 0.45324021577835083
Norm after each mp layer: 1.7393454313278198
Norm after each mp layer: 7.757978439331055
Norm after each mp layer: 34.47072982788086
Norm before input: 0.2552422881126404
Norm after input: 0.45324021577835083
Norm after each mp layer: 1.7393454313278198
Norm after each mp layer: 7.757978439331055
Norm after each mp layer: 34.47072982788086
Norm before input: 0.2552422881126404
Norm after input: 0.45323994755744934
Norm after each mp layer: 1.7390215396881104
Norm after each mp layer: 7.757029056549072
Norm after each mp layer: 34.46704864501953
Epoch: 455, Loss: 0.0192, Energy: 9617.5566, Train: 99.92%, Valid: 74.60%, Test: 75.50%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45323994755744934
Norm after each mp layer: 1.7390215396881104
Norm after each mp layer: 7.757029056549072
Norm after each mp layer: 34.46704864501953
Norm before input: 0.2552422881126404
Norm after input: 0.45323970913887024
Norm after each mp layer: 1.7386970520019531
Norm after each mp layer: 7.756069183349609
Norm after each mp layer: 34.463294982910156
Norm before input: 0.2552422881126404
Norm after input: 0.45323970913887024
Norm after each mp layer: 1.7386970520019531
Norm after each mp layer: 7.756069183349609
Norm after each mp layer: 34.463294982910156
Norm before input: 0.2552422881126404
Norm after input: 0.4532395005226135
Norm after each mp layer: 1.738371729850769
Norm after each mp layer: 7.755095958709717
Norm after each mp layer: 34.459476470947266
Norm before input: 0.2552422881126404
Norm after input: 0.4532395005226135
Norm after each mp layer: 1.738371729850769
Norm after each mp layer: 7.755095958709717
Norm after each mp layer: 34.459476470947266
Norm before input: 0.2552422881126404
Norm after input: 0.4532392621040344
Norm after each mp layer: 1.7380459308624268
Norm after each mp layer: 7.754110813140869
Norm after each mp layer: 34.45560073852539
Norm before input: 0.2552422881126404
Norm after input: 0.4532392621040344
Norm after each mp layer: 1.7380459308624268
Norm after each mp layer: 7.754110813140869
Norm after each mp layer: 34.45560073852539
Norm before input: 0.2552422881126404
Norm after input: 0.45323893427848816
Norm after each mp layer: 1.737719178199768
Norm after each mp layer: 7.75311279296875
Norm after each mp layer: 34.45165252685547
Norm before input: 0.2552422881126404
Norm after input: 0.45323893427848816
Norm after each mp layer: 1.737719178199768
Norm after each mp layer: 7.75311279296875
Norm after each mp layer: 34.45165252685547
Norm before input: 0.2552422881126404
Norm after input: 0.45323875546455383
Norm after each mp layer: 1.7373919486999512
Norm after each mp layer: 7.752101421356201
Norm after each mp layer: 34.44763946533203
Epoch: 460, Loss: 0.0188, Energy: 9503.0381, Train: 99.92%, Valid: 74.40%, Test: 75.50%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45323875546455383
Norm after each mp layer: 1.7373919486999512
Norm after each mp layer: 7.752101421356201
Norm after each mp layer: 34.44763946533203
Norm before input: 0.2552422881126404
Norm after input: 0.45323848724365234
Norm after each mp layer: 1.7370637655258179
Norm after each mp layer: 7.751077175140381
Norm after each mp layer: 34.44356155395508
Norm before input: 0.2552422881126404
Norm after input: 0.45323848724365234
Norm after each mp layer: 1.7370637655258179
Norm after each mp layer: 7.751077175140381
Norm after each mp layer: 34.44356155395508
Norm before input: 0.2552422881126404
Norm after input: 0.45323821902275085
Norm after each mp layer: 1.7367347478866577
Norm after each mp layer: 7.750040054321289
Norm after each mp layer: 34.439414978027344
Norm before input: 0.2552422881126404
Norm after input: 0.45323821902275085
Norm after each mp layer: 1.7367347478866577
Norm after each mp layer: 7.750040054321289
Norm after each mp layer: 34.439414978027344
Norm before input: 0.2552422881126404
Norm after input: 0.45323795080184937
Norm after each mp layer: 1.7364048957824707
Norm after each mp layer: 7.748988151550293
Norm after each mp layer: 34.43519973754883
Norm before input: 0.2552422881126404
Norm after input: 0.45323795080184937
Norm after each mp layer: 1.7364048957824707
Norm after each mp layer: 7.748988151550293
Norm after each mp layer: 34.43519973754883
Norm before input: 0.2552422881126404
Norm after input: 0.45323774218559265
Norm after each mp layer: 1.736074447631836
Norm after each mp layer: 7.747921943664551
Norm after each mp layer: 34.43091583251953
Norm before input: 0.2552422881126404
Norm after input: 0.45323774218559265
Norm after each mp layer: 1.736074447631836
Norm after each mp layer: 7.747921943664551
Norm after each mp layer: 34.43091583251953
Norm before input: 0.2552422881126404
Norm after input: 0.45323747396469116
Norm after each mp layer: 1.7357428073883057
Norm after each mp layer: 7.746842384338379
Norm after each mp layer: 34.42655563354492
Epoch: 465, Loss: 0.0185, Energy: 9390.7852, Train: 99.92%, Valid: 74.40%, Test: 75.40%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45323747396469116
Norm after each mp layer: 1.7357428073883057
Norm after each mp layer: 7.746842384338379
Norm after each mp layer: 34.42655563354492
Norm before input: 0.2552422881126404
Norm after input: 0.45323723554611206
Norm after each mp layer: 1.7354100942611694
Norm after each mp layer: 7.745746612548828
Norm after each mp layer: 34.42212677001953
Norm before input: 0.2552422881126404
Norm after input: 0.45323723554611206
Norm after each mp layer: 1.7354100942611694
Norm after each mp layer: 7.745746612548828
Norm after each mp layer: 34.42212677001953
Norm before input: 0.2552422881126404
Norm after input: 0.4532369077205658
Norm after each mp layer: 1.7350767850875854
Norm after each mp layer: 7.744636058807373
Norm after each mp layer: 34.41762161254883
Norm before input: 0.2552422881126404
Norm after input: 0.4532369077205658
Norm after each mp layer: 1.7350767850875854
Norm after each mp layer: 7.744636058807373
Norm after each mp layer: 34.41762161254883
Norm before input: 0.2552422881126404
Norm after input: 0.4532366991043091
Norm after each mp layer: 1.734742283821106
Norm after each mp layer: 7.7435102462768555
Norm after each mp layer: 34.41303634643555
Norm before input: 0.2552422881126404
Norm after input: 0.4532366991043091
Norm after each mp layer: 1.734742283821106
Norm after each mp layer: 7.7435102462768555
Norm after each mp layer: 34.41303634643555
Norm before input: 0.2552422881126404
Norm after input: 0.4532364308834076
Norm after each mp layer: 1.7344067096710205
Norm after each mp layer: 7.742368221282959
Norm after each mp layer: 34.408382415771484
Norm before input: 0.2552422881126404
Norm after input: 0.4532364308834076
Norm after each mp layer: 1.7344067096710205
Norm after each mp layer: 7.742368221282959
Norm after each mp layer: 34.408382415771484
Norm before input: 0.2552422881126404
Norm after input: 0.4532361626625061
Norm after each mp layer: 1.7340704202651978
Norm after each mp layer: 7.7412109375
Norm after each mp layer: 34.403648376464844
Epoch: 470, Loss: 0.0182, Energy: 9280.5918, Train: 99.92%, Valid: 74.40%, Test: 75.50%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532361626625061
Norm after each mp layer: 1.7340704202651978
Norm after each mp layer: 7.7412109375
Norm after each mp layer: 34.403648376464844
Norm before input: 0.2552422881126404
Norm after input: 0.4532358944416046
Norm after each mp layer: 1.73373281955719
Norm after each mp layer: 7.740036487579346
Norm after each mp layer: 34.39883041381836
Norm before input: 0.2552422881126404
Norm after input: 0.4532358944416046
Norm after each mp layer: 1.73373281955719
Norm after each mp layer: 7.740036487579346
Norm after each mp layer: 34.39883041381836
Norm before input: 0.2552422881126404
Norm after input: 0.4532356560230255
Norm after each mp layer: 1.7333943843841553
Norm after each mp layer: 7.738846778869629
Norm after each mp layer: 34.39393997192383
Norm before input: 0.2552422881126404
Norm after input: 0.4532356560230255
Norm after each mp layer: 1.7333943843841553
Norm after each mp layer: 7.738846778869629
Norm after each mp layer: 34.39393997192383
Norm before input: 0.2552422881126404
Norm after input: 0.4532354176044464
Norm after each mp layer: 1.733054757118225
Norm after each mp layer: 7.737640857696533
Norm after each mp layer: 34.38896942138672
Norm before input: 0.2552422881126404
Norm after input: 0.4532354176044464
Norm after each mp layer: 1.733054757118225
Norm after each mp layer: 7.737640857696533
Norm after each mp layer: 34.38896942138672
Norm before input: 0.2552422881126404
Norm after input: 0.4532352089881897
Norm after each mp layer: 1.7327139377593994
Norm after each mp layer: 7.7364182472229
Norm after each mp layer: 34.38391876220703
Norm before input: 0.2552422881126404
Norm after input: 0.4532352089881897
Norm after each mp layer: 1.7327139377593994
Norm after each mp layer: 7.7364182472229
Norm after each mp layer: 34.38391876220703
Norm before input: 0.2552422881126404
Norm after input: 0.4532349705696106
Norm after each mp layer: 1.7323724031448364
Norm after each mp layer: 7.735179901123047
Norm after each mp layer: 34.37879180908203
Epoch: 475, Loss: 0.0178, Energy: 9172.2598, Train: 99.92%, Valid: 74.40%, Test: 75.40%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532349705696106
Norm after each mp layer: 1.7323724031448364
Norm after each mp layer: 7.735179901123047
Norm after each mp layer: 34.37879180908203
Norm before input: 0.2552422881126404
Norm after input: 0.45323479175567627
Norm after each mp layer: 1.732029676437378
Norm after each mp layer: 7.733924865722656
Norm after each mp layer: 34.37358474731445
Norm before input: 0.2552422881126404
Norm after input: 0.45323479175567627
Norm after each mp layer: 1.732029676437378
Norm after each mp layer: 7.733924865722656
Norm after each mp layer: 34.37358474731445
Norm before input: 0.2552422881126404
Norm after input: 0.45323461294174194
Norm after each mp layer: 1.731685996055603
Norm after each mp layer: 7.732654094696045
Norm after each mp layer: 34.36830139160156
Norm before input: 0.2552422881126404
Norm after input: 0.45323461294174194
Norm after each mp layer: 1.731685996055603
Norm after each mp layer: 7.732654094696045
Norm after each mp layer: 34.36830139160156
Norm before input: 0.2552422881126404
Norm after input: 0.4532344341278076
Norm after each mp layer: 1.7313413619995117
Norm after each mp layer: 7.731367111206055
Norm after each mp layer: 34.36294174194336
Norm before input: 0.2552422881126404
Norm after input: 0.4532344341278076
Norm after each mp layer: 1.7313413619995117
Norm after each mp layer: 7.731367111206055
Norm after each mp layer: 34.36294174194336
Norm before input: 0.2552422881126404
Norm after input: 0.4532342553138733
Norm after each mp layer: 1.730995535850525
Norm after each mp layer: 7.730064868927002
Norm after each mp layer: 34.35749816894531
Norm before input: 0.2552422881126404
Norm after input: 0.4532342553138733
Norm after each mp layer: 1.730995535850525
Norm after each mp layer: 7.730064868927002
Norm after each mp layer: 34.35749816894531
Norm before input: 0.2552422881126404
Norm after input: 0.45323416590690613
Norm after each mp layer: 1.7306489944458008
Norm after each mp layer: 7.7287468910217285
Norm after each mp layer: 34.35198211669922
Epoch: 480, Loss: 0.0175, Energy: 9065.6992, Train: 99.92%, Valid: 74.40%, Test: 75.40%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45323416590690613
Norm after each mp layer: 1.7306489944458008
Norm after each mp layer: 7.7287468910217285
Norm after each mp layer: 34.35198211669922
Norm before input: 0.2552422881126404
Norm after input: 0.45323407649993896
Norm after each mp layer: 1.7303013801574707
Norm after each mp layer: 7.727414131164551
Norm after each mp layer: 34.34638977050781
Norm before input: 0.2552422881126404
Norm after input: 0.45323407649993896
Norm after each mp layer: 1.7303013801574707
Norm after each mp layer: 7.727414131164551
Norm after each mp layer: 34.34638977050781
Norm before input: 0.2552422881126404
Norm after input: 0.4532339870929718
Norm after each mp layer: 1.7299526929855347
Norm after each mp layer: 7.726065635681152
Norm after each mp layer: 34.340721130371094
Norm before input: 0.2552422881126404
Norm after input: 0.4532339870929718
Norm after each mp layer: 1.7299526929855347
Norm after each mp layer: 7.726065635681152
Norm after each mp layer: 34.340721130371094
Norm before input: 0.2552422881126404
Norm after input: 0.4532339870929718
Norm after each mp layer: 1.7296031713485718
Norm after each mp layer: 7.724702835083008
Norm after each mp layer: 34.33497619628906
Norm before input: 0.2552422881126404
Norm after input: 0.4532339870929718
Norm after each mp layer: 1.7296031713485718
Norm after each mp layer: 7.724702835083008
Norm after each mp layer: 34.33498001098633
Norm before input: 0.2552422881126404
Norm after input: 0.4532339870929718
Norm after each mp layer: 1.7292529344558716
Norm after each mp layer: 7.723324775695801
Norm after each mp layer: 34.32916259765625
Norm before input: 0.2552422881126404
Norm after input: 0.4532339870929718
Norm after each mp layer: 1.7292529344558716
Norm after each mp layer: 7.723324775695801
Norm after each mp layer: 34.32916259765625
Norm before input: 0.2552422881126404
Norm after input: 0.4532339870929718
Norm after each mp layer: 1.7289015054702759
Norm after each mp layer: 7.721932888031006
Norm after each mp layer: 34.323272705078125
Epoch: 485, Loss: 0.0172, Energy: 8960.8799, Train: 99.92%, Valid: 74.40%, Test: 75.40%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532339870929718
Norm after each mp layer: 1.7289015054702759
Norm after each mp layer: 7.721932888031006
Norm after each mp layer: 34.323272705078125
Norm before input: 0.2552422881126404
Norm after input: 0.45323407649993896
Norm after each mp layer: 1.7285494804382324
Norm after each mp layer: 7.720526218414307
Norm after each mp layer: 34.31731033325195
Norm before input: 0.2552422881126404
Norm after input: 0.45323407649993896
Norm after each mp layer: 1.7285494804382324
Norm after each mp layer: 7.720526218414307
Norm after each mp layer: 34.31731033325195
Norm before input: 0.2552422881126404
Norm after input: 0.45323416590690613
Norm after each mp layer: 1.7281968593597412
Norm after each mp layer: 7.719106197357178
Norm after each mp layer: 34.311275482177734
Norm before input: 0.2552422881126404
Norm after input: 0.45323416590690613
Norm after each mp layer: 1.7281968593597412
Norm after each mp layer: 7.719106197357178
Norm after each mp layer: 34.311275482177734
Norm before input: 0.2552422881126404
Norm after input: 0.4532342553138733
Norm after each mp layer: 1.7278430461883545
Norm after each mp layer: 7.717672348022461
Norm after each mp layer: 34.30517578125
Norm before input: 0.2552422881126404
Norm after input: 0.4532342553138733
Norm after each mp layer: 1.7278430461883545
Norm after each mp layer: 7.717672348022461
Norm after each mp layer: 34.30517578125
Norm before input: 0.2552422881126404
Norm after input: 0.4532344341278076
Norm after each mp layer: 1.7274885177612305
Norm after each mp layer: 7.716225624084473
Norm after each mp layer: 34.29899978637695
Norm before input: 0.2552422881126404
Norm after input: 0.4532344341278076
Norm after each mp layer: 1.7274885177612305
Norm after each mp layer: 7.716225624084473
Norm after each mp layer: 34.29899978637695
Norm before input: 0.2552422881126404
Norm after input: 0.4532346725463867
Norm after each mp layer: 1.7271332740783691
Norm after each mp layer: 7.714765548706055
Norm after each mp layer: 34.292762756347656
Epoch: 490, Loss: 0.0170, Energy: 8857.8125, Train: 99.92%, Valid: 74.40%, Test: 75.50%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.4532346725463867
Norm after each mp layer: 1.7271332740783691
Norm after each mp layer: 7.714765548706055
Norm after each mp layer: 34.292762756347656
Norm before input: 0.2552422881126404
Norm after input: 0.45323488116264343
Norm after each mp layer: 1.72677743434906
Norm after each mp layer: 7.713293075561523
Norm after each mp layer: 34.28644943237305
Norm before input: 0.2552422881126404
Norm after input: 0.45323488116264343
Norm after each mp layer: 1.72677743434906
Norm after each mp layer: 7.713293075561523
Norm after each mp layer: 34.28644943237305
Norm before input: 0.2552422881126404
Norm after input: 0.4532351493835449
Norm after each mp layer: 1.7264207601547241
Norm after each mp layer: 7.7118072509765625
Norm after each mp layer: 34.28007888793945
Norm before input: 0.2552422881126404
Norm after input: 0.4532351493835449
Norm after each mp layer: 1.7264207601547241
Norm after each mp layer: 7.7118072509765625
Norm after each mp layer: 34.28007888793945
Norm before input: 0.2552422881126404
Norm after input: 0.4532355070114136
Norm after each mp layer: 1.7260634899139404
Norm after each mp layer: 7.710309982299805
Norm after each mp layer: 34.27363204956055
Norm before input: 0.2552422881126404
Norm after input: 0.4532355070114136
Norm after each mp layer: 1.7260634899139404
Norm after each mp layer: 7.710309982299805
Norm after each mp layer: 34.27363204956055
Norm before input: 0.2552422881126404
Norm after input: 0.45323583483695984
Norm after each mp layer: 1.725705623626709
Norm after each mp layer: 7.708800792694092
Norm after each mp layer: 34.26713180541992
Norm before input: 0.2552422881126404
Norm after input: 0.45323583483695984
Norm after each mp layer: 1.725705623626709
Norm after each mp layer: 7.708800792694092
Norm after each mp layer: 34.26713180541992
Norm before input: 0.2552422881126404
Norm after input: 0.45323625206947327
Norm after each mp layer: 1.7253468036651611
Norm after each mp layer: 7.707279205322266
Norm after each mp layer: 34.26055908203125
Epoch: 495, Loss: 0.0167, Energy: 8756.4795, Train: 99.92%, Valid: 74.20%, Test: 75.50%, Best Valid: 83.00%, Best Test: 81.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45323625206947327
Norm after each mp layer: 1.7253468036651611
Norm after each mp layer: 7.707279205322266
Norm after each mp layer: 34.26055908203125
Norm before input: 0.2552422881126404
Norm after input: 0.4532366394996643
Norm after each mp layer: 1.7249876260757446
Norm after each mp layer: 7.705747127532959
Norm after each mp layer: 34.25392532348633
Norm before input: 0.2552422881126404
Norm after input: 0.4532366394996643
Norm after each mp layer: 1.7249876260757446
Norm after each mp layer: 7.705747127532959
Norm after each mp layer: 34.25392532348633
Norm before input: 0.2552422881126404
Norm after input: 0.4532371461391449
Norm after each mp layer: 1.7246277332305908
Norm after each mp layer: 7.704202175140381
Norm after each mp layer: 34.24723434448242
Norm before input: 0.2552422881126404
Norm after input: 0.4532371461391449
Norm after each mp layer: 1.7246277332305908
Norm after each mp layer: 7.704202175140381
Norm after each mp layer: 34.24723434448242
Norm before input: 0.2552422881126404
Norm after input: 0.4532375931739807
Norm after each mp layer: 1.7242674827575684
Norm after each mp layer: 7.7026472091674805
Norm after each mp layer: 34.240478515625
Norm before input: 0.2552422881126404
Norm after input: 0.4532375931739807
Norm after each mp layer: 1.7242674827575684
Norm after each mp layer: 7.7026472091674805
Norm after each mp layer: 34.240478515625
Norm before input: 0.2552422881126404
Norm after input: 0.4532381296157837
Norm after each mp layer: 1.7239065170288086
Norm after each mp layer: 7.701080799102783
Norm after each mp layer: 34.23366165161133
train_accuracy_list: [0.28228476821192056, 0.28228476821192056, 0.16225165562913907, 0.16225165562913907, 0.28228476821192056, 0.16225165562913907, 0.11423841059602649, 0.09850993377483444, 0.28228476821192056, 0.20695364238410596, 0.25662251655629137, 0.2259933774834437, 0.11506622516556292, 0.11506622516556292, 0.28642384105960267, 0.2897350993377483, 0.2847682119205298, 0.28311258278145696, 0.31374172185430466, 0.28228476821192056, 0.2847682119205298, 0.3294701986754967, 0.28807947019867547, 0.32367549668874174, 0.40480132450331124, 0.43211920529801323, 0.4644039735099338, 0.4817880794701987, 0.4900662251655629, 0.5049668874172185, 0.5099337748344371, 0.5074503311258278, 0.5372516556291391, 0.543046357615894, 0.543046357615894, 0.5811258278145696, 0.5993377483443708, 0.6043046357615894, 0.5951986754966887, 0.5968543046357616, 0.6316225165562914, 0.6589403973509934, 0.6721854304635762, 0.6920529801324503, 0.7185430463576159, 0.75, 0.75, 0.75, 0.7690397350993378, 0.7682119205298014, 0.7847682119205298, 0.7988410596026491, 0.7831125827814569, 0.793046357615894, 0.8129139072847682, 0.8153973509933775, 0.8360927152317881, 0.8551324503311258, 0.8551324503311258, 0.8576158940397351, 0.8683774834437086, 0.8716887417218543, 0.8791390728476821, 0.8841059602649006, 0.8807947019867549, 0.8899006622516556, 0.8940397350993378, 0.8932119205298014, 0.9072847682119205, 0.9064569536423841, 0.9072847682119205, 0.9130794701986755, 0.9147350993377483, 0.9172185430463576, 0.9221854304635762, 0.9230132450331126, 0.9271523178807947, 0.9263245033112583, 0.9321192052980133, 0.9346026490066225, 0.9362582781456954, 0.9420529801324503, 0.9370860927152318, 0.9412251655629139, 0.9403973509933775, 0.9445364238410596, 0.945364238410596, 0.9437086092715232, 0.9470198675496688, 0.9370860927152318, 0.9503311258278145, 0.9461920529801324, 0.9544701986754967, 0.9544701986754967, 0.9503311258278145, 0.9552980132450332, 0.9577814569536424, 0.9552980132450332, 0.9594370860927153, 0.9610927152317881, 0.9627483443708609, 0.9627483443708609, 0.9644039735099338, 0.9652317880794702, 0.9668874172185431, 0.9627483443708609, 0.9594370860927153, 0.9635761589403974, 0.9710264900662252, 0.9652317880794702, 0.9710264900662252, 0.972682119205298, 0.9644039735099338, 0.9685430463576159, 0.9718543046357616, 0.9677152317880795, 0.9693708609271523, 0.9710264900662252, 0.9677152317880795, 0.9743377483443708, 0.9710264900662252, 0.9685430463576159, 0.9594370860927153, 0.9610927152317881, 0.9701986754966887, 0.9759933774834437, 0.9668874172185431, 0.9743377483443708, 0.9751655629139073, 0.9743377483443708, 0.9718543046357616, 0.9735099337748344, 0.9759933774834437, 0.972682119205298, 0.9784768211920529, 0.9793046357615894, 0.9735099337748344, 0.9776490066225165, 0.9801324503311258, 0.9801324503311258, 0.9743377483443708, 0.9784768211920529, 0.9784768211920529, 0.9768211920529801, 0.9793046357615894, 0.9801324503311258, 0.9759933774834437, 0.9826158940397351, 0.9834437086092715, 0.9784768211920529, 0.9809602649006622, 0.9834437086092715, 0.9801324503311258, 0.9817880794701986, 0.9826158940397351, 0.9826158940397351, 0.984271523178808, 0.9834437086092715, 0.9834437086092715, 0.9850993377483444, 0.984271523178808, 0.9834437086092715, 0.984271523178808, 0.9859271523178808, 0.9867549668874173, 0.984271523178808, 0.9850993377483444, 0.9900662251655629, 0.9900662251655629, 0.9850993377483444, 0.9900662251655629, 0.9892384105960265, 0.9875827814569537, 0.9892384105960265, 0.9900662251655629, 0.9892384105960265, 0.9908940397350994, 0.9925496688741722, 0.9900662251655629, 0.9917218543046358, 0.9925496688741722, 0.9884105960264901, 0.9925496688741722, 0.9917218543046358, 0.9759933774834437, 0.9644039735099338, 0.6829470198675497, 0.8286423841059603, 0.7566225165562914, 0.9039735099337748, 0.7475165562913907, 0.8253311258278145, 0.9403973509933775, 0.9188741721854304, 0.8170529801324503, 0.8394039735099338, 0.9246688741721855, 0.9354304635761589, 0.9139072847682119, 0.9197019867549668, 0.9197019867549668, 0.9312913907284768, 0.9362582781456954, 0.9511589403973509, 0.956953642384106, 0.9478476821192053, 0.945364238410596, 0.9486754966887417, 0.9577814569536424, 0.9635761589403974, 0.9644039735099338, 0.9685430463576159, 0.9677152317880795, 0.9660596026490066, 0.9668874172185431, 0.9701986754966887, 0.9735099337748344, 0.9768211920529801, 0.9751655629139073, 0.9776490066225165, 0.9759933774834437, 0.9768211920529801, 0.9759933774834437, 0.9768211920529801, 0.9776490066225165, 0.9809602649006622, 0.9826158940397351, 0.9859271523178808, 0.984271523178808, 0.9867549668874173, 0.9867549668874173, 0.9875827814569537, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9900662251655629, 0.9900662251655629, 0.9900662251655629, 0.9900662251655629, 0.9900662251655629, 0.9908940397350994, 0.9908940397350994, 0.9925496688741722, 0.9925496688741722, 0.9933774834437086, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9950331125827815, 0.9950331125827815, 0.9950331125827815, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636, 0.9991721854304636]
valid_accuracy_list: [0.316, 0.316, 0.156, 0.162, 0.316, 0.156, 0.114, 0.086, 0.316, 0.25, 0.278, 0.264, 0.116, 0.116, 0.322, 0.328, 0.308, 0.312, 0.348, 0.316, 0.316, 0.374, 0.318, 0.334, 0.444, 0.45, 0.476, 0.5, 0.504, 0.512, 0.52, 0.514, 0.54, 0.538, 0.532, 0.568, 0.574, 0.576, 0.588, 0.596, 0.618, 0.63, 0.642, 0.644, 0.684, 0.724, 0.712, 0.702, 0.704, 0.708, 0.72, 0.748, 0.732, 0.742, 0.756, 0.746, 0.768, 0.794, 0.792, 0.794, 0.796, 0.792, 0.794, 0.8, 0.812, 0.81, 0.808, 0.806, 0.814, 0.806, 0.804, 0.804, 0.81, 0.804, 0.804, 0.8, 0.788, 0.8, 0.79, 0.8, 0.794, 0.796, 0.802, 0.796, 0.8, 0.802, 0.794, 0.796, 0.788, 0.806, 0.784, 0.794, 0.792, 0.786, 0.794, 0.786, 0.786, 0.792, 0.786, 0.782, 0.788, 0.786, 0.786, 0.782, 0.78, 0.784, 0.768, 0.776, 0.78, 0.776, 0.774, 0.782, 0.768, 0.778, 0.784, 0.766, 0.784, 0.768, 0.764, 0.78, 0.762, 0.758, 0.764, 0.748, 0.766, 0.764, 0.752, 0.764, 0.764, 0.754, 0.752, 0.764, 0.754, 0.752, 0.756, 0.754, 0.746, 0.748, 0.756, 0.752, 0.75, 0.752, 0.752, 0.748, 0.752, 0.744, 0.742, 0.752, 0.75, 0.74, 0.742, 0.748, 0.744, 0.742, 0.744, 0.742, 0.742, 0.742, 0.74, 0.742, 0.742, 0.742, 0.742, 0.748, 0.742, 0.74, 0.744, 0.74, 0.736, 0.738, 0.738, 0.734, 0.738, 0.736, 0.734, 0.734, 0.73, 0.732, 0.734, 0.734, 0.73, 0.736, 0.736, 0.728, 0.722, 0.706, 0.562, 0.664, 0.59, 0.736, 0.62, 0.65, 0.76, 0.752, 0.672, 0.69, 0.77, 0.768, 0.742, 0.766, 0.776, 0.786, 0.8, 0.808, 0.82, 0.808, 0.79, 0.794, 0.792, 0.796, 0.804, 0.816, 0.818, 0.82, 0.816, 0.824, 0.828, 0.83, 0.822, 0.822, 0.822, 0.822, 0.822, 0.822, 0.816, 0.818, 0.816, 0.81, 0.808, 0.808, 0.808, 0.81, 0.81, 0.81, 0.812, 0.808, 0.806, 0.806, 0.802, 0.804, 0.804, 0.804, 0.802, 0.802, 0.804, 0.804, 0.802, 0.802, 0.802, 0.8, 0.802, 0.802, 0.802, 0.802, 0.802, 0.8, 0.8, 0.794, 0.794, 0.794, 0.794, 0.794, 0.79, 0.79, 0.792, 0.792, 0.792, 0.79, 0.788, 0.788, 0.788, 0.786, 0.786, 0.784, 0.782, 0.782, 0.782, 0.78, 0.78, 0.78, 0.78, 0.78, 0.78, 0.782, 0.782, 0.782, 0.782, 0.78, 0.78, 0.778, 0.778, 0.778, 0.778, 0.778, 0.78, 0.78, 0.78, 0.776, 0.776, 0.776, 0.776, 0.776, 0.776, 0.776, 0.776, 0.776, 0.776, 0.776, 0.776, 0.778, 0.778, 0.778, 0.778, 0.778, 0.778, 0.778, 0.774, 0.772, 0.772, 0.772, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.768, 0.77, 0.77, 0.77, 0.772, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.77, 0.768, 0.768, 0.768, 0.768, 0.766, 0.766, 0.764, 0.764, 0.764, 0.764, 0.762, 0.762, 0.762, 0.76, 0.76, 0.76, 0.756, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.75, 0.75, 0.75, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.746, 0.746, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.746, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742]
test_accuracy_list: [0.319, 0.319, 0.144, 0.149, 0.319, 0.144, 0.104, 0.116, 0.319, 0.238, 0.288, 0.26, 0.104, 0.104, 0.321, 0.33, 0.305, 0.31, 0.354, 0.319, 0.32, 0.352, 0.31, 0.336, 0.421, 0.44, 0.462, 0.482, 0.489, 0.507, 0.507, 0.506, 0.525, 0.538, 0.535, 0.558, 0.591, 0.598, 0.599, 0.593, 0.615, 0.638, 0.649, 0.661, 0.683, 0.705, 0.697, 0.696, 0.712, 0.723, 0.73, 0.74, 0.716, 0.733, 0.75, 0.745, 0.761, 0.779, 0.782, 0.782, 0.776, 0.773, 0.78, 0.782, 0.784, 0.79, 0.79, 0.787, 0.794, 0.788, 0.784, 0.789, 0.787, 0.784, 0.785, 0.779, 0.775, 0.776, 0.778, 0.781, 0.779, 0.774, 0.781, 0.77, 0.778, 0.775, 0.77, 0.779, 0.768, 0.782, 0.774, 0.781, 0.776, 0.771, 0.776, 0.774, 0.77, 0.769, 0.767, 0.763, 0.765, 0.758, 0.759, 0.757, 0.755, 0.761, 0.75, 0.762, 0.758, 0.755, 0.755, 0.754, 0.753, 0.757, 0.752, 0.751, 0.753, 0.753, 0.751, 0.749, 0.751, 0.753, 0.748, 0.746, 0.753, 0.753, 0.746, 0.753, 0.747, 0.752, 0.749, 0.75, 0.755, 0.749, 0.751, 0.748, 0.748, 0.745, 0.74, 0.744, 0.741, 0.741, 0.747, 0.738, 0.744, 0.742, 0.735, 0.741, 0.743, 0.736, 0.739, 0.738, 0.74, 0.74, 0.735, 0.732, 0.733, 0.73, 0.729, 0.726, 0.729, 0.73, 0.724, 0.727, 0.728, 0.725, 0.729, 0.725, 0.717, 0.725, 0.723, 0.716, 0.722, 0.719, 0.715, 0.718, 0.714, 0.716, 0.719, 0.717, 0.708, 0.715, 0.714, 0.703, 0.717, 0.69, 0.565, 0.626, 0.587, 0.72, 0.622, 0.677, 0.752, 0.735, 0.682, 0.687, 0.748, 0.76, 0.735, 0.762, 0.767, 0.779, 0.78, 0.783, 0.795, 0.789, 0.787, 0.79, 0.792, 0.793, 0.798, 0.802, 0.802, 0.806, 0.817, 0.816, 0.814, 0.814, 0.812, 0.812, 0.809, 0.807, 0.803, 0.801, 0.802, 0.8, 0.8, 0.8, 0.804, 0.805, 0.803, 0.803, 0.801, 0.801, 0.802, 0.798, 0.799, 0.797, 0.798, 0.797, 0.797, 0.797, 0.794, 0.794, 0.794, 0.792, 0.793, 0.793, 0.793, 0.793, 0.792, 0.792, 0.792, 0.79, 0.79, 0.79, 0.791, 0.791, 0.79, 0.788, 0.787, 0.787, 0.786, 0.788, 0.788, 0.787, 0.788, 0.787, 0.787, 0.785, 0.785, 0.786, 0.786, 0.786, 0.786, 0.786, 0.785, 0.783, 0.78, 0.78, 0.779, 0.778, 0.777, 0.777, 0.777, 0.777, 0.777, 0.775, 0.776, 0.776, 0.775, 0.775, 0.774, 0.774, 0.773, 0.773, 0.774, 0.773, 0.773, 0.773, 0.773, 0.773, 0.774, 0.774, 0.774, 0.774, 0.776, 0.776, 0.776, 0.776, 0.776, 0.776, 0.776, 0.777, 0.777, 0.777, 0.777, 0.777, 0.777, 0.776, 0.777, 0.776, 0.776, 0.775, 0.775, 0.775, 0.774, 0.774, 0.774, 0.773, 0.773, 0.773, 0.773, 0.771, 0.771, 0.771, 0.771, 0.771, 0.771, 0.77, 0.77, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.769, 0.77, 0.77, 0.771, 0.771, 0.771, 0.771, 0.771, 0.771, 0.771, 0.77, 0.77, 0.77, 0.769, 0.769, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.767, 0.767, 0.767, 0.767, 0.767, 0.767, 0.767, 0.767, 0.767, 0.768, 0.768, 0.768, 0.767, 0.767, 0.766, 0.766, 0.765, 0.765, 0.764, 0.765, 0.765, 0.765, 0.765, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.763, 0.764, 0.763, 0.763, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.76, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.757, 0.757, 0.757, 0.757, 0.756, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.755, 0.755, 0.755, 0.755, 0.755, 0.754, 0.753, 0.753, 0.753, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755]
best validation: 0.83
best test: 0.817
Experiment run
dataset: cora
num_mp_layers: 3
mp_hidden_dim: 4000
optimizer_lr: 0.001
loss_func: CrossEntropyLoss
total_epoch: 500
energy_lambda: 1e-05
Norm before input: 0.2552422881126404
Norm after input: 0.9947524070739746
Norm after each mp layer: 0.6566787362098694
Norm after each mp layer: 0.6222918033599854
Norm after each mp layer: 0.6184736490249634
Norm before input: 0.2552422881126404
Norm after input: 1.001779317855835
Norm after each mp layer: 1.8241080045700073
Norm after each mp layer: 1.6365679502487183
Norm after each mp layer: 1.6241487264633179
Norm before input: 0.2552422881126404
Norm after input: 1.001779317855835
Norm after each mp layer: 1.8241080045700073
Norm after each mp layer: 1.6365679502487183
Norm after each mp layer: 1.6241487264633179
Norm before input: 0.2552422881126404
Norm after input: 1.053762435913086
Norm after each mp layer: 2.7993175983428955
Norm after each mp layer: 5.263521194458008
Norm after each mp layer: 14.253692626953125
Norm before input: 0.2552422881126404
Norm after input: 1.053762435913086
Norm after each mp layer: 2.7993175983428955
Norm after each mp layer: 5.263521194458008
Norm after each mp layer: 14.253692626953125
Norm before input: 0.2552422881126404
Norm after input: 1.1226898431777954
Norm after each mp layer: 4.213434219360352
Norm after each mp layer: 11.945268630981445
Norm after each mp layer: 43.04161834716797
Norm before input: 0.2552422881126404
Norm after input: 1.1226898431777954
Norm after each mp layer: 4.213434219360352
Norm after each mp layer: 11.945268630981445
Norm after each mp layer: 43.04161834716797
Norm before input: 0.2552422881126404
Norm after input: 1.0722075700759888
Norm after each mp layer: 3.558468818664551
Norm after each mp layer: 7.827006816864014
Norm after each mp layer: 26.253883361816406
Norm before input: 0.2552422881126404
Norm after input: 1.0722075700759888
Norm after each mp layer: 3.558468818664551
Norm after each mp layer: 7.827006816864014
Norm after each mp layer: 26.253883361816406
Norm before input: 0.2552422881126404
Norm after input: 1.0157253742218018
Norm after each mp layer: 3.0826709270477295
Norm after each mp layer: 5.1834845542907715
Norm after each mp layer: 9.415900230407715
Epoch: 05, Loss: 3.7400, Energy: 99406.2812, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 1.0157253742218018
Norm after each mp layer: 3.0826709270477295
Norm after each mp layer: 5.1834845542907715
Norm after each mp layer: 9.415900230407715
Norm before input: 0.2552422881126404
Norm after input: 0.9813019633293152
Norm after each mp layer: 2.846177577972412
Norm after each mp layer: 5.970116138458252
Norm after each mp layer: 15.441522598266602
Norm before input: 0.2552422881126404
Norm after input: 0.9813019633293152
Norm after each mp layer: 2.846177577972412
Norm after each mp layer: 5.970116138458252
Norm after each mp layer: 15.441522598266602
Norm before input: 0.2552422881126404
Norm after input: 0.9555144309997559
Norm after each mp layer: 2.766556739807129
Norm after each mp layer: 7.137587070465088
Norm after each mp layer: 22.432130813598633
Norm before input: 0.2552422881126404
Norm after input: 0.9555144309997559
Norm after each mp layer: 2.766556739807129
Norm after each mp layer: 7.137587070465088
Norm after each mp layer: 22.432130813598633
Norm before input: 0.2552422881126404
Norm after input: 0.9241836071014404
Norm after each mp layer: 2.6216952800750732
Norm after each mp layer: 7.702203273773193
Norm after each mp layer: 25.30707550048828
Norm before input: 0.2552422881126404
Norm after input: 0.9241836071014404
Norm after each mp layer: 2.6216952800750732
Norm after each mp layer: 7.702203273773193
Norm after each mp layer: 25.30707550048828
Norm before input: 0.2552422881126404
Norm after input: 0.887587308883667
Norm after each mp layer: 2.3917136192321777
Norm after each mp layer: 7.548224925994873
Norm after each mp layer: 25.181747436523438
Norm before input: 0.2552422881126404
Norm after input: 0.887587308883667
Norm after each mp layer: 2.3917136192321777
Norm after each mp layer: 7.548224925994873
Norm after each mp layer: 25.181747436523438
Norm before input: 0.2552422881126404
Norm after input: 0.8418604135513306
Norm after each mp layer: 2.031122922897339
Norm after each mp layer: 5.68989896774292
Norm after each mp layer: 19.663803100585938
Epoch: 10, Loss: 3.0555, Energy: 75746.7188, Train: 28.23%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.8418604135513306
Norm after each mp layer: 2.031122922897339
Norm after each mp layer: 5.68989896774292
Norm after each mp layer: 19.663803100585938
Norm before input: 0.2552422881126404
Norm after input: 0.7971702218055725
Norm after each mp layer: 1.6873520612716675
Norm after each mp layer: 4.135075092315674
Norm after each mp layer: 14.308025360107422
Norm before input: 0.2552422881126404
Norm after input: 0.7971702218055725
Norm after each mp layer: 1.6873520612716675
Norm after each mp layer: 4.135075092315674
Norm after each mp layer: 14.308025360107422
Norm before input: 0.2552422881126404
Norm after input: 0.7574008107185364
Norm after each mp layer: 1.413580060005188
Norm after each mp layer: 3.107201099395752
Norm after each mp layer: 10.570087432861328
Norm before input: 0.2552422881126404
Norm after input: 0.7574008107185364
Norm after each mp layer: 1.413580060005188
Norm after each mp layer: 3.107201099395752
Norm after each mp layer: 10.570087432861328
Norm before input: 0.2552422881126404
Norm after input: 0.7258922457695007
Norm after each mp layer: 1.2668230533599854
Norm after each mp layer: 2.683807134628296
Norm after each mp layer: 9.050763130187988
Norm before input: 0.2552422881126404
Norm after input: 0.7258922457695007
Norm after each mp layer: 1.2668230533599854
Norm after each mp layer: 2.683807134628296
Norm after each mp layer: 9.050763130187988
Norm before input: 0.2552422881126404
Norm after input: 0.7033304572105408
Norm after each mp layer: 1.2568105459213257
Norm after each mp layer: 2.8616180419921875
Norm after each mp layer: 9.863492965698242
Norm before input: 0.2552422881126404
Norm after input: 0.7033304572105408
Norm after each mp layer: 1.2568105459213257
Norm after each mp layer: 2.8616180419921875
Norm after each mp layer: 9.863492965698242
Norm before input: 0.2552422881126404
Norm after input: 0.687501072883606
Norm after each mp layer: 1.3260868787765503
Norm after each mp layer: 3.435299873352051
Norm after each mp layer: 12.399020195007324
Epoch: 15, Loss: 2.5377, Energy: 9200.0234, Train: 16.23%, Valid: 16.20%, Test: 14.90%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.687501072883606
Norm after each mp layer: 1.3260868787765503
Norm after each mp layer: 3.435299873352051
Norm after each mp layer: 12.399020195007324
Norm before input: 0.2552422881126404
Norm after input: 0.6733313798904419
Norm after each mp layer: 1.3924580812454224
Norm after each mp layer: 3.999682903289795
Norm after each mp layer: 15.087150573730469
Norm before input: 0.2552422881126404
Norm after input: 0.6733313798904419
Norm after each mp layer: 1.3924580812454224
Norm after each mp layer: 3.999682903289795
Norm after each mp layer: 15.087150573730469
Norm before input: 0.2552422881126404
Norm after input: 0.6578993797302246
Norm after each mp layer: 1.4206898212432861
Norm after each mp layer: 4.350652694702148
Norm after each mp layer: 16.98967742919922
Norm before input: 0.2552422881126404
Norm after input: 0.6578993797302246
Norm after each mp layer: 1.4206898212432861
Norm after each mp layer: 4.350652694702148
Norm after each mp layer: 16.98967742919922
Norm before input: 0.2552422881126404
Norm after input: 0.6402400732040405
Norm after each mp layer: 1.404447317123413
Norm after each mp layer: 4.4291558265686035
Norm after each mp layer: 17.734031677246094
Norm before input: 0.2552422881126404
Norm after input: 0.6402400732040405
Norm after each mp layer: 1.404447317123413
Norm after each mp layer: 4.4291558265686035
Norm after each mp layer: 17.734031677246094
Norm before input: 0.2552422881126404
Norm after input: 0.6192885637283325
Norm after each mp layer: 1.3404537439346313
Norm after each mp layer: 4.197642803192139
Norm after each mp layer: 17.039838790893555
Norm before input: 0.2552422881126404
Norm after input: 0.6192885637283325
Norm after each mp layer: 1.3404537439346313
Norm after each mp layer: 4.197642803192139
Norm after each mp layer: 17.039838790893555
Norm before input: 0.2552422881126404
Norm after input: 0.5963659882545471
Norm after each mp layer: 1.2451666593551636
Norm after each mp layer: 3.7646684646606445
Norm after each mp layer: 15.381403923034668
Epoch: 20, Loss: 2.1708, Energy: 28978.2539, Train: 42.05%, Valid: 44.20%, Test: 44.20%, Best Valid: 44.20%, Best Test: 44.20%
Norm before input: 0.2552422881126404
Norm after input: 0.5963659882545471
Norm after each mp layer: 1.2451666593551636
Norm after each mp layer: 3.7646684646606445
Norm after each mp layer: 15.381403923034668
Norm before input: 0.2552422881126404
Norm after input: 0.5739086270332336
Norm after each mp layer: 1.1407759189605713
Norm after each mp layer: 3.275890827178955
Norm after each mp layer: 13.453361511230469
Norm before input: 0.2552422881126404
Norm after input: 0.5739086270332336
Norm after each mp layer: 1.1407759189605713
Norm after each mp layer: 3.275890827178955
Norm after each mp layer: 13.453361511230469
Norm before input: 0.2552422881126404
Norm after input: 0.5538289546966553
Norm after each mp layer: 1.0458914041519165
Norm after each mp layer: 2.8275411128997803
Norm after each mp layer: 11.695462226867676
Norm before input: 0.2552422881126404
Norm after input: 0.5538289546966553
Norm after each mp layer: 1.0458914041519165
Norm after each mp layer: 2.8275411128997803
Norm after each mp layer: 11.695462226867676
Norm before input: 0.2552422881126404
Norm after input: 0.5375760793685913
Norm after each mp layer: 0.9811540842056274
Norm after each mp layer: 2.5074589252471924
Norm after each mp layer: 10.470245361328125
Norm before input: 0.2552422881126404
Norm after input: 0.5375760793685913
Norm after each mp layer: 0.9811540842056274
Norm after each mp layer: 2.5074589252471924
Norm after each mp layer: 10.470245361328125
Norm before input: 0.2552422881126404
Norm after input: 0.525781512260437
Norm after each mp layer: 0.9640207290649414
Norm after each mp layer: 2.4099559783935547
Norm after each mp layer: 10.187776565551758
Norm before input: 0.2552422881126404
Norm after input: 0.525781512260437
Norm after each mp layer: 0.9640207290649414
Norm after each mp layer: 2.4099559783935547
Norm after each mp layer: 10.187776565551758
Norm before input: 0.2552422881126404
Norm after input: 0.517939031124115
Norm after each mp layer: 0.9904974102973938
Norm after each mp layer: 2.5334811210632324
Norm after each mp layer: 10.891556739807129
Epoch: 25, Loss: 1.9183, Energy: 7313.7871, Train: 43.96%, Valid: 46.60%, Test: 45.20%, Best Valid: 46.80%, Best Test: 45.30%
Norm before input: 0.2552422881126404
Norm after input: 0.517939031124115
Norm after each mp layer: 0.9904974102973938
Norm after each mp layer: 2.5334811210632324
Norm after each mp layer: 10.891556739807129
Norm before input: 0.2552422881126404
Norm after input: 0.5128768086433411
Norm after each mp layer: 1.0395426750183105
Norm after each mp layer: 2.768320322036743
Norm after each mp layer: 12.125492095947266
Norm before input: 0.2552422881126404
Norm after input: 0.5128768086433411
Norm after each mp layer: 1.0395426750183105
Norm after each mp layer: 2.768320322036743
Norm after each mp layer: 12.125493049621582
Norm before input: 0.2552422881126404
Norm after input: 0.5088565945625305
Norm after each mp layer: 1.08797287940979
Norm after each mp layer: 2.9982128143310547
Norm after each mp layer: 13.354703903198242
Norm before input: 0.2552422881126404
Norm after input: 0.5088565945625305
Norm after each mp layer: 1.08797287940979
Norm after each mp layer: 2.9982128143310547
Norm after each mp layer: 13.354703903198242
Norm before input: 0.2552422881126404
Norm after input: 0.5041881799697876
Norm after each mp layer: 1.1203265190124512
Norm after each mp layer: 3.1511781215667725
Norm after each mp layer: 14.216403007507324
Norm before input: 0.2552422881126404
Norm after input: 0.5041881799697876
Norm after each mp layer: 1.1203265190124512
Norm after each mp layer: 3.1511781215667725
Norm after each mp layer: 14.216403007507324
Norm before input: 0.2552422881126404
Norm after input: 0.49796345829963684
Norm after each mp layer: 1.131858229637146
Norm after each mp layer: 3.2009871006011963
Norm after each mp layer: 14.557384490966797
Norm before input: 0.2552422881126404
Norm after input: 0.49796345829963684
Norm after each mp layer: 1.131858229637146
Norm after each mp layer: 3.2009871006011963
Norm after each mp layer: 14.557384490966797
Norm before input: 0.2552422881126404
Norm after input: 0.490324467420578
Norm after each mp layer: 1.1266496181488037
Norm after each mp layer: 3.1621036529541016
Norm after each mp layer: 14.436847686767578
Epoch: 30, Loss: 1.6504, Energy: 20559.3535, Train: 53.73%, Valid: 53.80%, Test: 52.60%, Best Valid: 53.80%, Best Test: 52.60%
Norm before input: 0.2552422881126404
Norm after input: 0.490324467420578
Norm after each mp layer: 1.1266496181488037
Norm after each mp layer: 3.1621036529541016
Norm after each mp layer: 14.436847686767578
Norm before input: 0.2552422881126404
Norm after input: 0.4821524918079376
Norm after each mp layer: 1.1125938892364502
Norm after each mp layer: 3.075838565826416
Norm after each mp layer: 14.067692756652832
Norm before input: 0.2552422881126404
Norm after input: 0.4821524918079376
Norm after each mp layer: 1.1125938892364502
Norm after each mp layer: 3.075838565826416
Norm after each mp layer: 14.067692756652832
Norm before input: 0.2552422881126404
Norm after input: 0.47439101338386536
Norm after each mp layer: 1.0955687761306763
Norm after each mp layer: 2.979687452316284
Norm after each mp layer: 13.649757385253906
Norm before input: 0.2552422881126404
Norm after input: 0.47439101338386536
Norm after each mp layer: 1.0955687761306763
Norm after each mp layer: 2.979687452316284
Norm after each mp layer: 13.649757385253906
Norm before input: 0.2552422881126404
Norm after input: 0.46778199076652527
Norm after each mp layer: 1.0807876586914062
Norm after each mp layer: 2.899876594543457
Norm after each mp layer: 13.31103515625
Norm before input: 0.2552422881126404
Norm after input: 0.46778199076652527
Norm after each mp layer: 1.0807876586914062
Norm after each mp layer: 2.899876594543457
Norm after each mp layer: 13.311034202575684
Norm before input: 0.2552422881126404
Norm after input: 0.46276170015335083
Norm after each mp layer: 1.0744913816452026
Norm after each mp layer: 2.858457088470459
Norm after each mp layer: 13.148001670837402
Norm before input: 0.2552422881126404
Norm after input: 0.46276170015335083
Norm after each mp layer: 1.0744913816452026
Norm after each mp layer: 2.858457088470459
Norm after each mp layer: 13.148001670837402
Norm before input: 0.2552422881126404
Norm after input: 0.4591856002807617
Norm after each mp layer: 1.079009771347046
Norm after each mp layer: 2.862112045288086
Norm after each mp layer: 13.185047149658203
Epoch: 35, Loss: 1.4560, Energy: 15186.1885, Train: 55.55%, Valid: 55.60%, Test: 54.00%, Best Valid: 56.40%, Best Test: 55.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4591856002807617
Norm after each mp layer: 1.079009771347046
Norm after each mp layer: 2.862112045288086
Norm after each mp layer: 13.185047149658203
Norm before input: 0.2552422881126404
Norm after input: 0.4564301371574402
Norm after each mp layer: 1.0899760723114014
Norm after each mp layer: 2.8951518535614014
Norm after each mp layer: 13.344412803649902
Norm before input: 0.2552422881126404
Norm after input: 0.4564301371574402
Norm after each mp layer: 1.0899760723114014
Norm after each mp layer: 2.8951518535614014
Norm after each mp layer: 13.344412803649902
Norm before input: 0.2552422881126404
Norm after input: 0.45392128825187683
Norm after each mp layer: 1.1012728214263916
Norm after each mp layer: 2.938455104827881
Norm after each mp layer: 13.537769317626953
Norm before input: 0.2552422881126404
Norm after input: 0.45392128825187683
Norm after each mp layer: 1.1012728214263916
Norm after each mp layer: 2.938455104827881
Norm after each mp layer: 13.537769317626953
Norm before input: 0.2552422881126404
Norm after input: 0.4515572488307953
Norm after each mp layer: 1.1112126111984253
Norm after each mp layer: 2.9883408546447754
Norm after each mp layer: 13.75709342956543
Norm before input: 0.2552422881126404
Norm after input: 0.4515572488307953
Norm after each mp layer: 1.1112126111984253
Norm after each mp layer: 2.9883408546447754
Norm after each mp layer: 13.75709342956543
Norm before input: 0.2552422881126404
Norm after input: 0.4495435655117035
Norm after each mp layer: 1.1211990118026733
Norm after each mp layer: 3.04667329788208
Norm after each mp layer: 14.015938758850098
Norm before input: 0.2552422881126404
Norm after input: 0.4495435655117035
Norm after each mp layer: 1.1211990118026733
Norm after each mp layer: 3.04667329788208
Norm after each mp layer: 14.015938758850098
Norm before input: 0.2552422881126404
Norm after input: 0.44802579283714294
Norm after each mp layer: 1.1316882371902466
Norm after each mp layer: 3.108146905899048
Norm after each mp layer: 14.272891998291016
Epoch: 40, Loss: 1.2147, Energy: 13093.9668, Train: 60.68%, Valid: 58.40%, Test: 57.60%, Best Valid: 58.40%, Best Test: 57.60%
Norm before input: 0.2552422881126404
Norm after input: 0.44802579283714294
Norm after each mp layer: 1.1316882371902466
Norm after each mp layer: 3.108146905899048
Norm after each mp layer: 14.272891998291016
Norm before input: 0.2552422881126404
Norm after input: 0.44691815972328186
Norm after each mp layer: 1.1419610977172852
Norm after each mp layer: 3.169419527053833
Norm after each mp layer: 14.494132041931152
Norm before input: 0.2552422881126404
Norm after input: 0.44691815972328186
Norm after each mp layer: 1.1419610977172852
Norm after each mp layer: 3.169419527053833
Norm after each mp layer: 14.494132041931152
Norm before input: 0.2552422881126404
Norm after input: 0.44592592120170593
Norm after each mp layer: 1.151477336883545
Norm after each mp layer: 3.2346229553222656
Norm after each mp layer: 14.71576976776123
Norm before input: 0.2552422881126404
Norm after input: 0.44592592120170593
Norm after each mp layer: 1.151477336883545
Norm after each mp layer: 3.2346229553222656
Norm after each mp layer: 14.71576976776123
Norm before input: 0.2552422881126404
Norm after input: 0.44500434398651123
Norm after each mp layer: 1.1637412309646606
Norm after each mp layer: 3.3192965984344482
Norm after each mp layer: 15.047784805297852
Norm before input: 0.2552422881126404
Norm after input: 0.44500434398651123
Norm after each mp layer: 1.1637412309646606
Norm after each mp layer: 3.3192965984344482
Norm after each mp layer: 15.047783851623535
Norm before input: 0.2552422881126404
Norm after input: 0.4444867670536041
Norm after each mp layer: 1.184648036956787
Norm after each mp layer: 3.441344976425171
Norm after each mp layer: 15.579841613769531
Norm before input: 0.2552422881126404
Norm after input: 0.4444867670536041
Norm after each mp layer: 1.184648036956787
Norm after each mp layer: 3.441344976425171
Norm after each mp layer: 15.579841613769531
Norm before input: 0.2552422881126404
Norm after input: 0.4446413815021515
Norm after each mp layer: 1.2162699699401855
Norm after each mp layer: 3.6030075550079346
Norm after each mp layer: 16.30543327331543
Epoch: 45, Loss: 0.9488, Energy: 10807.6543, Train: 75.08%, Valid: 73.20%, Test: 71.20%, Best Valid: 73.20%, Best Test: 71.20%
Norm before input: 0.2552422881126404
Norm after input: 0.4446413815021515
Norm after each mp layer: 1.2162699699401855
Norm after each mp layer: 3.6030075550079346
Norm after each mp layer: 16.30543327331543
Norm before input: 0.2552422881126404
Norm after input: 0.4453008472919464
Norm after each mp layer: 1.2549279928207397
Norm after each mp layer: 3.790301561355591
Norm after each mp layer: 17.187744140625
Norm before input: 0.2552422881126404
Norm after input: 0.4453008472919464
Norm after each mp layer: 1.2549279928207397
Norm after each mp layer: 3.790301561355591
Norm after each mp layer: 17.187744140625
Norm before input: 0.2552422881126404
Norm after input: 0.4459037184715271
Norm after each mp layer: 1.2922301292419434
Norm after each mp layer: 3.975184202194214
Norm after each mp layer: 18.14146614074707
Norm before input: 0.2552422881126404
Norm after input: 0.4459037184715271
Norm after each mp layer: 1.2922301292419434
Norm after each mp layer: 3.975184202194214
Norm after each mp layer: 18.14146614074707
Norm before input: 0.2552422881126404
Norm after input: 0.4460470676422119
Norm after each mp layer: 1.3203308582305908
Norm after each mp layer: 4.129065036773682
Norm after each mp layer: 19.00208854675293
Norm before input: 0.2552422881126404
Norm after input: 0.4460470676422119
Norm after each mp layer: 1.3203308582305908
Norm after each mp layer: 4.129065036773682
Norm after each mp layer: 19.00208854675293
Norm before input: 0.2552422881126404
Norm after input: 0.4459417760372162
Norm after each mp layer: 1.3381909132003784
Norm after each mp layer: 4.241024017333984
Norm after each mp layer: 19.633037567138672
Norm before input: 0.2552422881126404
Norm after input: 0.4459417760372162
Norm after each mp layer: 1.3381909132003784
Norm after each mp layer: 4.241024017333984
Norm after each mp layer: 19.63303565979004
Norm before input: 0.2552422881126404
Norm after input: 0.4460899829864502
Norm after each mp layer: 1.3498831987380981
Norm after each mp layer: 4.317512035369873
Norm after each mp layer: 20.04416847229004
Epoch: 50, Loss: 0.7975, Energy: 15645.7549, Train: 83.69%, Valid: 76.80%, Test: 78.50%, Best Valid: 77.80%, Best Test: 78.50%
Norm before input: 0.2552422881126404
Norm after input: 0.4460899829864502
Norm after each mp layer: 1.3498831987380981
Norm after each mp layer: 4.317512035369873
Norm after each mp layer: 20.04416847229004
Norm before input: 0.2552422881126404
Norm after input: 0.44677498936653137
Norm after each mp layer: 1.3587932586669922
Norm after each mp layer: 4.3670830726623535
Norm after each mp layer: 20.33785057067871
Norm before input: 0.2552422881126404
Norm after input: 0.44677498936653137
Norm after each mp layer: 1.3587932586669922
Norm after each mp layer: 4.3670830726623535
Norm after each mp layer: 20.33785057067871
Norm before input: 0.2552422881126404
Norm after input: 0.44801777601242065
Norm after each mp layer: 1.3649318218231201
Norm after each mp layer: 4.391993045806885
Norm after each mp layer: 20.555078506469727
Norm before input: 0.2552422881126404
Norm after input: 0.44801777601242065
Norm after each mp layer: 1.3649318218231201
Norm after each mp layer: 4.391993045806885
Norm after each mp layer: 20.555078506469727
Norm before input: 0.2552422881126404
Norm after input: 0.44987544417381287
Norm after each mp layer: 1.3676103353500366
Norm after each mp layer: 4.396041393280029
Norm after each mp layer: 20.662691116333008
Norm before input: 0.2552422881126404
Norm after input: 0.44987544417381287
Norm after each mp layer: 1.3676103353500366
Norm after each mp layer: 4.396041393280029
Norm after each mp layer: 20.662691116333008
Norm before input: 0.2552422881126404
Norm after input: 0.4522475004196167
Norm after each mp layer: 1.3658311367034912
Norm after each mp layer: 4.381217002868652
Norm after each mp layer: 20.628700256347656
Norm before input: 0.2552422881126404
Norm after input: 0.4522475004196167
Norm after each mp layer: 1.3658311367034912
Norm after each mp layer: 4.381217002868652
Norm after each mp layer: 20.628700256347656
Norm before input: 0.2552422881126404
Norm after input: 0.454710990190506
Norm after each mp layer: 1.3592547178268433
Norm after each mp layer: 4.3500471115112305
Norm after each mp layer: 20.487674713134766
Epoch: 55, Loss: 0.6364, Energy: 12874.9922, Train: 87.25%, Valid: 79.00%, Test: 78.60%, Best Valid: 79.00%, Best Test: 78.60%
Norm before input: 0.2552422881126404
Norm after input: 0.454710990190506
Norm after each mp layer: 1.3592547178268433
Norm after each mp layer: 4.3500471115112305
Norm after each mp layer: 20.487674713134766
Norm before input: 0.2552422881126404
Norm after input: 0.45676591992378235
Norm after each mp layer: 1.3505247831344604
Norm after each mp layer: 4.31198787689209
Norm after each mp layer: 20.30894660949707
Norm before input: 0.2552422881126404
Norm after input: 0.45676591992378235
Norm after each mp layer: 1.3505247831344604
Norm after each mp layer: 4.31198787689209
Norm after each mp layer: 20.30894660949707
Norm before input: 0.2552422881126404
Norm after input: 0.458114355802536
Norm after each mp layer: 1.3429704904556274
Norm after each mp layer: 4.275164604187012
Norm after each mp layer: 20.10041618347168
Norm before input: 0.2552422881126404
Norm after input: 0.458114355802536
Norm after each mp layer: 1.3429704904556274
Norm after each mp layer: 4.275164604187012
Norm after each mp layer: 20.10041618347168
Norm before input: 0.2552422881126404
Norm after input: 0.45843446254730225
Norm after each mp layer: 1.3379039764404297
Norm after each mp layer: 4.2451910972595215
Norm after each mp layer: 19.8984375
Norm before input: 0.2552422881126404
Norm after input: 0.45843446254730225
Norm after each mp layer: 1.3379039764404297
Norm after each mp layer: 4.2451910972595215
Norm after each mp layer: 19.8984375
Norm before input: 0.2552422881126404
Norm after input: 0.457487016916275
Norm after each mp layer: 1.336089015007019
Norm after each mp layer: 4.233150959014893
Norm after each mp layer: 19.829408645629883
Norm before input: 0.2552422881126404
Norm after input: 0.457487016916275
Norm after each mp layer: 1.336089015007019
Norm after each mp layer: 4.233150959014893
Norm after each mp layer: 19.829408645629883
Norm before input: 0.2552422881126404
Norm after input: 0.4559383690357208
Norm after each mp layer: 1.3360285758972168
Norm after each mp layer: 4.234874725341797
Norm after each mp layer: 19.8609619140625
Epoch: 60, Loss: 0.5293, Energy: 9220.2266, Train: 89.24%, Valid: 81.20%, Test: 78.80%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4559383690357208
Norm after each mp layer: 1.3360285758972168
Norm after each mp layer: 4.234874725341797
Norm after each mp layer: 19.8609619140625
Norm before input: 0.2552422881126404
Norm after input: 0.4547811448574066
Norm after each mp layer: 1.3370400667190552
Norm after each mp layer: 4.244858741760254
Norm after each mp layer: 19.931793212890625
Norm before input: 0.2552422881126404
Norm after input: 0.4547811448574066
Norm after each mp layer: 1.3370400667190552
Norm after each mp layer: 4.244858741760254
Norm after each mp layer: 19.931793212890625
Norm before input: 0.2552422881126404
Norm after input: 0.45439472794532776
Norm after each mp layer: 1.3396544456481934
Norm after each mp layer: 4.264131546020508
Norm after each mp layer: 20.073421478271484
Norm before input: 0.2552422881126404
Norm after input: 0.45439472794532776
Norm after each mp layer: 1.3396544456481934
Norm after each mp layer: 4.264131546020508
Norm after each mp layer: 20.073421478271484
Norm before input: 0.2552422881126404
Norm after input: 0.4549403488636017
Norm after each mp layer: 1.3427543640136719
Norm after each mp layer: 4.286925315856934
Norm after each mp layer: 20.23822593688965
Norm before input: 0.2552422881126404
Norm after input: 0.4549403488636017
Norm after each mp layer: 1.3427543640136719
Norm after each mp layer: 4.286925315856934
Norm after each mp layer: 20.23822593688965
Norm before input: 0.2552422881126404
Norm after input: 0.45639917254447937
Norm after each mp layer: 1.3447532653808594
Norm after each mp layer: 4.3054351806640625
Norm after each mp layer: 20.3361759185791
Norm before input: 0.2552422881126404
Norm after input: 0.45639917254447937
Norm after each mp layer: 1.3447532653808594
Norm after each mp layer: 4.305434703826904
Norm after each mp layer: 20.3361759185791
Norm before input: 0.2552422881126404
Norm after input: 0.4581213891506195
Norm after each mp layer: 1.34669029712677
Norm after each mp layer: 4.327635765075684
Norm after each mp layer: 20.44442367553711
Epoch: 65, Loss: 0.4351, Energy: 8014.7109, Train: 91.23%, Valid: 77.60%, Test: 78.60%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4581213891506195
Norm after each mp layer: 1.34669029712677
Norm after each mp layer: 4.327635765075684
Norm after each mp layer: 20.44442367553711
Norm before input: 0.2552422881126404
Norm after input: 0.45937463641166687
Norm after each mp layer: 1.3514243364334106
Norm after each mp layer: 4.3685784339904785
Norm after each mp layer: 20.686033248901367
Norm before input: 0.2552422881126404
Norm after input: 0.45937463641166687
Norm after each mp layer: 1.3514243364334106
Norm after each mp layer: 4.3685784339904785
Norm after each mp layer: 20.686033248901367
Norm before input: 0.2552422881126404
Norm after input: 0.46009692549705505
Norm after each mp layer: 1.3592932224273682
Norm after each mp layer: 4.42460823059082
Norm after each mp layer: 21.01805305480957
Norm before input: 0.2552422881126404
Norm after input: 0.46009692549705505
Norm after each mp layer: 1.3592932224273682
Norm after each mp layer: 4.42460823059082
Norm after each mp layer: 21.01805305480957
Norm before input: 0.2552422881126404
Norm after input: 0.4603770077228546
Norm after each mp layer: 1.3704317808151245
Norm after each mp layer: 4.495176792144775
Norm after each mp layer: 21.456308364868164
Norm before input: 0.2552422881126404
Norm after input: 0.4603770077228546
Norm after each mp layer: 1.3704317808151245
Norm after each mp layer: 4.495176792144775
Norm after each mp layer: 21.456308364868164
Norm before input: 0.2552422881126404
Norm after input: 0.4605632424354553
Norm after each mp layer: 1.3826146125793457
Norm after each mp layer: 4.570766925811768
Norm after each mp layer: 21.953907012939453
Norm before input: 0.2552422881126404
Norm after input: 0.4605632424354553
Norm after each mp layer: 1.3826146125793457
Norm after each mp layer: 4.570766925811768
Norm after each mp layer: 21.953907012939453
Norm before input: 0.2552422881126404
Norm after input: 0.4610133171081543
Norm after each mp layer: 1.3904985189437866
Norm after each mp layer: 4.624599456787109
Norm after each mp layer: 22.31190299987793
Epoch: 70, Loss: 0.3656, Energy: 7958.1846, Train: 92.14%, Valid: 77.40%, Test: 78.20%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4610133171081543
Norm after each mp layer: 1.3904985189437866
Norm after each mp layer: 4.624599456787109
Norm after each mp layer: 22.31190299987793
Norm before input: 0.2552422881126404
Norm after input: 0.46138378977775574
Norm after each mp layer: 1.39298677444458
Norm after each mp layer: 4.654690742492676
Norm after each mp layer: 22.535245895385742
Norm before input: 0.2552422881126404
Norm after input: 0.46138378977775574
Norm after each mp layer: 1.39298677444458
Norm after each mp layer: 4.654690742492676
Norm after each mp layer: 22.535245895385742
Norm before input: 0.2552422881126404
Norm after input: 0.4615364968776703
Norm after each mp layer: 1.3930574655532837
Norm after each mp layer: 4.671072483062744
Norm after each mp layer: 22.685684204101562
Norm before input: 0.2552422881126404
Norm after input: 0.4615364968776703
Norm after each mp layer: 1.3930573463439941
Norm after each mp layer: 4.671072483062744
Norm after each mp layer: 22.685684204101562
Norm before input: 0.2552422881126404
Norm after input: 0.4617585241794586
Norm after each mp layer: 1.3925175666809082
Norm after each mp layer: 4.672197341918945
Norm after each mp layer: 22.704744338989258
Norm before input: 0.2552422881126404
Norm after input: 0.4617585241794586
Norm after each mp layer: 1.3925175666809082
Norm after each mp layer: 4.672197341918945
Norm after each mp layer: 22.704744338989258
Norm before input: 0.2552422881126404
Norm after input: 0.4617561101913452
Norm after each mp layer: 1.3934166431427002
Norm after each mp layer: 4.676033020019531
Norm after each mp layer: 22.747692108154297
Norm before input: 0.2552422881126404
Norm after input: 0.4617561101913452
Norm after each mp layer: 1.3934166431427002
Norm after each mp layer: 4.676033020019531
Norm after each mp layer: 22.747692108154297
Norm before input: 0.2552422881126404
Norm after input: 0.46158769726753235
Norm after each mp layer: 1.3926994800567627
Norm after each mp layer: 4.6730146408081055
Norm after each mp layer: 22.746078491210938
Epoch: 75, Loss: 0.3151, Energy: 7940.8423, Train: 93.63%, Valid: 77.00%, Test: 77.20%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.46158769726753235
Norm after each mp layer: 1.3926994800567627
Norm after each mp layer: 4.6730146408081055
Norm after each mp layer: 22.746078491210938
Norm before input: 0.2552422881126404
Norm after input: 0.46148866415023804
Norm after each mp layer: 1.3911137580871582
Norm after each mp layer: 4.669572830200195
Norm after each mp layer: 22.747745513916016
Norm before input: 0.2552422881126404
Norm after input: 0.46148866415023804
Norm after each mp layer: 1.3911137580871582
Norm after each mp layer: 4.669572830200195
Norm after each mp layer: 22.747745513916016
Norm before input: 0.2552422881126404
Norm after input: 0.4617796838283539
Norm after each mp layer: 1.3912553787231445
Norm after each mp layer: 4.674792289733887
Norm after each mp layer: 22.800291061401367
Norm before input: 0.2552422881126404
Norm after input: 0.4617796838283539
Norm after each mp layer: 1.3912553787231445
Norm after each mp layer: 4.674792289733887
Norm after each mp layer: 22.800291061401367
Norm before input: 0.2552422881126404
Norm after input: 0.4623657166957855
Norm after each mp layer: 1.3926892280578613
Norm after each mp layer: 4.682927131652832
Norm after each mp layer: 22.84393882751465
Norm before input: 0.2552422881126404
Norm after input: 0.4623657166957855
Norm after each mp layer: 1.3926892280578613
Norm after each mp layer: 4.682927131652832
Norm after each mp layer: 22.84393882751465
Norm before input: 0.2552422881126404
Norm after input: 0.46232953667640686
Norm after each mp layer: 1.3974297046661377
Norm after each mp layer: 4.715841770172119
Norm after each mp layer: 23.094234466552734
Norm before input: 0.2552422881126404
Norm after input: 0.46232953667640686
Norm after each mp layer: 1.3974297046661377
Norm after each mp layer: 4.715841770172119
Norm after each mp layer: 23.094234466552734
Norm before input: 0.2552422881126404
Norm after input: 0.4628956615924835
Norm after each mp layer: 1.400523066520691
Norm after each mp layer: 4.724908828735352
Norm after each mp layer: 23.099355697631836
Epoch: 80, Loss: 0.2804, Energy: 7636.8120, Train: 94.62%, Valid: 76.80%, Test: 76.80%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4628956615924835
Norm after each mp layer: 1.400523066520691
Norm after each mp layer: 4.724908828735352
Norm after each mp layer: 23.099355697631836
Norm before input: 0.2552422881126404
Norm after input: 0.462930291891098
Norm after each mp layer: 1.409191608428955
Norm after each mp layer: 4.780725479125977
Norm after each mp layer: 23.52899932861328
Norm before input: 0.2552422881126404
Norm after input: 0.462930291891098
Norm after each mp layer: 1.409191608428955
Norm after each mp layer: 4.780725479125977
Norm after each mp layer: 23.52899932861328
Norm before input: 0.2552422881126404
Norm after input: 0.4640621840953827
Norm after each mp layer: 1.4050090312957764
Norm after each mp layer: 4.735750198364258
Norm after each mp layer: 23.06437110900879
Norm before input: 0.2552422881126404
Norm after input: 0.4640621840953827
Norm after each mp layer: 1.4050090312957764
Norm after each mp layer: 4.735750198364258
Norm after each mp layer: 23.06437110900879
Norm before input: 0.2552422881126404
Norm after input: 0.46356695890426636
Norm after each mp layer: 1.4270411729812622
Norm after each mp layer: 4.899266719818115
Norm after each mp layer: 24.413715362548828
Norm before input: 0.2552422881126404
Norm after input: 0.46356695890426636
Norm after each mp layer: 1.4270411729812622
Norm after each mp layer: 4.899266719818115
Norm after each mp layer: 24.413715362548828
Norm before input: 0.2552422881126404
Norm after input: 0.4634250998497009
Norm after each mp layer: 1.4108721017837524
Norm after each mp layer: 4.770949840545654
Norm after each mp layer: 23.290748596191406
Norm before input: 0.2552422881126404
Norm after input: 0.4634250998497009
Norm after each mp layer: 1.4108721017837524
Norm after each mp layer: 4.770949840545654
Norm after each mp layer: 23.290748596191406
Norm before input: 0.2552422881126404
Norm after input: 0.4649392068386078
Norm after each mp layer: 1.4118623733520508
Norm after each mp layer: 4.7444329261779785
Norm after each mp layer: 22.91731071472168
Epoch: 85, Loss: 0.2564, Energy: 7592.8462, Train: 93.87%, Valid: 76.20%, Test: 75.60%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4649392068386078
Norm after each mp layer: 1.4118623733520508
Norm after each mp layer: 4.7444329261779785
Norm after each mp layer: 22.91731071472168
Norm before input: 0.2552422881126404
Norm after input: 0.4651740789413452
Norm after each mp layer: 1.4393301010131836
Norm after each mp layer: 4.925108432769775
Norm after each mp layer: 24.37715721130371
Norm before input: 0.2552422881126404
Norm after input: 0.4651740789413452
Norm after each mp layer: 1.4393301010131836
Norm after each mp layer: 4.925108432769775
Norm after each mp layer: 24.37715721130371
Norm before input: 0.2552422881126404
Norm after input: 0.4630061984062195
Norm after each mp layer: 1.428720235824585
Norm after each mp layer: 4.870811462402344
Norm after each mp layer: 24.001893997192383
Norm before input: 0.2552422881126404
Norm after input: 0.4630061984062195
Norm after each mp layer: 1.428720235824585
Norm after each mp layer: 4.870811462402344
Norm after each mp layer: 24.001893997192383
Norm before input: 0.2552422881126404
Norm after input: 0.46172431111335754
Norm after each mp layer: 1.4085341691970825
Norm after each mp layer: 4.7281060218811035
Norm after each mp layer: 22.751359939575195
Norm before input: 0.2552422881126404
Norm after input: 0.46172431111335754
Norm after each mp layer: 1.4085341691970825
Norm after each mp layer: 4.7281060218811035
Norm after each mp layer: 22.751359939575195
Norm before input: 0.2552422881126404
Norm after input: 0.4633052349090576
Norm after each mp layer: 1.4142799377441406
Norm after each mp layer: 4.707959175109863
Norm after each mp layer: 22.395883560180664
Norm before input: 0.2552422881126404
Norm after input: 0.4633052349090576
Norm after each mp layer: 1.4142799377441406
Norm after each mp layer: 4.707959175109863
Norm after each mp layer: 22.395883560180664
Norm before input: 0.2552422881126404
Norm after input: 0.46253570914268494
Norm after each mp layer: 1.4299856424331665
Norm after each mp layer: 4.8263959884643555
Norm after each mp layer: 23.409025192260742
Epoch: 90, Loss: 0.3064, Energy: 7662.6860, Train: 94.62%, Valid: 77.20%, Test: 76.00%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.46253570914268494
Norm after each mp layer: 1.4299856424331665
Norm after each mp layer: 4.8263959884643555
Norm after each mp layer: 23.409025192260742
Norm before input: 0.2552422881126404
Norm after input: 0.46109122037887573
Norm after each mp layer: 1.4368889331817627
Norm after each mp layer: 4.880414009094238
Norm after each mp layer: 23.806854248046875
Norm before input: 0.2552422881126404
Norm after input: 0.46109122037887573
Norm after each mp layer: 1.4368889331817627
Norm after each mp layer: 4.880414009094238
Norm after each mp layer: 23.806854248046875
Norm before input: 0.2552422881126404
Norm after input: 0.4583685100078583
Norm after each mp layer: 1.4144423007965088
Norm after each mp layer: 4.724133491516113
Norm after each mp layer: 22.570287704467773
Norm before input: 0.2552422881126404
Norm after input: 0.4583685100078583
Norm after each mp layer: 1.4144423007965088
Norm after each mp layer: 4.724133491516113
Norm after each mp layer: 22.570289611816406
Norm before input: 0.2552422881126404
Norm after input: 0.45812997221946716
Norm after each mp layer: 1.4085454940795898
Norm after each mp layer: 4.644041061401367
Norm after each mp layer: 21.73276138305664
Norm before input: 0.2552422881126404
Norm after input: 0.45812997221946716
Norm after each mp layer: 1.4085454940795898
Norm after each mp layer: 4.644041061401367
Norm after each mp layer: 21.73276138305664
Norm before input: 0.2552422881126404
Norm after input: 0.4580022692680359
Norm after each mp layer: 1.415691614151001
Norm after each mp layer: 4.64272928237915
Norm after each mp layer: 21.572378158569336
Norm before input: 0.2552422881126404
Norm after input: 0.4580022692680359
Norm after each mp layer: 1.415691614151001
Norm after each mp layer: 4.64272928237915
Norm after each mp layer: 21.572378158569336
Norm before input: 0.2552422881126404
Norm after input: 0.4555020332336426
Norm after each mp layer: 1.4183387756347656
Norm after each mp layer: 4.674350261688232
Norm after each mp layer: 21.85787582397461
Epoch: 95, Loss: 0.2910, Energy: 6732.4258, Train: 95.36%, Valid: 77.00%, Test: 76.00%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4555020332336426
Norm after each mp layer: 1.4183387756347656
Norm after each mp layer: 4.674350261688232
Norm after each mp layer: 21.85787582397461
Norm before input: 0.2552422881126404
Norm after input: 0.45278096199035645
Norm after each mp layer: 1.4193297624588013
Norm after each mp layer: 4.721471309661865
Norm after each mp layer: 22.277311325073242
Norm before input: 0.2552422881126404
Norm after input: 0.45278096199035645
Norm after each mp layer: 1.4193297624588013
Norm after each mp layer: 4.721471309661865
Norm after each mp layer: 22.277311325073242
Norm before input: 0.2552422881126404
Norm after input: 0.45080551505088806
Norm after each mp layer: 1.41044020652771
Norm after each mp layer: 4.684998035430908
Norm after each mp layer: 22.01142692565918
Norm before input: 0.2552422881126404
Norm after input: 0.45080551505088806
Norm after each mp layer: 1.41044020652771
Norm after each mp layer: 4.684998035430908
Norm after each mp layer: 22.01142692565918
Norm before input: 0.2552422881126404
Norm after input: 0.45090410113334656
Norm after each mp layer: 1.400376319885254
Norm after each mp layer: 4.6011457443237305
Norm after each mp layer: 21.255712509155273
Norm before input: 0.2552422881126404
Norm after input: 0.45090410113334656
Norm after each mp layer: 1.400376319885254
Norm after each mp layer: 4.6011457443237305
Norm after each mp layer: 21.255712509155273
Norm before input: 0.2552422881126404
Norm after input: 0.45199188590049744
Norm after each mp layer: 1.4024637937545776
Norm after each mp layer: 4.573647499084473
Norm after each mp layer: 20.860607147216797
Norm before input: 0.2552422881126404
Norm after input: 0.45199188590049744
Norm after each mp layer: 1.4024637937545776
Norm after each mp layer: 4.573647499084473
Norm after each mp layer: 20.860607147216797
Norm before input: 0.2552422881126404
Norm after input: 0.4423803389072418
Norm after each mp layer: 1.4165328741073608
Norm after each mp layer: 4.723761081695557
Norm after each mp layer: 22.131256103515625
Epoch: 100, Loss: 0.2516, Energy: 5836.7109, Train: 94.78%, Valid: 74.80%, Test: 76.20%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4423803389072418
Norm after each mp layer: 1.4165328741073608
Norm after each mp layer: 4.723761081695557
Norm after each mp layer: 22.131256103515625
Norm before input: 0.2552422881126404
Norm after input: 0.4384579062461853
Norm after each mp layer: 1.4358030557632446
Norm after each mp layer: 4.860134601593018
Norm after each mp layer: 23.117647171020508
Norm before input: 0.2552422881126404
Norm after input: 0.4384579062461853
Norm after each mp layer: 1.4358030557632446
Norm after each mp layer: 4.860134601593018
Norm after each mp layer: 23.117647171020508
Norm before input: 0.2552422881126404
Norm after input: 0.43925920128822327
Norm after each mp layer: 1.4515180587768555
Norm after each mp layer: 4.934938907623291
Norm after each mp layer: 23.50271987915039
Norm before input: 0.2552422881126404
Norm after input: 0.43925920128822327
Norm after each mp layer: 1.4515180587768555
Norm after each mp layer: 4.934938907623291
Norm after each mp layer: 23.50271987915039
Norm before input: 0.2552422881126404
Norm after input: 0.4401724338531494
Norm after each mp layer: 1.4442427158355713
Norm after each mp layer: 4.898903846740723
Norm after each mp layer: 23.147464752197266
Norm before input: 0.2552422881126404
Norm after input: 0.4401724338531494
Norm after each mp layer: 1.4442427158355713
Norm after each mp layer: 4.898903846740723
Norm after each mp layer: 23.147464752197266
Norm before input: 0.2552422881126404
Norm after input: 0.44036978483200073
Norm after each mp layer: 1.420538306236267
Norm after each mp layer: 4.789052963256836
Norm after each mp layer: 22.337078094482422
Norm before input: 0.2552422881126404
Norm after input: 0.44036978483200073
Norm after each mp layer: 1.420538306236267
Norm after each mp layer: 4.789052963256836
Norm after each mp layer: 22.337078094482422
Norm before input: 0.2552422881126404
Norm after input: 0.4421795904636383
Norm after each mp layer: 1.40550696849823
Norm after each mp layer: 4.701169967651367
Norm after each mp layer: 21.606950759887695
Epoch: 105, Loss: 0.2275, Energy: 7880.4814, Train: 95.28%, Valid: 75.80%, Test: 74.00%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4421795904636383
Norm after each mp layer: 1.40550696849823
Norm after each mp layer: 4.701169967651367
Norm after each mp layer: 21.606950759887695
Norm before input: 0.2552422881126404
Norm after input: 0.4446442723274231
Norm after each mp layer: 1.4075148105621338
Norm after each mp layer: 4.671389102935791
Norm after each mp layer: 21.207429885864258
Norm before input: 0.2552422881126404
Norm after input: 0.4446442723274231
Norm after each mp layer: 1.4075148105621338
Norm after each mp layer: 4.671389102935791
Norm after each mp layer: 21.207429885864258
Norm before input: 0.2552422881126404
Norm after input: 0.44404807686805725
Norm after each mp layer: 1.409110188484192
Norm after each mp layer: 4.665558338165283
Norm after each mp layer: 21.075284957885742
Norm before input: 0.2552422881126404
Norm after input: 0.44404807686805725
Norm after each mp layer: 1.409110188484192
Norm after each mp layer: 4.665558338165283
Norm after each mp layer: 21.075284957885742
Norm before input: 0.2552422881126404
Norm after input: 0.43918779492378235
Norm after each mp layer: 1.398207664489746
Norm after each mp layer: 4.634270191192627
Norm after each mp layer: 20.95018196105957
Norm before input: 0.2552422881126404
Norm after input: 0.43918779492378235
Norm after each mp layer: 1.398207664489746
Norm after each mp layer: 4.634270191192627
Norm after each mp layer: 20.95018196105957
Norm before input: 0.2552422881126404
Norm after input: 0.43519484996795654
Norm after each mp layer: 1.391836404800415
Norm after each mp layer: 4.6218791007995605
Norm after each mp layer: 20.92537498474121
Norm before input: 0.2552422881126404
Norm after input: 0.43519484996795654
Norm after each mp layer: 1.391836404800415
Norm after each mp layer: 4.6218791007995605
Norm after each mp layer: 20.92537498474121
Norm before input: 0.2552422881126404
Norm after input: 0.43516793847084045
Norm after each mp layer: 1.3962732553482056
Norm after each mp layer: 4.625649929046631
Norm after each mp layer: 20.869613647460938
Epoch: 110, Loss: 0.2207, Energy: 5545.5093, Train: 95.86%, Valid: 76.40%, Test: 74.30%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.43516793847084045
Norm after each mp layer: 1.3962732553482056
Norm after each mp layer: 4.625649929046631
Norm after each mp layer: 20.869613647460938
Norm before input: 0.2552422881126404
Norm after input: 0.4365651309490204
Norm after each mp layer: 1.401772379875183
Norm after each mp layer: 4.6362786293029785
Norm after each mp layer: 20.84061050415039
Norm before input: 0.2552422881126404
Norm after input: 0.4365651309490204
Norm after each mp layer: 1.401772379875183
Norm after each mp layer: 4.6362786293029785
Norm after each mp layer: 20.84061050415039
Norm before input: 0.2552422881126404
Norm after input: 0.4354954659938812
Norm after each mp layer: 1.3949600458145142
Norm after each mp layer: 4.62993860244751
Norm after each mp layer: 20.837371826171875
Norm before input: 0.2552422881126404
Norm after input: 0.4354954659938812
Norm after each mp layer: 1.3949600458145142
Norm after each mp layer: 4.62993860244751
Norm after each mp layer: 20.837371826171875
Norm before input: 0.2552422881126404
Norm after input: 0.4331757724285126
Norm after each mp layer: 1.3879350423812866
Norm after each mp layer: 4.64057731628418
Norm after each mp layer: 21.01070213317871
Norm before input: 0.2552422881126404
Norm after input: 0.4331757724285126
Norm after each mp layer: 1.3879350423812866
Norm after each mp layer: 4.6405768394470215
Norm after each mp layer: 21.01070213317871
Norm before input: 0.2552422881126404
Norm after input: 0.4324353039264679
Norm after each mp layer: 1.394754409790039
Norm after each mp layer: 4.694179534912109
Norm after each mp layer: 21.38439178466797
Norm before input: 0.2552422881126404
Norm after input: 0.4324353039264679
Norm after each mp layer: 1.394754409790039
Norm after each mp layer: 4.694179534912109
Norm after each mp layer: 21.38439178466797
Norm before input: 0.2552422881126404
Norm after input: 0.43245503306388855
Norm after each mp layer: 1.4094860553741455
Norm after each mp layer: 4.7669677734375
Norm after each mp layer: 21.819961547851562
Epoch: 115, Loss: 0.1881, Energy: 5096.7495, Train: 96.27%, Valid: 75.80%, Test: 74.10%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.43245503306388855
Norm after each mp layer: 1.4094860553741455
Norm after each mp layer: 4.7669677734375
Norm after each mp layer: 21.819961547851562
Norm before input: 0.2552422881126404
Norm after input: 0.43075063824653625
Norm after each mp layer: 1.4164001941680908
Norm after each mp layer: 4.810203552246094
Norm after each mp layer: 22.11193084716797
Norm before input: 0.2552422881126404
Norm after input: 0.43075063824653625
Norm after each mp layer: 1.4164001941680908
Norm after each mp layer: 4.810203552246094
Norm after each mp layer: 22.11193084716797
Norm before input: 0.2552422881126404
Norm after input: 0.4281245470046997
Norm after each mp layer: 1.41415536403656
Norm after each mp layer: 4.8240838050842285
Norm after each mp layer: 22.25936508178711
Norm before input: 0.2552422881126404
Norm after input: 0.4281245470046997
Norm after each mp layer: 1.41415536403656
Norm after each mp layer: 4.8240838050842285
Norm after each mp layer: 22.25936508178711
Norm before input: 0.2552422881126404
Norm after input: 0.4277445673942566
Norm after each mp layer: 1.4156321287155151
Norm after each mp layer: 4.840464115142822
Norm after each mp layer: 22.352615356445312
Norm before input: 0.2552422881126404
Norm after input: 0.4277445673942566
Norm after each mp layer: 1.4156321287155151
Norm after each mp layer: 4.840464115142822
Norm after each mp layer: 22.352615356445312
Norm before input: 0.2552422881126404
Norm after input: 0.42965954542160034
Norm after each mp layer: 1.4209403991699219
Norm after each mp layer: 4.858726978302002
Norm after each mp layer: 22.382673263549805
Norm before input: 0.2552422881126404
Norm after input: 0.42965954542160034
Norm after each mp layer: 1.4209403991699219
Norm after each mp layer: 4.858726978302002
Norm after each mp layer: 22.382673263549805
Norm before input: 0.2552422881126404
Norm after input: 0.4313531219959259
Norm after each mp layer: 1.4212477207183838
Norm after each mp layer: 4.862232685089111
Norm after each mp layer: 22.338380813598633
Epoch: 120, Loss: 0.1682, Energy: 5643.5596, Train: 97.10%, Valid: 76.60%, Test: 75.10%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4313531219959259
Norm after each mp layer: 1.4212477207183838
Norm after each mp layer: 4.862232685089111
Norm after each mp layer: 22.338380813598633
Norm before input: 0.2552422881126404
Norm after input: 0.4313971996307373
Norm after each mp layer: 1.4143524169921875
Norm after each mp layer: 4.84358024597168
Norm after each mp layer: 22.213302612304688
Norm before input: 0.2552422881126404
Norm after input: 0.4313971996307373
Norm after each mp layer: 1.4143524169921875
Norm after each mp layer: 4.84358024597168
Norm after each mp layer: 22.213302612304688
Norm before input: 0.2552422881126404
Norm after input: 0.4311901032924652
Norm after each mp layer: 1.4119454622268677
Norm after each mp layer: 4.8430280685424805
Norm after each mp layer: 22.197799682617188
Norm before input: 0.2552422881126404
Norm after input: 0.4311901032924652
Norm after each mp layer: 1.4119454622268677
Norm after each mp layer: 4.8430280685424805
Norm after each mp layer: 22.197799682617188
Norm before input: 0.2552422881126404
Norm after input: 0.43127959966659546
Norm after each mp layer: 1.4184507131576538
Norm after each mp layer: 4.878161907196045
Norm after each mp layer: 22.385318756103516
Norm before input: 0.2552422881126404
Norm after input: 0.43127959966659546
Norm after each mp layer: 1.4184507131576538
Norm after each mp layer: 4.878161907196045
Norm after each mp layer: 22.385318756103516
Norm before input: 0.2552422881126404
Norm after input: 0.43035465478897095
Norm after each mp layer: 1.425522804260254
Norm after each mp layer: 4.9283294677734375
Norm after each mp layer: 22.71973419189453
Norm before input: 0.2552422881126404
Norm after input: 0.43035465478897095
Norm after each mp layer: 1.425522804260254
Norm after each mp layer: 4.9283294677734375
Norm after each mp layer: 22.71973419189453
Norm before input: 0.2552422881126404
Norm after input: 0.4281894564628601
Norm after each mp layer: 1.4258123636245728
Norm after each mp layer: 4.964359760284424
Norm after each mp layer: 23.0406551361084
Epoch: 125, Loss: 0.1577, Energy: 5312.5537, Train: 97.76%, Valid: 75.60%, Test: 73.60%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4281894564628601
Norm after each mp layer: 1.4258123636245728
Norm after each mp layer: 4.964359760284424
Norm after each mp layer: 23.0406551361084
Norm before input: 0.2552422881126404
Norm after input: 0.4268745183944702
Norm after each mp layer: 1.4257912635803223
Norm after each mp layer: 4.992653846740723
Norm after each mp layer: 23.284423828125
Norm before input: 0.2552422881126404
Norm after input: 0.4268745183944702
Norm after each mp layer: 1.4257912635803223
Norm after each mp layer: 4.992653846740723
Norm after each mp layer: 23.284423828125
Norm before input: 0.2552422881126404
Norm after input: 0.42742860317230225
Norm after each mp layer: 1.4292821884155273
Norm after each mp layer: 5.016051292419434
Norm after each mp layer: 23.403249740600586
Norm before input: 0.2552422881126404
Norm after input: 0.42742860317230225
Norm after each mp layer: 1.4292821884155273
Norm after each mp layer: 5.016051292419434
Norm after each mp layer: 23.403249740600586
Norm before input: 0.2552422881126404
Norm after input: 0.42820772528648376
Norm after each mp layer: 1.429779291152954
Norm after each mp layer: 5.022214412689209
Norm after each mp layer: 23.40491485595703
Norm before input: 0.2552422881126404
Norm after input: 0.42820772528648376
Norm after each mp layer: 1.429779291152954
Norm after each mp layer: 5.022214412689209
Norm after each mp layer: 23.40491485595703
Norm before input: 0.2552422881126404
Norm after input: 0.4282056391239166
Norm after each mp layer: 1.4251924753189087
Norm after each mp layer: 5.012756824493408
Norm after each mp layer: 23.3535099029541
Norm before input: 0.2552422881126404
Norm after input: 0.4282056391239166
Norm after each mp layer: 1.4251924753189087
Norm after each mp layer: 5.012756824493408
Norm after each mp layer: 23.3535099029541
Norm before input: 0.2552422881126404
Norm after input: 0.4282626211643219
Norm after each mp layer: 1.4237760305404663
Norm after each mp layer: 5.014638900756836
Norm after each mp layer: 23.3642520904541
Epoch: 130, Loss: 0.1486, Energy: 5189.5938, Train: 97.85%, Valid: 75.20%, Test: 73.60%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4282626211643219
Norm after each mp layer: 1.4237760305404663
Norm after each mp layer: 5.014638900756836
Norm after each mp layer: 23.3642520904541
Norm before input: 0.2552422881126404
Norm after input: 0.42834341526031494
Norm after each mp layer: 1.425779938697815
Norm after each mp layer: 5.0263543128967285
Norm after each mp layer: 23.41572380065918
Norm before input: 0.2552422881126404
Norm after input: 0.42834341526031494
Norm after each mp layer: 1.425779938697815
Norm after each mp layer: 5.0263543128967285
Norm after each mp layer: 23.41572380065918
Norm before input: 0.2552422881126404
Norm after input: 0.4275663197040558
Norm after each mp layer: 1.4241132736206055
Norm after each mp layer: 5.0226545333862305
Norm after each mp layer: 23.39250373840332
Norm before input: 0.2552422881126404
Norm after input: 0.4275663197040558
Norm after each mp layer: 1.4241132736206055
Norm after each mp layer: 5.0226545333862305
Norm after each mp layer: 23.39250373840332
Norm before input: 0.2552422881126404
Norm after input: 0.4268842935562134
Norm after each mp layer: 1.422154188156128
Norm after each mp layer: 5.016557693481445
Norm after each mp layer: 23.347421646118164
Norm before input: 0.2552422881126404
Norm after input: 0.4268842935562134
Norm after each mp layer: 1.422154188156128
Norm after each mp layer: 5.016557693481445
Norm after each mp layer: 23.347421646118164
Norm before input: 0.2552422881126404
Norm after input: 0.42705172300338745
Norm after each mp layer: 1.422619342803955
Norm after each mp layer: 5.018359184265137
Norm after each mp layer: 23.324207305908203
Norm before input: 0.2552422881126404
Norm after input: 0.42705172300338745
Norm after each mp layer: 1.422619342803955
Norm after each mp layer: 5.018359184265137
Norm after each mp layer: 23.324207305908203
Norm before input: 0.2552422881126404
Norm after input: 0.4274913966655731
Norm after each mp layer: 1.4224684238433838
Norm after each mp layer: 5.021699905395508
Norm after each mp layer: 23.318603515625
Epoch: 135, Loss: 0.1390, Energy: 4939.1084, Train: 97.68%, Valid: 75.00%, Test: 73.00%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4274913966655731
Norm after each mp layer: 1.4224684238433838
Norm after each mp layer: 5.021699905395508
Norm after each mp layer: 23.318603515625
Norm before input: 0.2552422881126404
Norm after input: 0.42763635516166687
Norm after each mp layer: 1.4202617406845093
Norm after each mp layer: 5.021777629852295
Norm after each mp layer: 23.317277908325195
Norm before input: 0.2552422881126404
Norm after input: 0.42763635516166687
Norm after each mp layer: 1.4202617406845093
Norm after each mp layer: 5.021777629852295
Norm after each mp layer: 23.317277908325195
Norm before input: 0.2552422881126404
Norm after input: 0.4277794063091278
Norm after each mp layer: 1.4195131063461304
Norm after each mp layer: 5.02643346786499
Norm after each mp layer: 23.335355758666992
Norm before input: 0.2552422881126404
Norm after input: 0.4277794063091278
Norm after each mp layer: 1.4195131063461304
Norm after each mp layer: 5.02643346786499
Norm after each mp layer: 23.335355758666992
Norm before input: 0.2552422881126404
Norm after input: 0.4278591275215149
Norm after each mp layer: 1.4211070537567139
Norm after each mp layer: 5.039003849029541
Norm after each mp layer: 23.391151428222656
Norm before input: 0.2552422881126404
Norm after input: 0.4278591275215149
Norm after each mp layer: 1.4211070537567139
Norm after each mp layer: 5.039003849029541
Norm after each mp layer: 23.391151428222656
Norm before input: 0.2552422881126404
Norm after input: 0.42731964588165283
Norm after each mp layer: 1.4220082759857178
Norm after each mp layer: 5.053299903869629
Norm after each mp layer: 23.48429298400879
Norm before input: 0.2552422881126404
Norm after input: 0.42731964588165283
Norm after each mp layer: 1.4220082759857178
Norm after each mp layer: 5.053299903869629
Norm after each mp layer: 23.48429298400879
Norm before input: 0.2552422881126404
Norm after input: 0.4264940023422241
Norm after each mp layer: 1.4224765300750732
Norm after each mp layer: 5.069886684417725
Norm after each mp layer: 23.61077880859375
Epoch: 140, Loss: 0.1300, Energy: 4751.9219, Train: 98.01%, Valid: 73.80%, Test: 72.50%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4264940023422241
Norm after each mp layer: 1.4224765300750732
Norm after each mp layer: 5.069886684417725
Norm after each mp layer: 23.61077880859375
Norm before input: 0.2552422881126404
Norm after input: 0.42606621980667114
Norm after each mp layer: 1.4244606494903564
Norm after each mp layer: 5.090179920196533
Norm after each mp layer: 23.742206573486328
Norm before input: 0.2552422881126404
Norm after input: 0.42606621980667114
Norm after each mp layer: 1.4244606494903564
Norm after each mp layer: 5.090179920196533
Norm after each mp layer: 23.742206573486328
Norm before input: 0.2552422881126404
Norm after input: 0.42576172947883606
Norm after each mp layer: 1.4247437715530396
Norm after each mp layer: 5.0988450050354
Norm after each mp layer: 23.7921085357666
Norm before input: 0.2552422881126404
Norm after input: 0.42576172947883606
Norm after each mp layer: 1.4247437715530396
Norm after each mp layer: 5.0988450050354
Norm after each mp layer: 23.7921085357666
Norm before input: 0.2552422881126404
Norm after input: 0.4256328344345093
Norm after each mp layer: 1.4233192205429077
Norm after each mp layer: 5.095548629760742
Norm after each mp layer: 23.756404876708984
Norm before input: 0.2552422881126404
Norm after input: 0.4256328344345093
Norm after each mp layer: 1.4233192205429077
Norm after each mp layer: 5.095548629760742
Norm after each mp layer: 23.756404876708984
Norm before input: 0.2552422881126404
Norm after input: 0.4260120987892151
Norm after each mp layer: 1.4231630563735962
Norm after each mp layer: 5.094665050506592
Norm after each mp layer: 23.71365737915039
Norm before input: 0.2552422881126404
Norm after input: 0.4260120987892151
Norm after each mp layer: 1.4231630563735962
Norm after each mp layer: 5.094665050506592
Norm after each mp layer: 23.71365737915039
Norm before input: 0.2552422881126404
Norm after input: 0.4263141453266144
Norm after each mp layer: 1.4224306344985962
Norm after each mp layer: 5.0947265625
Norm after each mp layer: 23.689151763916016
Epoch: 145, Loss: 0.1229, Energy: 4636.7744, Train: 98.26%, Valid: 73.60%, Test: 72.20%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4263141453266144
Norm after each mp layer: 1.4224306344985962
Norm after each mp layer: 5.0947265625
Norm after each mp layer: 23.689151763916016
Norm before input: 0.2552422881126404
Norm after input: 0.4263174533843994
Norm after each mp layer: 1.4213836193084717
Norm after each mp layer: 5.096712589263916
Norm after each mp layer: 23.691434860229492
Norm before input: 0.2552422881126404
Norm after input: 0.4263174533843994
Norm after each mp layer: 1.4213836193084717
Norm after each mp layer: 5.096712589263916
Norm after each mp layer: 23.691434860229492
Norm before input: 0.2552422881126404
Norm after input: 0.42626917362213135
Norm after each mp layer: 1.4220026731491089
Norm after each mp layer: 5.104696750640869
Norm after each mp layer: 23.72136878967285
Norm before input: 0.2552422881126404
Norm after input: 0.42626917362213135
Norm after each mp layer: 1.4220026731491089
Norm after each mp layer: 5.104696750640869
Norm after each mp layer: 23.72136878967285
Norm before input: 0.2552422881126404
Norm after input: 0.42595595121383667
Norm after each mp layer: 1.4223870038986206
Norm after each mp layer: 5.112104892730713
Norm after each mp layer: 23.75428581237793
Norm before input: 0.2552422881126404
Norm after input: 0.42595595121383667
Norm after each mp layer: 1.4223870038986206
Norm after each mp layer: 5.112104892730713
Norm after each mp layer: 23.75428581237793
Norm before input: 0.2552422881126404
Norm after input: 0.4254855215549469
Norm after each mp layer: 1.421929121017456
Norm after each mp layer: 5.11806058883667
Norm after each mp layer: 23.790536880493164
Norm before input: 0.2552422881126404
Norm after input: 0.4254855215549469
Norm after each mp layer: 1.421929121017456
Norm after each mp layer: 5.11806058883667
Norm after each mp layer: 23.790536880493164
Norm before input: 0.2552422881126404
Norm after input: 0.42532041668891907
Norm after each mp layer: 1.422361969947815
Norm after each mp layer: 5.127666473388672
Norm after each mp layer: 23.840087890625
Epoch: 150, Loss: 0.1164, Energy: 4472.2461, Train: 98.51%, Valid: 74.00%, Test: 72.40%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.42532041668891907
Norm after each mp layer: 1.422361969947815
Norm after each mp layer: 5.127666473388672
Norm after each mp layer: 23.840087890625
Norm before input: 0.2552422881126404
Norm after input: 0.4252583384513855
Norm after each mp layer: 1.4224029779434204
Norm after each mp layer: 5.135358810424805
Norm after each mp layer: 23.877397537231445
Norm before input: 0.2552422881126404
Norm after input: 0.4252583384513855
Norm after each mp layer: 1.4224029779434204
Norm after each mp layer: 5.135358810424805
Norm after each mp layer: 23.877397537231445
Norm before input: 0.2552422881126404
Norm after input: 0.42507874965667725
Norm after each mp layer: 1.4213910102844238
Norm after each mp layer: 5.137404441833496
Norm after each mp layer: 23.884464263916016
Norm before input: 0.2552422881126404
Norm after input: 0.42507874965667725
Norm after each mp layer: 1.4213910102844238
Norm after each mp layer: 5.137404441833496
Norm after each mp layer: 23.884464263916016
Norm before input: 0.2552422881126404
Norm after input: 0.42502447962760925
Norm after each mp layer: 1.4217238426208496
Norm after each mp layer: 5.142837047576904
Norm after each mp layer: 23.8986873626709
Norm before input: 0.2552422881126404
Norm after input: 0.42502447962760925
Norm after each mp layer: 1.4217238426208496
Norm after each mp layer: 5.142837047576904
Norm after each mp layer: 23.8986873626709
Norm before input: 0.2552422881126404
Norm after input: 0.4247559905052185
Norm after each mp layer: 1.4215854406356812
Norm after each mp layer: 5.148500919342041
Norm after each mp layer: 23.92700958251953
Norm before input: 0.2552422881126404
Norm after input: 0.4247559905052185
Norm after each mp layer: 1.4215854406356812
Norm after each mp layer: 5.148500919342041
Norm after each mp layer: 23.92700958251953
Norm before input: 0.2552422881126404
Norm after input: 0.42442503571510315
Norm after each mp layer: 1.4214403629302979
Norm after each mp layer: 5.156241416931152
Norm after each mp layer: 23.975006103515625
Epoch: 155, Loss: 0.1100, Energy: 4254.9160, Train: 98.68%, Valid: 73.60%, Test: 72.00%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.42442503571510315
Norm after each mp layer: 1.4214403629302979
Norm after each mp layer: 5.156241416931152
Norm after each mp layer: 23.975006103515625
Norm before input: 0.2552422881126404
Norm after input: 0.4243304133415222
Norm after each mp layer: 1.4223331212997437
Norm after each mp layer: 5.166940689086914
Norm after each mp layer: 24.027835845947266
Norm before input: 0.2552422881126404
Norm after input: 0.4243304133415222
Norm after each mp layer: 1.4223331212997437
Norm after each mp layer: 5.166940689086914
Norm after each mp layer: 24.027835845947266
Norm before input: 0.2552422881126404
Norm after input: 0.42417269945144653
Norm after each mp layer: 1.4216779470443726
Norm after each mp layer: 5.169677257537842
Norm after each mp layer: 24.03725242614746
Norm before input: 0.2552422881126404
Norm after input: 0.42417269945144653
Norm after each mp layer: 1.4216779470443726
Norm after each mp layer: 5.169677257537842
Norm after each mp layer: 24.03725242614746
Norm before input: 0.2552422881126404
Norm after input: 0.4242664873600006
Norm after each mp layer: 1.4215103387832642
Norm after each mp layer: 5.1718010902404785
Norm after each mp layer: 24.028772354125977
Norm before input: 0.2552422881126404
Norm after input: 0.4242664873600006
Norm after each mp layer: 1.4215103387832642
Norm after each mp layer: 5.1718010902404785
Norm after each mp layer: 24.028772354125977
Norm before input: 0.2552422881126404
Norm after input: 0.4244312047958374
Norm after each mp layer: 1.4213107824325562
Norm after each mp layer: 5.174015998840332
Norm after each mp layer: 24.019819259643555
Norm before input: 0.2552422881126404
Norm after input: 0.4244312047958374
Norm after each mp layer: 1.4213107824325562
Norm after each mp layer: 5.174015998840332
Norm after each mp layer: 24.019819259643555
Norm before input: 0.2552422881126404
Norm after input: 0.4244275987148285
Norm after each mp layer: 1.4205948114395142
Norm after each mp layer: 5.176296710968018
Norm after each mp layer: 24.02241325378418
Epoch: 160, Loss: 0.1040, Energy: 4095.4873, Train: 98.76%, Valid: 73.60%, Test: 71.80%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4244275987148285
Norm after each mp layer: 1.4205948114395142
Norm after each mp layer: 5.176296710968018
Norm after each mp layer: 24.02241325378418
Norm before input: 0.2552422881126404
Norm after input: 0.42441773414611816
Norm after each mp layer: 1.4213019609451294
Norm after each mp layer: 5.185091495513916
Norm after each mp layer: 24.058673858642578
Norm before input: 0.2552422881126404
Norm after input: 0.42441773414611816
Norm after each mp layer: 1.4213019609451294
Norm after each mp layer: 5.185091495513916
Norm after each mp layer: 24.058673858642578
Norm before input: 0.2552422881126404
Norm after input: 0.424073189496994
Norm after each mp layer: 1.421127200126648
Norm after each mp layer: 5.191812038421631
Norm after each mp layer: 24.09859848022461
Norm before input: 0.2552422881126404
Norm after input: 0.424073189496994
Norm after each mp layer: 1.421127200126648
Norm after each mp layer: 5.191812038421631
Norm after each mp layer: 24.09859848022461
Norm before input: 0.2552422881126404
Norm after input: 0.4238030016422272
Norm after each mp layer: 1.4219120740890503
Norm after each mp layer: 5.202508449554443
Norm after each mp layer: 24.15674591064453
Norm before input: 0.2552422881126404
Norm after input: 0.4238030016422272
Norm after each mp layer: 1.4219120740890503
Norm after each mp layer: 5.202508449554443
Norm after each mp layer: 24.15674591064453
Norm before input: 0.2552422881126404
Norm after input: 0.4235818684101105
Norm after each mp layer: 1.4224046468734741
Norm after each mp layer: 5.212663173675537
Norm after each mp layer: 24.21429443359375
Norm before input: 0.2552422881126404
Norm after input: 0.4235818684101105
Norm after each mp layer: 1.4224046468734741
Norm after each mp layer: 5.212663173675537
Norm after each mp layer: 24.21429443359375
Norm before input: 0.2552422881126404
Norm after input: 0.423491895198822
Norm after each mp layer: 1.4226988554000854
Norm after each mp layer: 5.221631050109863
Norm after each mp layer: 24.262231826782227
Epoch: 165, Loss: 0.0985, Energy: 3966.1897, Train: 98.84%, Valid: 73.00%, Test: 71.90%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.423491895198822
Norm after each mp layer: 1.4226988554000854
Norm after each mp layer: 5.221631050109863
Norm after each mp layer: 24.262231826782227
Norm before input: 0.2552422881126404
Norm after input: 0.42355015873908997
Norm after each mp layer: 1.4232468605041504
Norm after each mp layer: 5.2298078536987305
Norm after each mp layer: 24.296384811401367
Norm before input: 0.2552422881126404
Norm after input: 0.42355015873908997
Norm after each mp layer: 1.4232468605041504
Norm after each mp layer: 5.2298078536987305
Norm after each mp layer: 24.296384811401367
Norm before input: 0.2552422881126404
Norm after input: 0.4234656095504761
Norm after each mp layer: 1.4226789474487305
Norm after each mp layer: 5.232334136962891
Norm after each mp layer: 24.304046630859375
Norm before input: 0.2552422881126404
Norm after input: 0.4234656095504761
Norm after each mp layer: 1.4226789474487305
Norm after each mp layer: 5.232334136962891
Norm after each mp layer: 24.304046630859375
Norm before input: 0.2552422881126404
Norm after input: 0.42351233959198
Norm after each mp layer: 1.4234191179275513
Norm after each mp layer: 5.239531993865967
Norm after each mp layer: 24.327131271362305
Norm before input: 0.2552422881126404
Norm after input: 0.42351233959198
Norm after each mp layer: 1.4234191179275513
Norm after each mp layer: 5.239531993865967
Norm after each mp layer: 24.327131271362305
Norm before input: 0.2552422881126404
Norm after input: 0.42325058579444885
Norm after each mp layer: 1.4223166704177856
Norm after each mp layer: 5.241195201873779
Norm after each mp layer: 24.339557647705078
Norm before input: 0.2552422881126404
Norm after input: 0.42325058579444885
Norm after each mp layer: 1.4223166704177856
Norm after each mp layer: 5.241195201873779
Norm after each mp layer: 24.339557647705078
Norm before input: 0.2552422881126404
Norm after input: 0.42336001992225647
Norm after each mp layer: 1.4240511655807495
Norm after each mp layer: 5.25358772277832
Norm after each mp layer: 24.388612747192383
Epoch: 170, Loss: 0.0935, Energy: 3847.3250, Train: 98.76%, Valid: 72.20%, Test: 71.70%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.42336001992225647
Norm after each mp layer: 1.4240511655807495
Norm after each mp layer: 5.25358772277832
Norm after each mp layer: 24.388612747192383
Norm before input: 0.2552422881126404
Norm after input: 0.42292240262031555
Norm after each mp layer: 1.4217413663864136
Norm after each mp layer: 5.250836372375488
Norm after each mp layer: 24.388999938964844
Norm before input: 0.2552422881126404
Norm after input: 0.42292240262031555
Norm after each mp layer: 1.4217413663864136
Norm after each mp layer: 5.250836372375488
Norm after each mp layer: 24.388999938964844
Norm before input: 0.2552422881126404
Norm after input: 0.4233706593513489
Norm after each mp layer: 1.4257798194885254
Norm after each mp layer: 5.271167755126953
Norm after each mp layer: 24.46269416809082
Norm before input: 0.2552422881126404
Norm after input: 0.4233706593513489
Norm after each mp layer: 1.4257798194885254
Norm after each mp layer: 5.271167755126953
Norm after each mp layer: 24.46269416809082
Norm before input: 0.2552422881126404
Norm after input: 0.4223991632461548
Norm after each mp layer: 1.4197567701339722
Norm after each mp layer: 5.256208896636963
Norm after each mp layer: 24.437063217163086
Norm before input: 0.2552422881126404
Norm after input: 0.4223991632461548
Norm after each mp layer: 1.4197567701339722
Norm after each mp layer: 5.256208896636963
Norm after each mp layer: 24.437063217163086
Norm before input: 0.2552422881126404
Norm after input: 0.42405566573143005
Norm after each mp layer: 1.434636116027832
Norm after each mp layer: 5.322876930236816
Norm after each mp layer: 24.715593338012695
Norm before input: 0.2552422881126404
Norm after input: 0.42405566573143005
Norm after each mp layer: 1.434636116027832
Norm after each mp layer: 5.322876930236816
Norm after each mp layer: 24.715593338012695
Norm before input: 0.2552422881126404
Norm after input: 0.41699934005737305
Norm after each mp layer: 1.4369906187057495
Norm after each mp layer: 5.540098667144775
Norm after each mp layer: 27.169727325439453
Epoch: 175, Loss: 0.1352, Energy: 4127.4180, Train: 71.03%, Valid: 53.60%, Test: 53.30%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.41699934005737305
Norm after each mp layer: 1.4369906187057495
Norm after each mp layer: 5.540098667144775
Norm after each mp layer: 27.169727325439453
Norm before input: 0.2552422881126404
Norm after input: 0.4417342245578766
Norm after each mp layer: 1.572825312614441
Norm after each mp layer: 6.049478054046631
Norm after each mp layer: 28.701923370361328
Norm before input: 0.2552422881126404
Norm after input: 0.4417342245578766
Norm after each mp layer: 1.572825312614441
Norm after each mp layer: 6.049478054046631
Norm after each mp layer: 28.701923370361328
Norm before input: 0.2552422881126404
Norm after input: 0.4323374330997467
Norm after each mp layer: 1.5082108974456787
Norm after each mp layer: 5.53861141204834
Norm after each mp layer: 24.92938995361328
Norm before input: 0.2552422881126404
Norm after input: 0.4323374330997467
Norm after each mp layer: 1.5082108974456787
Norm after each mp layer: 5.53861141204834
Norm after each mp layer: 24.92938995361328
Norm before input: 0.2552422881126404
Norm after input: 0.41698727011680603
Norm after each mp layer: 1.4330506324768066
Norm after each mp layer: 5.2873148918151855
Norm after each mp layer: 23.95363426208496
Norm before input: 0.2552422881126404
Norm after input: 0.41698727011680603
Norm after each mp layer: 1.4330506324768066
Norm after each mp layer: 5.2873148918151855
Norm after each mp layer: 23.95363426208496
Norm before input: 0.2552422881126404
Norm after input: 0.40937554836273193
Norm after each mp layer: 1.4410771131515503
Norm after each mp layer: 5.445000171661377
Norm after each mp layer: 25.73348045349121
Norm before input: 0.2552422881126404
Norm after input: 0.40937554836273193
Norm after each mp layer: 1.4410771131515503
Norm after each mp layer: 5.445000171661377
Norm after each mp layer: 25.73348045349121
Norm before input: 0.2552422881126404
Norm after input: 0.41046857833862305
Norm after each mp layer: 1.4314548969268799
Norm after each mp layer: 5.212534427642822
Norm after each mp layer: 23.594697952270508
Epoch: 180, Loss: 1.2977, Energy: 19307.2148, Train: 76.32%, Valid: 62.00%, Test: 59.80%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.41046857833862305
Norm after each mp layer: 1.4314548969268799
Norm after each mp layer: 5.212534427642822
Norm after each mp layer: 23.594697952270508
Norm before input: 0.2552422881126404
Norm after input: 0.4165653884410858
Norm after each mp layer: 1.416652798652649
Norm after each mp layer: 4.885961055755615
Norm after each mp layer: 20.50357437133789
Norm before input: 0.2552422881126404
Norm after input: 0.4165653884410858
Norm after each mp layer: 1.416652798652649
Norm after each mp layer: 4.885960578918457
Norm after each mp layer: 20.50357437133789
Norm before input: 0.2552422881126404
Norm after input: 0.4192134737968445
Norm after each mp layer: 1.4177560806274414
Norm after each mp layer: 4.659031867980957
Norm after each mp layer: 18.590917587280273
Norm before input: 0.2552422881126404
Norm after input: 0.4192134737968445
Norm after each mp layer: 1.4177560806274414
Norm after each mp layer: 4.659031867980957
Norm after each mp layer: 18.590917587280273
Norm before input: 0.2552422881126404
Norm after input: 0.4194530248641968
Norm after each mp layer: 1.4553718566894531
Norm after each mp layer: 4.670345306396484
Norm after each mp layer: 18.585174560546875
Norm before input: 0.2552422881126404
Norm after input: 0.4194530248641968
Norm after each mp layer: 1.4553718566894531
Norm after each mp layer: 4.670345306396484
Norm after each mp layer: 18.585174560546875
Norm before input: 0.2552422881126404
Norm after input: 0.42179518938064575
Norm after each mp layer: 1.506577491760254
Norm after each mp layer: 4.77157735824585
Norm after each mp layer: 19.08949089050293
Norm before input: 0.2552422881126404
Norm after input: 0.42179518938064575
Norm after each mp layer: 1.506577491760254
Norm after each mp layer: 4.77157735824585
Norm after each mp layer: 19.08949089050293
Norm before input: 0.2552422881126404
Norm after input: 0.4245894253253937
Norm after each mp layer: 1.5449622869491577
Norm after each mp layer: 4.8283891677856445
Norm after each mp layer: 19.220449447631836
Epoch: 185, Loss: 0.5171, Energy: 6525.4473, Train: 82.20%, Valid: 70.60%, Test: 69.80%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4245894253253937
Norm after each mp layer: 1.5449622869491577
Norm after each mp layer: 4.8283891677856445
Norm after each mp layer: 19.220449447631836
Norm before input: 0.2552422881126404
Norm after input: 0.4265042543411255
Norm after each mp layer: 1.5585832595825195
Norm after each mp layer: 4.782304763793945
Norm after each mp layer: 18.58678436279297
Norm before input: 0.2552422881126404
Norm after input: 0.4265042543411255
Norm after each mp layer: 1.5585832595825195
Norm after each mp layer: 4.782304286956787
Norm after each mp layer: 18.58678436279297
Norm before input: 0.2552422881126404
Norm after input: 0.4281531274318695
Norm after each mp layer: 1.558571696281433
Norm after each mp layer: 4.710317134857178
Norm after each mp layer: 17.697799682617188
Norm before input: 0.2552422881126404
Norm after input: 0.4281531274318695
Norm after each mp layer: 1.558571696281433
Norm after each mp layer: 4.710317134857178
Norm after each mp layer: 17.697799682617188
Norm before input: 0.2552422881126404
Norm after input: 0.42997264862060547
Norm after each mp layer: 1.5578652620315552
Norm after each mp layer: 4.686800479888916
Norm after each mp layer: 17.270244598388672
Norm before input: 0.2552422881126404
Norm after input: 0.42997264862060547
Norm after each mp layer: 1.5578652620315552
Norm after each mp layer: 4.686800479888916
Norm after each mp layer: 17.270244598388672
Norm before input: 0.2552422881126404
Norm after input: 0.43158742785453796
Norm after each mp layer: 1.5625388622283936
Norm after each mp layer: 4.7154083251953125
Norm after each mp layer: 17.408443450927734
Norm before input: 0.2552422881126404
Norm after input: 0.43158742785453796
Norm after each mp layer: 1.5625388622283936
Norm after each mp layer: 4.7154083251953125
Norm after each mp layer: 17.408443450927734
Norm before input: 0.2552422881126404
Norm after input: 0.43267297744750977
Norm after each mp layer: 1.5732675790786743
Norm after each mp layer: 4.763469696044922
Norm after each mp layer: 17.727882385253906
Epoch: 190, Loss: 0.3857, Energy: 8714.0254, Train: 93.71%, Valid: 79.60%, Test: 76.70%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.43267297744750977
Norm after each mp layer: 1.5732675790786743
Norm after each mp layer: 4.763469696044922
Norm after each mp layer: 17.727882385253906
Norm before input: 0.2552422881126404
Norm after input: 0.4333816170692444
Norm after each mp layer: 1.5840494632720947
Norm after each mp layer: 4.785125255584717
Norm after each mp layer: 17.850271224975586
Norm before input: 0.2552422881126404
Norm after input: 0.4333816170692444
Norm after each mp layer: 1.5840494632720947
Norm after each mp layer: 4.785125255584717
Norm after each mp layer: 17.850271224975586
Norm before input: 0.2552422881126404
Norm after input: 0.4340488016605377
Norm after each mp layer: 1.591493010520935
Norm after each mp layer: 4.770268440246582
Norm after each mp layer: 17.68472671508789
Norm before input: 0.2552422881126404
Norm after input: 0.4340488016605377
Norm after each mp layer: 1.591493010520935
Norm after each mp layer: 4.770268440246582
Norm after each mp layer: 17.68472671508789
Norm before input: 0.2552422881126404
Norm after input: 0.4350728392601013
Norm after each mp layer: 1.6000031232833862
Norm after each mp layer: 4.768739223480225
Norm after each mp layer: 17.5390625
Norm before input: 0.2552422881126404
Norm after input: 0.4350728392601013
Norm after each mp layer: 1.6000031232833862
Norm after each mp layer: 4.768739223480225
Norm after each mp layer: 17.5390625
Norm before input: 0.2552422881126404
Norm after input: 0.4364277422428131
Norm after each mp layer: 1.6108765602111816
Norm after each mp layer: 4.800485610961914
Norm after each mp layer: 17.576045989990234
Norm before input: 0.2552422881126404
Norm after input: 0.4364277422428131
Norm after each mp layer: 1.6108765602111816
Norm after each mp layer: 4.800485610961914
Norm after each mp layer: 17.576045989990234
Norm before input: 0.2552422881126404
Norm after input: 0.43751442432403564
Norm after each mp layer: 1.6174094676971436
Norm after each mp layer: 4.795187950134277
Norm after each mp layer: 17.5582275390625
Epoch: 195, Loss: 0.3394, Energy: 6744.9644, Train: 93.63%, Valid: 79.60%, Test: 77.40%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.43751442432403564
Norm after each mp layer: 1.6174094676971436
Norm after each mp layer: 4.795187950134277
Norm after each mp layer: 17.5582275390625
Norm before input: 0.2552422881126404
Norm after input: 0.4388178288936615
Norm after each mp layer: 1.6259069442749023
Norm after each mp layer: 4.810483932495117
Norm after each mp layer: 17.6600284576416
Norm before input: 0.2552422881126404
Norm after input: 0.4388178288936615
Norm after each mp layer: 1.6259069442749023
Norm after each mp layer: 4.810483932495117
Norm after each mp layer: 17.6600284576416
Norm before input: 0.2552422881126404
Norm after input: 0.4404243528842926
Norm after each mp layer: 1.6341190338134766
Norm after each mp layer: 4.833389759063721
Norm after each mp layer: 17.741544723510742
Norm before input: 0.2552422881126404
Norm after input: 0.4404243528842926
Norm after each mp layer: 1.6341190338134766
Norm after each mp layer: 4.833389759063721
Norm after each mp layer: 17.741544723510742
Norm before input: 0.2552422881126404
Norm after input: 0.44222310185432434
Norm after each mp layer: 1.639251708984375
Norm after each mp layer: 4.840236186981201
Norm after each mp layer: 17.644554138183594
Norm before input: 0.2552422881126404
Norm after input: 0.44222310185432434
Norm after each mp layer: 1.639251708984375
Norm after each mp layer: 4.840236186981201
Norm after each mp layer: 17.644554138183594
Norm before input: 0.2552422881126404
Norm after input: 0.4441358745098114
Norm after each mp layer: 1.64207124710083
Norm after each mp layer: 4.830447196960449
Norm after each mp layer: 17.381013870239258
Norm before input: 0.2552422881126404
Norm after input: 0.4441358745098114
Norm after each mp layer: 1.64207124710083
Norm after each mp layer: 4.830447196960449
Norm after each mp layer: 17.381013870239258
Norm before input: 0.2552422881126404
Norm after input: 0.44609421491622925
Norm after each mp layer: 1.6456310749053955
Norm after each mp layer: 4.822015285491943
Norm after each mp layer: 17.09617805480957
Epoch: 200, Loss: 0.2424, Energy: 8269.8281, Train: 94.70%, Valid: 80.20%, Test: 76.80%, Best Valid: 81.20%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44609421491622925
Norm after each mp layer: 1.6456310749053955
Norm after each mp layer: 4.822015285491943
Norm after each mp layer: 17.09617805480957
Norm before input: 0.2552422881126404
Norm after input: 0.44792887568473816
Norm after each mp layer: 1.6513335704803467
Norm after each mp layer: 4.82455587387085
Norm after each mp layer: 16.89432144165039
Norm before input: 0.2552422881126404
Norm after input: 0.44792887568473816
Norm after each mp layer: 1.6513335704803467
Norm after each mp layer: 4.82455587387085
Norm after each mp layer: 16.89432144165039
Norm before input: 0.2552422881126404
Norm after input: 0.4494702219963074
Norm after each mp layer: 1.6588579416275024
Norm after each mp layer: 4.839053630828857
Norm after each mp layer: 16.814922332763672
Norm before input: 0.2552422881126404
Norm after input: 0.4494702219963074
Norm after each mp layer: 1.6588579416275024
Norm after each mp layer: 4.839053630828857
Norm after each mp layer: 16.814922332763672
Norm before input: 0.2552422881126404
Norm after input: 0.45061832666397095
Norm after each mp layer: 1.6672412157058716
Norm after each mp layer: 4.864731311798096
Norm after each mp layer: 16.86229705810547
Norm before input: 0.2552422881126404
Norm after input: 0.45061832666397095
Norm after each mp layer: 1.6672412157058716
Norm after each mp layer: 4.864731311798096
Norm after each mp layer: 16.86229705810547
Norm before input: 0.2552422881126404
Norm after input: 0.4512970447540283
Norm after each mp layer: 1.6753302812576294
Norm after each mp layer: 4.8984832763671875
Norm after each mp layer: 17.00722312927246
Norm before input: 0.2552422881126404
Norm after input: 0.4512970447540283
Norm after each mp layer: 1.6753302812576294
Norm after each mp layer: 4.8984832763671875
Norm after each mp layer: 17.00722312927246
Norm before input: 0.2552422881126404
Norm after input: 0.4515136480331421
Norm after each mp layer: 1.6816399097442627
Norm after each mp layer: 4.934173583984375
Norm after each mp layer: 17.194799423217773
Epoch: 205, Loss: 0.2127, Energy: 5402.2852, Train: 95.86%, Valid: 81.20%, Test: 78.00%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4515136480331421
Norm after each mp layer: 1.6816399097442627
Norm after each mp layer: 4.934173583984375
Norm after each mp layer: 17.194799423217773
Norm before input: 0.2552422881126404
Norm after input: 0.45149755477905273
Norm after each mp layer: 1.6857447624206543
Norm after each mp layer: 4.969052791595459
Norm after each mp layer: 17.38508415222168
Norm before input: 0.2552422881126404
Norm after input: 0.45149755477905273
Norm after each mp layer: 1.6857447624206543
Norm after each mp layer: 4.969052791595459
Norm after each mp layer: 17.38508415222168
Norm before input: 0.2552422881126404
Norm after input: 0.4514842927455902
Norm after each mp layer: 1.6878602504730225
Norm after each mp layer: 5.001468181610107
Norm after each mp layer: 17.554901123046875
Norm before input: 0.2552422881126404
Norm after input: 0.4514842927455902
Norm after each mp layer: 1.6878602504730225
Norm after each mp layer: 5.001468181610107
Norm after each mp layer: 17.554901123046875
Norm before input: 0.2552422881126404
Norm after input: 0.4515708386898041
Norm after each mp layer: 1.688362956047058
Norm after each mp layer: 5.029838562011719
Norm after each mp layer: 17.695728302001953
Norm before input: 0.2552422881126404
Norm after input: 0.4515708386898041
Norm after each mp layer: 1.688362956047058
Norm after each mp layer: 5.029838562011719
Norm after each mp layer: 17.695728302001953
Norm before input: 0.2552422881126404
Norm after input: 0.45173898339271545
Norm after each mp layer: 1.6878607273101807
Norm after each mp layer: 5.055006980895996
Norm after each mp layer: 17.822614669799805
Norm before input: 0.2552422881126404
Norm after input: 0.45173898339271545
Norm after each mp layer: 1.6878607273101807
Norm after each mp layer: 5.055006980895996
Norm after each mp layer: 17.822614669799805
Norm before input: 0.2552422881126404
Norm after input: 0.45190420746803284
Norm after each mp layer: 1.6869055032730103
Norm after each mp layer: 5.079247951507568
Norm after each mp layer: 17.965394973754883
Epoch: 210, Loss: 0.1821, Energy: 6118.3564, Train: 96.36%, Valid: 79.80%, Test: 77.80%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.45190420746803284
Norm after each mp layer: 1.6869055032730103
Norm after each mp layer: 5.079247951507568
Norm after each mp layer: 17.965394973754883
Norm before input: 0.2552422881126404
Norm after input: 0.45196595788002014
Norm after each mp layer: 1.6857547760009766
Norm after each mp layer: 5.1020894050598145
Norm after each mp layer: 18.135053634643555
Norm before input: 0.2552422881126404
Norm after input: 0.45196595788002014
Norm after each mp layer: 1.6857547760009766
Norm after each mp layer: 5.1020894050598145
Norm after each mp layer: 18.135053634643555
Norm before input: 0.2552422881126404
Norm after input: 0.45181503891944885
Norm after each mp layer: 1.6841557025909424
Norm after each mp layer: 5.117764472961426
Norm after each mp layer: 18.303991317749023
Norm before input: 0.2552422881126404
Norm after input: 0.45181503891944885
Norm after each mp layer: 1.6841557025909424
Norm after each mp layer: 5.117764472961426
Norm after each mp layer: 18.303991317749023
Norm before input: 0.2552422881126404
Norm after input: 0.45143070816993713
Norm after each mp layer: 1.6818926334381104
Norm after each mp layer: 5.1208038330078125
Norm after each mp layer: 18.435375213623047
Norm before input: 0.2552422881126404
Norm after input: 0.45143070816993713
Norm after each mp layer: 1.6818926334381104
Norm after each mp layer: 5.1208038330078125
Norm after each mp layer: 18.435375213623047
Norm before input: 0.2552422881126404
Norm after input: 0.4509310722351074
Norm after each mp layer: 1.67922842502594
Norm after each mp layer: 5.11295223236084
Norm after each mp layer: 18.524024963378906
Norm before input: 0.2552422881126404
Norm after input: 0.4509310722351074
Norm after each mp layer: 1.67922842502594
Norm after each mp layer: 5.112951755523682
Norm after each mp layer: 18.524024963378906
Norm before input: 0.2552422881126404
Norm after input: 0.45050308108329773
Norm after each mp layer: 1.6765334606170654
Norm after each mp layer: 5.099977970123291
Norm after each mp layer: 18.58136749267578
Epoch: 215, Loss: 0.1665, Energy: 5863.1963, Train: 96.85%, Valid: 79.60%, Test: 78.00%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.45050308108329773
Norm after each mp layer: 1.6765334606170654
Norm after each mp layer: 5.099977970123291
Norm after each mp layer: 18.58136749267578
Norm before input: 0.2552422881126404
Norm after input: 0.45030033588409424
Norm after each mp layer: 1.6737267971038818
Norm after each mp layer: 5.084893703460693
Norm after each mp layer: 18.60159683227539
Norm before input: 0.2552422881126404
Norm after input: 0.45030033588409424
Norm after each mp layer: 1.6737267971038818
Norm after each mp layer: 5.084893703460693
Norm after each mp layer: 18.60159683227539
Norm before input: 0.2552422881126404
Norm after input: 0.4503835439682007
Norm after each mp layer: 1.6707634925842285
Norm after each mp layer: 5.068592071533203
Norm after each mp layer: 18.575458526611328
Norm before input: 0.2552422881126404
Norm after input: 0.4503835439682007
Norm after each mp layer: 1.6707634925842285
Norm after each mp layer: 5.068592071533203
Norm after each mp layer: 18.575458526611328
Norm before input: 0.2552422881126404
Norm after input: 0.45074641704559326
Norm after each mp layer: 1.6677454710006714
Norm after each mp layer: 5.051981449127197
Norm after each mp layer: 18.50526237487793
Norm before input: 0.2552422881126404
Norm after input: 0.45074641704559326
Norm after each mp layer: 1.6677454710006714
Norm after each mp layer: 5.051981449127197
Norm after each mp layer: 18.50526237487793
Norm before input: 0.2552422881126404
Norm after input: 0.45131972432136536
Norm after each mp layer: 1.6648962497711182
Norm after each mp layer: 5.036623001098633
Norm after each mp layer: 18.408449172973633
Norm before input: 0.2552422881126404
Norm after input: 0.45131972432136536
Norm after each mp layer: 1.6648962497711182
Norm after each mp layer: 5.036623001098633
Norm after each mp layer: 18.408447265625
Norm before input: 0.2552422881126404
Norm after input: 0.45197299122810364
Norm after each mp layer: 1.6624679565429688
Norm after each mp layer: 5.024142742156982
Norm after each mp layer: 18.310298919677734
Epoch: 220, Loss: 0.1541, Energy: 5150.1152, Train: 97.52%, Valid: 78.80%, Test: 77.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.45197299122810364
Norm after each mp layer: 1.6624679565429688
Norm after each mp layer: 5.024142742156982
Norm after each mp layer: 18.310298919677734
Norm before input: 0.2552422881126404
Norm after input: 0.45253393054008484
Norm after each mp layer: 1.6606724262237549
Norm after each mp layer: 5.015841960906982
Norm after each mp layer: 18.236257553100586
Norm before input: 0.2552422881126404
Norm after input: 0.45253393054008484
Norm after each mp layer: 1.6606724262237549
Norm after each mp layer: 5.015841960906982
Norm after each mp layer: 18.236257553100586
Norm before input: 0.2552422881126404
Norm after input: 0.452839732170105
Norm after each mp layer: 1.659719705581665
Norm after each mp layer: 5.013383865356445
Norm after each mp layer: 18.211915969848633
Norm before input: 0.2552422881126404
Norm after input: 0.452839732170105
Norm after each mp layer: 1.659719705581665
Norm after each mp layer: 5.013383865356445
Norm after each mp layer: 18.211915969848633
Norm before input: 0.2552422881126404
Norm after input: 0.4528212547302246
Norm after each mp layer: 1.6598161458969116
Norm after each mp layer: 5.018671989440918
Norm after each mp layer: 18.257762908935547
Norm before input: 0.2552422881126404
Norm after input: 0.4528212547302246
Norm after each mp layer: 1.6598161458969116
Norm after each mp layer: 5.018671989440918
Norm after each mp layer: 18.257762908935547
Norm before input: 0.2552422881126404
Norm after input: 0.4525541365146637
Norm after each mp layer: 1.6609604358673096
Norm after each mp layer: 5.031903266906738
Norm after each mp layer: 18.372316360473633
Norm before input: 0.2552422881126404
Norm after input: 0.4525541365146637
Norm after each mp layer: 1.6609604358673096
Norm after each mp layer: 5.031903266906738
Norm after each mp layer: 18.372316360473633
Norm before input: 0.2552422881126404
Norm after input: 0.4521912634372711
Norm after each mp layer: 1.662842869758606
Norm after each mp layer: 5.050817966461182
Norm after each mp layer: 18.529827117919922
Epoch: 225, Loss: 0.1433, Energy: 4743.5669, Train: 97.60%, Valid: 80.60%, Test: 77.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4521912634372711
Norm after each mp layer: 1.662842869758606
Norm after each mp layer: 5.050817966461182
Norm after each mp layer: 18.529827117919922
Norm before input: 0.2552422881126404
Norm after input: 0.45187124609947205
Norm after each mp layer: 1.6648547649383545
Norm after each mp layer: 5.071298122406006
Norm after each mp layer: 18.69061279296875
Norm before input: 0.2552422881126404
Norm after input: 0.45187124609947205
Norm after each mp layer: 1.6648547649383545
Norm after each mp layer: 5.071298122406006
Norm after each mp layer: 18.69061279296875
Norm before input: 0.2552422881126404
Norm after input: 0.45167458057403564
Norm after each mp layer: 1.666295051574707
Norm after each mp layer: 5.088777542114258
Norm after each mp layer: 18.815807342529297
Norm before input: 0.2552422881126404
Norm after input: 0.45167458057403564
Norm after each mp layer: 1.666295051574707
Norm after each mp layer: 5.088777542114258
Norm after each mp layer: 18.815807342529297
Norm before input: 0.2552422881126404
Norm after input: 0.45162129402160645
Norm after each mp layer: 1.6666929721832275
Norm after each mp layer: 5.100174903869629
Norm after each mp layer: 18.882509231567383
Norm before input: 0.2552422881126404
Norm after input: 0.45162129402160645
Norm after each mp layer: 1.6666929721832275
Norm after each mp layer: 5.100174903869629
Norm after each mp layer: 18.882509231567383
Norm before input: 0.2552422881126404
Norm after input: 0.4516826868057251
Norm after each mp layer: 1.666024923324585
Norm after each mp layer: 5.105320930480957
Norm after each mp layer: 18.893230438232422
Norm before input: 0.2552422881126404
Norm after input: 0.4516826868057251
Norm after each mp layer: 1.666024923324585
Norm after each mp layer: 5.105320930480957
Norm after each mp layer: 18.893230438232422
Norm before input: 0.2552422881126404
Norm after input: 0.45179295539855957
Norm after each mp layer: 1.6646651029586792
Norm after each mp layer: 5.106578826904297
Norm after each mp layer: 18.8724365234375
Epoch: 230, Loss: 0.1358, Energy: 4989.2466, Train: 98.01%, Valid: 80.20%, Test: 77.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.45179295539855957
Norm after each mp layer: 1.6646651029586792
Norm after each mp layer: 5.106578826904297
Norm after each mp layer: 18.8724365234375
Norm before input: 0.2552422881126404
Norm after input: 0.45186343789100647
Norm after each mp layer: 1.6630792617797852
Norm after each mp layer: 5.106759548187256
Norm after each mp layer: 18.85040283203125
Norm before input: 0.2552422881126404
Norm after input: 0.45186343789100647
Norm after each mp layer: 1.6630792617797852
Norm after each mp layer: 5.106759548187256
Norm after each mp layer: 18.85040283203125
Norm before input: 0.2552422881126404
Norm after input: 0.45181646943092346
Norm after each mp layer: 1.661557912826538
Norm after each mp layer: 5.107511043548584
Norm after each mp layer: 18.84746551513672
Norm before input: 0.2552422881126404
Norm after input: 0.45181646943092346
Norm after each mp layer: 1.661557912826538
Norm after each mp layer: 5.107511043548584
Norm after each mp layer: 18.84746551513672
Norm before input: 0.2552422881126404
Norm after input: 0.4516252875328064
Norm after each mp layer: 1.660164475440979
Norm after each mp layer: 5.109097957611084
Norm after each mp layer: 18.867656707763672
Norm before input: 0.2552422881126404
Norm after input: 0.4516252875328064
Norm after each mp layer: 1.660164475440979
Norm after each mp layer: 5.109097957611084
Norm after each mp layer: 18.867656707763672
Norm before input: 0.2552422881126404
Norm after input: 0.45132577419281006
Norm after each mp layer: 1.6588053703308105
Norm after each mp layer: 5.110777854919434
Norm after each mp layer: 18.900876998901367
Norm before input: 0.2552422881126404
Norm after input: 0.45132577419281006
Norm after each mp layer: 1.6588053703308105
Norm after each mp layer: 5.110777854919434
Norm after each mp layer: 18.900876998901367
Norm before input: 0.2552422881126404
Norm after input: 0.4509911835193634
Norm after each mp layer: 1.6573539972305298
Norm after each mp layer: 5.111566066741943
Norm after each mp layer: 18.931617736816406
Epoch: 235, Loss: 0.1289, Energy: 4821.5552, Train: 98.10%, Valid: 80.00%, Test: 78.00%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4509911835193634
Norm after each mp layer: 1.6573539972305298
Norm after each mp layer: 5.111566066741943
Norm after each mp layer: 18.931617736816406
Norm before input: 0.2552422881126404
Norm after input: 0.4507009983062744
Norm after each mp layer: 1.6557565927505493
Norm after each mp layer: 5.111029148101807
Norm after each mp layer: 18.948057174682617
Norm before input: 0.2552422881126404
Norm after input: 0.4507009983062744
Norm after each mp layer: 1.6557565927505493
Norm after each mp layer: 5.111029148101807
Norm after each mp layer: 18.948057174682617
Norm before input: 0.2552422881126404
Norm after input: 0.45051291584968567
Norm after each mp layer: 1.6540528535842896
Norm after each mp layer: 5.109440326690674
Norm after each mp layer: 18.946041107177734
Norm before input: 0.2552422881126404
Norm after input: 0.45051291584968567
Norm after each mp layer: 1.6540528535842896
Norm after each mp layer: 5.109440326690674
Norm after each mp layer: 18.946041107177734
Norm before input: 0.2552422881126404
Norm after input: 0.45044323801994324
Norm after each mp layer: 1.6523406505584717
Norm after each mp layer: 5.107502460479736
Norm after each mp layer: 18.92930030822754
Norm before input: 0.2552422881126404
Norm after input: 0.45044323801994324
Norm after each mp layer: 1.6523406505584717
Norm after each mp layer: 5.1075029373168945
Norm after each mp layer: 18.929298400878906
Norm before input: 0.2552422881126404
Norm after input: 0.45046377182006836
Norm after each mp layer: 1.6507350206375122
Norm after each mp layer: 5.1060638427734375
Norm after each mp layer: 18.90760040283203
Norm before input: 0.2552422881126404
Norm after input: 0.45046377182006836
Norm after each mp layer: 1.6507350206375122
Norm after each mp layer: 5.1060638427734375
Norm after each mp layer: 18.90760040283203
Norm before input: 0.2552422881126404
Norm after input: 0.45051705837249756
Norm after each mp layer: 1.6493271589279175
Norm after each mp layer: 5.105770587921143
Norm after each mp layer: 18.892141342163086
Epoch: 240, Loss: 0.1229, Energy: 4713.5454, Train: 98.34%, Valid: 79.80%, Test: 77.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.45051705837249756
Norm after each mp layer: 1.6493271589279175
Norm after each mp layer: 5.105770587921143
Norm after each mp layer: 18.892141342163086
Norm before input: 0.2552422881126404
Norm after input: 0.450538694858551
Norm after each mp layer: 1.6481444835662842
Norm after each mp layer: 5.106655597686768
Norm after each mp layer: 18.889915466308594
Norm before input: 0.2552422881126404
Norm after input: 0.450538694858551
Norm after each mp layer: 1.6481444835662842
Norm after each mp layer: 5.106655597686768
Norm after each mp layer: 18.889915466308594
Norm before input: 0.2552422881126404
Norm after input: 0.4504844844341278
Norm after each mp layer: 1.647109866142273
Norm after each mp layer: 5.107941150665283
Norm after each mp layer: 18.899717330932617
Norm before input: 0.2552422881126404
Norm after input: 0.4504844844341278
Norm after each mp layer: 1.647109866142273
Norm after each mp layer: 5.107941150665283
Norm after each mp layer: 18.899717330932617
Norm before input: 0.2552422881126404
Norm after input: 0.45035335421562195
Norm after each mp layer: 1.6460896730422974
Norm after each mp layer: 5.108644485473633
Norm after each mp layer: 18.91437339782715
Norm before input: 0.2552422881126404
Norm after input: 0.45035335421562195
Norm after each mp layer: 1.6460896730422974
Norm after each mp layer: 5.108644485473633
Norm after each mp layer: 18.91437339782715
Norm before input: 0.2552422881126404
Norm after input: 0.450186163187027
Norm after each mp layer: 1.645017147064209
Norm after each mp layer: 5.1085405349731445
Norm after each mp layer: 18.92754364013672
Norm before input: 0.2552422881126404
Norm after input: 0.450186163187027
Norm after each mp layer: 1.645017147064209
Norm after each mp layer: 5.1085405349731445
Norm after each mp layer: 18.92754364013672
Norm before input: 0.2552422881126404
Norm after input: 0.4500373899936676
Norm after each mp layer: 1.6438875198364258
Norm after each mp layer: 5.107914924621582
Norm after each mp layer: 18.934814453125
Epoch: 245, Loss: 0.1177, Energy: 4543.6445, Train: 98.43%, Valid: 79.20%, Test: 77.10%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4500373899936676
Norm after each mp layer: 1.6438875198364258
Norm after each mp layer: 5.107914924621582
Norm after each mp layer: 18.934814453125
Norm before input: 0.2552422881126404
Norm after input: 0.44994404911994934
Norm after each mp layer: 1.6426960229873657
Norm after each mp layer: 5.1069440841674805
Norm after each mp layer: 18.93266487121582
Norm before input: 0.2552422881126404
Norm after input: 0.44994404911994934
Norm after each mp layer: 1.6426960229873657
Norm after each mp layer: 5.1069440841674805
Norm after each mp layer: 18.93266487121582
Norm before input: 0.2552422881126404
Norm after input: 0.449911504983902
Norm after each mp layer: 1.6414419412612915
Norm after each mp layer: 5.105736255645752
Norm after each mp layer: 18.920337677001953
Norm before input: 0.2552422881126404
Norm after input: 0.449911504983902
Norm after each mp layer: 1.6414419412612915
Norm after each mp layer: 5.105736255645752
Norm after each mp layer: 18.920337677001953
Norm before input: 0.2552422881126404
Norm after input: 0.4499145448207855
Norm after each mp layer: 1.6401370763778687
Norm after each mp layer: 5.1044721603393555
Norm after each mp layer: 18.900924682617188
Norm before input: 0.2552422881126404
Norm after input: 0.4499145448207855
Norm after each mp layer: 1.6401370763778687
Norm after each mp layer: 5.1044721603393555
Norm after each mp layer: 18.900924682617188
Norm before input: 0.2552422881126404
Norm after input: 0.4499085545539856
Norm after each mp layer: 1.6387972831726074
Norm after each mp layer: 5.103344917297363
Norm after each mp layer: 18.87997817993164
Norm before input: 0.2552422881126404
Norm after input: 0.4499085545539856
Norm after each mp layer: 1.6387972831726074
Norm after each mp layer: 5.103344440460205
Norm after each mp layer: 18.87997817993164
Norm before input: 0.2552422881126404
Norm after input: 0.44984880089759827
Norm after each mp layer: 1.637447476387024
Norm after each mp layer: 5.102540969848633
Norm after each mp layer: 18.863597869873047
Epoch: 250, Loss: 0.1130, Energy: 4399.7358, Train: 98.51%, Valid: 79.20%, Test: 77.10%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44984880089759827
Norm after each mp layer: 1.637447476387024
Norm after each mp layer: 5.102540969848633
Norm after each mp layer: 18.863597869873047
Norm before input: 0.2552422881126404
Norm after input: 0.4497116506099701
Norm after each mp layer: 1.6361303329467773
Norm after each mp layer: 5.1023173332214355
Norm after each mp layer: 18.856903076171875
Norm before input: 0.2552422881126404
Norm after input: 0.4497116506099701
Norm after each mp layer: 1.6361303329467773
Norm after each mp layer: 5.1023173332214355
Norm after each mp layer: 18.856903076171875
Norm before input: 0.2552422881126404
Norm after input: 0.4495074152946472
Norm after each mp layer: 1.634886384010315
Norm after each mp layer: 5.102870464324951
Norm after each mp layer: 18.86168098449707
Norm before input: 0.2552422881126404
Norm after input: 0.4495074152946472
Norm after each mp layer: 1.634886384010315
Norm after each mp layer: 5.102870464324951
Norm after each mp layer: 18.86168098449707
Norm before input: 0.2552422881126404
Norm after input: 0.4492751657962799
Norm after each mp layer: 1.633711338043213
Norm after each mp layer: 5.104053974151611
Norm after each mp layer: 18.874347686767578
Norm before input: 0.2552422881126404
Norm after input: 0.4492751657962799
Norm after each mp layer: 1.633711338043213
Norm after each mp layer: 5.104053974151611
Norm after each mp layer: 18.874347686767578
Norm before input: 0.2552422881126404
Norm after input: 0.4490642249584198
Norm after each mp layer: 1.6325488090515137
Norm after each mp layer: 5.1053314208984375
Norm after each mp layer: 18.886850357055664
Norm before input: 0.2552422881126404
Norm after input: 0.4490642249584198
Norm after each mp layer: 1.6325488090515137
Norm after each mp layer: 5.1053314208984375
Norm after each mp layer: 18.886850357055664
Norm before input: 0.2552422881126404
Norm after input: 0.4489131569862366
Norm after each mp layer: 1.6313220262527466
Norm after each mp layer: 5.106027603149414
Norm after each mp layer: 18.890430450439453
Epoch: 255, Loss: 0.1088, Energy: 4288.9829, Train: 98.59%, Valid: 79.40%, Test: 77.80%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4489131569862366
Norm after each mp layer: 1.6313220262527466
Norm after each mp layer: 5.106027603149414
Norm after each mp layer: 18.89042854309082
Norm before input: 0.2552422881126404
Norm after input: 0.4488361179828644
Norm after each mp layer: 1.629975438117981
Norm after each mp layer: 5.105652332305908
Norm after each mp layer: 18.87973976135254
Norm before input: 0.2552422881126404
Norm after input: 0.4488361179828644
Norm after each mp layer: 1.629975438117981
Norm after each mp layer: 5.105652332305908
Norm after each mp layer: 18.879741668701172
Norm before input: 0.2552422881126404
Norm after input: 0.44881975650787354
Norm after each mp layer: 1.628503441810608
Norm after each mp layer: 5.104140281677246
Norm after each mp layer: 18.855321884155273
Norm before input: 0.2552422881126404
Norm after input: 0.44881975650787354
Norm after each mp layer: 1.628503441810608
Norm after each mp layer: 5.104140281677246
Norm after each mp layer: 18.855321884155273
Norm before input: 0.2552422881126404
Norm after input: 0.44883081316947937
Norm after each mp layer: 1.6269601583480835
Norm after each mp layer: 5.10189962387085
Norm after each mp layer: 18.823383331298828
Norm before input: 0.2552422881126404
Norm after input: 0.44883081316947937
Norm after each mp layer: 1.6269601583480835
Norm after each mp layer: 5.10189962387085
Norm after each mp layer: 18.823383331298828
Norm before input: 0.2552422881126404
Norm after input: 0.44883090257644653
Norm after each mp layer: 1.6254348754882812
Norm after each mp layer: 5.099628925323486
Norm after each mp layer: 18.792875289916992
Norm before input: 0.2552422881126404
Norm after input: 0.44883090257644653
Norm after each mp layer: 1.6254348754882812
Norm after each mp layer: 5.099628925323486
Norm after each mp layer: 18.792875289916992
Norm before input: 0.2552422881126404
Norm after input: 0.4487927258014679
Norm after each mp layer: 1.6240053176879883
Norm after each mp layer: 5.097947597503662
Norm after each mp layer: 18.77107810974121
Epoch: 260, Loss: 0.1049, Energy: 4132.8203, Train: 98.68%, Valid: 79.40%, Test: 77.40%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4487927258014679
Norm after each mp layer: 1.6240053176879883
Norm after each mp layer: 5.097947597503662
Norm after each mp layer: 18.77107810974121
Norm before input: 0.2552422881126404
Norm after input: 0.4487111568450928
Norm after each mp layer: 1.6227014064788818
Norm after each mp layer: 5.09709358215332
Norm after each mp layer: 18.760250091552734
Norm before input: 0.2552422881126404
Norm after input: 0.4487111568450928
Norm after each mp layer: 1.6227014064788818
Norm after each mp layer: 5.09709358215332
Norm after each mp layer: 18.760250091552734
Norm before input: 0.2552422881126404
Norm after input: 0.44860216975212097
Norm after each mp layer: 1.621497631072998
Norm after each mp layer: 5.096868991851807
Norm after each mp layer: 18.757198333740234
Norm before input: 0.2552422881126404
Norm after input: 0.44860216975212097
Norm after each mp layer: 1.6214977502822876
Norm after each mp layer: 5.096868991851807
Norm after each mp layer: 18.757198333740234
Norm before input: 0.2552422881126404
Norm after input: 0.4484907388687134
Norm after each mp layer: 1.6203382015228271
Norm after each mp layer: 5.0968122482299805
Norm after each mp layer: 18.75566864013672
Norm before input: 0.2552422881126404
Norm after input: 0.4484907388687134
Norm after each mp layer: 1.6203382015228271
Norm after each mp layer: 5.0968122482299805
Norm after each mp layer: 18.75566864013672
Norm before input: 0.2552422881126404
Norm after input: 0.44839751720428467
Norm after each mp layer: 1.619165062904358
Norm after each mp layer: 5.09647274017334
Norm after each mp layer: 18.749858856201172
Norm before input: 0.2552422881126404
Norm after input: 0.44839751720428467
Norm after each mp layer: 1.619165062904358
Norm after each mp layer: 5.09647274017334
Norm after each mp layer: 18.749858856201172
Norm before input: 0.2552422881126404
Norm after input: 0.44832953810691833
Norm after each mp layer: 1.6179453134536743
Norm after each mp layer: 5.095668315887451
Norm after each mp layer: 18.73738670349121
Epoch: 265, Loss: 0.1012, Energy: 3992.2600, Train: 98.84%, Valid: 79.20%, Test: 77.10%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44832953810691833
Norm after each mp layer: 1.6179453134536743
Norm after each mp layer: 5.095668315887451
Norm after each mp layer: 18.73738670349121
Norm before input: 0.2552422881126404
Norm after input: 0.4482786953449249
Norm after each mp layer: 1.6166859865188599
Norm after each mp layer: 5.094595909118652
Norm after each mp layer: 18.7203369140625
Norm before input: 0.2552422881126404
Norm after input: 0.4482786953449249
Norm after each mp layer: 1.6166859865188599
Norm after each mp layer: 5.094595909118652
Norm after each mp layer: 18.7203369140625
Norm before input: 0.2552422881126404
Norm after input: 0.4482272267341614
Norm after each mp layer: 1.615426778793335
Norm after each mp layer: 5.093719959259033
Norm after each mp layer: 18.703889846801758
Norm before input: 0.2552422881126404
Norm after input: 0.4482272267341614
Norm after each mp layer: 1.615426778793335
Norm after each mp layer: 5.093719959259033
Norm after each mp layer: 18.703889846801758
Norm before input: 0.2552422881126404
Norm after input: 0.44815778732299805
Norm after each mp layer: 1.6142194271087646
Norm after each mp layer: 5.093522548675537
Norm after each mp layer: 18.693410873413086
Norm before input: 0.2552422881126404
Norm after input: 0.44815778732299805
Norm after each mp layer: 1.6142194271087646
Norm after each mp layer: 5.093522071838379
Norm after each mp layer: 18.693410873413086
Norm before input: 0.2552422881126404
Norm after input: 0.4480631351470947
Norm after each mp layer: 1.613101601600647
Norm after each mp layer: 5.094254493713379
Norm after each mp layer: 18.69164276123047
Norm before input: 0.2552422881126404
Norm after input: 0.4480631351470947
Norm after each mp layer: 1.613101601600647
Norm after each mp layer: 5.094254493713379
Norm after each mp layer: 18.69164276123047
Norm before input: 0.2552422881126404
Norm after input: 0.44795089960098267
Norm after each mp layer: 1.6120824813842773
Norm after each mp layer: 5.095831394195557
Norm after each mp layer: 18.697355270385742
Epoch: 270, Loss: 0.0977, Energy: 3867.7178, Train: 98.84%, Valid: 79.00%, Test: 77.00%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44795089960098267
Norm after each mp layer: 1.6120824813842773
Norm after each mp layer: 5.095831394195557
Norm after each mp layer: 18.697355270385742
Norm before input: 0.2552422881126404
Norm after input: 0.4478393793106079
Norm after each mp layer: 1.6111438274383545
Norm after each mp layer: 5.097896575927734
Norm after each mp layer: 18.706209182739258
Norm before input: 0.2552422881126404
Norm after input: 0.4478393793106079
Norm after each mp layer: 1.6111438274383545
Norm after each mp layer: 5.097896575927734
Norm after each mp layer: 18.706209182739258
Norm before input: 0.2552422881126404
Norm after input: 0.44774767756462097
Norm after each mp layer: 1.610255241394043
Norm after each mp layer: 5.100034236907959
Norm after each mp layer: 18.713306427001953
Norm before input: 0.2552422881126404
Norm after input: 0.44774767756462097
Norm after each mp layer: 1.610255241394043
Norm after each mp layer: 5.100034236907959
Norm after each mp layer: 18.713306427001953
Norm before input: 0.2552422881126404
Norm after input: 0.4476853311061859
Norm after each mp layer: 1.6093950271606445
Norm after each mp layer: 5.102003574371338
Norm after each mp layer: 18.715961456298828
Norm before input: 0.2552422881126404
Norm after input: 0.4476853311061859
Norm after each mp layer: 1.6093950271606445
Norm after each mp layer: 5.102003574371338
Norm after each mp layer: 18.715961456298828
Norm before input: 0.2552422881126404
Norm after input: 0.44764795899391174
Norm after each mp layer: 1.6085574626922607
Norm after each mp layer: 5.1038408279418945
Norm after each mp layer: 18.71497344970703
Norm before input: 0.2552422881126404
Norm after input: 0.44764795899391174
Norm after each mp layer: 1.6085574626922607
Norm after each mp layer: 5.1038408279418945
Norm after each mp layer: 18.71497344970703
Norm before input: 0.2552422881126404
Norm after input: 0.44761964678764343
Norm after each mp layer: 1.607746958732605
Norm after each mp layer: 5.105776786804199
Norm after each mp layer: 18.713773727416992
Epoch: 275, Loss: 0.0945, Energy: 3777.7502, Train: 98.84%, Valid: 79.00%, Test: 76.80%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44761964678764343
Norm after each mp layer: 1.607746958732605
Norm after each mp layer: 5.105776786804199
Norm after each mp layer: 18.713773727416992
Norm before input: 0.2552422881126404
Norm after input: 0.44758135080337524
Norm after each mp layer: 1.6069631576538086
Norm after each mp layer: 5.108043193817139
Norm after each mp layer: 18.716129302978516
Norm before input: 0.2552422881126404
Norm after input: 0.44758135080337524
Norm after each mp layer: 1.6069631576538086
Norm after each mp layer: 5.108043193817139
Norm after each mp layer: 18.716129302978516
Norm before input: 0.2552422881126404
Norm after input: 0.44752031564712524
Norm after each mp layer: 1.6061910390853882
Norm after each mp layer: 5.1107048988342285
Norm after each mp layer: 18.723920822143555
Norm before input: 0.2552422881126404
Norm after input: 0.44752031564712524
Norm after each mp layer: 1.6061910390853882
Norm after each mp layer: 5.1107048988342285
Norm after each mp layer: 18.723920822143555
Norm before input: 0.2552422881126404
Norm after input: 0.4474359154701233
Norm after each mp layer: 1.605402946472168
Norm after each mp layer: 5.113602638244629
Norm after each mp layer: 18.736122131347656
Norm before input: 0.2552422881126404
Norm after input: 0.4474359154701233
Norm after each mp layer: 1.605402946472168
Norm after each mp layer: 5.113602638244629
Norm after each mp layer: 18.736122131347656
Norm before input: 0.2552422881126404
Norm after input: 0.447338730096817
Norm after each mp layer: 1.6045682430267334
Norm after each mp layer: 5.1164326667785645
Norm after each mp layer: 18.749481201171875
Norm before input: 0.2552422881126404
Norm after input: 0.447338730096817
Norm after each mp layer: 1.6045682430267334
Norm after each mp layer: 5.1164326667785645
Norm after each mp layer: 18.749481201171875
Norm before input: 0.2552422881126404
Norm after input: 0.44724345207214355
Norm after each mp layer: 1.6036680936813354
Norm after each mp layer: 5.118896007537842
Norm after each mp layer: 18.76034927368164
Epoch: 280, Loss: 0.0913, Energy: 3689.5388, Train: 98.84%, Valid: 78.80%, Test: 76.70%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44724345207214355
Norm after each mp layer: 1.6036680936813354
Norm after each mp layer: 5.118896007537842
Norm after each mp layer: 18.76034927368164
Norm before input: 0.2552422881126404
Norm after input: 0.4471608102321625
Norm after each mp layer: 1.602704405784607
Norm after each mp layer: 5.120850086212158
Norm after each mp layer: 18.766605377197266
Norm before input: 0.2552422881126404
Norm after input: 0.4471608102321625
Norm after each mp layer: 1.602704405784607
Norm after each mp layer: 5.120850086212158
Norm after each mp layer: 18.766605377197266
Norm before input: 0.2552422881126404
Norm after input: 0.44709232449531555
Norm after each mp layer: 1.6016987562179565
Norm after each mp layer: 5.122361660003662
Norm after each mp layer: 18.768638610839844
Norm before input: 0.2552422881126404
Norm after input: 0.44709232449531555
Norm after each mp layer: 1.6016987562179565
Norm after each mp layer: 5.122361660003662
Norm after each mp layer: 18.768638610839844
Norm before input: 0.2552422881126404
Norm after input: 0.4470304846763611
Norm after each mp layer: 1.6006836891174316
Norm after each mp layer: 5.123652458190918
Norm after each mp layer: 18.768949508666992
Norm before input: 0.2552422881126404
Norm after input: 0.4470304846763611
Norm after each mp layer: 1.6006836891174316
Norm after each mp layer: 5.123652458190918
Norm after each mp layer: 18.768949508666992
Norm before input: 0.2552422881126404
Norm after input: 0.4469638764858246
Norm after each mp layer: 1.5996886491775513
Norm after each mp layer: 5.124973773956299
Norm after each mp layer: 18.77060317993164
Norm before input: 0.2552422881126404
Norm after input: 0.4469638764858246
Norm after each mp layer: 1.5996886491775513
Norm after each mp layer: 5.124973773956299
Norm after each mp layer: 18.77060317993164
Norm before input: 0.2552422881126404
Norm after input: 0.44688481092453003
Norm after each mp layer: 1.5987274646759033
Norm after each mp layer: 5.126473426818848
Norm after each mp layer: 18.775468826293945
Epoch: 285, Loss: 0.0884, Energy: 3602.9585, Train: 98.84%, Valid: 78.60%, Test: 76.80%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44688481092453003
Norm after each mp layer: 1.5987274646759033
Norm after each mp layer: 5.126473426818848
Norm after each mp layer: 18.775468826293945
Norm before input: 0.2552422881126404
Norm after input: 0.446794331073761
Norm after each mp layer: 1.5977941751480103
Norm after each mp layer: 5.1281328201293945
Norm after each mp layer: 18.78318977355957
Norm before input: 0.2552422881126404
Norm after input: 0.446794331073761
Norm after each mp layer: 1.5977941751480103
Norm after each mp layer: 5.1281328201293945
Norm after each mp layer: 18.78318977355957
Norm before input: 0.2552422881126404
Norm after input: 0.44670137763023376
Norm after each mp layer: 1.5968701839447021
Norm after each mp layer: 5.129802703857422
Norm after each mp layer: 18.791522979736328
Norm before input: 0.2552422881126404
Norm after input: 0.44670137763023376
Norm after each mp layer: 1.5968701839447021
Norm after each mp layer: 5.129802703857422
Norm after each mp layer: 18.791522979736328
Norm before input: 0.2552422881126404
Norm after input: 0.4466171860694885
Norm after each mp layer: 1.5959347486495972
Norm after each mp layer: 5.131310939788818
Norm after each mp layer: 18.797780990600586
Norm before input: 0.2552422881126404
Norm after input: 0.4466171860694885
Norm after each mp layer: 1.5959347486495972
Norm after each mp layer: 5.131310939788818
Norm after each mp layer: 18.797780990600586
Norm before input: 0.2552422881126404
Norm after input: 0.44654855132102966
Norm after each mp layer: 1.5949764251708984
Norm after each mp layer: 5.132567882537842
Norm after each mp layer: 18.800432205200195
Norm before input: 0.2552422881126404
Norm after input: 0.44654855132102966
Norm after each mp layer: 1.5949764251708984
Norm after each mp layer: 5.132567882537842
Norm after each mp layer: 18.800432205200195
Norm before input: 0.2552422881126404
Norm after input: 0.44649362564086914
Norm after each mp layer: 1.593996286392212
Norm after each mp layer: 5.133617877960205
Norm after each mp layer: 18.799945831298828
Epoch: 290, Loss: 0.0855, Energy: 3515.7615, Train: 98.84%, Valid: 78.40%, Test: 76.80%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44649362564086914
Norm after each mp layer: 1.593996286392212
Norm after each mp layer: 5.133617877960205
Norm after each mp layer: 18.799945831298828
Norm before input: 0.2552422881126404
Norm after input: 0.4464431703090668
Norm after each mp layer: 1.5930052995681763
Norm after each mp layer: 5.1346025466918945
Norm after each mp layer: 18.798418045043945
Norm before input: 0.2552422881126404
Norm after input: 0.4464431703090668
Norm after each mp layer: 1.5930052995681763
Norm after each mp layer: 5.1346025466918945
Norm after each mp layer: 18.798418045043945
Norm before input: 0.2552422881126404
Norm after input: 0.4463860094547272
Norm after each mp layer: 1.5920157432556152
Norm after each mp layer: 5.135676383972168
Norm after each mp layer: 18.798295974731445
Norm before input: 0.2552422881126404
Norm after input: 0.4463860094547272
Norm after each mp layer: 1.5920157432556152
Norm after each mp layer: 5.135676383972168
Norm after each mp layer: 18.798295974731445
Norm before input: 0.2552422881126404
Norm after input: 0.4463154673576355
Norm after each mp layer: 1.5910348892211914
Norm after each mp layer: 5.136916637420654
Norm after each mp layer: 18.800968170166016
Norm before input: 0.2552422881126404
Norm after input: 0.4463154673576355
Norm after each mp layer: 1.5910348892211914
Norm after each mp layer: 5.136916637420654
Norm after each mp layer: 18.800968170166016
Norm before input: 0.2552422881126404
Norm after input: 0.44623276591300964
Norm after each mp layer: 1.590060830116272
Norm after each mp layer: 5.138288974761963
Norm after each mp layer: 18.806068420410156
Norm before input: 0.2552422881126404
Norm after input: 0.44623276591300964
Norm after each mp layer: 1.590060830116272
Norm after each mp layer: 5.138288974761963
Norm after each mp layer: 18.806068420410156
Norm before input: 0.2552422881126404
Norm after input: 0.44614535570144653
Norm after each mp layer: 1.5890870094299316
Norm after each mp layer: 5.139681339263916
Norm after each mp layer: 18.811891555786133
Epoch: 295, Loss: 0.0828, Energy: 3424.3250, Train: 98.92%, Valid: 78.60%, Test: 76.70%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44614535570144653
Norm after each mp layer: 1.5890870094299316
Norm after each mp layer: 5.139681339263916
Norm after each mp layer: 18.811891555786133
Norm before input: 0.2552422881126404
Norm after input: 0.4460621178150177
Norm after each mp layer: 1.5881062746047974
Norm after each mp layer: 5.140980243682861
Norm after each mp layer: 18.81656265258789
Norm before input: 0.2552422881126404
Norm after input: 0.4460621178150177
Norm after each mp layer: 1.5881062746047974
Norm after each mp layer: 5.140980243682861
Norm after each mp layer: 18.81656265258789
Norm before input: 0.2552422881126404
Norm after input: 0.445987343788147
Norm after each mp layer: 1.5871174335479736
Norm after each mp layer: 5.142138957977295
Norm after each mp layer: 18.819210052490234
Norm before input: 0.2552422881126404
Norm after input: 0.445987343788147
Norm after each mp layer: 1.5871174335479736
Norm after each mp layer: 5.142138957977295
Norm after each mp layer: 18.819210052490234
Norm before input: 0.2552422881126404
Norm after input: 0.4459191560745239
Norm after each mp layer: 1.5861246585845947
Norm after each mp layer: 5.143199920654297
Norm after each mp layer: 18.820415496826172
Norm before input: 0.2552422881126404
Norm after input: 0.4459191560745239
Norm after each mp layer: 1.5861246585845947
Norm after each mp layer: 5.143199920654297
Norm after each mp layer: 18.820415496826172
Norm before input: 0.2552422881126404
Norm after input: 0.44585126638412476
Norm after each mp layer: 1.5851372480392456
Norm after each mp layer: 5.144262313842773
Norm after each mp layer: 18.821699142456055
Norm before input: 0.2552422881126404
Norm after input: 0.44585126638412476
Norm after each mp layer: 1.5851372480392456
Norm after each mp layer: 5.144261360168457
Norm after each mp layer: 18.821699142456055
Norm before input: 0.2552422881126404
Norm after input: 0.445777952671051
Norm after each mp layer: 1.5841624736785889
Norm after each mp layer: 5.145405292510986
Norm after each mp layer: 18.824420928955078
Epoch: 300, Loss: 0.0802, Energy: 3340.3496, Train: 99.09%, Valid: 78.00%, Test: 76.40%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.445777952671051
Norm after each mp layer: 1.5841624736785889
Norm after each mp layer: 5.145405292510986
Norm after each mp layer: 18.824420928955078
Norm before input: 0.2552422881126404
Norm after input: 0.445698082447052
Norm after each mp layer: 1.5832023620605469
Norm after each mp layer: 5.146640777587891
Norm after each mp layer: 18.828868865966797
Norm before input: 0.2552422881126404
Norm after input: 0.445698082447052
Norm after each mp layer: 1.5832023620605469
Norm after each mp layer: 5.146640777587891
Norm after each mp layer: 18.828868865966797
Norm before input: 0.2552422881126404
Norm after input: 0.4456157684326172
Norm after each mp layer: 1.5822525024414062
Norm after each mp layer: 5.147905349731445
Norm after each mp layer: 18.834121704101562
Norm before input: 0.2552422881126404
Norm after input: 0.4456157684326172
Norm after each mp layer: 1.5822525024414062
Norm after each mp layer: 5.147905349731445
Norm after each mp layer: 18.834121704101562
Norm before input: 0.2552422881126404
Norm after input: 0.4455373287200928
Norm after each mp layer: 1.581305980682373
Norm after each mp layer: 5.149106025695801
Norm after each mp layer: 18.838726043701172
Norm before input: 0.2552422881126404
Norm after input: 0.4455373287200928
Norm after each mp layer: 1.581305980682373
Norm after each mp layer: 5.149106025695801
Norm after each mp layer: 18.838726043701172
Norm before input: 0.2552422881126404
Norm after input: 0.4454667568206787
Norm after each mp layer: 1.5803574323654175
Norm after each mp layer: 5.1501898765563965
Norm after each mp layer: 18.84174156188965
Norm before input: 0.2552422881126404
Norm after input: 0.4454667568206787
Norm after each mp layer: 1.5803574323654175
Norm after each mp layer: 5.1501898765563965
Norm after each mp layer: 18.84174156188965
Norm before input: 0.2552422881126404
Norm after input: 0.4454030990600586
Norm after each mp layer: 1.5794049501419067
Norm after each mp layer: 5.151177406311035
Norm after each mp layer: 18.843347549438477
Epoch: 305, Loss: 0.0777, Energy: 3257.1750, Train: 99.09%, Valid: 77.60%, Test: 76.20%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4454030990600586
Norm after each mp layer: 1.5794049501419067
Norm after each mp layer: 5.151177406311035
Norm after each mp layer: 18.843347549438477
Norm before input: 0.2552422881126404
Norm after input: 0.4453416168689728
Norm after each mp layer: 1.578452229499817
Norm after each mp layer: 5.152153968811035
Norm after each mp layer: 18.84469223022461
Norm before input: 0.2552422881126404
Norm after input: 0.4453416168689728
Norm after each mp layer: 1.578452229499817
Norm after each mp layer: 5.152153968811035
Norm after each mp layer: 18.84469223022461
Norm before input: 0.2552422881126404
Norm after input: 0.4452766180038452
Norm after each mp layer: 1.5775034427642822
Norm after each mp layer: 5.153212070465088
Norm after each mp layer: 18.847082138061523
Norm before input: 0.2552422881126404
Norm after input: 0.4452766180038452
Norm after each mp layer: 1.5775034427642822
Norm after each mp layer: 5.153212070465088
Norm after each mp layer: 18.847082138061523
Norm before input: 0.2552422881126404
Norm after input: 0.4452056586742401
Norm after each mp layer: 1.576561689376831
Norm after each mp layer: 5.154393196105957
Norm after each mp layer: 18.851104736328125
Norm before input: 0.2552422881126404
Norm after input: 0.4452056586742401
Norm after each mp layer: 1.576561689376831
Norm after each mp layer: 5.154393196105957
Norm after each mp layer: 18.851104736328125
Norm before input: 0.2552422881126404
Norm after input: 0.4451308250427246
Norm after each mp layer: 1.575627088546753
Norm after each mp layer: 5.155668258666992
Norm after each mp layer: 18.85631561279297
Norm before input: 0.2552422881126404
Norm after input: 0.4451308250427246
Norm after each mp layer: 1.575627326965332
Norm after each mp layer: 5.155668258666992
Norm after each mp layer: 18.85631561279297
Norm before input: 0.2552422881126404
Norm after input: 0.44505661725997925
Norm after each mp layer: 1.5746979713439941
Norm after each mp layer: 5.156959056854248
Norm after each mp layer: 18.86162567138672
Epoch: 310, Loss: 0.0752, Energy: 3174.0938, Train: 99.17%, Valid: 76.80%, Test: 76.10%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44505661725997925
Norm after each mp layer: 1.5746979713439941
Norm after each mp layer: 5.156959056854248
Norm after each mp layer: 18.86162567138672
Norm before input: 0.2552422881126404
Norm after input: 0.44498685002326965
Norm after each mp layer: 1.573772668838501
Norm after each mp layer: 5.158197402954102
Norm after each mp layer: 18.866117477416992
Norm before input: 0.2552422881126404
Norm after input: 0.44498685002326965
Norm after each mp layer: 1.573772668838501
Norm after each mp layer: 5.158197402954102
Norm after each mp layer: 18.866117477416992
Norm before input: 0.2552422881126404
Norm after input: 0.44492170214653015
Norm after each mp layer: 1.572852373123169
Norm after each mp layer: 5.159371376037598
Norm after each mp layer: 18.869674682617188
Norm before input: 0.2552422881126404
Norm after input: 0.44492170214653015
Norm after each mp layer: 1.572852373123169
Norm after each mp layer: 5.159371376037598
Norm after each mp layer: 18.869674682617188
Norm before input: 0.2552422881126404
Norm after input: 0.44485825300216675
Norm after each mp layer: 1.571940302848816
Norm after each mp layer: 5.160524845123291
Norm after each mp layer: 18.873001098632812
Norm before input: 0.2552422881126404
Norm after input: 0.44485825300216675
Norm after each mp layer: 1.571940302848816
Norm after each mp layer: 5.160524845123291
Norm after each mp layer: 18.873001098632812
Norm before input: 0.2552422881126404
Norm after input: 0.44479283690452576
Norm after each mp layer: 1.5710405111312866
Norm after each mp layer: 5.161726951599121
Norm after each mp layer: 18.877033233642578
Norm before input: 0.2552422881126404
Norm after input: 0.44479283690452576
Norm after each mp layer: 1.5710405111312866
Norm after each mp layer: 5.161726951599121
Norm after each mp layer: 18.877033233642578
Norm before input: 0.2552422881126404
Norm after input: 0.44472363591194153
Norm after each mp layer: 1.5701545476913452
Norm after each mp layer: 5.163020610809326
Norm after each mp layer: 18.88224220275879
Epoch: 315, Loss: 0.0729, Energy: 3094.1450, Train: 99.25%, Valid: 77.00%, Test: 75.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44472363591194153
Norm after each mp layer: 1.5701545476913452
Norm after each mp layer: 5.163020610809326
Norm after each mp layer: 18.88224220275879
Norm before input: 0.2552422881126404
Norm after input: 0.44465237855911255
Norm after each mp layer: 1.5692803859710693
Norm after each mp layer: 5.164397716522217
Norm after each mp layer: 18.888330459594727
Norm before input: 0.2552422881126404
Norm after input: 0.44465237855911255
Norm after each mp layer: 1.5692803859710693
Norm after each mp layer: 5.164397716522217
Norm after each mp layer: 18.888330459594727
Norm before input: 0.2552422881126404
Norm after input: 0.4445822238922119
Norm after each mp layer: 1.568415641784668
Norm after each mp layer: 5.165814399719238
Norm after each mp layer: 18.894508361816406
Norm before input: 0.2552422881126404
Norm after input: 0.4445822238922119
Norm after each mp layer: 1.568415641784668
Norm after each mp layer: 5.165814399719238
Norm after each mp layer: 18.894508361816406
Norm before input: 0.2552422881126404
Norm after input: 0.44451597332954407
Norm after each mp layer: 1.567557692527771
Norm after each mp layer: 5.167229175567627
Norm after each mp layer: 18.900136947631836
Norm before input: 0.2552422881126404
Norm after input: 0.44451597332954407
Norm after each mp layer: 1.567557692527771
Norm after each mp layer: 5.167229175567627
Norm after each mp layer: 18.900136947631836
Norm before input: 0.2552422881126404
Norm after input: 0.4444533884525299
Norm after each mp layer: 1.566707730293274
Norm after each mp layer: 5.168638706207275
Norm after each mp layer: 18.905187606811523
Norm before input: 0.2552422881126404
Norm after input: 0.4444533884525299
Norm after each mp layer: 1.566707730293274
Norm after each mp layer: 5.168638706207275
Norm after each mp layer: 18.90518569946289
Norm before input: 0.2552422881126404
Norm after input: 0.4443923234939575
Norm after each mp layer: 1.5658681392669678
Norm after each mp layer: 5.170074462890625
Norm after each mp layer: 18.91021728515625
Epoch: 320, Loss: 0.0706, Energy: 3018.7312, Train: 99.34%, Valid: 76.20%, Test: 76.10%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4443923234939575
Norm after each mp layer: 1.5658681392669678
Norm after each mp layer: 5.170074462890625
Norm after each mp layer: 18.91021728515625
Norm before input: 0.2552422881126404
Norm after input: 0.4443298578262329
Norm after each mp layer: 1.565042495727539
Norm after each mp layer: 5.171579837799072
Norm after each mp layer: 18.91590118408203
Norm before input: 0.2552422881126404
Norm after input: 0.4443298578262329
Norm after each mp layer: 1.565042495727539
Norm after each mp layer: 5.171579837799072
Norm after each mp layer: 18.91590118408203
Norm before input: 0.2552422881126404
Norm after input: 0.44426506757736206
Norm after each mp layer: 1.5642316341400146
Norm after each mp layer: 5.1731696128845215
Norm after each mp layer: 18.92249298095703
Norm before input: 0.2552422881126404
Norm after input: 0.44426506757736206
Norm after each mp layer: 1.5642316341400146
Norm after each mp layer: 5.1731696128845215
Norm after each mp layer: 18.92249298095703
Norm before input: 0.2552422881126404
Norm after input: 0.44419947266578674
Norm after each mp layer: 1.56343412399292
Norm after each mp layer: 5.174816608428955
Norm after each mp layer: 18.929636001586914
Norm before input: 0.2552422881126404
Norm after input: 0.44419947266578674
Norm after each mp layer: 1.56343412399292
Norm after each mp layer: 5.174816608428955
Norm after each mp layer: 18.929636001586914
Norm before input: 0.2552422881126404
Norm after input: 0.4441356062889099
Norm after each mp layer: 1.5626471042633057
Norm after each mp layer: 5.17647123336792
Norm after each mp layer: 18.9366512298584
Norm before input: 0.2552422881126404
Norm after input: 0.4441356062889099
Norm after each mp layer: 1.5626471042633057
Norm after each mp layer: 5.17647123336792
Norm after each mp layer: 18.9366512298584
Norm before input: 0.2552422881126404
Norm after input: 0.4440751075744629
Norm after each mp layer: 1.5618679523468018
Norm after each mp layer: 5.178092956542969
Norm after each mp layer: 18.943050384521484
Epoch: 325, Loss: 0.0685, Energy: 2946.4766, Train: 99.34%, Valid: 75.80%, Test: 75.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4440751075744629
Norm after each mp layer: 1.5618679523468018
Norm after each mp layer: 5.178092956542969
Norm after each mp layer: 18.943050384521484
Norm before input: 0.2552422881126404
Norm after input: 0.44401755928993225
Norm after each mp layer: 1.56109619140625
Norm after each mp layer: 5.1796793937683105
Norm after each mp layer: 18.94887351989746
Norm before input: 0.2552422881126404
Norm after input: 0.44401755928993225
Norm after each mp layer: 1.56109619140625
Norm after each mp layer: 5.1796793937683105
Norm after each mp layer: 18.94887351989746
Norm before input: 0.2552422881126404
Norm after input: 0.44396084547042847
Norm after each mp layer: 1.560333490371704
Norm after each mp layer: 5.181260585784912
Norm after each mp layer: 18.954601287841797
Norm before input: 0.2552422881126404
Norm after input: 0.44396084547042847
Norm after each mp layer: 1.560333490371704
Norm after each mp layer: 5.181260585784912
Norm after each mp layer: 18.954601287841797
Norm before input: 0.2552422881126404
Norm after input: 0.4439030885696411
Norm after each mp layer: 1.5595818758010864
Norm after each mp layer: 5.182872295379639
Norm after each mp layer: 18.960752487182617
Norm before input: 0.2552422881126404
Norm after input: 0.4439030885696411
Norm after each mp layer: 1.5595818758010864
Norm after each mp layer: 5.182872772216797
Norm after each mp layer: 18.960752487182617
Norm before input: 0.2552422881126404
Norm after input: 0.4438437819480896
Norm after each mp layer: 1.5588421821594238
Norm after each mp layer: 5.184532642364502
Norm after each mp layer: 18.967485427856445
Norm before input: 0.2552422881126404
Norm after input: 0.4438437819480896
Norm after each mp layer: 1.5588421821594238
Norm after each mp layer: 5.184532642364502
Norm after each mp layer: 18.967485427856445
Norm before input: 0.2552422881126404
Norm after input: 0.44378402829170227
Norm after each mp layer: 1.5581133365631104
Norm after each mp layer: 5.186225414276123
Norm after each mp layer: 18.974538803100586
Epoch: 330, Loss: 0.0664, Energy: 2876.7441, Train: 99.42%, Valid: 75.60%, Test: 75.80%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44378402829170227
Norm after each mp layer: 1.5581133365631104
Norm after each mp layer: 5.186225414276123
Norm after each mp layer: 18.974538803100586
Norm before input: 0.2552422881126404
Norm after input: 0.4437255859375
Norm after each mp layer: 1.5573935508728027
Norm after each mp layer: 5.18792200088501
Norm after each mp layer: 18.981483459472656
Norm before input: 0.2552422881126404
Norm after input: 0.4437255859375
Norm after each mp layer: 1.5573935508728027
Norm after each mp layer: 5.18792200088501
Norm after each mp layer: 18.981483459472656
Norm before input: 0.2552422881126404
Norm after input: 0.44366905093193054
Norm after each mp layer: 1.5566810369491577
Norm after each mp layer: 5.189602851867676
Norm after each mp layer: 18.988075256347656
Norm before input: 0.2552422881126404
Norm after input: 0.44366905093193054
Norm after each mp layer: 1.5566810369491577
Norm after each mp layer: 5.189602851867676
Norm after each mp layer: 18.988075256347656
Norm before input: 0.2552422881126404
Norm after input: 0.44361400604248047
Norm after each mp layer: 1.5559755563735962
Norm after each mp layer: 5.191273212432861
Norm after each mp layer: 18.994422912597656
Norm before input: 0.2552422881126404
Norm after input: 0.44361400604248047
Norm after each mp layer: 1.5559755563735962
Norm after each mp layer: 5.191273212432861
Norm after each mp layer: 18.994422912597656
Norm before input: 0.2552422881126404
Norm after input: 0.44355905055999756
Norm after each mp layer: 1.5552778244018555
Norm after each mp layer: 5.192953109741211
Norm after each mp layer: 19.000829696655273
Norm before input: 0.2552422881126404
Norm after input: 0.44355905055999756
Norm after each mp layer: 1.5552778244018555
Norm after each mp layer: 5.192953109741211
Norm after each mp layer: 19.000829696655273
Norm before input: 0.2552422881126404
Norm after input: 0.44350355863571167
Norm after each mp layer: 1.5545884370803833
Norm after each mp layer: 5.194659233093262
Norm after each mp layer: 19.00751495361328
Epoch: 335, Loss: 0.0644, Energy: 2809.6204, Train: 99.50%, Valid: 75.20%, Test: 75.80%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44350355863571167
Norm after each mp layer: 1.5545884370803833
Norm after each mp layer: 5.194659233093262
Norm after each mp layer: 19.00751304626465
Norm before input: 0.2552422881126404
Norm after input: 0.44344788789749146
Norm after each mp layer: 1.5539084672927856
Norm after each mp layer: 5.196391582489014
Norm after each mp layer: 19.014406204223633
Norm before input: 0.2552422881126404
Norm after input: 0.44344788789749146
Norm after each mp layer: 1.5539084672927856
Norm after each mp layer: 5.196391582489014
Norm after each mp layer: 19.014406204223633
Norm before input: 0.2552422881126404
Norm after input: 0.4433933198451996
Norm after each mp layer: 1.553237795829773
Norm after each mp layer: 5.1981329917907715
Norm after each mp layer: 19.02124786376953
Norm before input: 0.2552422881126404
Norm after input: 0.4433933198451996
Norm after each mp layer: 1.553237795829773
Norm after each mp layer: 5.1981329917907715
Norm after each mp layer: 19.02124786376953
Norm before input: 0.2552422881126404
Norm after input: 0.4433405101299286
Norm after each mp layer: 1.5525771379470825
Norm after each mp layer: 5.199869632720947
Norm after each mp layer: 19.02781867980957
Norm before input: 0.2552422881126404
Norm after input: 0.4433405101299286
Norm after each mp layer: 1.5525771379470825
Norm after each mp layer: 5.199869632720947
Norm after each mp layer: 19.02781867980957
Norm before input: 0.2552422881126404
Norm after input: 0.4432893991470337
Norm after each mp layer: 1.5519273281097412
Norm after each mp layer: 5.20159912109375
Norm after each mp layer: 19.034137725830078
Norm before input: 0.2552422881126404
Norm after input: 0.4432893991470337
Norm after each mp layer: 1.5519273281097412
Norm after each mp layer: 5.20159912109375
Norm after each mp layer: 19.034137725830078
Norm before input: 0.2552422881126404
Norm after input: 0.4432390630245209
Norm after each mp layer: 1.5512895584106445
Norm after each mp layer: 5.203334331512451
Norm after each mp layer: 19.04041862487793
Epoch: 340, Loss: 0.0625, Energy: 2745.0701, Train: 99.59%, Valid: 74.60%, Test: 75.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4432390630245209
Norm after each mp layer: 1.5512895584106445
Norm after each mp layer: 5.203334331512451
Norm after each mp layer: 19.04041862487793
Norm before input: 0.2552422881126404
Norm after input: 0.44318851828575134
Norm after each mp layer: 1.5506640672683716
Norm after each mp layer: 5.2050909996032715
Norm after each mp layer: 19.046886444091797
Norm before input: 0.2552422881126404
Norm after input: 0.44318851828575134
Norm after each mp layer: 1.5506640672683716
Norm after each mp layer: 5.2050909996032715
Norm after each mp layer: 19.046886444091797
Norm before input: 0.2552422881126404
Norm after input: 0.4431377053260803
Norm after each mp layer: 1.5500502586364746
Norm after each mp layer: 5.206873416900635
Norm after each mp layer: 19.053560256958008
Norm before input: 0.2552422881126404
Norm after input: 0.4431377053260803
Norm after each mp layer: 1.5500502586364746
Norm after each mp layer: 5.206873416900635
Norm after each mp layer: 19.053560256958008
Norm before input: 0.2552422881126404
Norm after input: 0.4430873692035675
Norm after each mp layer: 1.549446702003479
Norm after each mp layer: 5.208669662475586
Norm after each mp layer: 19.060258865356445
Norm before input: 0.2552422881126404
Norm after input: 0.4430873692035675
Norm after each mp layer: 1.549446702003479
Norm after each mp layer: 5.208669662475586
Norm after each mp layer: 19.060258865356445
Norm before input: 0.2552422881126404
Norm after input: 0.4430382251739502
Norm after each mp layer: 1.5488524436950684
Norm after each mp layer: 5.210464954376221
Norm after each mp layer: 19.06675148010254
Norm before input: 0.2552422881126404
Norm after input: 0.4430382251739502
Norm after each mp layer: 1.5488524436950684
Norm after each mp layer: 5.210464954376221
Norm after each mp layer: 19.06675148010254
Norm before input: 0.2552422881126404
Norm after input: 0.44299057126045227
Norm after each mp layer: 1.5482670068740845
Norm after each mp layer: 5.212252616882324
Norm after each mp layer: 19.072952270507812
Epoch: 345, Loss: 0.0606, Energy: 2683.0583, Train: 99.59%, Valid: 74.80%, Test: 75.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44299057126045227
Norm after each mp layer: 1.5482670068740845
Norm after each mp layer: 5.212252616882324
Norm after each mp layer: 19.072952270507812
Norm before input: 0.2552422881126404
Norm after input: 0.44294384121894836
Norm after each mp layer: 1.547690749168396
Norm after each mp layer: 5.214037895202637
Norm after each mp layer: 19.078977584838867
Norm before input: 0.2552422881126404
Norm after input: 0.44294384121894836
Norm after each mp layer: 1.547690749168396
Norm after each mp layer: 5.214037895202637
Norm after each mp layer: 19.078977584838867
Norm before input: 0.2552422881126404
Norm after input: 0.44289740920066833
Norm after each mp layer: 1.5471248626708984
Norm after each mp layer: 5.215834617614746
Norm after each mp layer: 19.085012435913086
Norm before input: 0.2552422881126404
Norm after input: 0.44289740920066833
Norm after each mp layer: 1.5471248626708984
Norm after each mp layer: 5.215834617614746
Norm after each mp layer: 19.085012435913086
Norm before input: 0.2552422881126404
Norm after input: 0.4428509771823883
Norm after each mp layer: 1.5465706586837769
Norm after each mp layer: 5.217650890350342
Norm after each mp layer: 19.091154098510742
Norm before input: 0.2552422881126404
Norm after input: 0.4428509771823883
Norm after each mp layer: 1.5465706586837769
Norm after each mp layer: 5.217650890350342
Norm after each mp layer: 19.091154098510742
Norm before input: 0.2552422881126404
Norm after input: 0.44280490279197693
Norm after each mp layer: 1.5460278987884521
Norm after each mp layer: 5.219484329223633
Norm after each mp layer: 19.097335815429688
Norm before input: 0.2552422881126404
Norm after input: 0.44280490279197693
Norm after each mp layer: 1.5460278987884521
Norm after each mp layer: 5.219484329223633
Norm after each mp layer: 19.097335815429688
Norm before input: 0.2552422881126404
Norm after input: 0.44275984168052673
Norm after each mp layer: 1.5454964637756348
Norm after each mp layer: 5.221325397491455
Norm after each mp layer: 19.10340690612793
Epoch: 350, Loss: 0.0588, Energy: 2623.6060, Train: 99.59%, Valid: 74.80%, Test: 75.50%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44275984168052673
Norm after each mp layer: 1.5454964637756348
Norm after each mp layer: 5.221325397491455
Norm after each mp layer: 19.10340690612793
Norm before input: 0.2552422881126404
Norm after input: 0.44271591305732727
Norm after each mp layer: 1.544975996017456
Norm after each mp layer: 5.223168849945068
Norm after each mp layer: 19.109281539916992
Norm before input: 0.2552422881126404
Norm after input: 0.44271591305732727
Norm after each mp layer: 1.544975996017456
Norm after each mp layer: 5.223168849945068
Norm after each mp layer: 19.109281539916992
Norm before input: 0.2552422881126404
Norm after input: 0.44267305731773376
Norm after each mp layer: 1.544466257095337
Norm after each mp layer: 5.225013732910156
Norm after each mp layer: 19.114999771118164
Norm before input: 0.2552422881126404
Norm after input: 0.44267305731773376
Norm after each mp layer: 1.544466257095337
Norm after each mp layer: 5.225013732910156
Norm after each mp layer: 19.114999771118164
Norm before input: 0.2552422881126404
Norm after input: 0.44263067841529846
Norm after each mp layer: 1.5439667701721191
Norm after each mp layer: 5.226864814758301
Norm after each mp layer: 19.12066078186035
Norm before input: 0.2552422881126404
Norm after input: 0.44263067841529846
Norm after each mp layer: 1.5439667701721191
Norm after each mp layer: 5.226864814758301
Norm after each mp layer: 19.12066078186035
Norm before input: 0.2552422881126404
Norm after input: 0.44258853793144226
Norm after each mp layer: 1.5434778928756714
Norm after each mp layer: 5.228724002838135
Norm after each mp layer: 19.12630844116211
Norm before input: 0.2552422881126404
Norm after input: 0.44258853793144226
Norm after each mp layer: 1.5434778928756714
Norm after each mp layer: 5.228723526000977
Norm after each mp layer: 19.12630844116211
Norm before input: 0.2552422881126404
Norm after input: 0.4425467848777771
Norm after each mp layer: 1.5429986715316772
Norm after each mp layer: 5.23058557510376
Norm after each mp layer: 19.13189125061035
Epoch: 355, Loss: 0.0572, Energy: 2567.0396, Train: 99.59%, Valid: 74.60%, Test: 75.20%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4425467848777771
Norm after each mp layer: 1.5429986715316772
Norm after each mp layer: 5.23058557510376
Norm after each mp layer: 19.13189125061035
Norm before input: 0.2552422881126404
Norm after input: 0.44250595569610596
Norm after each mp layer: 1.5425283908843994
Norm after each mp layer: 5.232443332672119
Norm after each mp layer: 19.137292861938477
Norm before input: 0.2552422881126404
Norm after input: 0.44250595569610596
Norm after each mp layer: 1.5425283908843994
Norm after each mp layer: 5.232443332672119
Norm after each mp layer: 19.137292861938477
Norm before input: 0.2552422881126404
Norm after input: 0.4424661099910736
Norm after each mp layer: 1.5420671701431274
Norm after each mp layer: 5.234291076660156
Norm after each mp layer: 19.142457962036133
Norm before input: 0.2552422881126404
Norm after input: 0.4424661099910736
Norm after each mp layer: 1.5420671701431274
Norm after each mp layer: 5.234291076660156
Norm after each mp layer: 19.142457962036133
Norm before input: 0.2552422881126404
Norm after input: 0.4424270689487457
Norm after each mp layer: 1.5416144132614136
Norm after each mp layer: 5.236133098602295
Norm after each mp layer: 19.14742660522461
Norm before input: 0.2552422881126404
Norm after input: 0.4424270689487457
Norm after each mp layer: 1.5416144132614136
Norm after each mp layer: 5.236133098602295
Norm after each mp layer: 19.14742660522461
Norm before input: 0.2552422881126404
Norm after input: 0.4423885643482208
Norm after each mp layer: 1.5411707162857056
Norm after each mp layer: 5.237973690032959
Norm after each mp layer: 19.152292251586914
Norm before input: 0.2552422881126404
Norm after input: 0.4423885643482208
Norm after each mp layer: 1.5411707162857056
Norm after each mp layer: 5.237973690032959
Norm after each mp layer: 19.152292251586914
Norm before input: 0.2552422881126404
Norm after input: 0.44235026836395264
Norm after each mp layer: 1.5407357215881348
Norm after each mp layer: 5.239816188812256
Norm after each mp layer: 19.15709686279297
Epoch: 360, Loss: 0.0555, Energy: 2513.0525, Train: 99.59%, Valid: 74.20%, Test: 74.80%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44235026836395264
Norm after each mp layer: 1.5407357215881348
Norm after each mp layer: 5.239816188812256
Norm after each mp layer: 19.15709686279297
Norm before input: 0.2552422881126404
Norm after input: 0.44231244921684265
Norm after each mp layer: 1.5403099060058594
Norm after each mp layer: 5.24165678024292
Norm after each mp layer: 19.161785125732422
Norm before input: 0.2552422881126404
Norm after input: 0.44231244921684265
Norm after each mp layer: 1.5403099060058594
Norm after each mp layer: 5.24165678024292
Norm after each mp layer: 19.161785125732422
Norm before input: 0.2552422881126404
Norm after input: 0.4422755241394043
Norm after each mp layer: 1.5398920774459839
Norm after each mp layer: 5.24348783493042
Norm after each mp layer: 19.166269302368164
Norm before input: 0.2552422881126404
Norm after input: 0.4422755241394043
Norm after each mp layer: 1.5398920774459839
Norm after each mp layer: 5.24348783493042
Norm after each mp layer: 19.166269302368164
Norm before input: 0.2552422881126404
Norm after input: 0.44223955273628235
Norm after each mp layer: 1.5394822359085083
Norm after each mp layer: 5.24530553817749
Norm after each mp layer: 19.17049217224121
Norm before input: 0.2552422881126404
Norm after input: 0.44223955273628235
Norm after each mp layer: 1.5394822359085083
Norm after each mp layer: 5.24530553817749
Norm after each mp layer: 19.17049217224121
Norm before input: 0.2552422881126404
Norm after input: 0.4422045350074768
Norm after each mp layer: 1.5390801429748535
Norm after each mp layer: 5.247112274169922
Norm after each mp layer: 19.17449188232422
Norm before input: 0.2552422881126404
Norm after input: 0.4422045350074768
Norm after each mp layer: 1.5390801429748535
Norm after each mp layer: 5.247112274169922
Norm after each mp layer: 19.17449188232422
Norm before input: 0.2552422881126404
Norm after input: 0.44217002391815186
Norm after each mp layer: 1.5386861562728882
Norm after each mp layer: 5.248910903930664
Norm after each mp layer: 19.178342819213867
Epoch: 365, Loss: 0.0540, Energy: 2461.2935, Train: 99.59%, Valid: 74.00%, Test: 74.40%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44217002391815186
Norm after each mp layer: 1.5386861562728882
Norm after each mp layer: 5.248910427093506
Norm after each mp layer: 19.178342819213867
Norm before input: 0.2552422881126404
Norm after input: 0.44213587045669556
Norm after each mp layer: 1.5383001565933228
Norm after each mp layer: 5.25070333480835
Norm after each mp layer: 19.182085037231445
Norm before input: 0.2552422881126404
Norm after input: 0.44213587045669556
Norm after each mp layer: 1.5383001565933228
Norm after each mp layer: 5.25070333480835
Norm after each mp layer: 19.182085037231445
Norm before input: 0.2552422881126404
Norm after input: 0.44210219383239746
Norm after each mp layer: 1.5379213094711304
Norm after each mp layer: 5.252487659454346
Norm after each mp layer: 19.185680389404297
Norm before input: 0.2552422881126404
Norm after input: 0.44210219383239746
Norm after each mp layer: 1.5379213094711304
Norm after each mp layer: 5.252487659454346
Norm after each mp layer: 19.185680389404297
Norm before input: 0.2552422881126404
Norm after input: 0.4420691430568695
Norm after each mp layer: 1.5375492572784424
Norm after each mp layer: 5.2542572021484375
Norm after each mp layer: 19.189062118530273
Norm before input: 0.2552422881126404
Norm after input: 0.4420691430568695
Norm after each mp layer: 1.5375492572784424
Norm after each mp layer: 5.2542572021484375
Norm after each mp layer: 19.189062118530273
Norm before input: 0.2552422881126404
Norm after input: 0.4420369565486908
Norm after each mp layer: 1.5371832847595215
Norm after each mp layer: 5.25601053237915
Norm after each mp layer: 19.192190170288086
Norm before input: 0.2552422881126404
Norm after input: 0.4420369565486908
Norm after each mp layer: 1.5371832847595215
Norm after each mp layer: 5.25601053237915
Norm after each mp layer: 19.192190170288086
Norm before input: 0.2552422881126404
Norm after input: 0.4420054256916046
Norm after each mp layer: 1.5368233919143677
Norm after each mp layer: 5.257749080657959
Norm after each mp layer: 19.195100784301758
Epoch: 370, Loss: 0.0525, Energy: 2411.6736, Train: 99.67%, Valid: 73.60%, Test: 73.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4420054256916046
Norm after each mp layer: 1.5368233919143677
Norm after each mp layer: 5.257749080657959
Norm after each mp layer: 19.195100784301758
Norm before input: 0.2552422881126404
Norm after input: 0.44197434186935425
Norm after each mp layer: 1.5364691019058228
Norm after each mp layer: 5.259474754333496
Norm after each mp layer: 19.19784164428711
Norm before input: 0.2552422881126404
Norm after input: 0.44197434186935425
Norm after each mp layer: 1.5364691019058228
Norm after each mp layer: 5.259474754333496
Norm after each mp layer: 19.19784164428711
Norm before input: 0.2552422881126404
Norm after input: 0.4419436454772949
Norm after each mp layer: 1.536120891571045
Norm after each mp layer: 5.2611870765686035
Norm after each mp layer: 19.20042610168457
Norm before input: 0.2552422881126404
Norm after input: 0.4419436454772949
Norm after each mp layer: 1.536120891571045
Norm after each mp layer: 5.2611870765686035
Norm after each mp layer: 19.20042610168457
Norm before input: 0.2552422881126404
Norm after input: 0.4419133961200714
Norm after each mp layer: 1.5357781648635864
Norm after each mp layer: 5.262881278991699
Norm after each mp layer: 19.20281219482422
Norm before input: 0.2552422881126404
Norm after input: 0.4419133961200714
Norm after each mp layer: 1.5357781648635864
Norm after each mp layer: 5.262881278991699
Norm after each mp layer: 19.20281219482422
Norm before input: 0.2552422881126404
Norm after input: 0.4418838918209076
Norm after each mp layer: 1.5354408025741577
Norm after each mp layer: 5.264553070068359
Norm after each mp layer: 19.204936981201172
Norm before input: 0.2552422881126404
Norm after input: 0.4418838918209076
Norm after each mp layer: 1.5354408025741577
Norm after each mp layer: 5.264553070068359
Norm after each mp layer: 19.204936981201172
Norm before input: 0.2552422881126404
Norm after input: 0.44185516238212585
Norm after each mp layer: 1.535108208656311
Norm after each mp layer: 5.266199588775635
Norm after each mp layer: 19.206785202026367
Epoch: 375, Loss: 0.0511, Energy: 2363.7979, Train: 99.67%, Valid: 73.60%, Test: 73.40%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44185516238212585
Norm after each mp layer: 1.535108208656311
Norm after each mp layer: 5.266199588775635
Norm after each mp layer: 19.206785202026367
Norm before input: 0.2552422881126404
Norm after input: 0.44182702898979187
Norm after each mp layer: 1.5347802639007568
Norm after each mp layer: 5.267824172973633
Norm after each mp layer: 19.208393096923828
Norm before input: 0.2552422881126404
Norm after input: 0.44182702898979187
Norm after each mp layer: 1.5347802639007568
Norm after each mp layer: 5.267824172973633
Norm after each mp layer: 19.208393096923828
Norm before input: 0.2552422881126404
Norm after input: 0.4417993724346161
Norm after each mp layer: 1.5344572067260742
Norm after each mp layer: 5.269428253173828
Norm after each mp layer: 19.209808349609375
Norm before input: 0.2552422881126404
Norm after input: 0.4417993724346161
Norm after each mp layer: 1.5344572067260742
Norm after each mp layer: 5.269428253173828
Norm after each mp layer: 19.209808349609375
Norm before input: 0.2552422881126404
Norm after input: 0.4417720437049866
Norm after each mp layer: 1.5341383218765259
Norm after each mp layer: 5.271013259887695
Norm after each mp layer: 19.211034774780273
Norm before input: 0.2552422881126404
Norm after input: 0.4417720437049866
Norm after each mp layer: 1.5341383218765259
Norm after each mp layer: 5.271013259887695
Norm after each mp layer: 19.211034774780273
Norm before input: 0.2552422881126404
Norm after input: 0.4417453110218048
Norm after each mp layer: 1.5338233709335327
Norm after each mp layer: 5.272575378417969
Norm after each mp layer: 19.212045669555664
Norm before input: 0.2552422881126404
Norm after input: 0.4417453110218048
Norm after each mp layer: 1.5338233709335327
Norm after each mp layer: 5.272575378417969
Norm after each mp layer: 19.212045669555664
Norm before input: 0.2552422881126404
Norm after input: 0.44171908497810364
Norm after each mp layer: 1.533511996269226
Norm after each mp layer: 5.274110794067383
Norm after each mp layer: 19.212785720825195
Epoch: 380, Loss: 0.0497, Energy: 2317.3699, Train: 99.67%, Valid: 73.60%, Test: 73.10%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44171908497810364
Norm after each mp layer: 1.533511996269226
Norm after each mp layer: 5.274110794067383
Norm after each mp layer: 19.212787628173828
Norm before input: 0.2552422881126404
Norm after input: 0.441693514585495
Norm after each mp layer: 1.5332038402557373
Norm after each mp layer: 5.275619029998779
Norm after each mp layer: 19.2132625579834
Norm before input: 0.2552422881126404
Norm after input: 0.441693514585495
Norm after each mp layer: 1.5332038402557373
Norm after each mp layer: 5.275619029998779
Norm after each mp layer: 19.2132625579834
Norm before input: 0.2552422881126404
Norm after input: 0.4416685104370117
Norm after each mp layer: 1.5328987836837769
Norm after each mp layer: 5.277102470397949
Norm after each mp layer: 19.213504791259766
Norm before input: 0.2552422881126404
Norm after input: 0.4416685104370117
Norm after each mp layer: 1.5328987836837769
Norm after each mp layer: 5.277102470397949
Norm after each mp layer: 19.213504791259766
Norm before input: 0.2552422881126404
Norm after input: 0.4416438937187195
Norm after each mp layer: 1.5325969457626343
Norm after each mp layer: 5.278562545776367
Norm after each mp layer: 19.213544845581055
Norm before input: 0.2552422881126404
Norm after input: 0.4416438937187195
Norm after each mp layer: 1.5325969457626343
Norm after each mp layer: 5.278562545776367
Norm after each mp layer: 19.213544845581055
Norm before input: 0.2552422881126404
Norm after input: 0.4416196048259735
Norm after each mp layer: 1.532297968864441
Norm after each mp layer: 5.279998302459717
Norm after each mp layer: 19.213382720947266
Norm before input: 0.2552422881126404
Norm after input: 0.4416196048259735
Norm after each mp layer: 1.532297968864441
Norm after each mp layer: 5.279998302459717
Norm after each mp layer: 19.213382720947266
Norm before input: 0.2552422881126404
Norm after input: 0.4415958821773529
Norm after each mp layer: 1.5320018529891968
Norm after each mp layer: 5.28140926361084
Norm after each mp layer: 19.212982177734375
Epoch: 385, Loss: 0.0484, Energy: 2272.1284, Train: 99.67%, Valid: 73.80%, Test: 73.10%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4415958821773529
Norm after each mp layer: 1.5320018529891968
Norm after each mp layer: 5.28140926361084
Norm after each mp layer: 19.212984085083008
Norm before input: 0.2552422881126404
Norm after input: 0.44157272577285767
Norm after each mp layer: 1.5317087173461914
Norm after each mp layer: 5.282792091369629
Norm after each mp layer: 19.21232795715332
Norm before input: 0.2552422881126404
Norm after input: 0.44157272577285767
Norm after each mp layer: 1.5317087173461914
Norm after each mp layer: 5.282792091369629
Norm after each mp layer: 19.21232795715332
Norm before input: 0.2552422881126404
Norm after input: 0.4415501356124878
Norm after each mp layer: 1.531417965888977
Norm after each mp layer: 5.284147262573242
Norm after each mp layer: 19.211427688598633
Norm before input: 0.2552422881126404
Norm after input: 0.4415501356124878
Norm after each mp layer: 1.531417965888977
Norm after each mp layer: 5.284147262573242
Norm after each mp layer: 19.211427688598633
Norm before input: 0.2552422881126404
Norm after input: 0.44152796268463135
Norm after each mp layer: 1.5311298370361328
Norm after each mp layer: 5.2854790687561035
Norm after each mp layer: 19.21031951904297
Norm before input: 0.2552422881126404
Norm after input: 0.44152796268463135
Norm after each mp layer: 1.5311298370361328
Norm after each mp layer: 5.2854790687561035
Norm after each mp layer: 19.21031951904297
Norm before input: 0.2552422881126404
Norm after input: 0.4415060877799988
Norm after each mp layer: 1.5308442115783691
Norm after each mp layer: 5.286787033081055
Norm after each mp layer: 19.209026336669922
Norm before input: 0.2552422881126404
Norm after input: 0.4415060877799988
Norm after each mp layer: 1.5308442115783691
Norm after each mp layer: 5.286787033081055
Norm after each mp layer: 19.209026336669922
Norm before input: 0.2552422881126404
Norm after input: 0.44148463010787964
Norm after each mp layer: 1.530560851097107
Norm after each mp layer: 5.2880706787109375
Norm after each mp layer: 19.207530975341797
Epoch: 390, Loss: 0.0471, Energy: 2227.8691, Train: 99.67%, Valid: 73.80%, Test: 72.70%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44148463010787964
Norm after each mp layer: 1.530560851097107
Norm after each mp layer: 5.2880706787109375
Norm after each mp layer: 19.207530975341797
Norm before input: 0.2552422881126404
Norm after input: 0.44146376848220825
Norm after each mp layer: 1.5302793979644775
Norm after each mp layer: 5.289327621459961
Norm after each mp layer: 19.205810546875
Norm before input: 0.2552422881126404
Norm after input: 0.44146376848220825
Norm after each mp layer: 1.5302793979644775
Norm after each mp layer: 5.289327621459961
Norm after each mp layer: 19.205808639526367
Norm before input: 0.2552422881126404
Norm after input: 0.4414432942867279
Norm after each mp layer: 1.5299997329711914
Norm after each mp layer: 5.290557861328125
Norm after each mp layer: 19.203853607177734
Norm before input: 0.2552422881126404
Norm after input: 0.4414432942867279
Norm after each mp layer: 1.5299997329711914
Norm after each mp layer: 5.290557861328125
Norm after each mp layer: 19.203853607177734
Norm before input: 0.2552422881126404
Norm after input: 0.4414232671260834
Norm after each mp layer: 1.5297218561172485
Norm after each mp layer: 5.291761875152588
Norm after each mp layer: 19.20168685913086
Norm before input: 0.2552422881126404
Norm after input: 0.4414232671260834
Norm after each mp layer: 1.5297218561172485
Norm after each mp layer: 5.291761875152588
Norm after each mp layer: 19.20168685913086
Norm before input: 0.2552422881126404
Norm after input: 0.4414036273956299
Norm after each mp layer: 1.529445767402649
Norm after each mp layer: 5.292943000793457
Norm after each mp layer: 19.19933319091797
Norm before input: 0.2552422881126404
Norm after input: 0.4414036273956299
Norm after each mp layer: 1.529445767402649
Norm after each mp layer: 5.292943000793457
Norm after each mp layer: 19.19933319091797
Norm before input: 0.2552422881126404
Norm after input: 0.4413844347000122
Norm after each mp layer: 1.529171347618103
Norm after each mp layer: 5.294101715087891
Norm after each mp layer: 19.196800231933594
Epoch: 395, Loss: 0.0459, Energy: 2184.5024, Train: 99.67%, Valid: 73.80%, Test: 72.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4413844347000122
Norm after each mp layer: 1.529171347618103
Norm after each mp layer: 5.294101715087891
Norm after each mp layer: 19.196800231933594
Norm before input: 0.2552422881126404
Norm after input: 0.4413655698299408
Norm after each mp layer: 1.5288989543914795
Norm after each mp layer: 5.295237064361572
Norm after each mp layer: 19.194076538085938
Norm before input: 0.2552422881126404
Norm after input: 0.4413655698299408
Norm after each mp layer: 1.5288989543914795
Norm after each mp layer: 5.295237064361572
Norm after each mp layer: 19.194076538085938
Norm before input: 0.2552422881126404
Norm after input: 0.4413471817970276
Norm after each mp layer: 1.5286282300949097
Norm after each mp layer: 5.29634952545166
Norm after each mp layer: 19.191150665283203
Norm before input: 0.2552422881126404
Norm after input: 0.4413471817970276
Norm after each mp layer: 1.5286282300949097
Norm after each mp layer: 5.29634952545166
Norm after each mp layer: 19.191150665283203
Norm before input: 0.2552422881126404
Norm after input: 0.44132933020591736
Norm after each mp layer: 1.5283591747283936
Norm after each mp layer: 5.297439098358154
Norm after each mp layer: 19.188034057617188
Norm before input: 0.2552422881126404
Norm after input: 0.44132933020591736
Norm after each mp layer: 1.5283591747283936
Norm after each mp layer: 5.297439098358154
Norm after each mp layer: 19.188034057617188
Norm before input: 0.2552422881126404
Norm after input: 0.4413117468357086
Norm after each mp layer: 1.528092384338379
Norm after each mp layer: 5.2985076904296875
Norm after each mp layer: 19.18475341796875
Norm before input: 0.2552422881126404
Norm after input: 0.4413117468357086
Norm after each mp layer: 1.528092384338379
Norm after each mp layer: 5.2985076904296875
Norm after each mp layer: 19.18475341796875
Norm before input: 0.2552422881126404
Norm after input: 0.4412946403026581
Norm after each mp layer: 1.527827262878418
Norm after each mp layer: 5.299557685852051
Norm after each mp layer: 19.18132209777832
Epoch: 400, Loss: 0.0448, Energy: 2141.9546, Train: 99.67%, Valid: 73.80%, Test: 72.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4412946403026581
Norm after each mp layer: 1.527827262878418
Norm after each mp layer: 5.299557685852051
Norm after each mp layer: 19.18132209777832
Norm before input: 0.2552422881126404
Norm after input: 0.44127777218818665
Norm after each mp layer: 1.5275639295578003
Norm after each mp layer: 5.300588607788086
Norm after each mp layer: 19.177736282348633
Norm before input: 0.2552422881126404
Norm after input: 0.44127777218818665
Norm after each mp layer: 1.5275639295578003
Norm after each mp layer: 5.300588607788086
Norm after each mp layer: 19.177736282348633
Norm before input: 0.2552422881126404
Norm after input: 0.4412614107131958
Norm after each mp layer: 1.5273023843765259
Norm after each mp layer: 5.301599979400635
Norm after each mp layer: 19.173982620239258
Norm before input: 0.2552422881126404
Norm after input: 0.4412614107131958
Norm after each mp layer: 1.5273023843765259
Norm after each mp layer: 5.301599979400635
Norm after each mp layer: 19.173982620239258
Norm before input: 0.2552422881126404
Norm after input: 0.441245436668396
Norm after each mp layer: 1.5270426273345947
Norm after each mp layer: 5.3025922775268555
Norm after each mp layer: 19.17007064819336
Norm before input: 0.2552422881126404
Norm after input: 0.441245436668396
Norm after each mp layer: 1.5270426273345947
Norm after each mp layer: 5.3025922775268555
Norm after each mp layer: 19.17007064819336
Norm before input: 0.2552422881126404
Norm after input: 0.44122982025146484
Norm after each mp layer: 1.5267844200134277
Norm after each mp layer: 5.303567409515381
Norm after each mp layer: 19.166015625
Norm before input: 0.2552422881126404
Norm after input: 0.44122982025146484
Norm after each mp layer: 1.5267844200134277
Norm after each mp layer: 5.303567886352539
Norm after each mp layer: 19.166015625
Norm before input: 0.2552422881126404
Norm after input: 0.44121456146240234
Norm after each mp layer: 1.5265283584594727
Norm after each mp layer: 5.304528713226318
Norm after each mp layer: 19.16183853149414
Epoch: 405, Loss: 0.0437, Energy: 2100.2734, Train: 99.67%, Valid: 73.80%, Test: 72.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44121456146240234
Norm after each mp layer: 1.5265283584594727
Norm after each mp layer: 5.304528713226318
Norm after each mp layer: 19.16183853149414
Norm before input: 0.2552422881126404
Norm after input: 0.4411996006965637
Norm after each mp layer: 1.5262744426727295
Norm after each mp layer: 5.30547571182251
Norm after each mp layer: 19.157546997070312
Norm before input: 0.2552422881126404
Norm after input: 0.4411996006965637
Norm after each mp layer: 1.5262744426727295
Norm after each mp layer: 5.30547571182251
Norm after each mp layer: 19.157546997070312
Norm before input: 0.2552422881126404
Norm after input: 0.4411850571632385
Norm after each mp layer: 1.5260224342346191
Norm after each mp layer: 5.306408882141113
Norm after each mp layer: 19.153133392333984
Norm before input: 0.2552422881126404
Norm after input: 0.4411850571632385
Norm after each mp layer: 1.5260224342346191
Norm after each mp layer: 5.306408882141113
Norm after each mp layer: 19.153133392333984
Norm before input: 0.2552422881126404
Norm after input: 0.4411708414554596
Norm after each mp layer: 1.5257726907730103
Norm after each mp layer: 5.307328701019287
Norm after each mp layer: 19.148591995239258
Norm before input: 0.2552422881126404
Norm after input: 0.4411708414554596
Norm after each mp layer: 1.5257726907730103
Norm after each mp layer: 5.307328701019287
Norm after each mp layer: 19.148591995239258
Norm before input: 0.2552422881126404
Norm after input: 0.4411570429801941
Norm after each mp layer: 1.525525450706482
Norm after each mp layer: 5.308237075805664
Norm after each mp layer: 19.143939971923828
Norm before input: 0.2552422881126404
Norm after input: 0.4411570429801941
Norm after each mp layer: 1.525525450706482
Norm after each mp layer: 5.308237075805664
Norm after each mp layer: 19.143939971923828
Norm before input: 0.2552422881126404
Norm after input: 0.44114363193511963
Norm after each mp layer: 1.525280475616455
Norm after each mp layer: 5.309134483337402
Norm after each mp layer: 19.13919448852539
Epoch: 410, Loss: 0.0426, Energy: 2059.5496, Train: 99.67%, Valid: 73.60%, Test: 72.30%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44114363193511963
Norm after each mp layer: 1.525280475616455
Norm after each mp layer: 5.309134483337402
Norm after each mp layer: 19.13919448852539
Norm before input: 0.2552422881126404
Norm after input: 0.44113045930862427
Norm after each mp layer: 1.5250378847122192
Norm after each mp layer: 5.310023784637451
Norm after each mp layer: 19.13436508178711
Norm before input: 0.2552422881126404
Norm after input: 0.44113045930862427
Norm after each mp layer: 1.5250378847122192
Norm after each mp layer: 5.310023784637451
Norm after each mp layer: 19.13436508178711
Norm before input: 0.2552422881126404
Norm after input: 0.44111764430999756
Norm after each mp layer: 1.5247976779937744
Norm after each mp layer: 5.310903549194336
Norm after each mp layer: 19.129444122314453
Norm before input: 0.2552422881126404
Norm after input: 0.44111764430999756
Norm after each mp layer: 1.5247976779937744
Norm after each mp layer: 5.310903549194336
Norm after each mp layer: 19.129444122314453
Norm before input: 0.2552422881126404
Norm after input: 0.4411052167415619
Norm after each mp layer: 1.5245598554611206
Norm after each mp layer: 5.3117756843566895
Norm after each mp layer: 19.124431610107422
Norm before input: 0.2552422881126404
Norm after input: 0.4411052167415619
Norm after each mp layer: 1.5245598554611206
Norm after each mp layer: 5.3117756843566895
Norm after each mp layer: 19.124431610107422
Norm before input: 0.2552422881126404
Norm after input: 0.4410931169986725
Norm after each mp layer: 1.5243247747421265
Norm after each mp layer: 5.3126397132873535
Norm after each mp layer: 19.11933708190918
Norm before input: 0.2552422881126404
Norm after input: 0.4410931169986725
Norm after each mp layer: 1.5243247747421265
Norm after each mp layer: 5.3126397132873535
Norm after each mp layer: 19.11933708190918
Norm before input: 0.2552422881126404
Norm after input: 0.44108131527900696
Norm after each mp layer: 1.524092197418213
Norm after each mp layer: 5.313498020172119
Norm after each mp layer: 19.114173889160156
Epoch: 415, Loss: 0.0416, Energy: 2019.8873, Train: 99.67%, Valid: 73.20%, Test: 72.00%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44108131527900696
Norm after each mp layer: 1.524092197418213
Norm after each mp layer: 5.313498497009277
Norm after each mp layer: 19.114173889160156
Norm before input: 0.2552422881126404
Norm after input: 0.4410697817802429
Norm after each mp layer: 1.5238624811172485
Norm after each mp layer: 5.314352035522461
Norm after each mp layer: 19.10895538330078
Norm before input: 0.2552422881126404
Norm after input: 0.4410697817802429
Norm after each mp layer: 1.5238624811172485
Norm after each mp layer: 5.314352035522461
Norm after each mp layer: 19.10895538330078
Norm before input: 0.2552422881126404
Norm after input: 0.44105860590934753
Norm after each mp layer: 1.5236355066299438
Norm after each mp layer: 5.315201759338379
Norm after each mp layer: 19.10367774963379
Norm before input: 0.2552422881126404
Norm after input: 0.44105860590934753
Norm after each mp layer: 1.5236355066299438
Norm after each mp layer: 5.315201759338379
Norm after each mp layer: 19.10367774963379
Norm before input: 0.2552422881126404
Norm after input: 0.441047728061676
Norm after each mp layer: 1.5234111547470093
Norm after each mp layer: 5.316047191619873
Norm after each mp layer: 19.098339080810547
Norm before input: 0.2552422881126404
Norm after input: 0.441047728061676
Norm after each mp layer: 1.5234111547470093
Norm after each mp layer: 5.316047191619873
Norm after each mp layer: 19.098339080810547
Norm before input: 0.2552422881126404
Norm after input: 0.4410371780395508
Norm after each mp layer: 1.523189902305603
Norm after each mp layer: 5.316888809204102
Norm after each mp layer: 19.092939376831055
Norm before input: 0.2552422881126404
Norm after input: 0.4410371780395508
Norm after each mp layer: 1.523189902305603
Norm after each mp layer: 5.316888809204102
Norm after each mp layer: 19.092939376831055
Norm before input: 0.2552422881126404
Norm after input: 0.4410269558429718
Norm after each mp layer: 1.522971749305725
Norm after each mp layer: 5.317728519439697
Norm after each mp layer: 19.087495803833008
Epoch: 420, Loss: 0.0406, Energy: 1981.3938, Train: 99.75%, Valid: 73.00%, Test: 72.10%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4410269558429718
Norm after each mp layer: 1.522971749305725
Norm after each mp layer: 5.317728519439697
Norm after each mp layer: 19.087495803833008
Norm before input: 0.2552422881126404
Norm after input: 0.44101694226264954
Norm after each mp layer: 1.5227564573287964
Norm after each mp layer: 5.318566799163818
Norm after each mp layer: 19.082012176513672
Norm before input: 0.2552422881126404
Norm after input: 0.44101694226264954
Norm after each mp layer: 1.5227564573287964
Norm after each mp layer: 5.318566799163818
Norm after each mp layer: 19.082012176513672
Norm before input: 0.2552422881126404
Norm after input: 0.44100722670555115
Norm after each mp layer: 1.522544264793396
Norm after each mp layer: 5.319404602050781
Norm after each mp layer: 19.076492309570312
Norm before input: 0.2552422881126404
Norm after input: 0.44100722670555115
Norm after each mp layer: 1.522544264793396
Norm after each mp layer: 5.319404602050781
Norm after each mp layer: 19.076492309570312
Norm before input: 0.2552422881126404
Norm after input: 0.440997838973999
Norm after each mp layer: 1.5223350524902344
Norm after each mp layer: 5.320241928100586
Norm after each mp layer: 19.070932388305664
Norm before input: 0.2552422881126404
Norm after input: 0.440997838973999
Norm after each mp layer: 1.5223350524902344
Norm after each mp layer: 5.320241928100586
Norm after each mp layer: 19.070932388305664
Norm before input: 0.2552422881126404
Norm after input: 0.4409886300563812
Norm after each mp layer: 1.5221291780471802
Norm after each mp layer: 5.321077823638916
Norm after each mp layer: 19.065332412719727
Norm before input: 0.2552422881126404
Norm after input: 0.4409886300563812
Norm after each mp layer: 1.5221291780471802
Norm after each mp layer: 5.321077823638916
Norm after each mp layer: 19.065332412719727
Norm before input: 0.2552422881126404
Norm after input: 0.4409797489643097
Norm after each mp layer: 1.5219262838363647
Norm after each mp layer: 5.321913719177246
Norm after each mp layer: 19.0596981048584
Epoch: 425, Loss: 0.0396, Energy: 1944.1432, Train: 99.75%, Valid: 72.80%, Test: 72.00%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4409797489643097
Norm after each mp layer: 1.5219262838363647
Norm after each mp layer: 5.321913719177246
Norm after each mp layer: 19.0596981048584
Norm before input: 0.2552422881126404
Norm after input: 0.44097116589546204
Norm after each mp layer: 1.5217267274856567
Norm after each mp layer: 5.322750091552734
Norm after each mp layer: 19.05403709411621
Norm before input: 0.2552422881126404
Norm after input: 0.44097116589546204
Norm after each mp layer: 1.5217267274856567
Norm after each mp layer: 5.322750091552734
Norm after each mp layer: 19.05403709411621
Norm before input: 0.2552422881126404
Norm after input: 0.4409627318382263
Norm after each mp layer: 1.5215301513671875
Norm after each mp layer: 5.3235883712768555
Norm after each mp layer: 19.0483455657959
Norm before input: 0.2552422881126404
Norm after input: 0.4409627318382263
Norm after each mp layer: 1.5215301513671875
Norm after each mp layer: 5.3235883712768555
Norm after each mp layer: 19.0483455657959
Norm before input: 0.2552422881126404
Norm after input: 0.4409545660018921
Norm after each mp layer: 1.5213366746902466
Norm after each mp layer: 5.324426174163818
Norm after each mp layer: 19.042621612548828
Norm before input: 0.2552422881126404
Norm after input: 0.4409545660018921
Norm after each mp layer: 1.5213366746902466
Norm after each mp layer: 5.324426174163818
Norm after each mp layer: 19.042621612548828
Norm before input: 0.2552422881126404
Norm after input: 0.44094663858413696
Norm after each mp layer: 1.5211466550827026
Norm after each mp layer: 5.3252644538879395
Norm after each mp layer: 19.036865234375
Norm before input: 0.2552422881126404
Norm after input: 0.44094663858413696
Norm after each mp layer: 1.5211466550827026
Norm after each mp layer: 5.3252644538879395
Norm after each mp layer: 19.036865234375
Norm before input: 0.2552422881126404
Norm after input: 0.4409389793872833
Norm after each mp layer: 1.520959496498108
Norm after each mp layer: 5.326103210449219
Norm after each mp layer: 19.03107452392578
Epoch: 430, Loss: 0.0387, Energy: 1908.1633, Train: 99.75%, Valid: 72.80%, Test: 71.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4409389793872833
Norm after each mp layer: 1.520959496498108
Norm after each mp layer: 5.326103210449219
Norm after each mp layer: 19.03107452392578
Norm before input: 0.2552422881126404
Norm after input: 0.4409314692020416
Norm after each mp layer: 1.5207756757736206
Norm after each mp layer: 5.3269429206848145
Norm after each mp layer: 19.02525520324707
Norm before input: 0.2552422881126404
Norm after input: 0.4409314692020416
Norm after each mp layer: 1.5207756757736206
Norm after each mp layer: 5.3269429206848145
Norm after each mp layer: 19.02525520324707
Norm before input: 0.2552422881126404
Norm after input: 0.4409242272377014
Norm after each mp layer: 1.520594835281372
Norm after each mp layer: 5.327782154083252
Norm after each mp layer: 19.01940155029297
Norm before input: 0.2552422881126404
Norm after input: 0.4409242272377014
Norm after each mp layer: 1.520594835281372
Norm after each mp layer: 5.327782154083252
Norm after each mp layer: 19.01940155029297
Norm before input: 0.2552422881126404
Norm after input: 0.44091713428497314
Norm after each mp layer: 1.5204170942306519
Norm after each mp layer: 5.328621864318848
Norm after each mp layer: 19.01350975036621
Norm before input: 0.2552422881126404
Norm after input: 0.44091713428497314
Norm after each mp layer: 1.5204170942306519
Norm after each mp layer: 5.328621864318848
Norm after each mp layer: 19.01350975036621
Norm before input: 0.2552422881126404
Norm after input: 0.4409102499485016
Norm after each mp layer: 1.5202423334121704
Norm after each mp layer: 5.329460144042969
Norm after each mp layer: 19.007577896118164
Norm before input: 0.2552422881126404
Norm after input: 0.4409102499485016
Norm after each mp layer: 1.5202423334121704
Norm after each mp layer: 5.329460144042969
Norm after each mp layer: 19.007577896118164
Norm before input: 0.2552422881126404
Norm after input: 0.44090354442596436
Norm after each mp layer: 1.5200703144073486
Norm after each mp layer: 5.330297470092773
Norm after each mp layer: 19.00160026550293
Epoch: 435, Loss: 0.0378, Energy: 1873.4143, Train: 99.75%, Valid: 72.80%, Test: 71.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44090354442596436
Norm after each mp layer: 1.5200703144073486
Norm after each mp layer: 5.330297470092773
Norm after each mp layer: 19.00160026550293
Norm before input: 0.2552422881126404
Norm after input: 0.44089704751968384
Norm after each mp layer: 1.5199013948440552
Norm after each mp layer: 5.3311333656311035
Norm after each mp layer: 18.995580673217773
Norm before input: 0.2552422881126404
Norm after input: 0.44089704751968384
Norm after each mp layer: 1.5199013948440552
Norm after each mp layer: 5.3311333656311035
Norm after each mp layer: 18.995580673217773
Norm before input: 0.2552422881126404
Norm after input: 0.4408906400203705
Norm after each mp layer: 1.5197349786758423
Norm after each mp layer: 5.331967353820801
Norm after each mp layer: 18.9895076751709
Norm before input: 0.2552422881126404
Norm after input: 0.4408906400203705
Norm after each mp layer: 1.5197349786758423
Norm after each mp layer: 5.331967353820801
Norm after each mp layer: 18.9895076751709
Norm before input: 0.2552422881126404
Norm after input: 0.44088447093963623
Norm after each mp layer: 1.519571304321289
Norm after each mp layer: 5.332798004150391
Norm after each mp layer: 18.98337745666504
Norm before input: 0.2552422881126404
Norm after input: 0.44088447093963623
Norm after each mp layer: 1.519571304321289
Norm after each mp layer: 5.332798004150391
Norm after each mp layer: 18.98337745666504
Norm before input: 0.2552422881126404
Norm after input: 0.44087842106819153
Norm after each mp layer: 1.519410252571106
Norm after each mp layer: 5.333624839782715
Norm after each mp layer: 18.977188110351562
Norm before input: 0.2552422881126404
Norm after input: 0.44087842106819153
Norm after each mp layer: 1.519410252571106
Norm after each mp layer: 5.333624839782715
Norm after each mp layer: 18.977188110351562
Norm before input: 0.2552422881126404
Norm after input: 0.4408724904060364
Norm after each mp layer: 1.5192517042160034
Norm after each mp layer: 5.334447860717773
Norm after each mp layer: 18.97093963623047
Epoch: 440, Loss: 0.0370, Energy: 1839.7784, Train: 99.75%, Valid: 72.80%, Test: 71.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4408724904060364
Norm after each mp layer: 1.5192517042160034
Norm after each mp layer: 5.334447860717773
Norm after each mp layer: 18.97093963623047
Norm before input: 0.2552422881126404
Norm after input: 0.44086670875549316
Norm after each mp layer: 1.5190951824188232
Norm after each mp layer: 5.335266590118408
Norm after each mp layer: 18.96462631225586
Norm before input: 0.2552422881126404
Norm after input: 0.44086670875549316
Norm after each mp layer: 1.5190951824188232
Norm after each mp layer: 5.335266590118408
Norm after each mp layer: 18.96462631225586
Norm before input: 0.2552422881126404
Norm after input: 0.4408610761165619
Norm after each mp layer: 1.5189416408538818
Norm after each mp layer: 5.336080074310303
Norm after each mp layer: 18.9582462310791
Norm before input: 0.2552422881126404
Norm after input: 0.4408610761165619
Norm after each mp layer: 1.5189416408538818
Norm after each mp layer: 5.336080074310303
Norm after each mp layer: 18.9582462310791
Norm before input: 0.2552422881126404
Norm after input: 0.4408555328845978
Norm after each mp layer: 1.5187897682189941
Norm after each mp layer: 5.336887359619141
Norm after each mp layer: 18.9517879486084
Norm before input: 0.2552422881126404
Norm after input: 0.4408555328845978
Norm after each mp layer: 1.5187897682189941
Norm after each mp layer: 5.336887359619141
Norm after each mp layer: 18.9517879486084
Norm before input: 0.2552422881126404
Norm after input: 0.44085007905960083
Norm after each mp layer: 1.5186402797698975
Norm after each mp layer: 5.337689399719238
Norm after each mp layer: 18.94525909423828
Norm before input: 0.2552422881126404
Norm after input: 0.44085007905960083
Norm after each mp layer: 1.5186402797698975
Norm after each mp layer: 5.337689399719238
Norm after each mp layer: 18.94525909423828
Norm before input: 0.2552422881126404
Norm after input: 0.4408448040485382
Norm after each mp layer: 1.5184929370880127
Norm after each mp layer: 5.338484287261963
Norm after each mp layer: 18.938655853271484
Epoch: 445, Loss: 0.0362, Energy: 1807.1005, Train: 99.75%, Valid: 72.60%, Test: 71.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4408448040485382
Norm after each mp layer: 1.5184929370880127
Norm after each mp layer: 5.338484287261963
Norm after each mp layer: 18.938655853271484
Norm before input: 0.2552422881126404
Norm after input: 0.440839558839798
Norm after each mp layer: 1.5183476209640503
Norm after each mp layer: 5.339273452758789
Norm after each mp layer: 18.931976318359375
Norm before input: 0.2552422881126404
Norm after input: 0.440839558839798
Norm after each mp layer: 1.5183476209640503
Norm after each mp layer: 5.339273452758789
Norm after each mp layer: 18.931976318359375
Norm before input: 0.2552422881126404
Norm after input: 0.4408344626426697
Norm after each mp layer: 1.5182044506072998
Norm after each mp layer: 5.340055465698242
Norm after each mp layer: 18.92521858215332
Norm before input: 0.2552422881126404
Norm after input: 0.4408344626426697
Norm after each mp layer: 1.5182044506072998
Norm after each mp layer: 5.340055465698242
Norm after each mp layer: 18.92521858215332
Norm before input: 0.2552422881126404
Norm after input: 0.44082948565483093
Norm after each mp layer: 1.5180635452270508
Norm after each mp layer: 5.340831756591797
Norm after each mp layer: 18.918380737304688
Norm before input: 0.2552422881126404
Norm after input: 0.44082948565483093
Norm after each mp layer: 1.5180635452270508
Norm after each mp layer: 5.340831279754639
Norm after each mp layer: 18.918380737304688
Norm before input: 0.2552422881126404
Norm after input: 0.44082456827163696
Norm after each mp layer: 1.5179243087768555
Norm after each mp layer: 5.3416008949279785
Norm after each mp layer: 18.91147232055664
Norm before input: 0.2552422881126404
Norm after input: 0.44082456827163696
Norm after each mp layer: 1.5179243087768555
Norm after each mp layer: 5.3416008949279785
Norm after each mp layer: 18.91147232055664
Norm before input: 0.2552422881126404
Norm after input: 0.4408196806907654
Norm after each mp layer: 1.517787218093872
Norm after each mp layer: 5.342364311218262
Norm after each mp layer: 18.904489517211914
Epoch: 450, Loss: 0.0354, Energy: 1775.2500, Train: 99.75%, Valid: 72.40%, Test: 71.90%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.4408196806907654
Norm after each mp layer: 1.517787218093872
Norm after each mp layer: 5.342364311218262
Norm after each mp layer: 18.904489517211914
Norm before input: 0.2552422881126404
Norm after input: 0.4408149719238281
Norm after each mp layer: 1.5176520347595215
Norm after each mp layer: 5.343121528625488
Norm after each mp layer: 18.897436141967773
Norm before input: 0.2552422881126404
Norm after input: 0.4408149719238281
Norm after each mp layer: 1.5176520347595215
Norm after each mp layer: 5.343121528625488
Norm after each mp layer: 18.897436141967773
Norm before input: 0.2552422881126404
Norm after input: 0.44081032276153564
Norm after each mp layer: 1.5175193548202515
Norm after each mp layer: 5.343873977661133
Norm after each mp layer: 18.89031219482422
Norm before input: 0.2552422881126404
Norm after input: 0.44081032276153564
Norm after each mp layer: 1.5175193548202515
Norm after each mp layer: 5.343873977661133
Norm after each mp layer: 18.89031219482422
Norm before input: 0.2552422881126404
Norm after input: 0.44080570340156555
Norm after each mp layer: 1.5173885822296143
Norm after each mp layer: 5.344621181488037
Norm after each mp layer: 18.883121490478516
Norm before input: 0.2552422881126404
Norm after input: 0.44080570340156555
Norm after each mp layer: 1.5173885822296143
Norm after each mp layer: 5.344621181488037
Norm after each mp layer: 18.883121490478516
Norm before input: 0.2552422881126404
Norm after input: 0.440801203250885
Norm after each mp layer: 1.5172600746154785
Norm after each mp layer: 5.345364093780518
Norm after each mp layer: 18.87586784362793
Norm before input: 0.2552422881126404
Norm after input: 0.440801203250885
Norm after each mp layer: 1.5172600746154785
Norm after each mp layer: 5.345364093780518
Norm after each mp layer: 18.87586784362793
Norm before input: 0.2552422881126404
Norm after input: 0.44079670310020447
Norm after each mp layer: 1.5171335935592651
Norm after each mp layer: 5.346102714538574
Norm after each mp layer: 18.86855697631836
Epoch: 455, Loss: 0.0347, Energy: 1744.1805, Train: 99.75%, Valid: 71.60%, Test: 71.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44079670310020447
Norm after each mp layer: 1.5171335935592651
Norm after each mp layer: 5.346102714538574
Norm after each mp layer: 18.86855697631836
Norm before input: 0.2552422881126404
Norm after input: 0.44079238176345825
Norm after each mp layer: 1.5170093774795532
Norm after each mp layer: 5.346838474273682
Norm after each mp layer: 18.861188888549805
Norm before input: 0.2552422881126404
Norm after input: 0.44079238176345825
Norm after each mp layer: 1.5170093774795532
Norm after each mp layer: 5.346838474273682
Norm after each mp layer: 18.861188888549805
Norm before input: 0.2552422881126404
Norm after input: 0.44078806042671204
Norm after each mp layer: 1.516887903213501
Norm after each mp layer: 5.34757137298584
Norm after each mp layer: 18.853771209716797
Norm before input: 0.2552422881126404
Norm after input: 0.44078806042671204
Norm after each mp layer: 1.516887903213501
Norm after each mp layer: 5.34757137298584
Norm after each mp layer: 18.853771209716797
Norm before input: 0.2552422881126404
Norm after input: 0.44078388810157776
Norm after each mp layer: 1.516768455505371
Norm after each mp layer: 5.348302364349365
Norm after each mp layer: 18.8463077545166
Norm before input: 0.2552422881126404
Norm after input: 0.44078388810157776
Norm after each mp layer: 1.516768455505371
Norm after each mp layer: 5.348302364349365
Norm after each mp layer: 18.8463077545166
Norm before input: 0.2552422881126404
Norm after input: 0.44077974557876587
Norm after each mp layer: 1.5166516304016113
Norm after each mp layer: 5.349032402038574
Norm after each mp layer: 18.838796615600586
Norm before input: 0.2552422881126404
Norm after input: 0.44077974557876587
Norm after each mp layer: 1.5166516304016113
Norm after each mp layer: 5.349032402038574
Norm after each mp layer: 18.838796615600586
Norm before input: 0.2552422881126404
Norm after input: 0.44077569246292114
Norm after each mp layer: 1.5165371894836426
Norm after each mp layer: 5.34976053237915
Norm after each mp layer: 18.831247329711914
Epoch: 460, Loss: 0.0339, Energy: 1713.9216, Train: 99.75%, Valid: 71.60%, Test: 71.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44077569246292114
Norm after each mp layer: 1.5165371894836426
Norm after each mp layer: 5.34976053237915
Norm after each mp layer: 18.831247329711914
Norm before input: 0.2552422881126404
Norm after input: 0.4407717287540436
Norm after each mp layer: 1.5164254903793335
Norm after each mp layer: 5.350490093231201
Norm after each mp layer: 18.82366371154785
Norm before input: 0.2552422881126404
Norm after input: 0.4407717287540436
Norm after each mp layer: 1.516425371170044
Norm after each mp layer: 5.350490093231201
Norm after each mp layer: 18.82366371154785
Norm before input: 0.2552422881126404
Norm after input: 0.440767765045166
Norm after each mp layer: 1.5163159370422363
Norm after each mp layer: 5.351218223571777
Norm after each mp layer: 18.8160457611084
Norm before input: 0.2552422881126404
Norm after input: 0.440767765045166
Norm after each mp layer: 1.5163159370422363
Norm after each mp layer: 5.351218223571777
Norm after each mp layer: 18.8160457611084
Norm before input: 0.2552422881126404
Norm after input: 0.4407639801502228
Norm after each mp layer: 1.5162094831466675
Norm after each mp layer: 5.351947784423828
Norm after each mp layer: 18.80839729309082
Norm before input: 0.2552422881126404
Norm after input: 0.4407639801502228
Norm after each mp layer: 1.5162094831466675
Norm after each mp layer: 5.351947784423828
Norm after each mp layer: 18.808399200439453
Norm before input: 0.2552422881126404
Norm after input: 0.44076022505760193
Norm after each mp layer: 1.5161055326461792
Norm after each mp layer: 5.352678298950195
Norm after each mp layer: 18.800724029541016
Norm before input: 0.2552422881126404
Norm after input: 0.44076022505760193
Norm after each mp layer: 1.5161055326461792
Norm after each mp layer: 5.352678298950195
Norm after each mp layer: 18.800724029541016
Norm before input: 0.2552422881126404
Norm after input: 0.44075658917427063
Norm after each mp layer: 1.516003966331482
Norm after each mp layer: 5.353410720825195
Norm after each mp layer: 18.79302406311035
Epoch: 465, Loss: 0.0332, Energy: 1684.5317, Train: 99.75%, Valid: 71.60%, Test: 71.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44075658917427063
Norm after each mp layer: 1.516003966331482
Norm after each mp layer: 5.353410720825195
Norm after each mp layer: 18.79302406311035
Norm before input: 0.2552422881126404
Norm after input: 0.4407529830932617
Norm after each mp layer: 1.5159053802490234
Norm after each mp layer: 5.35414457321167
Norm after each mp layer: 18.785303115844727
Norm before input: 0.2552422881126404
Norm after input: 0.4407529830932617
Norm after each mp layer: 1.5159053802490234
Norm after each mp layer: 5.35414457321167
Norm after each mp layer: 18.785303115844727
Norm before input: 0.2552422881126404
Norm after input: 0.44074946641921997
Norm after each mp layer: 1.5158092975616455
Norm after each mp layer: 5.3548808097839355
Norm after each mp layer: 18.777559280395508
Norm before input: 0.2552422881126404
Norm after input: 0.44074946641921997
Norm after each mp layer: 1.5158092975616455
Norm after each mp layer: 5.3548808097839355
Norm after each mp layer: 18.777559280395508
Norm before input: 0.2552422881126404
Norm after input: 0.4407460391521454
Norm after each mp layer: 1.5157160758972168
Norm after each mp layer: 5.355619430541992
Norm after each mp layer: 18.76980209350586
Norm before input: 0.2552422881126404
Norm after input: 0.4407460391521454
Norm after each mp layer: 1.5157160758972168
Norm after each mp layer: 5.355619430541992
Norm after each mp layer: 18.76980209350586
Norm before input: 0.2552422881126404
Norm after input: 0.4407426118850708
Norm after each mp layer: 1.5156253576278687
Norm after each mp layer: 5.356360912322998
Norm after each mp layer: 18.762027740478516
Norm before input: 0.2552422881126404
Norm after input: 0.4407426118850708
Norm after each mp layer: 1.5156253576278687
Norm after each mp layer: 5.356360912322998
Norm after each mp layer: 18.762027740478516
Norm before input: 0.2552422881126404
Norm after input: 0.44073936343193054
Norm after each mp layer: 1.5155375003814697
Norm after each mp layer: 5.357104301452637
Norm after each mp layer: 18.75423812866211
Epoch: 470, Loss: 0.0326, Energy: 1656.0526, Train: 99.83%, Valid: 71.80%, Test: 71.50%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44073936343193054
Norm after each mp layer: 1.5155375003814697
Norm after each mp layer: 5.357104301452637
Norm after each mp layer: 18.75423812866211
Norm before input: 0.2552422881126404
Norm after input: 0.4407361149787903
Norm after each mp layer: 1.5154523849487305
Norm after each mp layer: 5.357851505279541
Norm after each mp layer: 18.746437072753906
Norm before input: 0.2552422881126404
Norm after input: 0.4407361149787903
Norm after each mp layer: 1.5154523849487305
Norm after each mp layer: 5.357851505279541
Norm after each mp layer: 18.746437072753906
Norm before input: 0.2552422881126404
Norm after input: 0.4407329559326172
Norm after each mp layer: 1.5153700113296509
Norm after each mp layer: 5.3586015701293945
Norm after each mp layer: 18.738622665405273
Norm before input: 0.2552422881126404
Norm after input: 0.4407329559326172
Norm after each mp layer: 1.5153700113296509
Norm after each mp layer: 5.3586015701293945
Norm after each mp layer: 18.738622665405273
Norm before input: 0.2552422881126404
Norm after input: 0.44072988629341125
Norm after each mp layer: 1.5152899026870728
Norm after each mp layer: 5.3593549728393555
Norm after each mp layer: 18.730798721313477
Norm before input: 0.2552422881126404
Norm after input: 0.44072988629341125
Norm after each mp layer: 1.5152899026870728
Norm after each mp layer: 5.3593549728393555
Norm after each mp layer: 18.730798721313477
Norm before input: 0.2552422881126404
Norm after input: 0.44072696566581726
Norm after each mp layer: 1.5152130126953125
Norm after each mp layer: 5.360111713409424
Norm after each mp layer: 18.722965240478516
Norm before input: 0.2552422881126404
Norm after input: 0.44072696566581726
Norm after each mp layer: 1.5152130126953125
Norm after each mp layer: 5.360111713409424
Norm after each mp layer: 18.722965240478516
Norm before input: 0.2552422881126404
Norm after input: 0.44072404503822327
Norm after each mp layer: 1.5151385068893433
Norm after each mp layer: 5.360872268676758
Norm after each mp layer: 18.715124130249023
Epoch: 475, Loss: 0.0319, Energy: 1628.4996, Train: 99.83%, Valid: 71.60%, Test: 71.50%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44072404503822327
Norm after each mp layer: 1.5151385068893433
Norm after each mp layer: 5.360872268676758
Norm after each mp layer: 18.715124130249023
Norm before input: 0.2552422881126404
Norm after input: 0.44072115421295166
Norm after each mp layer: 1.5150665044784546
Norm after each mp layer: 5.361635684967041
Norm after each mp layer: 18.707271575927734
Norm before input: 0.2552422881126404
Norm after input: 0.44072115421295166
Norm after each mp layer: 1.5150665044784546
Norm after each mp layer: 5.361635684967041
Norm after each mp layer: 18.707271575927734
Norm before input: 0.2552422881126404
Norm after input: 0.440718412399292
Norm after each mp layer: 1.5149974822998047
Norm after each mp layer: 5.362402439117432
Norm after each mp layer: 18.69941520690918
Norm before input: 0.2552422881126404
Norm after input: 0.440718412399292
Norm after each mp layer: 1.5149974822998047
Norm after each mp layer: 5.362402439117432
Norm after each mp layer: 18.69941520690918
Norm before input: 0.2552422881126404
Norm after input: 0.4407157301902771
Norm after each mp layer: 1.5149307250976562
Norm after each mp layer: 5.363173484802246
Norm after each mp layer: 18.691553115844727
Norm before input: 0.2552422881126404
Norm after input: 0.4407157301902771
Norm after each mp layer: 1.5149307250976562
Norm after each mp layer: 5.363173484802246
Norm after each mp layer: 18.691553115844727
Norm before input: 0.2552422881126404
Norm after input: 0.4407130777835846
Norm after each mp layer: 1.5148669481277466
Norm after each mp layer: 5.36394739151001
Norm after each mp layer: 18.683685302734375
Norm before input: 0.2552422881126404
Norm after input: 0.4407130777835846
Norm after each mp layer: 1.5148669481277466
Norm after each mp layer: 5.36394739151001
Norm after each mp layer: 18.683685302734375
Norm before input: 0.2552422881126404
Norm after input: 0.44071051478385925
Norm after each mp layer: 1.5148056745529175
Norm after each mp layer: 5.364725589752197
Norm after each mp layer: 18.675811767578125
Epoch: 480, Loss: 0.0313, Energy: 1601.8621, Train: 99.83%, Valid: 71.40%, Test: 71.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44071051478385925
Norm after each mp layer: 1.5148056745529175
Norm after each mp layer: 5.364725589752197
Norm after each mp layer: 18.675811767578125
Norm before input: 0.2552422881126404
Norm after input: 0.4407079815864563
Norm after each mp layer: 1.5147470235824585
Norm after each mp layer: 5.365507125854492
Norm after each mp layer: 18.66793441772461
Norm before input: 0.2552422881126404
Norm after input: 0.4407079815864563
Norm after each mp layer: 1.5147470235824585
Norm after each mp layer: 5.365507125854492
Norm after each mp layer: 18.66793441772461
Norm before input: 0.2552422881126404
Norm after input: 0.4407055675983429
Norm after each mp layer: 1.5146909952163696
Norm after each mp layer: 5.366292476654053
Norm after each mp layer: 18.660051345825195
Norm before input: 0.2552422881126404
Norm after input: 0.4407055675983429
Norm after each mp layer: 1.5146909952163696
Norm after each mp layer: 5.366292476654053
Norm after each mp layer: 18.660051345825195
Norm before input: 0.2552422881126404
Norm after input: 0.44070321321487427
Norm after each mp layer: 1.5146373510360718
Norm after each mp layer: 5.367081165313721
Norm after each mp layer: 18.652170181274414
Norm before input: 0.2552422881126404
Norm after input: 0.44070321321487427
Norm after each mp layer: 1.5146373510360718
Norm after each mp layer: 5.367081165313721
Norm after each mp layer: 18.652170181274414
Norm before input: 0.2552422881126404
Norm after input: 0.44070085883140564
Norm after each mp layer: 1.514586329460144
Norm after each mp layer: 5.367873191833496
Norm after each mp layer: 18.644283294677734
Norm before input: 0.2552422881126404
Norm after input: 0.44070085883140564
Norm after each mp layer: 1.514586329460144
Norm after each mp layer: 5.367873191833496
Norm after each mp layer: 18.644283294677734
Norm before input: 0.2552422881126404
Norm after input: 0.44069865345954895
Norm after each mp layer: 1.514538049697876
Norm after each mp layer: 5.368669509887695
Norm after each mp layer: 18.63639259338379
Epoch: 485, Loss: 0.0307, Energy: 1576.1124, Train: 99.83%, Valid: 71.40%, Test: 71.60%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44069865345954895
Norm after each mp layer: 1.514538049697876
Norm after each mp layer: 5.368669509887695
Norm after each mp layer: 18.63639259338379
Norm before input: 0.2552422881126404
Norm after input: 0.44069644808769226
Norm after each mp layer: 1.5144922733306885
Norm after each mp layer: 5.369468688964844
Norm after each mp layer: 18.628501892089844
Norm before input: 0.2552422881126404
Norm after input: 0.44069644808769226
Norm after each mp layer: 1.5144922733306885
Norm after each mp layer: 5.369468688964844
Norm after each mp layer: 18.628501892089844
Norm before input: 0.2552422881126404
Norm after input: 0.44069427251815796
Norm after each mp layer: 1.5144487619400024
Norm after each mp layer: 5.3702712059021
Norm after each mp layer: 18.620609283447266
Norm before input: 0.2552422881126404
Norm after input: 0.44069427251815796
Norm after each mp layer: 1.5144487619400024
Norm after each mp layer: 5.3702712059021
Norm after each mp layer: 18.620609283447266
Norm before input: 0.2552422881126404
Norm after input: 0.4406922161579132
Norm after each mp layer: 1.5144078731536865
Norm after each mp layer: 5.371077537536621
Norm after each mp layer: 18.612716674804688
Norm before input: 0.2552422881126404
Norm after input: 0.4406922161579132
Norm after each mp layer: 1.5144078731536865
Norm after each mp layer: 5.371077537536621
Norm after each mp layer: 18.612716674804688
Norm before input: 0.2552422881126404
Norm after input: 0.44069021940231323
Norm after each mp layer: 1.5143694877624512
Norm after each mp layer: 5.371887683868408
Norm after each mp layer: 18.60482406616211
Norm before input: 0.2552422881126404
Norm after input: 0.44069021940231323
Norm after each mp layer: 1.5143694877624512
Norm after each mp layer: 5.371888160705566
Norm after each mp layer: 18.60482406616211
Norm before input: 0.2552422881126404
Norm after input: 0.44068825244903564
Norm after each mp layer: 1.5143336057662964
Norm after each mp layer: 5.372701168060303
Norm after each mp layer: 18.5969295501709
Epoch: 490, Loss: 0.0301, Energy: 1551.2123, Train: 99.83%, Valid: 71.40%, Test: 71.50%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44068825244903564
Norm after each mp layer: 1.5143336057662964
Norm after each mp layer: 5.372701168060303
Norm after each mp layer: 18.5969295501709
Norm before input: 0.2552422881126404
Norm after input: 0.44068631529808044
Norm after each mp layer: 1.514299988746643
Norm after each mp layer: 5.3735175132751465
Norm after each mp layer: 18.589035034179688
Norm before input: 0.2552422881126404
Norm after input: 0.44068631529808044
Norm after each mp layer: 1.514299988746643
Norm after each mp layer: 5.3735175132751465
Norm after each mp layer: 18.589035034179688
Norm before input: 0.2552422881126404
Norm after input: 0.44068440794944763
Norm after each mp layer: 1.5142689943313599
Norm after each mp layer: 5.374337673187256
Norm after each mp layer: 18.581144332885742
Norm before input: 0.2552422881126404
Norm after input: 0.44068440794944763
Norm after each mp layer: 1.5142689943313599
Norm after each mp layer: 5.374337673187256
Norm after each mp layer: 18.581144332885742
Norm before input: 0.2552422881126404
Norm after input: 0.4406825602054596
Norm after each mp layer: 1.5142403841018677
Norm after each mp layer: 5.375161647796631
Norm after each mp layer: 18.57325553894043
Norm before input: 0.2552422881126404
Norm after input: 0.4406825602054596
Norm after each mp layer: 1.5142403841018677
Norm after each mp layer: 5.375161647796631
Norm after each mp layer: 18.57325553894043
Norm before input: 0.2552422881126404
Norm after input: 0.44068077206611633
Norm after each mp layer: 1.514214277267456
Norm after each mp layer: 5.375988483428955
Norm after each mp layer: 18.565364837646484
Norm before input: 0.2552422881126404
Norm after input: 0.44068077206611633
Norm after each mp layer: 1.514214277267456
Norm after each mp layer: 5.375988483428955
Norm after each mp layer: 18.565364837646484
Norm before input: 0.2552422881126404
Norm after input: 0.44067904353141785
Norm after each mp layer: 1.514190435409546
Norm after each mp layer: 5.376819133758545
Norm after each mp layer: 18.557477951049805
Epoch: 495, Loss: 0.0295, Energy: 1527.1199, Train: 99.83%, Valid: 71.60%, Test: 71.40%, Best Valid: 82.00%, Best Test: 79.40%
Norm before input: 0.2552422881126404
Norm after input: 0.44067904353141785
Norm after each mp layer: 1.514190435409546
Norm after each mp layer: 5.376819133758545
Norm after each mp layer: 18.557477951049805
Norm before input: 0.2552422881126404
Norm after input: 0.44067734479904175
Norm after each mp layer: 1.5141689777374268
Norm after each mp layer: 5.377654075622559
Norm after each mp layer: 18.549592971801758
Norm before input: 0.2552422881126404
Norm after input: 0.44067734479904175
Norm after each mp layer: 1.5141689777374268
Norm after each mp layer: 5.377654075622559
Norm after each mp layer: 18.549592971801758
Norm before input: 0.2552422881126404
Norm after input: 0.4406757056713104
Norm after each mp layer: 1.5141500234603882
Norm after each mp layer: 5.378492832183838
Norm after each mp layer: 18.541711807250977
Norm before input: 0.2552422881126404
Norm after input: 0.4406757056713104
Norm after each mp layer: 1.5141500234603882
Norm after each mp layer: 5.378492832183838
Norm after each mp layer: 18.541711807250977
Norm before input: 0.2552422881126404
Norm after input: 0.4406740963459015
Norm after each mp layer: 1.5141332149505615
Norm after each mp layer: 5.379335880279541
Norm after each mp layer: 18.533832550048828
Norm before input: 0.2552422881126404
Norm after input: 0.4406740963459015
Norm after each mp layer: 1.5141332149505615
Norm after each mp layer: 5.379335880279541
Norm after each mp layer: 18.533832550048828
Norm before input: 0.2552422881126404
Norm after input: 0.44067254662513733
Norm after each mp layer: 1.514119029045105
Norm after each mp layer: 5.380183696746826
Norm after each mp layer: 18.525955200195312
train_accuracy_list: [0.28228476821192056, 0.28228476821192056, 0.16225165562913907, 0.16225165562913907, 0.28228476821192056, 0.28228476821192056, 0.28228476821192056, 0.13245033112582782, 0.16225165562913907, 0.28228476821192056, 0.28228476821192056, 0.28228476821192056, 0.2814569536423841, 0.1630794701986755, 0.16225165562913907, 0.16225165562913907, 0.30629139072847683, 0.3890728476821192, 0.41804635761589404, 0.4205298013245033, 0.42549668874172186, 0.4337748344370861, 0.43956953642384106, 0.4412251655629139, 0.43956953642384106, 0.4470198675496689, 0.478476821192053, 0.4966887417218543, 0.5231788079470199, 0.5372516556291391, 0.5521523178807947, 0.5687086092715232, 0.5596026490066225, 0.5455298013245033, 0.5554635761589404, 0.5554635761589404, 0.5587748344370861, 0.5596026490066225, 0.5753311258278145, 0.6067880794701986, 0.6357615894039735, 0.6614238410596026, 0.6895695364238411, 0.7475165562913907, 0.7508278145695364, 0.7574503311258278, 0.7938741721854304, 0.8145695364238411, 0.8269867549668874, 0.8369205298013245, 0.8460264900662252, 0.8518211920529801, 0.8509933774834437, 0.8667218543046358, 0.8725165562913907, 0.8799668874172185, 0.8849337748344371, 0.8841059602649006, 0.8857615894039735, 0.8923841059602649, 0.8998344370860927, 0.8981788079470199, 0.9006622516556292, 0.9105960264900662, 0.9122516556291391, 0.9130794701986755, 0.9139072847682119, 0.918046357615894, 0.9197019867549668, 0.9213576158940397, 0.9221854304635762, 0.9271523178807947, 0.9321192052980133, 0.9337748344370861, 0.9362582781456954, 0.9370860927152318, 0.9395695364238411, 0.9420529801324503, 0.9478476821192053, 0.9461920529801324, 0.9486754966887417, 0.9461920529801324, 0.9288079470198676, 0.956953642384106, 0.9387417218543046, 0.9238410596026491, 0.9461920529801324, 0.9445364238410596, 0.9354304635761589, 0.9461920529801324, 0.9205298013245033, 0.9561258278145696, 0.9412251655629139, 0.9370860927152318, 0.9536423841059603, 0.9395695364238411, 0.9428807947019867, 0.9586092715231788, 0.9519867549668874, 0.9478476821192053, 0.9519867549668874, 0.9470198675496688, 0.9536423841059603, 0.9602649006622517, 0.9528145695364238, 0.9577814569536424, 0.9544701986754967, 0.9619205298013245, 0.9561258278145696, 0.9586092715231788, 0.9652317880794702, 0.9668874172185431, 0.9660596026490066, 0.9627483443708609, 0.9627483443708609, 0.9652317880794702, 0.9677152317880795, 0.9668874172185431, 0.9685430463576159, 0.9710264900662252, 0.9701986754966887, 0.9718543046357616, 0.9693708609271523, 0.9685430463576159, 0.9776490066225165, 0.9768211920529801, 0.9784768211920529, 0.9735099337748344, 0.9759933774834437, 0.9784768211920529, 0.9801324503311258, 0.9801324503311258, 0.9793046357615894, 0.9768211920529801, 0.9768211920529801, 0.9784768211920529, 0.9809602649006622, 0.9809602649006622, 0.9793046357615894, 0.9801324503311258, 0.9817880794701986, 0.9826158940397351, 0.9826158940397351, 0.9826158940397351, 0.9826158940397351, 0.9817880794701986, 0.9834437086092715, 0.9826158940397351, 0.9859271523178808, 0.9850993377483444, 0.9859271523178808, 0.9867549668874173, 0.9867549668874173, 0.9859271523178808, 0.9867549668874173, 0.9875827814569537, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9875827814569537, 0.9875827814569537, 0.9875827814569537, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9875827814569537, 0.9875827814569537, 0.9892384105960265, 0.9875827814569537, 0.9917218543046358, 0.9776490066225165, 0.7102649006622517, 0.5951986754966887, 0.8236754966887417, 0.9056291390728477, 0.7392384105960265, 0.7632450331125827, 0.8311258278145696, 0.9379139072847682, 0.929635761589404, 0.8625827814569537, 0.8220198675496688, 0.8543046357615894, 0.8981788079470199, 0.9105960264900662, 0.9130794701986755, 0.9370860927152318, 0.9395695364238411, 0.9387417218543046, 0.9304635761589404, 0.9221854304635762, 0.9362582781456954, 0.9445364238410596, 0.9420529801324503, 0.9445364238410596, 0.9486754966887417, 0.9470198675496688, 0.9495033112582781, 0.9536423841059603, 0.9610927152317881, 0.9586092715231788, 0.9586092715231788, 0.9594370860927153, 0.9619205298013245, 0.9619205298013245, 0.9619205298013245, 0.9635761589403974, 0.9668874172185431, 0.9644039735099338, 0.9652317880794702, 0.9677152317880795, 0.9685430463576159, 0.9693708609271523, 0.9710264900662252, 0.972682119205298, 0.9759933774834437, 0.9751655629139073, 0.9784768211920529, 0.9759933774834437, 0.9759933774834437, 0.9751655629139073, 0.9759933774834437, 0.9793046357615894, 0.9784768211920529, 0.9793046357615894, 0.9793046357615894, 0.9801324503311258, 0.9776490066225165, 0.9793046357615894, 0.9793046357615894, 0.9817880794701986, 0.9809602649006622, 0.9826158940397351, 0.9817880794701986, 0.9817880794701986, 0.9826158940397351, 0.9834437086092715, 0.9834437086092715, 0.9834437086092715, 0.984271523178808, 0.984271523178808, 0.984271523178808, 0.984271523178808, 0.984271523178808, 0.9850993377483444, 0.9859271523178808, 0.9850993377483444, 0.9850993377483444, 0.9859271523178808, 0.9859271523178808, 0.9859271523178808, 0.9859271523178808, 0.9859271523178808, 0.9859271523178808, 0.9867549668874173, 0.9875827814569537, 0.9867549668874173, 0.9867549668874173, 0.9867549668874173, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9875827814569537, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9884105960264901, 0.9892384105960265, 0.9892384105960265, 0.9900662251655629, 0.9900662251655629, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9917218543046358, 0.9917218543046358, 0.9917218543046358, 0.9917218543046358, 0.9917218543046358, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9950331125827815, 0.9950331125827815, 0.9950331125827815, 0.9950331125827815, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9975165562913907, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272, 0.9983443708609272]
valid_accuracy_list: [0.316, 0.316, 0.156, 0.162, 0.316, 0.316, 0.316, 0.122, 0.156, 0.316, 0.316, 0.316, 0.314, 0.164, 0.162, 0.164, 0.34, 0.402, 0.436, 0.442, 0.45, 0.458, 0.464, 0.468, 0.466, 0.462, 0.472, 0.514, 0.532, 0.538, 0.548, 0.564, 0.562, 0.548, 0.556, 0.562, 0.564, 0.554, 0.568, 0.584, 0.602, 0.622, 0.668, 0.724, 0.732, 0.736, 0.756, 0.768, 0.778, 0.768, 0.768, 0.774, 0.774, 0.79, 0.79, 0.792, 0.802, 0.808, 0.812, 0.812, 0.802, 0.788, 0.784, 0.78, 0.776, 0.784, 0.778, 0.776, 0.78, 0.774, 0.774, 0.776, 0.778, 0.776, 0.77, 0.772, 0.776, 0.78, 0.78, 0.768, 0.78, 0.768, 0.77, 0.774, 0.762, 0.774, 0.77, 0.766, 0.752, 0.772, 0.766, 0.768, 0.76, 0.756, 0.77, 0.766, 0.77, 0.762, 0.768, 0.748, 0.754, 0.756, 0.76, 0.77, 0.758, 0.768, 0.756, 0.76, 0.756, 0.764, 0.746, 0.758, 0.762, 0.754, 0.758, 0.76, 0.758, 0.762, 0.77, 0.766, 0.762, 0.754, 0.758, 0.754, 0.756, 0.756, 0.746, 0.748, 0.744, 0.752, 0.752, 0.748, 0.744, 0.744, 0.75, 0.746, 0.744, 0.748, 0.744, 0.738, 0.744, 0.74, 0.734, 0.732, 0.736, 0.738, 0.74, 0.738, 0.74, 0.74, 0.736, 0.74, 0.736, 0.736, 0.736, 0.736, 0.738, 0.738, 0.736, 0.736, 0.736, 0.734, 0.732, 0.732, 0.73, 0.728, 0.728, 0.724, 0.724, 0.722, 0.728, 0.724, 0.73, 0.722, 0.536, 0.494, 0.636, 0.672, 0.61, 0.62, 0.662, 0.728, 0.776, 0.742, 0.706, 0.716, 0.772, 0.786, 0.794, 0.796, 0.778, 0.798, 0.784, 0.772, 0.796, 0.796, 0.804, 0.808, 0.808, 0.802, 0.798, 0.816, 0.814, 0.82, 0.812, 0.814, 0.806, 0.804, 0.8, 0.798, 0.796, 0.798, 0.796, 0.794, 0.796, 0.796, 0.794, 0.794, 0.792, 0.788, 0.784, 0.788, 0.792, 0.796, 0.806, 0.804, 0.808, 0.806, 0.806, 0.802, 0.802, 0.8, 0.798, 0.798, 0.8, 0.802, 0.804, 0.802, 0.8, 0.798, 0.798, 0.798, 0.798, 0.794, 0.792, 0.79, 0.79, 0.792, 0.792, 0.792, 0.796, 0.798, 0.796, 0.794, 0.794, 0.796, 0.796, 0.798, 0.796, 0.794, 0.794, 0.792, 0.79, 0.79, 0.792, 0.792, 0.792, 0.788, 0.79, 0.79, 0.788, 0.79, 0.79, 0.79, 0.79, 0.79, 0.79, 0.79, 0.788, 0.788, 0.786, 0.786, 0.786, 0.786, 0.786, 0.786, 0.786, 0.786, 0.784, 0.784, 0.784, 0.784, 0.784, 0.786, 0.786, 0.784, 0.782, 0.78, 0.78, 0.78, 0.78, 0.778, 0.778, 0.776, 0.776, 0.774, 0.774, 0.774, 0.768, 0.768, 0.768, 0.768, 0.77, 0.77, 0.77, 0.77, 0.766, 0.764, 0.764, 0.762, 0.762, 0.762, 0.76, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.756, 0.756, 0.756, 0.756, 0.754, 0.752, 0.748, 0.748, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.746, 0.746, 0.746, 0.746, 0.744, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.74, 0.74, 0.74, 0.738, 0.738, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.736, 0.736, 0.736, 0.734, 0.734, 0.732, 0.732, 0.73, 0.73, 0.73, 0.73, 0.73, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.726, 0.726, 0.726, 0.724, 0.724, 0.724, 0.724, 0.724, 0.724, 0.72, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.718, 0.718, 0.718, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.714, 0.714, 0.714, 0.714, 0.714, 0.714, 0.714, 0.714, 0.714, 0.714, 0.714, 0.714, 0.714, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716]
test_accuracy_list: [0.319, 0.319, 0.144, 0.149, 0.319, 0.319, 0.319, 0.13, 0.144, 0.319, 0.319, 0.319, 0.317, 0.153, 0.149, 0.151, 0.348, 0.408, 0.441, 0.442, 0.442, 0.45, 0.452, 0.453, 0.452, 0.459, 0.463, 0.494, 0.521, 0.526, 0.537, 0.544, 0.551, 0.539, 0.54, 0.546, 0.543, 0.537, 0.549, 0.576, 0.599, 0.629, 0.648, 0.7, 0.712, 0.718, 0.739, 0.763, 0.772, 0.785, 0.778, 0.77, 0.767, 0.772, 0.786, 0.793, 0.794, 0.788, 0.791, 0.788, 0.79, 0.784, 0.787, 0.787, 0.786, 0.788, 0.79, 0.787, 0.784, 0.782, 0.779, 0.775, 0.775, 0.776, 0.772, 0.772, 0.77, 0.767, 0.763, 0.768, 0.763, 0.761, 0.758, 0.766, 0.756, 0.759, 0.758, 0.766, 0.753, 0.76, 0.752, 0.765, 0.747, 0.756, 0.76, 0.757, 0.757, 0.77, 0.757, 0.762, 0.756, 0.754, 0.753, 0.745, 0.74, 0.747, 0.748, 0.75, 0.739, 0.743, 0.749, 0.747, 0.744, 0.742, 0.741, 0.744, 0.747, 0.75, 0.752, 0.751, 0.74, 0.741, 0.739, 0.738, 0.736, 0.74, 0.74, 0.732, 0.736, 0.736, 0.737, 0.735, 0.734, 0.731, 0.73, 0.73, 0.729, 0.728, 0.726, 0.725, 0.726, 0.726, 0.723, 0.722, 0.722, 0.724, 0.723, 0.722, 0.722, 0.724, 0.722, 0.722, 0.721, 0.721, 0.72, 0.72, 0.719, 0.719, 0.72, 0.718, 0.717, 0.719, 0.721, 0.72, 0.719, 0.717, 0.715, 0.718, 0.712, 0.717, 0.71, 0.717, 0.707, 0.709, 0.533, 0.512, 0.649, 0.662, 0.582, 0.598, 0.649, 0.739, 0.758, 0.732, 0.698, 0.72, 0.747, 0.756, 0.764, 0.767, 0.782, 0.775, 0.771, 0.757, 0.774, 0.786, 0.78, 0.775, 0.769, 0.768, 0.767, 0.771, 0.781, 0.784, 0.78, 0.778, 0.778, 0.784, 0.784, 0.778, 0.77, 0.773, 0.773, 0.777, 0.78, 0.781, 0.778, 0.782, 0.78, 0.779, 0.777, 0.776, 0.777, 0.777, 0.779, 0.783, 0.784, 0.783, 0.782, 0.779, 0.782, 0.78, 0.781, 0.78, 0.78, 0.78, 0.779, 0.778, 0.778, 0.779, 0.778, 0.777, 0.776, 0.777, 0.771, 0.771, 0.771, 0.771, 0.771, 0.771, 0.77, 0.771, 0.776, 0.777, 0.778, 0.776, 0.776, 0.775, 0.773, 0.774, 0.774, 0.773, 0.772, 0.771, 0.771, 0.771, 0.77, 0.769, 0.77, 0.77, 0.771, 0.771, 0.771, 0.77, 0.768, 0.767, 0.767, 0.766, 0.767, 0.767, 0.769, 0.769, 0.769, 0.769, 0.768, 0.769, 0.768, 0.768, 0.768, 0.768, 0.767, 0.766, 0.766, 0.766, 0.767, 0.767, 0.766, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.762, 0.762, 0.762, 0.762, 0.761, 0.761, 0.76, 0.76, 0.76, 0.759, 0.759, 0.759, 0.761, 0.761, 0.761, 0.761, 0.76, 0.759, 0.76, 0.759, 0.759, 0.758, 0.758, 0.758, 0.758, 0.758, 0.757, 0.757, 0.757, 0.757, 0.758, 0.758, 0.757, 0.757, 0.757, 0.756, 0.755, 0.755, 0.754, 0.755, 0.756, 0.756, 0.755, 0.755, 0.755, 0.755, 0.753, 0.753, 0.752, 0.752, 0.752, 0.753, 0.751, 0.75, 0.75, 0.748, 0.747, 0.746, 0.746, 0.744, 0.744, 0.742, 0.74, 0.739, 0.737, 0.736, 0.736, 0.735, 0.735, 0.735, 0.734, 0.734, 0.733, 0.733, 0.731, 0.731, 0.731, 0.732, 0.732, 0.731, 0.731, 0.732, 0.73, 0.73, 0.728, 0.727, 0.727, 0.726, 0.726, 0.726, 0.726, 0.725, 0.725, 0.725, 0.725, 0.726, 0.727, 0.727, 0.727, 0.727, 0.726, 0.726, 0.726, 0.724, 0.723, 0.723, 0.72, 0.719, 0.719, 0.72, 0.72, 0.72, 0.72, 0.72, 0.721, 0.721, 0.721, 0.72, 0.72, 0.72, 0.72, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.718, 0.718, 0.718, 0.719, 0.719, 0.718, 0.718, 0.718, 0.719, 0.719, 0.718, 0.718, 0.718, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.715, 0.714, 0.714, 0.716, 0.716, 0.716, 0.716, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.717, 0.717, 0.716, 0.715, 0.715, 0.715, 0.715, 0.714, 0.714, 0.714, 0.714, 0.714, 0.713]
best validation: 0.82
best test: 0.794
Experiment run
dataset: cora
num_mp_layers: 3
mp_hidden_dim: 4000
optimizer_lr: 0.001
loss_func: CrossEntropyLoss
total_epoch: 500
energy_lambda: 0.0001
Norm before input: 0.2552422881126404
Norm after input: 0.9947524070739746
Norm after each mp layer: 0.6566787362098694
Norm after each mp layer: 0.6222918033599854
Norm after each mp layer: 0.6184736490249634
Norm before input: 0.2552422881126404
Norm after input: 1.0012873411178589
Norm after each mp layer: 1.8204997777938843
Norm after each mp layer: 1.6149559020996094
Norm after each mp layer: 1.5747946500778198
Norm before input: 0.2552422881126404
Norm after input: 1.0012873411178589
Norm after each mp layer: 1.8204997777938843
Norm after each mp layer: 1.6149559020996094
Norm after each mp layer: 1.5747946500778198
Norm before input: 0.2552422881126404
Norm after input: 1.0478028059005737
Norm after each mp layer: 2.792241334915161
Norm after each mp layer: 5.163978576660156
Norm after each mp layer: 13.720152854919434
Norm before input: 0.2552422881126404
Norm after input: 1.0478028059005737
Norm after each mp layer: 2.792241334915161
Norm after each mp layer: 5.163978576660156
Norm after each mp layer: 13.720152854919434
Norm before input: 0.2552422881126404
Norm after input: 0.9884743690490723
Norm after each mp layer: 2.0103678703308105
Norm after each mp layer: 3.218209981918335
Norm after each mp layer: 6.030322074890137
Norm before input: 0.2552422881126404
Norm after input: 0.9884743690490723
Norm after each mp layer: 2.0103678703308105
Norm after each mp layer: 3.218209981918335
Norm after each mp layer: 6.030322074890137
Norm before input: 0.2552422881126404
Norm after input: 0.9567639827728271
Norm after each mp layer: 1.88925302028656
Norm after each mp layer: 3.7599072456359863
Norm after each mp layer: 8.61077880859375
Norm before input: 0.2552422881126404
Norm after input: 0.9567639827728271
Norm after each mp layer: 1.88925302028656
Norm after each mp layer: 3.7599072456359863
Norm after each mp layer: 8.61077880859375
Norm before input: 0.2552422881126404
Norm after input: 0.9439951181411743
Norm after each mp layer: 2.2218079566955566
Norm after each mp layer: 5.568711280822754
Norm after each mp layer: 16.603384017944336
Epoch: 05, Loss: 5.3418, Energy: 8429.5762, Train: 16.23%, Valid: 15.60%, Test: 14.40%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.9439951181411743
Norm after each mp layer: 2.2218079566955566
Norm after each mp layer: 5.568711280822754
Norm after each mp layer: 16.603384017944336
Norm before input: 0.2552422881126404
Norm after input: 0.8996334671974182
Norm after each mp layer: 1.7393807172775269
Norm after each mp layer: 3.250600814819336
Norm after each mp layer: 9.231205940246582
Norm before input: 0.2552422881126404
Norm after input: 0.8996334671974182
Norm after each mp layer: 1.7393807172775269
Norm after each mp layer: 3.250600814819336
Norm after each mp layer: 9.231205940246582
Norm before input: 0.2552422881126404
Norm after input: 0.8711276054382324
Norm after each mp layer: 1.5943410396575928
Norm after each mp layer: 2.4008777141571045
Norm after each mp layer: 6.701023101806641
Norm before input: 0.2552422881126404
Norm after input: 0.8711276054382324
Norm after each mp layer: 1.5943410396575928
Norm after each mp layer: 2.4008777141571045
Norm after each mp layer: 6.701023101806641
Norm before input: 0.2552422881126404
Norm after input: 0.8585504293441772
Norm after each mp layer: 1.7844364643096924
Norm after each mp layer: 2.9909000396728516
Norm after each mp layer: 9.045377731323242
Norm before input: 0.2552422881126404
Norm after input: 0.8585504293441772
Norm after each mp layer: 1.7844364643096924
Norm after each mp layer: 2.9909000396728516
Norm after each mp layer: 9.045377731323242
Norm before input: 0.2552422881126404
Norm after input: 0.8528599143028259
Norm after each mp layer: 2.0368077754974365
Norm after each mp layer: 3.8482749462127686
Norm after each mp layer: 12.708044052124023
Norm before input: 0.2552422881126404
Norm after input: 0.8528599143028259
Norm after each mp layer: 2.0368077754974365
Norm after each mp layer: 3.8482749462127686
Norm after each mp layer: 12.708044052124023
Norm before input: 0.2552422881126404
Norm after input: 0.8351338505744934
Norm after each mp layer: 2.023674964904785
Norm after each mp layer: 3.5879690647125244
Norm after each mp layer: 11.257453918457031
Epoch: 10, Loss: 3.4813, Energy: 12045.9521, Train: 20.28%, Valid: 23.60%, Test: 22.30%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.8351338505744934
Norm after each mp layer: 2.023674964904785
Norm after each mp layer: 3.5879690647125244
Norm after each mp layer: 11.257453918457031
Norm before input: 0.2552422881126404
Norm after input: 0.8145154118537903
Norm after each mp layer: 1.945807933807373
Norm after each mp layer: 3.1077163219451904
Norm after each mp layer: 8.719141960144043
Norm before input: 0.2552422881126404
Norm after input: 0.8145154118537903
Norm after each mp layer: 1.945807933807373
Norm after each mp layer: 3.1077163219451904
Norm after each mp layer: 8.719141960144043
Norm before input: 0.2552422881126404
Norm after input: 0.7963459491729736
Norm after each mp layer: 1.929540991783142
Norm after each mp layer: 2.8946897983551025
Norm after each mp layer: 8.039734840393066
Norm before input: 0.2552422881126404
Norm after input: 0.7963459491729736
Norm after each mp layer: 1.929540991783142
Norm after each mp layer: 2.8946895599365234
Norm after each mp layer: 8.039734840393066
Norm before input: 0.2552422881126404
Norm after input: 0.7815361618995667
Norm after each mp layer: 1.9883755445480347
Norm after each mp layer: 3.0242016315460205
Norm after each mp layer: 9.751030921936035
Norm before input: 0.2552422881126404
Norm after input: 0.7815361618995667
Norm after each mp layer: 1.9883755445480347
Norm after each mp layer: 3.0242016315460205
Norm after each mp layer: 9.751030921936035
Norm before input: 0.2552422881126404
Norm after input: 0.7676894664764404
Norm after each mp layer: 2.0248751640319824
Norm after each mp layer: 3.1246111392974854
Norm after each mp layer: 11.586836814880371
Norm before input: 0.2552422881126404
Norm after input: 0.7676894664764404
Norm after each mp layer: 2.0248751640319824
Norm after each mp layer: 3.1246111392974854
Norm after each mp layer: 11.586836814880371
Norm before input: 0.2552422881126404
Norm after input: 0.7491840124130249
Norm after each mp layer: 1.9334256649017334
Norm after each mp layer: 2.667442798614502
Norm after each mp layer: 10.201431274414062
Epoch: 15, Loss: 2.5264, Energy: 5272.3462, Train: 28.31%, Valid: 31.60%, Test: 32.10%, Best Valid: 31.60%, Best Test: 32.10%
Norm before input: 0.2552422881126404
Norm after input: 0.7491840124130249
Norm after each mp layer: 1.9334256649017334
Norm after each mp layer: 2.667442798614502
Norm after each mp layer: 10.201431274414062
Norm before input: 0.2552422881126404
Norm after input: 0.7319605946540833
Norm after each mp layer: 1.8148274421691895
Norm after each mp layer: 2.0363974571228027
Norm after each mp layer: 7.436028480529785
Norm before input: 0.2552422881126404
Norm after input: 0.7319605946540833
Norm after each mp layer: 1.8148274421691895
Norm after each mp layer: 2.0363974571228027
Norm after each mp layer: 7.436028480529785
Norm before input: 0.2552422881126404
Norm after input: 0.7213667035102844
Norm after each mp layer: 1.853097677230835
Norm after each mp layer: 2.170471668243408
Norm after each mp layer: 8.490766525268555
Norm before input: 0.2552422881126404
Norm after input: 0.7213667035102844
Norm after each mp layer: 1.853097677230835
Norm after each mp layer: 2.170471668243408
Norm after each mp layer: 8.490766525268555
Norm before input: 0.2552422881126404
Norm after input: 0.7159395813941956
Norm after each mp layer: 1.967866063117981
Norm after each mp layer: 2.680980682373047
Norm after each mp layer: 11.543124198913574
Norm before input: 0.2552422881126404
Norm after input: 0.7159395813941956
Norm after each mp layer: 1.967866063117981
Norm after each mp layer: 2.680980682373047
Norm after each mp layer: 11.543124198913574
Norm before input: 0.2552422881126404
Norm after input: 0.7096956372261047
Norm after each mp layer: 2.0225131511688232
Norm after each mp layer: 2.859422445297241
Norm after each mp layer: 12.673003196716309
Norm before input: 0.2552422881126404
Norm after input: 0.7096956372261047
Norm after each mp layer: 2.0225131511688232
Norm after each mp layer: 2.859422445297241
Norm after each mp layer: 12.673003196716309
Norm before input: 0.2552422881126404
Norm after input: 0.7004786729812622
Norm after each mp layer: 1.9896206855773926
Norm after each mp layer: 2.569457530975342
Norm after each mp layer: 10.992445945739746
Epoch: 20, Loss: 2.2836, Energy: 3662.7896, Train: 28.81%, Valid: 31.80%, Test: 32.30%, Best Valid: 33.40%, Best Test: 34.10%
Norm before input: 0.2552422881126404
Norm after input: 0.7004786729812622
Norm after each mp layer: 1.9896206855773926
Norm after each mp layer: 2.569457530975342
Norm after each mp layer: 10.992445945739746
Norm before input: 0.2552422881126404
Norm after input: 0.6929367184638977
Norm after each mp layer: 1.947904109954834
Norm after each mp layer: 2.1993050575256348
Norm after each mp layer: 8.560872077941895
Norm before input: 0.2552422881126404
Norm after input: 0.6929367184638977
Norm after each mp layer: 1.947904109954834
Norm after each mp layer: 2.199305295944214
Norm after each mp layer: 8.560872077941895
Norm before input: 0.2552422881126404
Norm after input: 0.6880258917808533
Norm after each mp layer: 1.9720895290374756
Norm after each mp layer: 2.2013747692108154
Norm after each mp layer: 8.377569198608398
Norm before input: 0.2552422881126404
Norm after input: 0.6880258917808533
Norm after each mp layer: 1.9720895290374756
Norm after each mp layer: 2.2013747692108154
Norm after each mp layer: 8.377569198608398
Norm before input: 0.2552422881126404
Norm after input: 0.684504508972168
Norm after each mp layer: 2.040142297744751
Norm after each mp layer: 2.465130567550659
Norm after each mp layer: 10.045400619506836
Norm before input: 0.2552422881126404
Norm after input: 0.684504508972168
Norm after each mp layer: 2.040142297744751
Norm after each mp layer: 2.465130567550659
Norm after each mp layer: 10.045400619506836
Norm before input: 0.2552422881126404
Norm after input: 0.6826958060264587
Norm after each mp layer: 2.0909342765808105
Norm after each mp layer: 2.562234878540039
Norm after each mp layer: 10.664311408996582
Norm before input: 0.2552422881126404
Norm after input: 0.6826958060264587
Norm after each mp layer: 2.0909342765808105
Norm after each mp layer: 2.562234878540039
Norm after each mp layer: 10.664311408996582
Norm before input: 0.2552422881126404
Norm after input: 0.6824145317077637
Norm after each mp layer: 2.108297824859619
Norm after each mp layer: 2.360682487487793
Norm after each mp layer: 9.193798065185547
Epoch: 25, Loss: 2.0254, Energy: 1842.2764, Train: 28.81%, Valid: 31.80%, Test: 32.80%, Best Valid: 35.00%, Best Test: 36.50%
Norm before input: 0.2552422881126404
Norm after input: 0.6824145317077637
Norm after each mp layer: 2.108297824859619
Norm after each mp layer: 2.360682487487793
Norm after each mp layer: 9.193798065185547
Norm before input: 0.2552422881126404
Norm after input: 0.6839230060577393
Norm after each mp layer: 2.1243698596954346
Norm after each mp layer: 2.114530086517334
Norm after each mp layer: 7.131221294403076
Norm before input: 0.2552422881126404
Norm after input: 0.6839230060577393
Norm after each mp layer: 2.1243698596954346
Norm after each mp layer: 2.114530086517334
Norm after each mp layer: 7.131221294403076
Norm before input: 0.2552422881126404
Norm after input: 0.6819445490837097
Norm after each mp layer: 2.1496636867523193
Norm after each mp layer: 2.1127593517303467
Norm after each mp layer: 7.229312419891357
Norm before input: 0.2552422881126404
Norm after input: 0.6819445490837097
Norm after each mp layer: 2.1496636867523193
Norm after each mp layer: 2.1127593517303467
Norm after each mp layer: 7.229312419891357
Norm before input: 0.2552422881126404
Norm after input: 0.6773465871810913
Norm after each mp layer: 2.1763036251068115
Norm after each mp layer: 2.279963731765747
Norm after each mp layer: 8.913629531860352
Norm before input: 0.2552422881126404
Norm after input: 0.6773465871810913
Norm after each mp layer: 2.1763036251068115
Norm after each mp layer: 2.279963731765747
Norm after each mp layer: 8.913629531860352
Norm before input: 0.2552422881126404
Norm after input: 0.6762158870697021
Norm after each mp layer: 2.1963984966278076
Norm after each mp layer: 2.2780449390411377
Norm after each mp layer: 8.97992992401123
Norm before input: 0.2552422881126404
Norm after input: 0.6762158870697021
Norm after each mp layer: 2.1963984966278076
Norm after each mp layer: 2.2780449390411377
Norm after each mp layer: 8.97992992401123
Norm before input: 0.2552422881126404
Norm after input: 0.6788219213485718
Norm after each mp layer: 2.2117514610290527
Norm after each mp layer: 2.1045374870300293
Norm after each mp layer: 7.198596954345703
Epoch: 30, Loss: 1.8729, Energy: 712.5112, Train: 29.14%, Valid: 31.80%, Test: 33.70%, Best Valid: 36.20%, Best Test: 37.30%
Norm before input: 0.2552422881126404
Norm after input: 0.6788219213485718
Norm after each mp layer: 2.2117514610290527
Norm after each mp layer: 2.1045374870300293
Norm after each mp layer: 7.198596954345703
Norm before input: 0.2552422881126404
Norm after input: 0.6751377582550049
Norm after each mp layer: 2.214916229248047
Norm after each mp layer: 2.1485626697540283
Norm after each mp layer: 7.591692924499512
Norm before input: 0.2552422881126404
Norm after input: 0.6751377582550049
Norm after each mp layer: 2.214916229248047
Norm after each mp layer: 2.1485626697540283
Norm after each mp layer: 7.591692924499512
Norm before input: 0.2552422881126404
Norm after input: 0.6672669649124146
Norm after each mp layer: 2.208087205886841
Norm after each mp layer: 2.3293824195861816
Norm after each mp layer: 9.359840393066406
Norm before input: 0.2552422881126404
Norm after input: 0.6672669649124146
Norm after each mp layer: 2.208087205886841
Norm after each mp layer: 2.3293824195861816
Norm after each mp layer: 9.359840393066406
Norm before input: 0.2552422881126404
Norm after input: 0.6655876636505127
Norm after each mp layer: 2.199493885040283
Norm after each mp layer: 2.25648832321167
Norm after each mp layer: 8.37332534790039
Norm before input: 0.2552422881126404
Norm after input: 0.6655876636505127
Norm after each mp layer: 2.199493885040283
Norm after each mp layer: 2.25648832321167
Norm after each mp layer: 8.37332534790039
Norm before input: 0.2552422881126404
Norm after input: 0.6644296646118164
Norm after each mp layer: 2.195514678955078
Norm after each mp layer: 2.2198851108551025
Norm after each mp layer: 7.553387641906738
Norm before input: 0.2552422881126404
Norm after input: 0.6644296646118164
Norm after each mp layer: 2.195514678955078
Norm after each mp layer: 2.2198851108551025
Norm after each mp layer: 7.553387641906738
Norm before input: 0.2552422881126404
Norm after input: 0.6494410037994385
Norm after each mp layer: 2.186283588409424
Norm after each mp layer: 2.6801695823669434
Norm after each mp layer: 12.00474739074707
Epoch: 35, Loss: 1.7496, Energy: 675.7792, Train: 29.47%, Valid: 32.80%, Test: 33.70%, Best Valid: 42.80%, Best Test: 41.00%
Norm before input: 0.2552422881126404
Norm after input: 0.6494410037994385
Norm after each mp layer: 2.186283588409424
Norm after each mp layer: 2.6801695823669434
Norm after each mp layer: 12.00474739074707
Norm before input: 0.2552422881126404
Norm after input: 0.6457005739212036
Norm after each mp layer: 2.1662957668304443
Norm after each mp layer: 2.6242783069610596
Norm after each mp layer: 11.034553527832031
Norm before input: 0.2552422881126404
Norm after input: 0.6457005739212036
Norm after each mp layer: 2.1662957668304443
Norm after each mp layer: 2.6242783069610596
Norm after each mp layer: 11.034554481506348
Norm before input: 0.2552422881126404
Norm after input: 0.648077130317688
Norm after each mp layer: 2.150528907775879
Norm after each mp layer: 2.4143459796905518
Norm after each mp layer: 8.147639274597168
Norm before input: 0.2552422881126404
Norm after input: 0.648077130317688
Norm after each mp layer: 2.150528907775879
Norm after each mp layer: 2.4143459796905518
Norm after each mp layer: 8.147639274597168
Norm before input: 0.2552422881126404
Norm after input: 0.636631190776825
Norm after each mp layer: 2.1233749389648438
Norm after each mp layer: 2.534015655517578
Norm after each mp layer: 9.772608757019043
Norm before input: 0.2552422881126404
Norm after input: 0.636631190776825
Norm after each mp layer: 2.1233749389648438
Norm after each mp layer: 2.534015655517578
Norm after each mp layer: 9.772608757019043
Norm before input: 0.2552422881126404
Norm after input: 0.625301718711853
Norm after each mp layer: 2.0930097103118896
Norm after each mp layer: 2.628981113433838
Norm after each mp layer: 11.290289878845215
Norm before input: 0.2552422881126404
Norm after input: 0.625301718711853
Norm after each mp layer: 2.0930097103118896
Norm after each mp layer: 2.628981113433838
Norm after each mp layer: 11.290289878845215
Norm before input: 0.2552422881126404
Norm after input: 0.62003493309021
Norm after each mp layer: 2.0521583557128906
Norm after each mp layer: 2.4730148315429688
Norm after each mp layer: 10.156497955322266
Epoch: 40, Loss: 1.6119, Energy: 1764.7145, Train: 45.45%, Valid: 46.40%, Test: 47.20%, Best Valid: 49.80%, Best Test: 48.30%
Norm before input: 0.2552422881126404
Norm after input: 0.62003493309021
Norm after each mp layer: 2.0521583557128906
Norm after each mp layer: 2.4730148315429688
Norm after each mp layer: 10.156497955322266
Norm before input: 0.2552422881126404
Norm after input: 0.6192455291748047
Norm after each mp layer: 2.0198006629943848
Norm after each mp layer: 2.277733087539673
Norm after each mp layer: 8.172160148620605
Norm before input: 0.2552422881126404
Norm after input: 0.6192455291748047
Norm after each mp layer: 2.0198006629943848
Norm after each mp layer: 2.277733087539673
Norm after each mp layer: 8.172160148620605
Norm before input: 0.2552422881126404
Norm after input: 0.6010021567344666
Norm after each mp layer: 2.0117452144622803
Norm after each mp layer: 2.737901449203491
Norm after each mp layer: 12.808850288391113
Norm before input: 0.2552422881126404
Norm after input: 0.6010021567344666
Norm after each mp layer: 2.0117452144622803
Norm after each mp layer: 2.737901449203491
Norm after each mp layer: 12.808850288391113
Norm before input: 0.2552422881126404
Norm after input: 0.5951768755912781
Norm after each mp layer: 1.9905940294265747
Norm after each mp layer: 2.763503074645996
Norm after each mp layer: 12.591034889221191
Norm before input: 0.2552422881126404
Norm after input: 0.5951768755912781
Norm after each mp layer: 1.9905940294265747
Norm after each mp layer: 2.763503074645996
Norm after each mp layer: 12.591034889221191
Norm before input: 0.2552422881126404
Norm after input: 0.5982346534729004
Norm after each mp layer: 1.973830223083496
Norm after each mp layer: 2.5819056034088135
Norm after each mp layer: 10.179280281066895
Norm before input: 0.2552422881126404
Norm after input: 0.5982346534729004
Norm after each mp layer: 1.973830223083496
Norm after each mp layer: 2.5819056034088135
Norm after each mp layer: 10.179280281066895
Norm before input: 0.2552422881126404
Norm after input: 0.5989015102386475
Norm after each mp layer: 1.954565405845642
Norm after each mp layer: 2.4889209270477295
Norm after each mp layer: 8.726619720458984
Epoch: 45, Loss: 1.4689, Energy: 2245.9951, Train: 52.40%, Valid: 48.80%, Test: 48.60%, Best Valid: 58.20%, Best Test: 59.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5989015102386475
Norm after each mp layer: 1.954565405845642
Norm after each mp layer: 2.4889211654663086
Norm after each mp layer: 8.726619720458984
Norm before input: 0.2552422881126404
Norm after input: 0.5864177346229553
Norm after each mp layer: 1.8827542066574097
Norm after each mp layer: 2.378431797027588
Norm after each mp layer: 8.318679809570312
Norm before input: 0.2552422881126404
Norm after input: 0.5864177346229553
Norm after each mp layer: 1.8827542066574097
Norm after each mp layer: 2.378431797027588
Norm after each mp layer: 8.318679809570312
Norm before input: 0.2552422881126404
Norm after input: 0.5637244582176208
Norm after each mp layer: 1.7679316997528076
Norm after each mp layer: 2.2648980617523193
Norm after each mp layer: 8.505630493164062
Norm before input: 0.2552422881126404
Norm after input: 0.5637244582176208
Norm after each mp layer: 1.7679316997528076
Norm after each mp layer: 2.2648980617523193
Norm after each mp layer: 8.505630493164062
Norm before input: 0.2552422881126404
Norm after input: 0.5433653593063354
Norm after each mp layer: 1.6639833450317383
Norm after each mp layer: 2.219280481338501
Norm after each mp layer: 8.670536041259766
Norm before input: 0.2552422881126404
Norm after input: 0.5433653593063354
Norm after each mp layer: 1.6639833450317383
Norm after each mp layer: 2.219280481338501
Norm after each mp layer: 8.670536041259766
Norm before input: 0.2552422881126404
Norm after input: 0.5349635481834412
Norm after each mp layer: 1.611296534538269
Norm after each mp layer: 2.148664951324463
Norm after each mp layer: 8.192306518554688
Norm before input: 0.2552422881126404
Norm after input: 0.5349635481834412
Norm after each mp layer: 1.611296534538269
Norm after each mp layer: 2.148664951324463
Norm after each mp layer: 8.192306518554688
Norm before input: 0.2552422881126404
Norm after input: 0.5388789176940918
Norm after each mp layer: 1.6248737573623657
Norm after each mp layer: 2.1064157485961914
Norm after each mp layer: 7.587254524230957
Epoch: 50, Loss: 1.3198, Energy: 2103.1506, Train: 67.38%, Valid: 61.80%, Test: 63.40%, Best Valid: 61.80%, Best Test: 63.40%
Norm before input: 0.2552422881126404
Norm after input: 0.5388789176940918
Norm after each mp layer: 1.6248737573623657
Norm after each mp layer: 2.1064157485961914
Norm after each mp layer: 7.587254524230957
Norm before input: 0.2552422881126404
Norm after input: 0.5475431084632874
Norm after each mp layer: 1.6919111013412476
Norm after each mp layer: 2.261500120162964
Norm after each mp layer: 7.928125858306885
Norm before input: 0.2552422881126404
Norm after input: 0.5475431084632874
Norm after each mp layer: 1.6919111013412476
Norm after each mp layer: 2.261500120162964
Norm after each mp layer: 7.928125858306885
Norm before input: 0.2552422881126404
Norm after input: 0.5465125441551208
Norm after each mp layer: 1.7391128540039062
Norm after each mp layer: 2.487422466278076
Norm after each mp layer: 8.99791145324707
Norm before input: 0.2552422881126404
Norm after input: 0.5465125441551208
Norm after each mp layer: 1.7391128540039062
Norm after each mp layer: 2.487422466278076
Norm after each mp layer: 8.99791145324707
Norm before input: 0.2552422881126404
Norm after input: 0.5359479188919067
Norm after each mp layer: 1.7327725887298584
Norm after each mp layer: 2.6148056983947754
Norm after each mp layer: 9.849721908569336
Norm before input: 0.2552422881126404
Norm after input: 0.5359479188919067
Norm after each mp layer: 1.7327725887298584
Norm after each mp layer: 2.6148056983947754
Norm after each mp layer: 9.849721908569336
Norm before input: 0.2552422881126404
Norm after input: 0.5215301513671875
Norm after each mp layer: 1.6898629665374756
Norm after each mp layer: 2.667279005050659
Norm after each mp layer: 10.366661071777344
Norm before input: 0.2552422881126404
Norm after input: 0.5215301513671875
Norm after each mp layer: 1.6898629665374756
Norm after each mp layer: 2.667279005050659
Norm after each mp layer: 10.366661071777344
Norm before input: 0.2552422881126404
Norm after input: 0.5109666585922241
Norm after each mp layer: 1.6440778970718384
Norm after each mp layer: 2.674936294555664
Norm after each mp layer: 10.486699104309082
Epoch: 55, Loss: 1.0411, Energy: 1879.4478, Train: 67.47%, Valid: 65.60%, Test: 64.40%, Best Valid: 69.40%, Best Test: 69.90%
Norm before input: 0.2552422881126404
Norm after input: 0.5109666585922241
Norm after each mp layer: 1.6440778970718384
Norm after each mp layer: 2.674936294555664
Norm after each mp layer: 10.486698150634766
Norm before input: 0.2552422881126404
Norm after input: 0.5099694728851318
Norm after each mp layer: 1.6319692134857178
Norm after each mp layer: 2.682295083999634
Norm after each mp layer: 10.366179466247559
Norm before input: 0.2552422881126404
Norm after input: 0.5099694728851318
Norm after each mp layer: 1.6319692134857178
Norm after each mp layer: 2.682295083999634
Norm after each mp layer: 10.366179466247559
Norm before input: 0.2552422881126404
Norm after input: 0.5166124105453491
Norm after each mp layer: 1.6549149751663208
Norm after each mp layer: 2.7223727703094482
Norm after each mp layer: 10.234383583068848
Norm before input: 0.2552422881126404
Norm after input: 0.5166124105453491
Norm after each mp layer: 1.6549149751663208
Norm after each mp layer: 2.7223727703094482
Norm after each mp layer: 10.234383583068848
Norm before input: 0.2552422881126404
Norm after input: 0.5225164890289307
Norm after each mp layer: 1.6758710145950317
Norm after each mp layer: 2.7706751823425293
Norm after each mp layer: 10.146895408630371
Norm before input: 0.2552422881126404
Norm after input: 0.5225164890289307
Norm after each mp layer: 1.6758710145950317
Norm after each mp layer: 2.7706751823425293
Norm after each mp layer: 10.146895408630371
Norm before input: 0.2552422881126404
Norm after input: 0.5186261534690857
Norm after each mp layer: 1.6516222953796387
Norm after each mp layer: 2.7512142658233643
Norm after each mp layer: 9.990960121154785
Norm before input: 0.2552422881126404
Norm after input: 0.5186261534690857
Norm after each mp layer: 1.6516222953796387
Norm after each mp layer: 2.7512142658233643
Norm after each mp layer: 9.990960121154785
Norm before input: 0.2552422881126404
Norm after input: 0.5053400993347168
Norm after each mp layer: 1.594723105430603
Norm after each mp layer: 2.706822156906128
Norm after each mp layer: 10.021879196166992
Epoch: 60, Loss: 0.8815, Energy: 1155.5463, Train: 74.09%, Valid: 68.60%, Test: 69.60%, Best Valid: 72.60%, Best Test: 71.60%
Norm before input: 0.2552422881126404
Norm after input: 0.5053400993347168
Norm after each mp layer: 1.594723105430603
Norm after each mp layer: 2.706822156906128
Norm after each mp layer: 10.021879196166992
Norm before input: 0.2552422881126404
Norm after input: 0.4969925582408905
Norm after each mp layer: 1.5651538372039795
Norm after each mp layer: 2.7202577590942383
Norm after each mp layer: 10.22509765625
Norm before input: 0.2552422881126404
Norm after input: 0.4969925582408905
Norm after each mp layer: 1.5651538372039795
Norm after each mp layer: 2.7202577590942383
Norm after each mp layer: 10.22509765625
Norm before input: 0.2552422881126404
Norm after input: 0.498495876789093
Norm after each mp layer: 1.5813745260238647
Norm after each mp layer: 2.7781221866607666
Norm after each mp layer: 10.427534103393555
Norm before input: 0.2552422881126404
Norm after input: 0.498495876789093
Norm after each mp layer: 1.5813745260238647
Norm after each mp layer: 2.7781221866607666
Norm after each mp layer: 10.427534103393555
Norm before input: 0.2552422881126404
Norm after input: 0.5055927038192749
Norm after each mp layer: 1.6226948499679565
Norm after each mp layer: 2.8722429275512695
Norm after each mp layer: 10.66027545928955
Norm before input: 0.2552422881126404
Norm after input: 0.5055927038192749
Norm after each mp layer: 1.6226948499679565
Norm after each mp layer: 2.8722429275512695
Norm after each mp layer: 10.66027545928955
Norm before input: 0.2552422881126404
Norm after input: 0.5072839856147766
Norm after each mp layer: 1.6367504596710205
Norm after each mp layer: 2.9314818382263184
Norm after each mp layer: 10.867055892944336
Norm before input: 0.2552422881126404
Norm after input: 0.5072839856147766
Norm after each mp layer: 1.6367504596710205
Norm after each mp layer: 2.9314815998077393
Norm after each mp layer: 10.86705493927002
Norm before input: 0.2552422881126404
Norm after input: 0.49953651428222656
Norm after each mp layer: 1.6064139604568481
Norm after each mp layer: 2.9187633991241455
Norm after each mp layer: 10.994186401367188
Epoch: 65, Loss: 0.7655, Energy: 1157.2793, Train: 78.56%, Valid: 70.80%, Test: 71.80%, Best Valid: 72.60%, Best Test: 73.10%
Norm before input: 0.2552422881126404
Norm after input: 0.49953651428222656
Norm after each mp layer: 1.6064139604568481
Norm after each mp layer: 2.9187633991241455
Norm after each mp layer: 10.994186401367188
Norm before input: 0.2552422881126404
Norm after input: 0.4913012981414795
Norm after each mp layer: 1.5784989595413208
Norm after each mp layer: 2.920858383178711
Norm after each mp layer: 11.216830253601074
Norm before input: 0.2552422881126404
Norm after input: 0.4913012981414795
Norm after each mp layer: 1.5784989595413208
Norm after each mp layer: 2.920858383178711
Norm after each mp layer: 11.216830253601074
Norm before input: 0.2552422881126404
Norm after input: 0.4897124469280243
Norm after each mp layer: 1.581539273262024
Norm after each mp layer: 2.9525489807128906
Norm after each mp layer: 11.426727294921875
Norm before input: 0.2552422881126404
Norm after input: 0.4897124469280243
Norm after each mp layer: 1.581539273262024
Norm after each mp layer: 2.9525489807128906
Norm after each mp layer: 11.426727294921875
Norm before input: 0.2552422881126404
Norm after input: 0.4940814673900604
Norm after each mp layer: 1.6030042171478271
Norm after each mp layer: 2.9693539142608643
Norm after each mp layer: 11.421935081481934
Norm before input: 0.2552422881126404
Norm after input: 0.4940814673900604
Norm after each mp layer: 1.6030042171478271
Norm after each mp layer: 2.9693539142608643
Norm after each mp layer: 11.42193603515625
Norm before input: 0.2552422881126404
Norm after input: 0.4977901875972748
Norm after each mp layer: 1.6138455867767334
Norm after each mp layer: 2.9493720531463623
Norm after each mp layer: 11.262353897094727
Norm before input: 0.2552422881126404
Norm after input: 0.4977901875972748
Norm after each mp layer: 1.6138455867767334
Norm after each mp layer: 2.9493720531463623
Norm after each mp layer: 11.262353897094727
Norm before input: 0.2552422881126404
Norm after input: 0.49492502212524414
Norm after each mp layer: 1.5981807708740234
Norm after each mp layer: 2.9116673469543457
Norm after each mp layer: 11.19736385345459
Epoch: 70, Loss: 0.6556, Energy: 1020.3120, Train: 82.20%, Valid: 72.20%, Test: 72.80%, Best Valid: 74.00%, Best Test: 74.40%
Norm before input: 0.2552422881126404
Norm after input: 0.49492502212524414
Norm after each mp layer: 1.5981807708740234
Norm after each mp layer: 2.9116673469543457
Norm after each mp layer: 11.19736385345459
Norm before input: 0.2552422881126404
Norm after input: 0.4892342686653137
Norm after each mp layer: 1.5748963356018066
Norm after each mp layer: 2.893314838409424
Norm after each mp layer: 11.281942367553711
Norm before input: 0.2552422881126404
Norm after input: 0.4892342686653137
Norm after each mp layer: 1.5748963356018066
Norm after each mp layer: 2.893314838409424
Norm after each mp layer: 11.281942367553711
Norm before input: 0.2552422881126404
Norm after input: 0.48718318343162537
Norm after each mp layer: 1.562994360923767
Norm after each mp layer: 2.8853302001953125
Norm after each mp layer: 11.283041000366211
Norm before input: 0.2552422881126404
Norm after input: 0.48718318343162537
Norm after each mp layer: 1.562994360923767
Norm after each mp layer: 2.8853302001953125
Norm after each mp layer: 11.283041000366211
Norm before input: 0.2552422881126404
Norm after input: 0.489060640335083
Norm after each mp layer: 1.558764100074768
Norm after each mp layer: 2.853667736053467
Norm after each mp layer: 11.047284126281738
Norm before input: 0.2552422881126404
Norm after input: 0.489060640335083
Norm after each mp layer: 1.558764100074768
Norm after each mp layer: 2.853667736053467
Norm after each mp layer: 11.047284126281738
Norm before input: 0.2552422881126404
Norm after input: 0.49006614089012146
Norm after each mp layer: 1.5547685623168945
Norm after each mp layer: 2.8392133712768555
Norm after each mp layer: 10.932760238647461
Norm before input: 0.2552422881126404
Norm after input: 0.49006614089012146
Norm after each mp layer: 1.5547685623168945
Norm after each mp layer: 2.8392133712768555
Norm after each mp layer: 10.932760238647461
Norm before input: 0.2552422881126404
Norm after input: 0.48689937591552734
Norm after each mp layer: 1.5472058057785034
Norm after each mp layer: 2.8653743267059326
Norm after each mp layer: 11.145546913146973
Epoch: 75, Loss: 0.5772, Energy: 906.4915, Train: 84.85%, Valid: 74.80%, Test: 73.70%, Best Valid: 74.80%, Best Test: 74.40%
Norm before input: 0.2552422881126404
Norm after input: 0.48689937591552734
Norm after each mp layer: 1.5472058057785034
Norm after each mp layer: 2.8653743267059326
Norm after each mp layer: 11.145546913146973
Norm before input: 0.2552422881126404
Norm after input: 0.4831860363483429
Norm after each mp layer: 1.5436079502105713
Norm after each mp layer: 2.916395902633667
Norm after each mp layer: 11.517436027526855
Norm before input: 0.2552422881126404
Norm after input: 0.4831860363483429
Norm after each mp layer: 1.5436079502105713
Norm after each mp layer: 2.916395902633667
Norm after each mp layer: 11.517436027526855
Norm before input: 0.2552422881126404
Norm after input: 0.4837912321090698
Norm after each mp layer: 1.5521916151046753
Norm after each mp layer: 2.947850465774536
Norm after each mp layer: 11.687804222106934
Norm before input: 0.2552422881126404
Norm after input: 0.4837912321090698
Norm after each mp layer: 1.5521916151046753
Norm after each mp layer: 2.947850465774536
Norm after each mp layer: 11.687804222106934
Norm before input: 0.2552422881126404
Norm after input: 0.4868771731853485
Norm after each mp layer: 1.5682525634765625
Norm after each mp layer: 2.9757840633392334
Norm after each mp layer: 11.765012741088867
Norm before input: 0.2552422881126404
Norm after input: 0.4868771731853485
Norm after each mp layer: 1.5682525634765625
Norm after each mp layer: 2.9757840633392334
Norm after each mp layer: 11.765012741088867
Norm before input: 0.2552422881126404
Norm after input: 0.48600897192955017
Norm after each mp layer: 1.5723413228988647
Norm after each mp layer: 3.016366720199585
Norm after each mp layer: 12.008543968200684
Norm before input: 0.2552422881126404
Norm after input: 0.48600897192955017
Norm after each mp layer: 1.5723413228988647
Norm after each mp layer: 3.016366720199585
Norm after each mp layer: 12.008543968200684
Norm before input: 0.2552422881126404
Norm after input: 0.48252278566360474
Norm after each mp layer: 1.565441608428955
Norm after each mp layer: 3.0557076930999756
Norm after each mp layer: 12.314862251281738
Epoch: 80, Loss: 0.4968, Energy: 940.2203, Train: 86.75%, Valid: 75.40%, Test: 75.10%, Best Valid: 75.40%, Best Test: 75.10%
Norm before input: 0.2552422881126404
Norm after input: 0.48252278566360474
Norm after each mp layer: 1.565441608428955
Norm after each mp layer: 3.0557076930999756
Norm after each mp layer: 12.314862251281738
Norm before input: 0.2552422881126404
Norm after input: 0.4813249409198761
Norm after each mp layer: 1.559596300125122
Norm after each mp layer: 3.0655126571655273
Norm after each mp layer: 12.399864196777344
Norm before input: 0.2552422881126404
Norm after input: 0.4813249409198761
Norm after each mp layer: 1.559596300125122
Norm after each mp layer: 3.0655126571655273
Norm after each mp layer: 12.399864196777344
Norm before input: 0.2552422881126404
Norm after input: 0.4834509491920471
Norm after each mp layer: 1.56267249584198
Norm after each mp layer: 3.0602452754974365
Norm after each mp layer: 12.283854484558105
Norm before input: 0.2552422881126404
Norm after input: 0.4834509491920471
Norm after each mp layer: 1.56267249584198
Norm after each mp layer: 3.0602452754974365
Norm after each mp layer: 12.283854484558105
Norm before input: 0.2552422881126404
Norm after input: 0.4839288890361786
Norm after each mp layer: 1.5618468523025513
Norm after each mp layer: 3.062743663787842
Norm after each mp layer: 12.236418724060059
Norm before input: 0.2552422881126404
Norm after input: 0.4839288890361786
Norm after each mp layer: 1.5618468523025513
Norm after each mp layer: 3.062743663787842
Norm after each mp layer: 12.236418724060059
Norm before input: 0.2552422881126404
Norm after input: 0.4807467758655548
Norm after each mp layer: 1.5482896566390991
Norm after each mp layer: 3.0648908615112305
Norm after each mp layer: 12.334797859191895
Norm before input: 0.2552422881126404
Norm after input: 0.4807467758655548
Norm after each mp layer: 1.5482896566390991
Norm after each mp layer: 3.0648908615112305
Norm after each mp layer: 12.334797859191895
Norm before input: 0.2552422881126404
Norm after input: 0.48000985383987427
Norm after each mp layer: 1.5434197187423706
Norm after each mp layer: 3.0604350566864014
Norm after each mp layer: 12.33830451965332
Epoch: 85, Loss: 0.4446, Energy: 973.9670, Train: 88.33%, Valid: 74.80%, Test: 74.30%, Best Valid: 75.60%, Best Test: 75.70%
Norm before input: 0.2552422881126404
Norm after input: 0.48000985383987427
Norm after each mp layer: 1.5434197187423706
Norm after each mp layer: 3.0604350566864014
Norm after each mp layer: 12.33830451965332
Norm before input: 0.2552422881126404
Norm after input: 0.4829609990119934
Norm after each mp layer: 1.553517460823059
Norm after each mp layer: 3.055173873901367
Norm after each mp layer: 12.199263572692871
Norm before input: 0.2552422881126404
Norm after input: 0.4829609990119934
Norm after each mp layer: 1.553517460823059
Norm after each mp layer: 3.055173873901367
Norm after each mp layer: 12.199261665344238
Norm before input: 0.2552422881126404
Norm after input: 0.4832378029823303
Norm after each mp layer: 1.555521011352539
Norm after each mp layer: 3.059697151184082
Norm after each mp layer: 12.217901229858398
Norm before input: 0.2552422881126404
Norm after input: 0.4832378029823303
Norm after each mp layer: 1.555521011352539
Norm after each mp layer: 3.059697151184082
Norm after each mp layer: 12.217901229858398
Norm before input: 0.2552422881126404
Norm after input: 0.4806865453720093
Norm after each mp layer: 1.5478107929229736
Norm after each mp layer: 3.0724761486053467
Norm after each mp layer: 12.385414123535156
Norm before input: 0.2552422881126404
Norm after input: 0.4806865453720093
Norm after each mp layer: 1.5478107929229736
Norm after each mp layer: 3.0724761486053467
Norm after each mp layer: 12.385414123535156
Norm before input: 0.2552422881126404
Norm after input: 0.4811416566371918
Norm after each mp layer: 1.5500946044921875
Norm after each mp layer: 3.079197883605957
Norm after each mp layer: 12.376545906066895
Norm before input: 0.2552422881126404
Norm after input: 0.4811416566371918
Norm after each mp layer: 1.5500946044921875
Norm after each mp layer: 3.079197883605957
Norm after each mp layer: 12.376544952392578
Norm before input: 0.2552422881126404
Norm after input: 0.4828929305076599
Norm after each mp layer: 1.5533697605133057
Norm after each mp layer: 3.0698482990264893
Norm after each mp layer: 12.253625869750977
Epoch: 90, Loss: 0.4019, Energy: 928.4919, Train: 90.48%, Valid: 75.60%, Test: 77.20%, Best Valid: 75.80%, Best Test: 77.20%
Norm before input: 0.2552422881126404
Norm after input: 0.4828929305076599
Norm after each mp layer: 1.5533697605133057
Norm after each mp layer: 3.0698482990264893
Norm after each mp layer: 12.253625869750977
Norm before input: 0.2552422881126404
Norm after input: 0.48054635524749756
Norm after each mp layer: 1.5413776636123657
Norm after each mp layer: 3.068267822265625
Norm after each mp layer: 12.351051330566406
Norm before input: 0.2552422881126404
Norm after input: 0.48054635524749756
Norm after each mp layer: 1.5413776636123657
Norm after each mp layer: 3.068267822265625
Norm after each mp layer: 12.351051330566406
Norm before input: 0.2552422881126404
Norm after input: 0.4810648560523987
Norm after each mp layer: 1.5427404642105103
Norm after each mp layer: 3.0730221271514893
Norm after each mp layer: 12.27913761138916
Norm before input: 0.2552422881126404
Norm after input: 0.4810648560523987
Norm after each mp layer: 1.5427404642105103
Norm after each mp layer: 3.0730221271514893
Norm after each mp layer: 12.27913761138916
Norm before input: 0.2552422881126404
Norm after input: 0.4813425838947296
Norm after each mp layer: 1.541358232498169
Norm after each mp layer: 3.0679783821105957
Norm after each mp layer: 12.23809814453125
Norm before input: 0.2552422881126404
Norm after input: 0.4813425838947296
Norm after each mp layer: 1.541358232498169
Norm after each mp layer: 3.0679783821105957
Norm after each mp layer: 12.23809814453125
Norm before input: 0.2552422881126404
Norm after input: 0.48060810565948486
Norm after each mp layer: 1.537187099456787
Norm after each mp layer: 3.0683557987213135
Norm after each mp layer: 12.309268951416016
Norm before input: 0.2552422881126404
Norm after input: 0.48060810565948486
Norm after each mp layer: 1.537187099456787
Norm after each mp layer: 3.0683557987213135
Norm after each mp layer: 12.309268951416016
Norm before input: 0.2552422881126404
Norm after input: 0.4814543128013611
Norm after each mp layer: 1.5432045459747314
Norm after each mp layer: 3.0844473838806152
Norm after each mp layer: 12.305642127990723
Epoch: 95, Loss: 0.3701, Energy: 879.7443, Train: 92.72%, Valid: 75.80%, Test: 77.20%, Best Valid: 76.00%, Best Test: 77.20%
Norm before input: 0.2552422881126404
Norm after input: 0.4814543128013611
Norm after each mp layer: 1.5432045459747314
Norm after each mp layer: 3.0844473838806152
Norm after each mp layer: 12.305642127990723
Norm before input: 0.2552422881126404
Norm after input: 0.4811761975288391
Norm after each mp layer: 1.5439918041229248
Norm after each mp layer: 3.099489688873291
Norm after each mp layer: 12.398127555847168
Norm before input: 0.2552422881126404
Norm after input: 0.4811761975288391
Norm after each mp layer: 1.5439918041229248
Norm after each mp layer: 3.099489688873291
Norm after each mp layer: 12.398127555847168
Norm before input: 0.2552422881126404
Norm after input: 0.48119792342185974
Norm after each mp layer: 1.5439335107803345
Norm after each mp layer: 3.1074419021606445
Norm after each mp layer: 12.467138290405273
Norm before input: 0.2552422881126404
Norm after input: 0.48119792342185974
Norm after each mp layer: 1.5439335107803345
Norm after each mp layer: 3.1074419021606445
Norm after each mp layer: 12.467140197753906
Norm before input: 0.2552422881126404
Norm after input: 0.4825771152973175
Norm after each mp layer: 1.5501735210418701
Norm after each mp layer: 3.1204302310943604
Norm after each mp layer: 12.422030448913574
Norm before input: 0.2552422881126404
Norm after input: 0.4825771152973175
Norm after each mp layer: 1.5501735210418701
Norm after each mp layer: 3.1204302310943604
Norm after each mp layer: 12.422030448913574
Norm before input: 0.2552422881126404
Norm after input: 0.4811888635158539
Norm after each mp layer: 1.544930338859558
Norm after each mp layer: 3.1352221965789795
Norm after each mp layer: 12.551843643188477
Norm before input: 0.2552422881126404
Norm after input: 0.4811888635158539
Norm after each mp layer: 1.544930338859558
Norm after each mp layer: 3.1352224349975586
Norm after each mp layer: 12.551843643188477
Norm before input: 0.2552422881126404
Norm after input: 0.48184001445770264
Norm after each mp layer: 1.5457801818847656
Norm after each mp layer: 3.137126922607422
Norm after each mp layer: 12.526320457458496
Epoch: 100, Loss: 0.3331, Energy: 865.5193, Train: 93.29%, Valid: 76.00%, Test: 77.40%, Best Valid: 76.40%, Best Test: 77.40%
Norm before input: 0.2552422881126404
Norm after input: 0.48184001445770264
Norm after each mp layer: 1.5457801818847656
Norm after each mp layer: 3.1371264457702637
Norm after each mp layer: 12.526320457458496
Norm before input: 0.2552422881126404
Norm after input: 0.48325175046920776
Norm after each mp layer: 1.5510683059692383
Norm after each mp layer: 3.1442458629608154
Norm after each mp layer: 12.465682983398438
Norm before input: 0.2552422881126404
Norm after input: 0.48325175046920776
Norm after each mp layer: 1.5510683059692383
Norm after each mp layer: 3.1442458629608154
Norm after each mp layer: 12.465682983398438
Norm before input: 0.2552422881126404
Norm after input: 0.4799521565437317
Norm after each mp layer: 1.5412709712982178
Norm after each mp layer: 3.1740212440490723
Norm after each mp layer: 12.790298461914062
Norm before input: 0.2552422881126404
Norm after input: 0.4799521565437317
Norm after each mp layer: 1.5412709712982178
Norm after each mp layer: 3.1740212440490723
Norm after each mp layer: 12.790298461914062
Norm before input: 0.2552422881126404
Norm after input: 0.48336949944496155
Norm after each mp layer: 1.5552138090133667
Norm after each mp layer: 3.1776421070098877
Norm after each mp layer: 12.5968017578125
Norm before input: 0.2552422881126404
Norm after input: 0.48336949944496155
Norm after each mp layer: 1.5552138090133667
Norm after each mp layer: 3.1776421070098877
Norm after each mp layer: 12.5968017578125
Norm before input: 0.2552422881126404
Norm after input: 0.4826124906539917
Norm after each mp layer: 1.552791714668274
Norm after each mp layer: 3.1876282691955566
Norm after each mp layer: 12.697925567626953
Norm before input: 0.2552422881126404
Norm after input: 0.4826124906539917
Norm after each mp layer: 1.552791714668274
Norm after each mp layer: 3.1876282691955566
Norm after each mp layer: 12.697927474975586
Norm before input: 0.2552422881126404
Norm after input: 0.4810757637023926
Norm after each mp layer: 1.5492291450500488
Norm after each mp layer: 3.214289426803589
Norm after each mp layer: 12.868107795715332
Epoch: 105, Loss: 0.3058, Energy: 847.4224, Train: 94.70%, Valid: 76.00%, Test: 76.80%, Best Valid: 76.40%, Best Test: 77.60%
Norm before input: 0.2552422881126404
Norm after input: 0.4810757637023926
Norm after each mp layer: 1.5492291450500488
Norm after each mp layer: 3.214289426803589
Norm after each mp layer: 12.868107795715332
Norm before input: 0.2552422881126404
Norm after input: 0.4829616844654083
Norm after each mp layer: 1.5548295974731445
Norm after each mp layer: 3.2135274410247803
Norm after each mp layer: 12.72702693939209
Norm before input: 0.2552422881126404
Norm after input: 0.4829616844654083
Norm after each mp layer: 1.5548295974731445
Norm after each mp layer: 3.2135274410247803
Norm after each mp layer: 12.72702693939209
Norm before input: 0.2552422881126404
Norm after input: 0.4824315011501312
Norm after each mp layer: 1.5501047372817993
Norm after each mp layer: 3.209960460662842
Norm after each mp layer: 12.740055084228516
Norm before input: 0.2552422881126404
Norm after input: 0.4824315011501312
Norm after each mp layer: 1.5501047372817993
Norm after each mp layer: 3.209960460662842
Norm after each mp layer: 12.740055084228516
Norm before input: 0.2552422881126404
Norm after input: 0.4821823239326477
Norm after each mp layer: 1.5505706071853638
Norm after each mp layer: 3.229574680328369
Norm after each mp layer: 12.777350425720215
Norm before input: 0.2552422881126404
Norm after input: 0.4821823239326477
Norm after each mp layer: 1.5505706071853638
Norm after each mp layer: 3.229574680328369
Norm after each mp layer: 12.777350425720215
Norm before input: 0.2552422881126404
Norm after input: 0.48249557614326477
Norm after each mp layer: 1.5498665571212769
Norm after each mp layer: 3.222100019454956
Norm after each mp layer: 12.720284461975098
Norm before input: 0.2552422881126404
Norm after input: 0.48249557614326477
Norm after each mp layer: 1.5498665571212769
Norm after each mp layer: 3.222100019454956
Norm after each mp layer: 12.720284461975098
Norm before input: 0.2552422881126404
Norm after input: 0.48328879475593567
Norm after each mp layer: 1.552274227142334
Norm after each mp layer: 3.2203266620635986
Norm after each mp layer: 12.653185844421387
Epoch: 110, Loss: 0.2763, Energy: 836.9145, Train: 95.86%, Valid: 75.80%, Test: 77.30%, Best Valid: 76.40%, Best Test: 77.70%
Norm before input: 0.2552422881126404
Norm after input: 0.48328879475593567
Norm after each mp layer: 1.552274227142334
Norm after each mp layer: 3.2203266620635986
Norm after each mp layer: 12.653185844421387
Norm before input: 0.2552422881126404
Norm after input: 0.48209622502326965
Norm after each mp layer: 1.550168514251709
Norm after each mp layer: 3.2424190044403076
Norm after each mp layer: 12.781682014465332
Norm before input: 0.2552422881126404
Norm after input: 0.48209622502326965
Norm after each mp layer: 1.550168514251709
Norm after each mp layer: 3.2424190044403076
Norm after each mp layer: 12.781682014465332
Norm before input: 0.2552422881126404
Norm after input: 0.4831293225288391
Norm after each mp layer: 1.5521987676620483
Norm after each mp layer: 3.2314045429229736
Norm after each mp layer: 12.669365882873535
Norm before input: 0.2552422881126404
Norm after input: 0.4831293225288391
Norm after each mp layer: 1.5521987676620483
Norm after each mp layer: 3.2314045429229736
Norm after each mp layer: 12.669365882873535
Norm before input: 0.2552422881126404
Norm after input: 0.48319149017333984
Norm after each mp layer: 1.5522605180740356
Norm after each mp layer: 3.2366890907287598
Norm after each mp layer: 12.657156944274902
Norm before input: 0.2552422881126404
Norm after input: 0.48319149017333984
Norm after each mp layer: 1.5522605180740356
Norm after each mp layer: 3.2366890907287598
Norm after each mp layer: 12.657156944274902
Norm before input: 0.2552422881126404
Norm after input: 0.48184335231781006
Norm after each mp layer: 1.5479508638381958
Norm after each mp layer: 3.2537896633148193
Norm after each mp layer: 12.771339416503906
Norm before input: 0.2552422881126404
Norm after input: 0.48184335231781006
Norm after each mp layer: 1.5479508638381958
Norm after each mp layer: 3.2537896633148193
Norm after each mp layer: 12.771339416503906
Norm before input: 0.2552422881126404
Norm after input: 0.483642578125
Norm after each mp layer: 1.5519613027572632
Norm after each mp layer: 3.237661600112915
Norm after each mp layer: 12.575899124145508
Epoch: 115, Loss: 0.2531, Energy: 807.9745, Train: 96.52%, Valid: 74.40%, Test: 77.10%, Best Valid: 76.40%, Best Test: 77.70%
Norm before input: 0.2552422881126404
Norm after input: 0.483642578125
Norm after each mp layer: 1.5519613027572632
Norm after each mp layer: 3.237661600112915
Norm after each mp layer: 12.575899124145508
Norm before input: 0.2552422881126404
Norm after input: 0.48206421732902527
Norm after each mp layer: 1.5467133522033691
Norm after each mp layer: 3.2541370391845703
Norm after each mp layer: 12.699723243713379
Norm before input: 0.2552422881126404
Norm after input: 0.48206421732902527
Norm after each mp layer: 1.5467133522033691
Norm after each mp layer: 3.2541370391845703
Norm after each mp layer: 12.699723243713379
Norm before input: 0.2552422881126404
Norm after input: 0.48270949721336365
Norm after each mp layer: 1.548499345779419
Norm after each mp layer: 3.255167007446289
Norm after each mp layer: 12.622636795043945
Norm before input: 0.2552422881126404
Norm after input: 0.48270949721336365
Norm after each mp layer: 1.548499345779419
Norm after each mp layer: 3.255167007446289
Norm after each mp layer: 12.622636795043945
Norm before input: 0.2552422881126404
Norm after input: 0.4826332926750183
Norm after each mp layer: 1.5457768440246582
Norm after each mp layer: 3.24484920501709
Norm after each mp layer: 12.574844360351562
Norm before input: 0.2552422881126404
Norm after input: 0.4826332926750183
Norm after each mp layer: 1.5457768440246582
Norm after each mp layer: 3.2448487281799316
Norm after each mp layer: 12.574846267700195
Norm before input: 0.2552422881126404
Norm after input: 0.4830836355686188
Norm after each mp layer: 1.5488897562026978
Norm after each mp layer: 3.263045072555542
Norm after each mp layer: 12.553196907043457
Norm before input: 0.2552422881126404
Norm after input: 0.4830836355686188
Norm after each mp layer: 1.5488897562026978
Norm after each mp layer: 3.263045072555542
Norm after each mp layer: 12.553196907043457
Norm before input: 0.2552422881126404
Norm after input: 0.47694888710975647
Norm after each mp layer: 1.5269343852996826
Norm after each mp layer: 3.3080368041992188
Norm after each mp layer: 13.206043243408203
Epoch: 120, Loss: 0.2554, Energy: 815.2114, Train: 90.31%, Valid: 67.80%, Test: 68.50%, Best Valid: 76.40%, Best Test: 77.70%
Norm before input: 0.2552422881126404
Norm after input: 0.47694888710975647
Norm after each mp layer: 1.5269343852996826
Norm after each mp layer: 3.3080368041992188
Norm after each mp layer: 13.20604419708252
Norm before input: 0.2552422881126404
Norm after input: 0.4888613820075989
Norm after each mp layer: 1.589877963066101
Norm after each mp layer: 3.4461302757263184
Norm after each mp layer: 13.260444641113281
Norm before input: 0.2552422881126404
Norm after input: 0.4888613820075989
Norm after each mp layer: 1.589877963066101
Norm after each mp layer: 3.4461302757263184
Norm after each mp layer: 13.260446548461914
Norm before input: 0.2552422881126404
Norm after input: 0.46376824378967285
Norm after each mp layer: 1.497206449508667
Norm after each mp layer: 3.3433446884155273
Norm after each mp layer: 13.44140338897705
Norm before input: 0.2552422881126404
Norm after input: 0.46376824378967285
Norm after each mp layer: 1.497206449508667
Norm after each mp layer: 3.3433446884155273
Norm after each mp layer: 13.44140338897705
Norm before input: 0.2552422881126404
Norm after input: 0.45303618907928467
Norm after each mp layer: 1.4680864810943604
Norm after each mp layer: 3.36855411529541
Norm after each mp layer: 13.528255462646484
Norm before input: 0.2552422881126404
Norm after input: 0.45303618907928467
Norm after each mp layer: 1.4680864810943604
Norm after each mp layer: 3.36855411529541
Norm after each mp layer: 13.528255462646484
Norm before input: 0.2552422881126404
Norm after input: 0.4504162669181824
Norm after each mp layer: 1.4539883136749268
Norm after each mp layer: 3.2666053771972656
Norm after each mp layer: 12.717768669128418
Norm before input: 0.2552422881126404
Norm after input: 0.4504162669181824
Norm after each mp layer: 1.4539883136749268
Norm after each mp layer: 3.2666053771972656
Norm after each mp layer: 12.717768669128418
Norm before input: 0.2552422881126404
Norm after input: 0.45632055401802063
Norm after each mp layer: 1.4675559997558594
Norm after each mp layer: 3.2181456089019775
Norm after each mp layer: 11.76459789276123
Epoch: 125, Loss: 0.4116, Energy: 1459.1895, Train: 86.34%, Valid: 71.20%, Test: 72.90%, Best Valid: 76.40%, Best Test: 77.70%
Norm before input: 0.2552422881126404
Norm after input: 0.45632055401802063
Norm after each mp layer: 1.4675559997558594
Norm after each mp layer: 3.2181456089019775
Norm after each mp layer: 11.76459789276123
Norm before input: 0.2552422881126404
Norm after input: 0.45110124349594116
Norm after each mp layer: 1.4508941173553467
Norm after each mp layer: 3.223533868789673
Norm after each mp layer: 11.336008071899414
Norm before input: 0.2552422881126404
Norm after input: 0.45110124349594116
Norm after each mp layer: 1.4508941173553467
Norm after each mp layer: 3.223533868789673
Norm after each mp layer: 11.336008071899414
Norm before input: 0.2552422881126404
Norm after input: 0.44217801094055176
Norm after each mp layer: 1.3994084596633911
Norm after each mp layer: 3.0712475776672363
Norm after each mp layer: 10.317706108093262
Norm before input: 0.2552422881126404
Norm after input: 0.44217801094055176
Norm after each mp layer: 1.3994084596633911
Norm after each mp layer: 3.0712475776672363
Norm after each mp layer: 10.317706108093262
Norm before input: 0.2552422881126404
Norm after input: 0.4347425103187561
Norm after each mp layer: 1.3519691228866577
Norm after each mp layer: 2.914479970932007
Norm after each mp layer: 9.42764949798584
Norm before input: 0.2552422881126404
Norm after input: 0.4347425103187561
Norm after each mp layer: 1.3519691228866577
Norm after each mp layer: 2.914479970932007
Norm after each mp layer: 9.42764949798584
Norm before input: 0.2552422881126404
Norm after input: 0.4258978068828583
Norm after each mp layer: 1.3262691497802734
Norm after each mp layer: 2.8724758625030518
Norm after each mp layer: 9.24616527557373
Norm before input: 0.2552422881126404
Norm after input: 0.4258978068828583
Norm after each mp layer: 1.3262691497802734
Norm after each mp layer: 2.8724758625030518
Norm after each mp layer: 9.24616527557373
Norm before input: 0.2552422881126404
Norm after input: 0.41798335313796997
Norm after each mp layer: 1.3211479187011719
Norm after each mp layer: 2.901078224182129
Norm after each mp layer: 9.41189956665039
Epoch: 130, Loss: 0.4090, Energy: 1356.0398, Train: 91.47%, Valid: 75.80%, Test: 77.80%, Best Valid: 77.60%, Best Test: 78.30%
Norm before input: 0.2552422881126404
Norm after input: 0.41798335313796997
Norm after each mp layer: 1.3211479187011719
Norm after each mp layer: 2.901078224182129
Norm after each mp layer: 9.41189956665039
Norm before input: 0.2552422881126404
Norm after input: 0.4141779839992523
Norm after each mp layer: 1.3316115140914917
Norm after each mp layer: 2.930356979370117
Norm after each mp layer: 9.509033203125
Norm before input: 0.2552422881126404
Norm after input: 0.4141779839992523
Norm after each mp layer: 1.3316115140914917
Norm after each mp layer: 2.930356979370117
Norm after each mp layer: 9.509033203125
Norm before input: 0.2552422881126404
Norm after input: 0.4150616526603699
Norm after each mp layer: 1.3533289432525635
Norm after each mp layer: 2.9371843338012695
Norm after each mp layer: 9.3954496383667
Norm before input: 0.2552422881126404
Norm after input: 0.4150616526603699
Norm after each mp layer: 1.3533289432525635
Norm after each mp layer: 2.9371843338012695
Norm after each mp layer: 9.3954496383667
Norm before input: 0.2552422881126404
Norm after input: 0.4181583821773529
Norm after each mp layer: 1.3837003707885742
Norm after each mp layer: 2.9679677486419678
Norm after each mp layer: 9.32805347442627
Norm before input: 0.2552422881126404
Norm after input: 0.4181583821773529
Norm after each mp layer: 1.3837003707885742
Norm after each mp layer: 2.9679677486419678
Norm after each mp layer: 9.32805347442627
Norm before input: 0.2552422881126404
Norm after input: 0.4185064136981964
Norm after each mp layer: 1.4076597690582275
Norm after each mp layer: 3.0341811180114746
Norm after each mp layer: 9.461179733276367
Norm before input: 0.2552422881126404
Norm after input: 0.4185064136981964
Norm after each mp layer: 1.4076597690582275
Norm after each mp layer: 3.0341811180114746
Norm after each mp layer: 9.461179733276367
Norm before input: 0.2552422881126404
Norm after input: 0.41676729917526245
Norm after each mp layer: 1.4210734367370605
Norm after each mp layer: 3.099508285522461
Norm after each mp layer: 9.663867950439453
Epoch: 135, Loss: 0.3530, Energy: 1102.0311, Train: 95.12%, Valid: 79.80%, Test: 81.10%, Best Valid: 79.80%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.41676729917526245
Norm after each mp layer: 1.4210734367370605
Norm after each mp layer: 3.099508285522461
Norm after each mp layer: 9.663867950439453
Norm before input: 0.2552422881126404
Norm after input: 0.4160078167915344
Norm after each mp layer: 1.4263416528701782
Norm after each mp layer: 3.128660202026367
Norm after each mp layer: 9.704327583312988
Norm before input: 0.2552422881126404
Norm after input: 0.4160078167915344
Norm after each mp layer: 1.4263416528701782
Norm after each mp layer: 3.128660202026367
Norm after each mp layer: 9.704327583312988
Norm before input: 0.2552422881126404
Norm after input: 0.4171094298362732
Norm after each mp layer: 1.4244798421859741
Norm after each mp layer: 3.112222671508789
Norm after each mp layer: 9.492829322814941
Norm before input: 0.2552422881126404
Norm after input: 0.4171094298362732
Norm after each mp layer: 1.4244798421859741
Norm after each mp layer: 3.112222671508789
Norm after each mp layer: 9.492829322814941
Norm before input: 0.2552422881126404
Norm after input: 0.4187611937522888
Norm after each mp layer: 1.4200439453125
Norm after each mp layer: 3.0908985137939453
Norm after each mp layer: 9.23617935180664
Norm before input: 0.2552422881126404
Norm after input: 0.4187611937522888
Norm after each mp layer: 1.4200439453125
Norm after each mp layer: 3.0908985137939453
Norm after each mp layer: 9.23617935180664
Norm before input: 0.2552422881126404
Norm after input: 0.41879644989967346
Norm after each mp layer: 1.4161750078201294
Norm after each mp layer: 3.1034610271453857
Norm after each mp layer: 9.186405181884766
Norm before input: 0.2552422881126404
Norm after input: 0.41879644989967346
Norm after each mp layer: 1.4161750078201294
Norm after each mp layer: 3.1034610271453857
Norm after each mp layer: 9.186405181884766
Norm before input: 0.2552422881126404
Norm after input: 0.4180952310562134
Norm after each mp layer: 1.4173935651779175
Norm after each mp layer: 3.1499195098876953
Norm after each mp layer: 9.31081485748291
Epoch: 140, Loss: 0.3041, Energy: 663.3552, Train: 96.11%, Valid: 79.60%, Test: 79.30%, Best Valid: 79.80%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4180952310562134
Norm after each mp layer: 1.4173935651779175
Norm after each mp layer: 3.1499195098876953
Norm after each mp layer: 9.31081485748291
Norm before input: 0.2552422881126404
Norm after input: 0.4182714819908142
Norm after each mp layer: 1.4229038953781128
Norm after each mp layer: 3.196269989013672
Norm after each mp layer: 9.374777793884277
Norm before input: 0.2552422881126404
Norm after input: 0.4182714819908142
Norm after each mp layer: 1.4229038953781128
Norm after each mp layer: 3.196269989013672
Norm after each mp layer: 9.374777793884277
Norm before input: 0.2552422881126404
Norm after input: 0.41893213987350464
Norm after each mp layer: 1.4294259548187256
Norm after each mp layer: 3.2310445308685303
Norm after each mp layer: 9.382762908935547
Norm before input: 0.2552422881126404
Norm after input: 0.41893213987350464
Norm after each mp layer: 1.4294259548187256
Norm after each mp layer: 3.2310450077056885
Norm after each mp layer: 9.382762908935547
Norm before input: 0.2552422881126404
Norm after input: 0.41816166043281555
Norm after each mp layer: 1.4341216087341309
Norm after each mp layer: 3.2668330669403076
Norm after each mp layer: 9.524029731750488
Norm before input: 0.2552422881126404
Norm after input: 0.41816166043281555
Norm after each mp layer: 1.4341216087341309
Norm after each mp layer: 3.2668330669403076
Norm after each mp layer: 9.524029731750488
Norm before input: 0.2552422881126404
Norm after input: 0.4164246618747711
Norm after each mp layer: 1.436095952987671
Norm after each mp layer: 3.2977187633514404
Norm after each mp layer: 9.74820613861084
Norm before input: 0.2552422881126404
Norm after input: 0.4164246618747711
Norm after each mp layer: 1.436095952987671
Norm after each mp layer: 3.2977187633514404
Norm after each mp layer: 9.74820613861084
Norm before input: 0.2552422881126404
Norm after input: 0.4155918061733246
Norm after each mp layer: 1.4358994960784912
Norm after each mp layer: 3.3069491386413574
Norm after each mp layer: 9.866082191467285
Epoch: 145, Loss: 0.2579, Energy: 752.3641, Train: 95.94%, Valid: 79.40%, Test: 79.60%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4155918061733246
Norm after each mp layer: 1.4358994960784912
Norm after each mp layer: 3.3069491386413574
Norm after each mp layer: 9.866082191467285
Norm before input: 0.2552422881126404
Norm after input: 0.41581374406814575
Norm after each mp layer: 1.4351465702056885
Norm after each mp layer: 3.301643133163452
Norm after each mp layer: 9.873257637023926
Norm before input: 0.2552422881126404
Norm after input: 0.41581374406814575
Norm after each mp layer: 1.4351465702056885
Norm after each mp layer: 3.301643133163452
Norm after each mp layer: 9.873257637023926
Norm before input: 0.2552422881126404
Norm after input: 0.4160723388195038
Norm after each mp layer: 1.4360620975494385
Norm after each mp layer: 3.3066792488098145
Norm after each mp layer: 9.914563179016113
Norm before input: 0.2552422881126404
Norm after input: 0.4160723388195038
Norm after each mp layer: 1.4360620975494385
Norm after each mp layer: 3.3066792488098145
Norm after each mp layer: 9.914563179016113
Norm before input: 0.2552422881126404
Norm after input: 0.41445979475975037
Norm after each mp layer: 1.4384663105010986
Norm after each mp layer: 3.3482072353363037
Norm after each mp layer: 10.18685245513916
Norm before input: 0.2552422881126404
Norm after input: 0.41445979475975037
Norm after each mp layer: 1.4384663105010986
Norm after each mp layer: 3.3482072353363037
Norm after each mp layer: 10.18685245513916
Norm before input: 0.2552422881126404
Norm after input: 0.41413772106170654
Norm after each mp layer: 1.4411014318466187
Norm after each mp layer: 3.373481035232544
Norm after each mp layer: 10.33852767944336
Norm before input: 0.2552422881126404
Norm after input: 0.41413772106170654
Norm after each mp layer: 1.4411014318466187
Norm after each mp layer: 3.373481035232544
Norm after each mp layer: 10.33852767944336
Norm before input: 0.2552422881126404
Norm after input: 0.41536828875541687
Norm after each mp layer: 1.442637324333191
Norm after each mp layer: 3.370293140411377
Norm after each mp layer: 10.304871559143066
Epoch: 150, Loss: 0.2277, Energy: 831.9622, Train: 96.11%, Valid: 79.20%, Test: 80.10%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.41536828875541687
Norm after each mp layer: 1.442637324333191
Norm after each mp layer: 3.370293140411377
Norm after each mp layer: 10.304871559143066
Norm before input: 0.2552422881126404
Norm after input: 0.41626057028770447
Norm after each mp layer: 1.4438873529434204
Norm after each mp layer: 3.3687500953674316
Norm after each mp layer: 10.311318397521973
Norm before input: 0.2552422881126404
Norm after input: 0.41626057028770447
Norm after each mp layer: 1.4438873529434204
Norm after each mp layer: 3.3687496185302734
Norm after each mp layer: 10.311318397521973
Norm before input: 0.2552422881126404
Norm after input: 0.4147579073905945
Norm after each mp layer: 1.4427087306976318
Norm after each mp layer: 3.3858604431152344
Norm after each mp layer: 10.514849662780762
Norm before input: 0.2552422881126404
Norm after input: 0.4147579073905945
Norm after each mp layer: 1.4427087306976318
Norm after each mp layer: 3.3858604431152344
Norm after each mp layer: 10.514849662780762
Norm before input: 0.2552422881126404
Norm after input: 0.4137217402458191
Norm after each mp layer: 1.4388960599899292
Norm after each mp layer: 3.38806414604187
Norm after each mp layer: 10.631516456604004
Norm before input: 0.2552422881126404
Norm after input: 0.4137217402458191
Norm after each mp layer: 1.4388960599899292
Norm after each mp layer: 3.38806414604187
Norm after each mp layer: 10.631516456604004
Norm before input: 0.2552422881126404
Norm after input: 0.4141228199005127
Norm after each mp layer: 1.4315781593322754
Norm after each mp layer: 3.356287956237793
Norm after each mp layer: 10.516304016113281
Norm before input: 0.2552422881126404
Norm after input: 0.4141228199005127
Norm after each mp layer: 1.4315781593322754
Norm after each mp layer: 3.356287956237793
Norm after each mp layer: 10.516304016113281
Norm before input: 0.2552422881126404
Norm after input: 0.41477885842323303
Norm after each mp layer: 1.4233306646347046
Norm after each mp layer: 3.318676233291626
Norm after each mp layer: 10.355043411254883
Epoch: 155, Loss: 0.2027, Energy: 695.0742, Train: 96.44%, Valid: 77.60%, Test: 77.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.41477885842323303
Norm after each mp layer: 1.4233306646347046
Norm after each mp layer: 3.318676233291626
Norm after each mp layer: 10.355043411254883
Norm before input: 0.2552422881126404
Norm after input: 0.4133129417896271
Norm after each mp layer: 1.4160797595977783
Norm after each mp layer: 3.3118653297424316
Norm after each mp layer: 10.426424026489258
Norm before input: 0.2552422881126404
Norm after input: 0.4133129417896271
Norm after each mp layer: 1.4160797595977783
Norm after each mp layer: 3.3118653297424316
Norm after each mp layer: 10.426424026489258
Norm before input: 0.2552422881126404
Norm after input: 0.41214457154273987
Norm after each mp layer: 1.4104849100112915
Norm after each mp layer: 3.3079655170440674
Norm after each mp layer: 10.496715545654297
Norm before input: 0.2552422881126404
Norm after input: 0.41214457154273987
Norm after each mp layer: 1.4104849100112915
Norm after each mp layer: 3.3079655170440674
Norm after each mp layer: 10.496715545654297
Norm before input: 0.2552422881126404
Norm after input: 0.4122340977191925
Norm after each mp layer: 1.4056200981140137
Norm after each mp layer: 3.2885258197784424
Norm after each mp layer: 10.434176445007324
Norm before input: 0.2552422881126404
Norm after input: 0.4122340977191925
Norm after each mp layer: 1.4056200981140137
Norm after each mp layer: 3.2885258197784424
Norm after each mp layer: 10.434175491333008
Norm before input: 0.2552422881126404
Norm after input: 0.41286495327949524
Norm after each mp layer: 1.4024640321731567
Norm after each mp layer: 3.269153356552124
Norm after each mp layer: 10.348454475402832
Norm before input: 0.2552422881126404
Norm after input: 0.41286495327949524
Norm after each mp layer: 1.4024640321731567
Norm after each mp layer: 3.269153356552124
Norm after each mp layer: 10.348454475402832
Norm before input: 0.2552422881126404
Norm after input: 0.4126405119895935
Norm after each mp layer: 1.4013437032699585
Norm after each mp layer: 3.270939350128174
Norm after each mp layer: 10.394783973693848
Epoch: 160, Loss: 0.1908, Energy: 623.7807, Train: 96.77%, Valid: 77.80%, Test: 77.90%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4126405119895935
Norm after each mp layer: 1.4013437032699585
Norm after each mp layer: 3.270939350128174
Norm after each mp layer: 10.394783973693848
Norm before input: 0.2552422881126404
Norm after input: 0.41168108582496643
Norm after each mp layer: 1.4014232158660889
Norm after each mp layer: 3.2892041206359863
Norm after each mp layer: 10.546897888183594
Norm before input: 0.2552422881126404
Norm after input: 0.41168108582496643
Norm after each mp layer: 1.4014232158660889
Norm after each mp layer: 3.2892041206359863
Norm after each mp layer: 10.546897888183594
Norm before input: 0.2552422881126404
Norm after input: 0.4112991988658905
Norm after each mp layer: 1.400997519493103
Norm after each mp layer: 3.297858953475952
Norm after each mp layer: 10.630905151367188
Norm before input: 0.2552422881126404
Norm after input: 0.4112991988658905
Norm after each mp layer: 1.400997519493103
Norm after each mp layer: 3.297858953475952
Norm after each mp layer: 10.630905151367188
Norm before input: 0.2552422881126404
Norm after input: 0.4117994010448456
Norm after each mp layer: 1.3989547491073608
Norm after each mp layer: 3.2864177227020264
Norm after each mp layer: 10.58150863647461
Norm before input: 0.2552422881126404
Norm after input: 0.4117994010448456
Norm after each mp layer: 1.3989547491073608
Norm after each mp layer: 3.2864177227020264
Norm after each mp layer: 10.58150863647461
Norm before input: 0.2552422881126404
Norm after input: 0.4121088683605194
Norm after each mp layer: 1.3961474895477295
Norm after each mp layer: 3.2747232913970947
Norm after each mp layer: 10.537470817565918
Norm before input: 0.2552422881126404
Norm after input: 0.4121088683605194
Norm after each mp layer: 1.3961474895477295
Norm after each mp layer: 3.2747232913970947
Norm after each mp layer: 10.537470817565918
Norm before input: 0.2552422881126404
Norm after input: 0.4113463759422302
Norm after each mp layer: 1.392840027809143
Norm after each mp layer: 3.277028799057007
Norm after each mp layer: 10.598177909851074
Epoch: 165, Loss: 0.1798, Energy: 616.8014, Train: 97.27%, Valid: 78.20%, Test: 77.50%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4113463759422302
Norm after each mp layer: 1.392840027809143
Norm after each mp layer: 3.277028799057007
Norm after each mp layer: 10.598177909851074
Norm before input: 0.2552422881126404
Norm after input: 0.41067588329315186
Norm after each mp layer: 1.3889216184616089
Norm after each mp layer: 3.2766458988189697
Norm after each mp layer: 10.639403343200684
Norm before input: 0.2552422881126404
Norm after input: 0.41067588329315186
Norm after each mp layer: 1.3889216184616089
Norm after each mp layer: 3.2766458988189697
Norm after each mp layer: 10.639403343200684
Norm before input: 0.2552422881126404
Norm after input: 0.4108271300792694
Norm after each mp layer: 1.3841800689697266
Norm after each mp layer: 3.2610321044921875
Norm after each mp layer: 10.570098876953125
Norm before input: 0.2552422881126404
Norm after input: 0.4108271300792694
Norm after each mp layer: 1.3841800689697266
Norm after each mp layer: 3.2610321044921875
Norm after each mp layer: 10.570098876953125
Norm before input: 0.2552422881126404
Norm after input: 0.4112049639225006
Norm after each mp layer: 1.3797755241394043
Norm after each mp layer: 3.244009256362915
Norm after each mp layer: 10.484299659729004
Norm before input: 0.2552422881126404
Norm after input: 0.4112049639225006
Norm after each mp layer: 1.3797755241394043
Norm after each mp layer: 3.244009256362915
Norm after each mp layer: 10.484299659729004
Norm before input: 0.2552422881126404
Norm after input: 0.41034674644470215
Norm after each mp layer: 1.376511573791504
Norm after each mp layer: 3.2490551471710205
Norm after each mp layer: 10.56151008605957
Norm before input: 0.2552422881126404
Norm after input: 0.41034674644470215
Norm after each mp layer: 1.376511573791504
Norm after each mp layer: 3.2490551471710205
Norm after each mp layer: 10.56151008605957
Norm before input: 0.2552422881126404
Norm after input: 0.40990471839904785
Norm after each mp layer: 1.3741174936294556
Norm after each mp layer: 3.250596761703491
Norm after each mp layer: 10.606353759765625
Epoch: 170, Loss: 0.1673, Energy: 616.7043, Train: 97.68%, Valid: 78.40%, Test: 77.80%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40990471839904785
Norm after each mp layer: 1.3741174936294556
Norm after each mp layer: 3.250596761703491
Norm after each mp layer: 10.606353759765625
Norm before input: 0.2552422881126404
Norm after input: 0.41017743945121765
Norm after each mp layer: 1.372214436531067
Norm after each mp layer: 3.2416622638702393
Norm after each mp layer: 10.569419860839844
Norm before input: 0.2552422881126404
Norm after input: 0.41017743945121765
Norm after each mp layer: 1.372214436531067
Norm after each mp layer: 3.2416622638702393
Norm after each mp layer: 10.569419860839844
Norm before input: 0.2552422881126404
Norm after input: 0.41037043929100037
Norm after each mp layer: 1.3711413145065308
Norm after each mp layer: 3.2362992763519287
Norm after each mp layer: 10.552480697631836
Norm before input: 0.2552422881126404
Norm after input: 0.41037043929100037
Norm after each mp layer: 1.3711413145065308
Norm after each mp layer: 3.2362992763519287
Norm after each mp layer: 10.552480697631836
Norm before input: 0.2552422881126404
Norm after input: 0.4098225235939026
Norm after each mp layer: 1.370406150817871
Norm after each mp layer: 3.242888927459717
Norm after each mp layer: 10.623364448547363
Norm before input: 0.2552422881126404
Norm after input: 0.4098225235939026
Norm after each mp layer: 1.370406150817871
Norm after each mp layer: 3.242888927459717
Norm after each mp layer: 10.623364448547363
Norm before input: 0.2552422881126404
Norm after input: 0.4092463552951813
Norm after each mp layer: 1.3692069053649902
Norm after each mp layer: 3.2478864192962646
Norm after each mp layer: 10.687337875366211
Norm before input: 0.2552422881126404
Norm after input: 0.4092463552951813
Norm after each mp layer: 1.3692069053649902
Norm after each mp layer: 3.2478864192962646
Norm after each mp layer: 10.687337875366211
Norm before input: 0.2552422881126404
Norm after input: 0.40928804874420166
Norm after each mp layer: 1.3667194843292236
Norm after each mp layer: 3.2374379634857178
Norm after each mp layer: 10.647217750549316
Epoch: 175, Loss: 0.1586, Energy: 615.4731, Train: 97.76%, Valid: 77.60%, Test: 77.60%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40928804874420166
Norm after each mp layer: 1.3667194843292236
Norm after each mp layer: 3.2374379634857178
Norm after each mp layer: 10.647218704223633
Norm before input: 0.2552422881126404
Norm after input: 0.40946176648139954
Norm after each mp layer: 1.3630810976028442
Norm after each mp layer: 3.22037410736084
Norm after each mp layer: 10.568503379821777
Norm before input: 0.2552422881126404
Norm after input: 0.40946176648139954
Norm after each mp layer: 1.3630810976028442
Norm after each mp layer: 3.220374584197998
Norm after each mp layer: 10.568503379821777
Norm before input: 0.2552422881126404
Norm after input: 0.40898844599723816
Norm after each mp layer: 1.3585532903671265
Norm after each mp layer: 3.2102813720703125
Norm after each mp layer: 10.552631378173828
Norm before input: 0.2552422881126404
Norm after input: 0.40898844599723816
Norm after each mp layer: 1.3585532903671265
Norm after each mp layer: 3.2102813720703125
Norm after each mp layer: 10.552631378173828
Norm before input: 0.2552422881126404
Norm after input: 0.4083923399448395
Norm after each mp layer: 1.3536096811294556
Norm after each mp layer: 3.200814723968506
Norm after each mp layer: 10.54635238647461
Norm before input: 0.2552422881126404
Norm after input: 0.4083923399448395
Norm after each mp layer: 1.3536096811294556
Norm after each mp layer: 3.200814723968506
Norm after each mp layer: 10.54635238647461
Norm before input: 0.2552422881126404
Norm after input: 0.4083649516105652
Norm after each mp layer: 1.3486194610595703
Norm after each mp layer: 3.1818349361419678
Norm after each mp layer: 10.46647834777832
Norm before input: 0.2552422881126404
Norm after input: 0.4083649516105652
Norm after each mp layer: 1.3486194610595703
Norm after each mp layer: 3.1818349361419678
Norm after each mp layer: 10.46647834777832
Norm before input: 0.2552422881126404
Norm after input: 0.4084930419921875
Norm after each mp layer: 1.3442869186401367
Norm after each mp layer: 3.1632213592529297
Norm after each mp layer: 10.379793167114258
Epoch: 180, Loss: 0.1505, Energy: 571.6978, Train: 97.68%, Valid: 77.20%, Test: 77.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4084930419921875
Norm after each mp layer: 1.3442869186401367
Norm after each mp layer: 3.1632213592529297
Norm after each mp layer: 10.379793167114258
Norm before input: 0.2552422881126404
Norm after input: 0.4079088568687439
Norm after each mp layer: 1.340951681137085
Norm after each mp layer: 3.160579204559326
Norm after each mp layer: 10.404301643371582
Norm before input: 0.2552422881126404
Norm after input: 0.4079088568687439
Norm after each mp layer: 1.340951681137085
Norm after each mp layer: 3.160579204559326
Norm after each mp layer: 10.404301643371582
Norm before input: 0.2552422881126404
Norm after input: 0.40767696499824524
Norm after each mp layer: 1.3385136127471924
Norm after each mp layer: 3.156822681427002
Norm after each mp layer: 10.40428638458252
Norm before input: 0.2552422881126404
Norm after input: 0.40767696499824524
Norm after each mp layer: 1.3385136127471924
Norm after each mp layer: 3.156822681427002
Norm after each mp layer: 10.40428638458252
Norm before input: 0.2552422881126404
Norm after input: 0.40793681144714355
Norm after each mp layer: 1.3366470336914062
Norm after each mp layer: 3.1481359004974365
Norm after each mp layer: 10.35379695892334
Norm before input: 0.2552422881126404
Norm after input: 0.40793681144714355
Norm after each mp layer: 1.3366470336914062
Norm after each mp layer: 3.1481359004974365
Norm after each mp layer: 10.35379695892334
Norm before input: 0.2552422881126404
Norm after input: 0.4078756868839264
Norm after each mp layer: 1.3351812362670898
Norm after each mp layer: 3.1473655700683594
Norm after each mp layer: 10.357039451599121
Norm before input: 0.2552422881126404
Norm after input: 0.4078756868839264
Norm after each mp layer: 1.3351812362670898
Norm after each mp layer: 3.1473655700683594
Norm after each mp layer: 10.357039451599121
Norm before input: 0.2552422881126404
Norm after input: 0.407490611076355
Norm after each mp layer: 1.333724021911621
Norm after each mp layer: 3.1528053283691406
Norm after each mp layer: 10.406928062438965
Epoch: 185, Loss: 0.1438, Energy: 536.7766, Train: 97.85%, Valid: 76.80%, Test: 77.50%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.407490611076355
Norm after each mp layer: 1.333724021911621
Norm after each mp layer: 3.1528053283691406
Norm after each mp layer: 10.406928062438965
Norm before input: 0.2552422881126404
Norm after input: 0.4074473977088928
Norm after each mp layer: 1.3318524360656738
Norm after each mp layer: 3.1509885787963867
Norm after each mp layer: 10.405092239379883
Norm before input: 0.2552422881126404
Norm after input: 0.4074473977088928
Norm after each mp layer: 1.3318524360656738
Norm after each mp layer: 3.1509885787963867
Norm after each mp layer: 10.405092239379883
Norm before input: 0.2552422881126404
Norm after input: 0.4075988829135895
Norm after each mp layer: 1.3295750617980957
Norm after each mp layer: 3.1440861225128174
Norm after each mp layer: 10.369929313659668
Norm before input: 0.2552422881126404
Norm after input: 0.4075988829135895
Norm after each mp layer: 1.3295750617980957
Norm after each mp layer: 3.1440861225128174
Norm after each mp layer: 10.369929313659668
Norm before input: 0.2552422881126404
Norm after input: 0.4073953330516815
Norm after each mp layer: 1.3271864652633667
Norm after each mp layer: 3.142624855041504
Norm after each mp layer: 10.380058288574219
Norm before input: 0.2552422881126404
Norm after input: 0.4073953330516815
Norm after each mp layer: 1.3271864652633667
Norm after each mp layer: 3.142624855041504
Norm after each mp layer: 10.380059242248535
Norm before input: 0.2552422881126404
Norm after input: 0.4070095419883728
Norm after each mp layer: 1.3248709440231323
Norm after each mp layer: 3.1443235874176025
Norm after each mp layer: 10.415821075439453
Norm before input: 0.2552422881126404
Norm after input: 0.4070095419883728
Norm after each mp layer: 1.3248709440231323
Norm after each mp layer: 3.1443235874176025
Norm after each mp layer: 10.415821075439453
Norm before input: 0.2552422881126404
Norm after input: 0.40691596269607544
Norm after each mp layer: 1.3226430416107178
Norm after each mp layer: 3.1406869888305664
Norm after each mp layer: 10.411391258239746
Epoch: 190, Loss: 0.1376, Energy: 525.9357, Train: 98.18%, Valid: 77.00%, Test: 76.80%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40691596269607544
Norm after each mp layer: 1.3226430416107178
Norm after each mp layer: 3.1406869888305664
Norm after each mp layer: 10.411391258239746
Norm before input: 0.2552422881126404
Norm after input: 0.40699493885040283
Norm after each mp layer: 1.320682406425476
Norm after each mp layer: 3.1346628665924072
Norm after each mp layer: 10.386510848999023
Norm before input: 0.2552422881126404
Norm after input: 0.40699493885040283
Norm after each mp layer: 1.320682406425476
Norm after each mp layer: 3.1346628665924072
Norm after each mp layer: 10.386510848999023
Norm before input: 0.2552422881126404
Norm after input: 0.40681973099708557
Norm after each mp layer: 1.3191299438476562
Norm after each mp layer: 3.1349010467529297
Norm after each mp layer: 10.404500007629395
Norm before input: 0.2552422881126404
Norm after input: 0.40681973099708557
Norm after each mp layer: 1.3191299438476562
Norm after each mp layer: 3.1349010467529297
Norm after each mp layer: 10.404500007629395
Norm before input: 0.2552422881126404
Norm after input: 0.4065271317958832
Norm after each mp layer: 1.3178263902664185
Norm after each mp layer: 3.1387104988098145
Norm after each mp layer: 10.445295333862305
Norm before input: 0.2552422881126404
Norm after input: 0.4065271317958832
Norm after each mp layer: 1.3178263902664185
Norm after each mp layer: 3.1387104988098145
Norm after each mp layer: 10.445295333862305
Norm before input: 0.2552422881126404
Norm after input: 0.40649664402008057
Norm after each mp layer: 1.3164714574813843
Norm after each mp layer: 3.1379239559173584
Norm after each mp layer: 10.449503898620605
Norm before input: 0.2552422881126404
Norm after input: 0.40649664402008057
Norm after each mp layer: 1.3164714574813843
Norm after each mp layer: 3.1379239559173584
Norm after each mp layer: 10.449503898620605
Norm before input: 0.2552422881126404
Norm after input: 0.40659767389297485
Norm after each mp layer: 1.314995527267456
Norm after each mp layer: 3.1346628665924072
Norm after each mp layer: 10.43454360961914
Epoch: 195, Loss: 0.1316, Energy: 512.4594, Train: 98.26%, Valid: 76.60%, Test: 76.50%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40659767389297485
Norm after each mp layer: 1.314995527267456
Norm after each mp layer: 3.1346628665924072
Norm after each mp layer: 10.43454360961914
Norm before input: 0.2552422881126404
Norm after input: 0.4064374268054962
Norm after each mp layer: 1.3134362697601318
Norm after each mp layer: 3.136279821395874
Norm after each mp layer: 10.45701789855957
Norm before input: 0.2552422881126404
Norm after input: 0.4064374268054962
Norm after each mp layer: 1.3134362697601318
Norm after each mp layer: 3.136279821395874
Norm after each mp layer: 10.45701789855957
Norm before input: 0.2552422881126404
Norm after input: 0.40622925758361816
Norm after each mp layer: 1.3117976188659668
Norm after each mp layer: 3.1383752822875977
Norm after each mp layer: 10.484819412231445
Norm before input: 0.2552422881126404
Norm after input: 0.40622925758361816
Norm after each mp layer: 1.3117976188659668
Norm after each mp layer: 3.1383752822875977
Norm after each mp layer: 10.484819412231445
Norm before input: 0.2552422881126404
Norm after input: 0.4062516689300537
Norm after each mp layer: 1.3100836277008057
Norm after each mp layer: 3.1350810527801514
Norm after each mp layer: 10.474645614624023
Norm before input: 0.2552422881126404
Norm after input: 0.4062516689300537
Norm after each mp layer: 1.3100836277008057
Norm after each mp layer: 3.1350810527801514
Norm after each mp layer: 10.474645614624023
Norm before input: 0.2552422881126404
Norm after input: 0.40624359250068665
Norm after each mp layer: 1.3084673881530762
Norm after each mp layer: 3.1319711208343506
Norm after each mp layer: 10.467279434204102
Norm before input: 0.2552422881126404
Norm after input: 0.40624359250068665
Norm after each mp layer: 1.3084673881530762
Norm after each mp layer: 3.1319711208343506
Norm after each mp layer: 10.467279434204102
Norm before input: 0.2552422881126404
Norm after input: 0.4059894382953644
Norm after each mp layer: 1.3070251941680908
Norm after each mp layer: 3.1337125301361084
Norm after each mp layer: 10.496471405029297
Epoch: 200, Loss: 0.1260, Energy: 501.7112, Train: 98.26%, Valid: 76.60%, Test: 76.60%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4059894382953644
Norm after each mp layer: 1.3070251941680908
Norm after each mp layer: 3.1337125301361084
Norm after each mp layer: 10.496471405029297
Norm before input: 0.2552422881126404
Norm after input: 0.40585339069366455
Norm after each mp layer: 1.3056551218032837
Norm after each mp layer: 3.133080005645752
Norm after each mp layer: 10.507596015930176
Norm before input: 0.2552422881126404
Norm after input: 0.40585339069366455
Norm after each mp layer: 1.3056551218032837
Norm after each mp layer: 3.133080005645752
Norm after each mp layer: 10.507596015930176
Norm before input: 0.2552422881126404
Norm after input: 0.40587422251701355
Norm after each mp layer: 1.3042510747909546
Norm after each mp layer: 3.1291470527648926
Norm after each mp layer: 10.493863105773926
Norm before input: 0.2552422881126404
Norm after input: 0.40587422251701355
Norm after each mp layer: 1.3042510747909546
Norm after each mp layer: 3.1291470527648926
Norm after each mp layer: 10.493863105773926
Norm before input: 0.2552422881126404
Norm after input: 0.40576085448265076
Norm after each mp layer: 1.3027888536453247
Norm after each mp layer: 3.128021240234375
Norm after each mp layer: 10.500373840332031
Norm before input: 0.2552422881126404
Norm after input: 0.40576085448265076
Norm after each mp layer: 1.3027888536453247
Norm after each mp layer: 3.128021240234375
Norm after each mp layer: 10.500373840332031
Norm before input: 0.2552422881126404
Norm after input: 0.40555670857429504
Norm after each mp layer: 1.3012044429779053
Norm after each mp layer: 3.1287240982055664
Norm after each mp layer: 10.520249366760254
Norm before input: 0.2552422881126404
Norm after input: 0.40555670857429504
Norm after each mp layer: 1.3012044429779053
Norm after each mp layer: 3.1287240982055664
Norm after each mp layer: 10.520249366760254
Norm before input: 0.2552422881126404
Norm after input: 0.4055138826370239
Norm after each mp layer: 1.2994614839553833
Norm after each mp layer: 3.125753879547119
Norm after each mp layer: 10.513590812683105
Epoch: 205, Loss: 0.1207, Energy: 488.9010, Train: 98.34%, Valid: 76.60%, Test: 75.80%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4055138826370239
Norm after each mp layer: 1.2994614839553833
Norm after each mp layer: 3.125753879547119
Norm after each mp layer: 10.513590812683105
Norm before input: 0.2552422881126404
Norm after input: 0.40553686022758484
Norm after each mp layer: 1.2976865768432617
Norm after each mp layer: 3.121426820755005
Norm after each mp layer: 10.497026443481445
Norm before input: 0.2552422881126404
Norm after input: 0.40553686022758484
Norm after each mp layer: 1.2976865768432617
Norm after each mp layer: 3.121426820755005
Norm after each mp layer: 10.497026443481445
Norm before input: 0.2552422881126404
Norm after input: 0.4054117798805237
Norm after each mp layer: 1.2960301637649536
Norm after each mp layer: 3.120708465576172
Norm after each mp layer: 10.506160736083984
Norm before input: 0.2552422881126404
Norm after input: 0.4054117798805237
Norm after each mp layer: 1.2960301637649536
Norm after each mp layer: 3.120708465576172
Norm after each mp layer: 10.506160736083984
Norm before input: 0.2552422881126404
Norm after input: 0.40529102087020874
Norm after each mp layer: 1.2945280075073242
Norm after each mp layer: 3.120339870452881
Norm after each mp layer: 10.517111778259277
Norm before input: 0.2552422881126404
Norm after input: 0.40529102087020874
Norm after each mp layer: 1.2945280075073242
Norm after each mp layer: 3.120339870452881
Norm after each mp layer: 10.517111778259277
Norm before input: 0.2552422881126404
Norm after input: 0.4053077697753906
Norm after each mp layer: 1.293135404586792
Norm after each mp layer: 3.117124080657959
Norm after each mp layer: 10.507038116455078
Norm before input: 0.2552422881126404
Norm after input: 0.4053077697753906
Norm after each mp layer: 1.293135404586792
Norm after each mp layer: 3.117124080657959
Norm after each mp layer: 10.507038116455078
Norm before input: 0.2552422881126404
Norm after input: 0.40527117252349854
Norm after each mp layer: 1.2918156385421753
Norm after each mp layer: 3.115173101425171
Norm after each mp layer: 10.50584602355957
Epoch: 210, Loss: 0.1157, Energy: 475.8070, Train: 98.34%, Valid: 76.40%, Test: 75.90%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40527117252349854
Norm after each mp layer: 1.2918156385421753
Norm after each mp layer: 3.115173101425171
Norm after each mp layer: 10.50584602355957
Norm before input: 0.2552422881126404
Norm after input: 0.4051177203655243
Norm after each mp layer: 1.2904824018478394
Norm after each mp layer: 3.115790367126465
Norm after each mp layer: 10.523041725158691
Norm before input: 0.2552422881126404
Norm after input: 0.4051177203655243
Norm after each mp layer: 1.2904824018478394
Norm after each mp layer: 3.115790367126465
Norm after each mp layer: 10.523041725158691
Norm before input: 0.2552422881126404
Norm after input: 0.40506675839424133
Norm after each mp layer: 1.2890558242797852
Norm after each mp layer: 3.1139793395996094
Norm after each mp layer: 10.522635459899902
Norm before input: 0.2552422881126404
Norm after input: 0.40506675839424133
Norm after each mp layer: 1.2890558242797852
Norm after each mp layer: 3.1139793395996094
Norm after each mp layer: 10.522635459899902
Norm before input: 0.2552422881126404
Norm after input: 0.4050709903240204
Norm after each mp layer: 1.28754460811615
Norm after each mp layer: 3.1108481884002686
Norm after each mp layer: 10.512572288513184
Norm before input: 0.2552422881126404
Norm after input: 0.4050709903240204
Norm after each mp layer: 1.28754460811615
Norm after each mp layer: 3.1108481884002686
Norm after each mp layer: 10.512572288513184
Norm before input: 0.2552422881126404
Norm after input: 0.40495455265045166
Norm after each mp layer: 1.2860108613967896
Norm after each mp layer: 3.1105573177337646
Norm after each mp layer: 10.522529602050781
Norm before input: 0.2552422881126404
Norm after input: 0.40495455265045166
Norm after each mp layer: 1.2860108613967896
Norm after each mp layer: 3.1105573177337646
Norm after each mp layer: 10.522529602050781
Norm before input: 0.2552422881126404
Norm after input: 0.40485718846321106
Norm after each mp layer: 1.2845196723937988
Norm after each mp layer: 3.110013961791992
Norm after each mp layer: 10.530226707458496
Epoch: 215, Loss: 0.1110, Energy: 463.2317, Train: 98.68%, Valid: 75.80%, Test: 75.50%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40485718846321106
Norm after each mp layer: 1.2845196723937988
Norm after each mp layer: 3.110013961791992
Norm after each mp layer: 10.530226707458496
Norm before input: 0.2552422881126404
Norm after input: 0.40485692024230957
Norm after each mp layer: 1.2831199169158936
Norm after each mp layer: 3.107408046722412
Norm after each mp layer: 10.522854804992676
Norm before input: 0.2552422881126404
Norm after input: 0.40485692024230957
Norm after each mp layer: 1.2831199169158936
Norm after each mp layer: 3.107408046722412
Norm after each mp layer: 10.522854804992676
Norm before input: 0.2552422881126404
Norm after input: 0.4047988951206207
Norm after each mp layer: 1.2818390130996704
Norm after each mp layer: 3.106477737426758
Norm after each mp layer: 10.526965141296387
Norm before input: 0.2552422881126404
Norm after input: 0.4047988951206207
Norm after each mp layer: 1.2818390130996704
Norm after each mp layer: 3.106477737426758
Norm after each mp layer: 10.526965141296387
Norm before input: 0.2552422881126404
Norm after input: 0.40468746423721313
Norm after each mp layer: 1.2806382179260254
Norm after each mp layer: 3.1070971488952637
Norm after each mp layer: 10.541773796081543
Norm before input: 0.2552422881126404
Norm after input: 0.40468746423721313
Norm after each mp layer: 1.2806382179260254
Norm after each mp layer: 3.1070971488952637
Norm after each mp layer: 10.541773796081543
Norm before input: 0.2552422881126404
Norm after input: 0.40466344356536865
Norm after each mp layer: 1.279443621635437
Norm after each mp layer: 3.105684757232666
Norm after each mp layer: 10.542227745056152
Norm before input: 0.2552422881126404
Norm after input: 0.40466344356536865
Norm after each mp layer: 1.279443621635437
Norm after each mp layer: 3.105684757232666
Norm after each mp layer: 10.542227745056152
Norm before input: 0.2552422881126404
Norm after input: 0.4046556353569031
Norm after each mp layer: 1.2782024145126343
Norm after each mp layer: 3.1038331985473633
Norm after each mp layer: 10.53964614868164
Epoch: 220, Loss: 0.1064, Energy: 450.9595, Train: 98.59%, Valid: 75.80%, Test: 74.90%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4046556353569031
Norm after each mp layer: 1.2782024145126343
Norm after each mp layer: 3.1038331985473633
Norm after each mp layer: 10.53964614868164
Norm before input: 0.2552422881126404
Norm after input: 0.40457040071487427
Norm after each mp layer: 1.276897668838501
Norm after each mp layer: 3.103820562362671
Norm after each mp layer: 10.549897193908691
Norm before input: 0.2552422881126404
Norm after input: 0.40457040071487427
Norm after each mp layer: 1.276897668838501
Norm after each mp layer: 3.103820562362671
Norm after each mp layer: 10.549897193908691
Norm before input: 0.2552422881126404
Norm after input: 0.404526948928833
Norm after each mp layer: 1.2755627632141113
Norm after each mp layer: 3.1027650833129883
Norm after each mp layer: 10.55253791809082
Norm before input: 0.2552422881126404
Norm after input: 0.404526948928833
Norm after each mp layer: 1.2755627632141113
Norm after each mp layer: 3.1027650833129883
Norm after each mp layer: 10.55253791809082
Norm before input: 0.2552422881126404
Norm after input: 0.4045361876487732
Norm after each mp layer: 1.274249792098999
Norm after each mp layer: 3.1004390716552734
Norm after each mp layer: 10.545878410339355
Norm before input: 0.2552422881126404
Norm after input: 0.4045361876487732
Norm after each mp layer: 1.274249792098999
Norm after each mp layer: 3.1004390716552734
Norm after each mp layer: 10.545878410339355
Norm before input: 0.2552422881126404
Norm after input: 0.4044762849807739
Norm after each mp layer: 1.2729873657226562
Norm after each mp layer: 3.099931001663208
Norm after each mp layer: 10.551474571228027
Norm before input: 0.2552422881126404
Norm after input: 0.4044762849807739
Norm after each mp layer: 1.2729873657226562
Norm after each mp layer: 3.099931001663208
Norm after each mp layer: 10.551474571228027
Norm before input: 0.2552422881126404
Norm after input: 0.4044102728366852
Norm after each mp layer: 1.2717770338058472
Norm after each mp layer: 3.0995819568634033
Norm after each mp layer: 10.557928085327148
Epoch: 225, Loss: 0.1021, Energy: 440.0227, Train: 98.76%, Valid: 75.40%, Test: 74.40%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4044102728366852
Norm after each mp layer: 1.2717770338058472
Norm after each mp layer: 3.0995819568634033
Norm after each mp layer: 10.557928085327148
Norm before input: 0.2552422881126404
Norm after input: 0.4044046998023987
Norm after each mp layer: 1.2705925703048706
Norm after each mp layer: 3.0975754261016846
Norm after each mp layer: 10.552918434143066
Norm before input: 0.2552422881126404
Norm after input: 0.4044046998023987
Norm after each mp layer: 1.2705925703048706
Norm after each mp layer: 3.0975754261016846
Norm after each mp layer: 10.552918434143066
Norm before input: 0.2552422881126404
Norm after input: 0.4043581783771515
Norm after each mp layer: 1.269391655921936
Norm after each mp layer: 3.09651780128479
Norm after each mp layer: 10.554631233215332
Norm before input: 0.2552422881126404
Norm after input: 0.4043581783771515
Norm after each mp layer: 1.269391655921936
Norm after each mp layer: 3.09651780128479
Norm after each mp layer: 10.554631233215332
Norm before input: 0.2552422881126404
Norm after input: 0.40428775548934937
Norm after each mp layer: 1.268159031867981
Norm after each mp layer: 3.0960445404052734
Norm after each mp layer: 10.560474395751953
Norm before input: 0.2552422881126404
Norm after input: 0.40428775548934937
Norm after each mp layer: 1.268159031867981
Norm after each mp layer: 3.0960445404052734
Norm after each mp layer: 10.560474395751953
Norm before input: 0.2552422881126404
Norm after input: 0.40427806973457336
Norm after each mp layer: 1.2669119834899902
Norm after each mp layer: 3.093991994857788
Norm after each mp layer: 10.55539608001709
Norm before input: 0.2552422881126404
Norm after input: 0.40427806973457336
Norm after each mp layer: 1.2669119834899902
Norm after each mp layer: 3.093991994857788
Norm after each mp layer: 10.55539608001709
Norm before input: 0.2552422881126404
Norm after input: 0.4042547047138214
Norm after each mp layer: 1.2656664848327637
Norm after each mp layer: 3.092411994934082
Norm after each mp layer: 10.553256034851074
Epoch: 230, Loss: 0.0979, Energy: 428.5612, Train: 99.01%, Valid: 75.20%, Test: 74.00%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4042547047138214
Norm after each mp layer: 1.2656664848327637
Norm after each mp layer: 3.092411994934082
Norm after each mp layer: 10.553256034851074
Norm before input: 0.2552422881126404
Norm after input: 0.40419813990592957
Norm after each mp layer: 1.264451503753662
Norm after each mp layer: 3.0919108390808105
Norm after each mp layer: 10.557982444763184
Norm before input: 0.2552422881126404
Norm after input: 0.40419813990592957
Norm after each mp layer: 1.264451503753662
Norm after each mp layer: 3.0919108390808105
Norm after each mp layer: 10.557982444763184
Norm before input: 0.2552422881126404
Norm after input: 0.4041890501976013
Norm after each mp layer: 1.2633017301559448
Norm after each mp layer: 3.090303421020508
Norm after each mp layer: 10.554696083068848
Norm before input: 0.2552422881126404
Norm after input: 0.4041890501976013
Norm after each mp layer: 1.2633017301559448
Norm after each mp layer: 3.090303421020508
Norm after each mp layer: 10.554696083068848
Norm before input: 0.2552422881126404
Norm after input: 0.4041779935359955
Norm after each mp layer: 1.2622032165527344
Norm after each mp layer: 3.088890314102173
Norm after each mp layer: 10.552380561828613
Norm before input: 0.2552422881126404
Norm after input: 0.4041779935359955
Norm after each mp layer: 1.2622032165527344
Norm after each mp layer: 3.088890314102173
Norm after each mp layer: 10.552380561828613
Norm before input: 0.2552422881126404
Norm after input: 0.40412846207618713
Norm after each mp layer: 1.261118769645691
Norm after each mp layer: 3.088606119155884
Norm after each mp layer: 10.557493209838867
Norm before input: 0.2552422881126404
Norm after input: 0.40412846207618713
Norm after each mp layer: 1.261118769645691
Norm after each mp layer: 3.088606119155884
Norm after each mp layer: 10.557493209838867
Norm before input: 0.2552422881126404
Norm after input: 0.40411674976348877
Norm after each mp layer: 1.2600359916687012
Norm after each mp layer: 3.0872552394866943
Norm after each mp layer: 10.555292129516602
Epoch: 235, Loss: 0.0939, Energy: 417.1272, Train: 99.09%, Valid: 74.80%, Test: 74.00%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40411674976348877
Norm after each mp layer: 1.2600359916687012
Norm after each mp layer: 3.0872552394866943
Norm after each mp layer: 10.555292129516602
Norm before input: 0.2552422881126404
Norm after input: 0.40411368012428284
Norm after each mp layer: 1.2589411735534668
Norm after each mp layer: 3.085669755935669
Norm after each mp layer: 10.551366806030273
Norm before input: 0.2552422881126404
Norm after input: 0.40411368012428284
Norm after each mp layer: 1.2589411735534668
Norm after each mp layer: 3.085669755935669
Norm after each mp layer: 10.551366806030273
Norm before input: 0.2552422881126404
Norm after input: 0.40407660603523254
Norm after each mp layer: 1.2578431367874146
Norm after each mp layer: 3.08516788482666
Norm after each mp layer: 10.554373741149902
Norm before input: 0.2552422881126404
Norm after input: 0.40407660603523254
Norm after each mp layer: 1.2578431367874146
Norm after each mp layer: 3.08516788482666
Norm after each mp layer: 10.554373741149902
Norm before input: 0.2552422881126404
Norm after input: 0.40407052636146545
Norm after each mp layer: 1.2567927837371826
Norm after each mp layer: 3.083967685699463
Norm after each mp layer: 10.552241325378418
Norm before input: 0.2552422881126404
Norm after input: 0.40407052636146545
Norm after each mp layer: 1.2567927837371826
Norm after each mp layer: 3.083967447280884
Norm after each mp layer: 10.552241325378418
Norm before input: 0.2552422881126404
Norm after input: 0.4040706753730774
Norm after each mp layer: 1.2558043003082275
Norm after each mp layer: 3.0827982425689697
Norm after each mp layer: 10.549720764160156
Norm before input: 0.2552422881126404
Norm after input: 0.4040706753730774
Norm after each mp layer: 1.2558043003082275
Norm after each mp layer: 3.0827982425689697
Norm after each mp layer: 10.549720764160156
Norm before input: 0.2552422881126404
Norm after input: 0.4040377736091614
Norm after each mp layer: 1.254867434501648
Norm after each mp layer: 3.082792043685913
Norm after each mp layer: 10.554303169250488
Epoch: 240, Loss: 0.0901, Energy: 405.7768, Train: 99.25%, Valid: 74.80%, Test: 73.10%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4040377736091614
Norm after each mp layer: 1.254867434501648
Norm after each mp layer: 3.082792043685913
Norm after each mp layer: 10.554302215576172
Norm before input: 0.2552422881126404
Norm after input: 0.4040319323539734
Norm after each mp layer: 1.253989577293396
Norm after each mp layer: 3.0821170806884766
Norm after each mp layer: 10.553996086120605
Norm before input: 0.2552422881126404
Norm after input: 0.4040319323539734
Norm after each mp layer: 1.253989577293396
Norm after each mp layer: 3.0821170806884766
Norm after each mp layer: 10.553996086120605
Norm before input: 0.2552422881126404
Norm after input: 0.4040270745754242
Norm after each mp layer: 1.2531358003616333
Norm after each mp layer: 3.0814859867095947
Norm after each mp layer: 10.553691864013672
Norm before input: 0.2552422881126404
Norm after input: 0.4040270745754242
Norm after each mp layer: 1.2531358003616333
Norm after each mp layer: 3.0814859867095947
Norm after each mp layer: 10.553691864013672
Norm before input: 0.2552422881126404
Norm after input: 0.4039989411830902
Norm after each mp layer: 1.2522709369659424
Norm after each mp layer: 3.08160662651062
Norm after each mp layer: 10.558074951171875
Norm before input: 0.2552422881126404
Norm after input: 0.4039989411830902
Norm after each mp layer: 1.2522709369659424
Norm after each mp layer: 3.08160662651062
Norm after each mp layer: 10.558074951171875
Norm before input: 0.2552422881126404
Norm after input: 0.404003769159317
Norm after each mp layer: 1.2514101266860962
Norm after each mp layer: 3.080766201019287
Norm after each mp layer: 10.555922508239746
Norm before input: 0.2552422881126404
Norm after input: 0.404003769159317
Norm after each mp layer: 1.2514101266860962
Norm after each mp layer: 3.080766201019287
Norm after each mp layer: 10.555922508239746
Norm before input: 0.2552422881126404
Norm after input: 0.40400683879852295
Norm after each mp layer: 1.2505475282669067
Norm after each mp layer: 3.0800704956054688
Norm after each mp layer: 10.554407119750977
Epoch: 245, Loss: 0.0864, Energy: 395.4692, Train: 99.25%, Valid: 74.80%, Test: 72.60%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40400683879852295
Norm after each mp layer: 1.2505475282669067
Norm after each mp layer: 3.0800704956054688
Norm after each mp layer: 10.554407119750977
Norm before input: 0.2552422881126404
Norm after input: 0.4039977788925171
Norm after each mp layer: 1.2496967315673828
Norm after each mp layer: 3.0798983573913574
Norm after each mp layer: 10.55593490600586
Norm before input: 0.2552422881126404
Norm after input: 0.4039977788925171
Norm after each mp layer: 1.2496967315673828
Norm after each mp layer: 3.0798983573913574
Norm after each mp layer: 10.55593490600586
Norm before input: 0.2552422881126404
Norm after input: 0.4040178954601288
Norm after each mp layer: 1.2488890886306763
Norm after each mp layer: 3.0789413452148438
Norm after each mp layer: 10.552019119262695
Norm before input: 0.2552422881126404
Norm after input: 0.4040178954601288
Norm after each mp layer: 1.2488890886306763
Norm after each mp layer: 3.0789413452148438
Norm after each mp layer: 10.552020072937012
Norm before input: 0.2552422881126404
Norm after input: 0.40402427315711975
Norm after each mp layer: 1.248098373413086
Norm after each mp layer: 3.0785248279571533
Norm after each mp layer: 10.551201820373535
Norm before input: 0.2552422881126404
Norm after input: 0.40402427315711975
Norm after each mp layer: 1.248098373413086
Norm after each mp layer: 3.0785248279571533
Norm after each mp layer: 10.551201820373535
Norm before input: 0.2552422881126404
Norm after input: 0.4040285050868988
Norm after each mp layer: 1.2473242282867432
Norm after each mp layer: 3.078242778778076
Norm after each mp layer: 10.550920486450195
Norm before input: 0.2552422881126404
Norm after input: 0.4040285050868988
Norm after each mp layer: 1.2473242282867432
Norm after each mp layer: 3.078242778778076
Norm after each mp layer: 10.550920486450195
Norm before input: 0.2552422881126404
Norm after input: 0.4040529429912567
Norm after each mp layer: 1.2465771436691284
Norm after each mp layer: 3.077346086502075
Norm after each mp layer: 10.546475410461426
Epoch: 250, Loss: 0.0829, Energy: 384.7837, Train: 99.34%, Valid: 74.80%, Test: 71.90%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4040529429912567
Norm after each mp layer: 1.2465771436691284
Norm after each mp layer: 3.077346086502075
Norm after each mp layer: 10.546475410461426
Norm before input: 0.2552422881126404
Norm after input: 0.40405669808387756
Norm after each mp layer: 1.2458302974700928
Norm after each mp layer: 3.0771870613098145
Norm after each mp layer: 10.546473503112793
Norm before input: 0.2552422881126404
Norm after input: 0.40405669808387756
Norm after each mp layer: 1.2458302974700928
Norm after each mp layer: 3.0771870613098145
Norm after each mp layer: 10.546473503112793
Norm before input: 0.2552422881126404
Norm after input: 0.40407422184944153
Norm after each mp layer: 1.2451095581054688
Norm after each mp layer: 3.0766448974609375
Norm after each mp layer: 10.54373550415039
Norm before input: 0.2552422881126404
Norm after input: 0.40407422184944153
Norm after each mp layer: 1.2451095581054688
Norm after each mp layer: 3.0766448974609375
Norm after each mp layer: 10.54373550415039
Norm before input: 0.2552422881126404
Norm after input: 0.4040974974632263
Norm after each mp layer: 1.24441659450531
Norm after each mp layer: 3.0760138034820557
Norm after each mp layer: 10.540061950683594
Norm before input: 0.2552422881126404
Norm after input: 0.4040974974632263
Norm after each mp layer: 1.24441659450531
Norm after each mp layer: 3.0760138034820557
Norm after each mp layer: 10.540061950683594
Norm before input: 0.2552422881126404
Norm after input: 0.4041067659854889
Norm after each mp layer: 1.2437429428100586
Norm after each mp layer: 3.075998067855835
Norm after each mp layer: 10.539854049682617
Norm before input: 0.2552422881126404
Norm after input: 0.4041067659854889
Norm after each mp layer: 1.2437429428100586
Norm after each mp layer: 3.075998067855835
Norm after each mp layer: 10.539854049682617
Norm before input: 0.2552422881126404
Norm after input: 0.4041364789009094
Norm after each mp layer: 1.2431156635284424
Norm after each mp layer: 3.0754196643829346
Norm after each mp layer: 10.535713195800781
Epoch: 255, Loss: 0.0794, Energy: 374.0332, Train: 99.34%, Valid: 74.40%, Test: 71.70%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4041364789009094
Norm after each mp layer: 1.2431156635284424
Norm after each mp layer: 3.0754196643829346
Norm after each mp layer: 10.535713195800781
Norm before input: 0.2552422881126404
Norm after input: 0.4041541516780853
Norm after each mp layer: 1.2425023317337036
Norm after each mp layer: 3.0754008293151855
Norm after each mp layer: 10.534590721130371
Norm before input: 0.2552422881126404
Norm after input: 0.4041541516780853
Norm after each mp layer: 1.2425023317337036
Norm after each mp layer: 3.0754008293151855
Norm after each mp layer: 10.534590721130371
Norm before input: 0.2552422881126404
Norm after input: 0.40417835116386414
Norm after each mp layer: 1.2419168949127197
Norm after each mp layer: 3.0752973556518555
Norm after each mp layer: 10.532490730285645
Norm before input: 0.2552422881126404
Norm after input: 0.40417835116386414
Norm after each mp layer: 1.2419167757034302
Norm after each mp layer: 3.0752973556518555
Norm after each mp layer: 10.532490730285645
Norm before input: 0.2552422881126404
Norm after input: 0.40421098470687866
Norm after each mp layer: 1.241364598274231
Norm after each mp layer: 3.0750362873077393
Norm after each mp layer: 10.529021263122559
Norm before input: 0.2552422881126404
Norm after input: 0.40421098470687866
Norm after each mp layer: 1.241364598274231
Norm after each mp layer: 3.0750362873077393
Norm after each mp layer: 10.529021263122559
Norm before input: 0.2552422881126404
Norm after input: 0.40423187613487244
Norm after each mp layer: 1.2408275604248047
Norm after each mp layer: 3.075343370437622
Norm after each mp layer: 10.528660774230957
Norm before input: 0.2552422881126404
Norm after input: 0.40423187613487244
Norm after each mp layer: 1.2408275604248047
Norm after each mp layer: 3.075343370437622
Norm after each mp layer: 10.528660774230957
Norm before input: 0.2552422881126404
Norm after input: 0.4042721092700958
Norm after each mp layer: 1.240333080291748
Norm after each mp layer: 3.0750653743743896
Norm after each mp layer: 10.524333000183105
Epoch: 260, Loss: 0.0762, Energy: 363.6006, Train: 99.34%, Valid: 74.20%, Test: 71.60%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4042721092700958
Norm after each mp layer: 1.240333080291748
Norm after each mp layer: 3.0750653743743896
Norm after each mp layer: 10.524333000183105
Norm before input: 0.2552422881126404
Norm after input: 0.40429985523223877
Norm after each mp layer: 1.239846110343933
Norm after each mp layer: 3.0753684043884277
Norm after each mp layer: 10.523153305053711
Norm before input: 0.2552422881126404
Norm after input: 0.40429985523223877
Norm after each mp layer: 1.239846110343933
Norm after each mp layer: 3.0753684043884277
Norm after each mp layer: 10.523153305053711
Norm before input: 0.2552422881126404
Norm after input: 0.40434014797210693
Norm after each mp layer: 1.2393933534622192
Norm after each mp layer: 3.075312852859497
Norm after each mp layer: 10.519427299499512
Norm before input: 0.2552422881126404
Norm after input: 0.40434014797210693
Norm after each mp layer: 1.2393933534622192
Norm after each mp layer: 3.075312852859497
Norm after each mp layer: 10.519427299499512
Norm before input: 0.2552422881126404
Norm after input: 0.4043775498867035
Norm after each mp layer: 1.238956093788147
Norm after each mp layer: 3.0754945278167725
Norm after each mp layer: 10.516709327697754
Norm before input: 0.2552422881126404
Norm after input: 0.4043775498867035
Norm after each mp layer: 1.238956093788147
Norm after each mp layer: 3.0754945278167725
Norm after each mp layer: 10.516709327697754
Norm before input: 0.2552422881126404
Norm after input: 0.40441712737083435
Norm after each mp layer: 1.2385398149490356
Norm after each mp layer: 3.0757408142089844
Norm after each mp layer: 10.513885498046875
Norm before input: 0.2552422881126404
Norm after input: 0.40441712737083435
Norm after each mp layer: 1.2385398149490356
Norm after each mp layer: 3.0757408142089844
Norm after each mp layer: 10.513885498046875
Norm before input: 0.2552422881126404
Norm after input: 0.4044649302959442
Norm after each mp layer: 1.2381553649902344
Norm after each mp layer: 3.075827121734619
Norm after each mp layer: 10.50961971282959
Epoch: 265, Loss: 0.0731, Energy: 353.5582, Train: 99.34%, Valid: 74.20%, Test: 71.40%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4044649302959442
Norm after each mp layer: 1.2381553649902344
Norm after each mp layer: 3.075827121734619
Norm after each mp layer: 10.50961971282959
Norm before input: 0.2552422881126404
Norm after input: 0.40450435876846313
Norm after each mp layer: 1.2377837896347046
Norm after each mp layer: 3.0763916969299316
Norm after each mp layer: 10.507741928100586
Norm before input: 0.2552422881126404
Norm after input: 0.40450435876846313
Norm after each mp layer: 1.2377837896347046
Norm after each mp layer: 3.0763916969299316
Norm after each mp layer: 10.507741928100586
Norm before input: 0.2552422881126404
Norm after input: 0.40456002950668335
Norm after each mp layer: 1.2374521493911743
Norm after each mp layer: 3.0764365196228027
Norm after each mp layer: 10.502340316772461
Norm before input: 0.2552422881126404
Norm after input: 0.40456002950668335
Norm after each mp layer: 1.2374521493911743
Norm after each mp layer: 3.0764365196228027
Norm after each mp layer: 10.502340316772461
Norm before input: 0.2552422881126404
Norm after input: 0.40459907054901123
Norm after each mp layer: 1.2371137142181396
Norm after each mp layer: 3.0772528648376465
Norm after each mp layer: 10.50105094909668
Norm before input: 0.2552422881126404
Norm after input: 0.40459907054901123
Norm after each mp layer: 1.2371137142181396
Norm after each mp layer: 3.0772528648376465
Norm after each mp layer: 10.50105094909668
Norm before input: 0.2552422881126404
Norm after input: 0.4046638011932373
Norm after each mp layer: 1.2368298768997192
Norm after each mp layer: 3.0771050453186035
Norm after each mp layer: 10.493736267089844
Norm before input: 0.2552422881126404
Norm after input: 0.4046638011932373
Norm after each mp layer: 1.2368298768997192
Norm after each mp layer: 3.0771050453186035
Norm after each mp layer: 10.493736267089844
Norm before input: 0.2552422881126404
Norm after input: 0.40469643473625183
Norm after each mp layer: 1.2365208864212036
Norm after each mp layer: 3.0783936977386475
Norm after each mp layer: 10.494401931762695
Epoch: 270, Loss: 0.0701, Energy: 343.6212, Train: 99.42%, Valid: 74.20%, Test: 71.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40469643473625183
Norm after each mp layer: 1.2365208864212036
Norm after each mp layer: 3.0783936977386475
Norm after each mp layer: 10.494401931762695
Norm before input: 0.2552422881126404
Norm after input: 0.4047788381576538
Norm after each mp layer: 1.2363089323043823
Norm after each mp layer: 3.0777249336242676
Norm after each mp layer: 10.483292579650879
Norm before input: 0.2552422881126404
Norm after input: 0.4047788381576538
Norm after each mp layer: 1.2363089323043823
Norm after each mp layer: 3.0777249336242676
Norm after each mp layer: 10.483292579650879
Norm before input: 0.2552422881126404
Norm after input: 0.4047917425632477
Norm after each mp layer: 1.2360106706619263
Norm after each mp layer: 3.0801703929901123
Norm after each mp layer: 10.489696502685547
Norm before input: 0.2552422881126404
Norm after input: 0.4047917425632477
Norm after each mp layer: 1.2360106706619263
Norm after each mp layer: 3.0801703929901123
Norm after each mp layer: 10.489696502685547
Norm before input: 0.2552422881126404
Norm after input: 0.4049201011657715
Norm after each mp layer: 1.2359225749969482
Norm after each mp layer: 3.0777945518493652
Norm after each mp layer: 10.468027114868164
Norm before input: 0.2552422881126404
Norm after input: 0.4049201011657715
Norm after each mp layer: 1.2359225749969482
Norm after each mp layer: 3.0777945518493652
Norm after each mp layer: 10.468027114868164
Norm before input: 0.2552422881126404
Norm after input: 0.40485960245132446
Norm after each mp layer: 1.2355425357818604
Norm after each mp layer: 3.083848714828491
Norm after each mp layer: 10.49405288696289
Norm before input: 0.2552422881126404
Norm after input: 0.40485960245132446
Norm after each mp layer: 1.2355425357818604
Norm after each mp layer: 3.083848714828491
Norm after each mp layer: 10.49405288696289
Norm before input: 0.2552422881126404
Norm after input: 0.4051320552825928
Norm after each mp layer: 1.2357733249664307
Norm after each mp layer: 3.075383186340332
Norm after each mp layer: 10.437446594238281
Epoch: 275, Loss: 0.0674, Energy: 335.7915, Train: 99.50%, Valid: 74.40%, Test: 70.60%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4051320552825928
Norm after each mp layer: 1.2357733249664307
Norm after each mp layer: 3.075383186340332
Norm after each mp layer: 10.437446594238281
Norm before input: 0.2552422881126404
Norm after input: 0.4047323167324066
Norm after each mp layer: 1.234886646270752
Norm after each mp layer: 3.0987541675567627
Norm after each mp layer: 10.561077117919922
Norm before input: 0.2552422881126404
Norm after input: 0.4047323167324066
Norm after each mp layer: 1.234886646270752
Norm after each mp layer: 3.0987541675567627
Norm after each mp layer: 10.561077117919922
Norm before input: 0.2552422881126404
Norm after input: 0.4058191776275635
Norm after each mp layer: 1.2372972965240479
Norm after each mp layer: 3.0577852725982666
Norm after each mp layer: 10.333314895629883
Norm before input: 0.2552422881126404
Norm after input: 0.4058191776275635
Norm after each mp layer: 1.2372972965240479
Norm after each mp layer: 3.0577852725982666
Norm after each mp layer: 10.333314895629883
Norm before input: 0.2552422881126404
Norm after input: 0.4014981687068939
Norm after each mp layer: 1.290738821029663
Norm after each mp layer: 3.8065598011016846
Norm after each mp layer: 14.471244812011719
Norm before input: 0.2552422881126404
Norm after input: 0.4014981687068939
Norm after each mp layer: 1.290738821029663
Norm after each mp layer: 3.8065598011016846
Norm after each mp layer: 14.471244812011719
Norm before input: 0.2552422881126404
Norm after input: 0.4253541827201843
Norm after each mp layer: 1.3886350393295288
Norm after each mp layer: 3.3341336250305176
Norm after each mp layer: 10.946810722351074
Norm before input: 0.2552422881126404
Norm after input: 0.4253541827201843
Norm after each mp layer: 1.3886350393295288
Norm after each mp layer: 3.3341336250305176
Norm after each mp layer: 10.946810722351074
Norm before input: 0.2552422881126404
Norm after input: 0.4216051697731018
Norm after each mp layer: 1.367444396018982
Norm after each mp layer: 3.2668845653533936
Norm after each mp layer: 10.584436416625977
Epoch: 280, Loss: 4.0449, Energy: 4246.2026, Train: 51.66%, Valid: 41.80%, Test: 42.40%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4216051697731018
Norm after each mp layer: 1.367444396018982
Norm after each mp layer: 3.2668845653533936
Norm after each mp layer: 10.584436416625977
Norm before input: 0.2552422881126404
Norm after input: 0.41357728838920593
Norm after each mp layer: 1.28420090675354
Norm after each mp layer: 3.0411951541900635
Norm after each mp layer: 10.127777099609375
Norm before input: 0.2552422881126404
Norm after input: 0.41357728838920593
Norm after each mp layer: 1.28420090675354
Norm after each mp layer: 3.0411951541900635
Norm after each mp layer: 10.127777099609375
Norm before input: 0.2552422881126404
Norm after input: 0.40529561042785645
Norm after each mp layer: 1.2788991928100586
Norm after each mp layer: 3.1409502029418945
Norm after each mp layer: 11.398669242858887
Norm before input: 0.2552422881126404
Norm after input: 0.40529561042785645
Norm after each mp layer: 1.2788991928100586
Norm after each mp layer: 3.1409502029418945
Norm after each mp layer: 11.398669242858887
Norm before input: 0.2552422881126404
Norm after input: 0.4002889096736908
Norm after each mp layer: 1.297686219215393
Norm after each mp layer: 3.1750636100769043
Norm after each mp layer: 11.467611312866211
Norm before input: 0.2552422881126404
Norm after input: 0.4002889096736908
Norm after each mp layer: 1.297686219215393
Norm after each mp layer: 3.1750636100769043
Norm after each mp layer: 11.467611312866211
Norm before input: 0.2552422881126404
Norm after input: 0.39872410893440247
Norm after each mp layer: 1.3085427284240723
Norm after each mp layer: 3.1605217456817627
Norm after each mp layer: 11.012862205505371
Norm before input: 0.2552422881126404
Norm after input: 0.39872410893440247
Norm after each mp layer: 1.3085427284240723
Norm after each mp layer: 3.1605217456817627
Norm after each mp layer: 11.012862205505371
Norm before input: 0.2552422881126404
Norm after input: 0.3991733491420746
Norm after each mp layer: 1.2871311902999878
Norm after each mp layer: 3.0449960231781006
Norm after each mp layer: 10.095385551452637
Epoch: 285, Loss: 0.6374, Energy: 2665.9131, Train: 90.89%, Valid: 75.60%, Test: 73.50%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3991733491420746
Norm after each mp layer: 1.2871311902999878
Norm after each mp layer: 3.0449960231781006
Norm after each mp layer: 10.095385551452637
Norm before input: 0.2552422881126404
Norm after input: 0.400861531496048
Norm after each mp layer: 1.2526344060897827
Norm after each mp layer: 2.9434926509857178
Norm after each mp layer: 9.424323081970215
Norm before input: 0.2552422881126404
Norm after input: 0.400861531496048
Norm after each mp layer: 1.2526344060897827
Norm after each mp layer: 2.9434926509857178
Norm after each mp layer: 9.424323081970215
Norm before input: 0.2552422881126404
Norm after input: 0.40374091267585754
Norm after each mp layer: 1.2263522148132324
Norm after each mp layer: 2.930403709411621
Norm after each mp layer: 9.319445610046387
Norm before input: 0.2552422881126404
Norm after input: 0.40374091267585754
Norm after each mp layer: 1.2263522148132324
Norm after each mp layer: 2.930403709411621
Norm after each mp layer: 9.319445610046387
Norm before input: 0.2552422881126404
Norm after input: 0.4074007272720337
Norm after each mp layer: 1.222895622253418
Norm after each mp layer: 3.039440393447876
Norm after each mp layer: 9.943242073059082
Norm before input: 0.2552422881126404
Norm after input: 0.4074007272720337
Norm after each mp layer: 1.222895622253418
Norm after each mp layer: 3.039440393447876
Norm after each mp layer: 9.943242073059082
Norm before input: 0.2552422881126404
Norm after input: 0.4096567928791046
Norm after each mp layer: 1.2651304006576538
Norm after each mp layer: 3.4246973991394043
Norm after each mp layer: 12.234167098999023
Norm before input: 0.2552422881126404
Norm after input: 0.4096567928791046
Norm after each mp layer: 1.2651304006576538
Norm after each mp layer: 3.4246973991394043
Norm after each mp layer: 12.234167098999023
Norm before input: 0.2552422881126404
Norm after input: 0.41260209679603577
Norm after each mp layer: 1.2717267274856567
Norm after each mp layer: 3.3937835693359375
Norm after each mp layer: 11.568374633789062
Epoch: 290, Loss: 0.5923, Energy: 2323.9517, Train: 95.12%, Valid: 78.00%, Test: 75.20%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.41260209679603577
Norm after each mp layer: 1.2717267274856567
Norm after each mp layer: 3.3937835693359375
Norm after each mp layer: 11.568374633789062
Norm before input: 0.2552422881126404
Norm after input: 0.412969708442688
Norm after each mp layer: 1.2622641324996948
Norm after each mp layer: 3.2731308937072754
Norm after each mp layer: 11.071797370910645
Norm before input: 0.2552422881126404
Norm after input: 0.412969708442688
Norm after each mp layer: 1.2622641324996948
Norm after each mp layer: 3.2731308937072754
Norm after each mp layer: 11.071797370910645
Norm before input: 0.2552422881126404
Norm after input: 0.410935640335083
Norm after each mp layer: 1.2609312534332275
Norm after each mp layer: 3.2711877822875977
Norm after each mp layer: 11.701591491699219
Norm before input: 0.2552422881126404
Norm after input: 0.410935640335083
Norm after each mp layer: 1.2609312534332275
Norm after each mp layer: 3.2711877822875977
Norm after each mp layer: 11.701591491699219
Norm before input: 0.2552422881126404
Norm after input: 0.4113263189792633
Norm after each mp layer: 1.246351718902588
Norm after each mp layer: 3.1213717460632324
Norm after each mp layer: 10.521867752075195
Norm before input: 0.2552422881126404
Norm after input: 0.4113263189792633
Norm after each mp layer: 1.246351718902588
Norm after each mp layer: 3.1213717460632324
Norm after each mp layer: 10.521867752075195
Norm before input: 0.2552422881126404
Norm after input: 0.4098776578903198
Norm after each mp layer: 1.2608258724212646
Norm after each mp layer: 3.257997751235962
Norm after each mp layer: 11.498050689697266
Norm before input: 0.2552422881126404
Norm after input: 0.4098776578903198
Norm after each mp layer: 1.2608258724212646
Norm after each mp layer: 3.257997751235962
Norm after each mp layer: 11.498050689697266
Norm before input: 0.2552422881126404
Norm after input: 0.41043734550476074
Norm after each mp layer: 1.2604124546051025
Norm after each mp layer: 3.270400285720825
Norm after each mp layer: 11.042322158813477
Epoch: 295, Loss: 0.3171, Energy: 1462.9224, Train: 94.95%, Valid: 76.80%, Test: 76.10%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.41043734550476074
Norm after each mp layer: 1.260412335395813
Norm after each mp layer: 3.270400285720825
Norm after each mp layer: 11.042322158813477
Norm before input: 0.2552422881126404
Norm after input: 0.41122233867645264
Norm after each mp layer: 1.258334755897522
Norm after each mp layer: 3.280007839202881
Norm after each mp layer: 10.924649238586426
Norm before input: 0.2552422881126404
Norm after input: 0.41122233867645264
Norm after each mp layer: 1.258334755897522
Norm after each mp layer: 3.280007839202881
Norm after each mp layer: 10.92464828491211
Norm before input: 0.2552422881126404
Norm after input: 0.41028380393981934
Norm after each mp layer: 1.2635869979858398
Norm after each mp layer: 3.356043815612793
Norm after each mp layer: 11.954015731811523
Norm before input: 0.2552422881126404
Norm after input: 0.41028380393981934
Norm after each mp layer: 1.2635869979858398
Norm after each mp layer: 3.356043815612793
Norm after each mp layer: 11.954015731811523
Norm before input: 0.2552422881126404
Norm after input: 0.41112297773361206
Norm after each mp layer: 1.2472020387649536
Norm after each mp layer: 3.2347538471221924
Norm after each mp layer: 11.143411636352539
Norm before input: 0.2552422881126404
Norm after input: 0.41112297773361206
Norm after each mp layer: 1.2472020387649536
Norm after each mp layer: 3.2347538471221924
Norm after each mp layer: 11.143411636352539
Norm before input: 0.2552422881126404
Norm after input: 0.4116177558898926
Norm after each mp layer: 1.237101435661316
Norm after each mp layer: 3.17136287689209
Norm after each mp layer: 10.286481857299805
Norm before input: 0.2552422881126404
Norm after input: 0.4116177558898926
Norm after each mp layer: 1.237101435661316
Norm after each mp layer: 3.17136287689209
Norm after each mp layer: 10.286481857299805
Norm before input: 0.2552422881126404
Norm after input: 0.41015273332595825
Norm after each mp layer: 1.2428945302963257
Norm after each mp layer: 3.2324721813201904
Norm after each mp layer: 10.497218132019043
Epoch: 300, Loss: 0.2951, Energy: 1290.5192, Train: 95.12%, Valid: 76.80%, Test: 76.70%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.41015273332595825
Norm after each mp layer: 1.2428945302963257
Norm after each mp layer: 3.2324721813201904
Norm after each mp layer: 10.497218132019043
Norm before input: 0.2552422881126404
Norm after input: 0.40945178270339966
Norm after each mp layer: 1.2421503067016602
Norm after each mp layer: 3.2372887134552
Norm after each mp layer: 10.52389907836914
Norm before input: 0.2552422881126404
Norm after input: 0.40945178270339966
Norm after each mp layer: 1.2421503067016602
Norm after each mp layer: 3.2372887134552
Norm after each mp layer: 10.52389907836914
Norm before input: 0.2552422881126404
Norm after input: 0.409457802772522
Norm after each mp layer: 1.2356302738189697
Norm after each mp layer: 3.1838648319244385
Norm after each mp layer: 10.085732460021973
Norm before input: 0.2552422881126404
Norm after input: 0.409457802772522
Norm after each mp layer: 1.2356302738189697
Norm after each mp layer: 3.1838648319244385
Norm after each mp layer: 10.085732460021973
Norm before input: 0.2552422881126404
Norm after input: 0.4093915522098541
Norm after each mp layer: 1.2341082096099854
Norm after each mp layer: 3.1546409130096436
Norm after each mp layer: 10.073139190673828
Norm before input: 0.2552422881126404
Norm after input: 0.4093915522098541
Norm after each mp layer: 1.2341082096099854
Norm after each mp layer: 3.1546409130096436
Norm after each mp layer: 10.073139190673828
Norm before input: 0.2552422881126404
Norm after input: 0.40877050161361694
Norm after each mp layer: 1.2400928735733032
Norm after each mp layer: 3.1622495651245117
Norm after each mp layer: 10.449357032775879
Norm before input: 0.2552422881126404
Norm after input: 0.40877050161361694
Norm after each mp layer: 1.2400928735733032
Norm after each mp layer: 3.1622495651245117
Norm after each mp layer: 10.449357032775879
Norm before input: 0.2552422881126404
Norm after input: 0.4083348214626312
Norm after each mp layer: 1.245080590248108
Norm after each mp layer: 3.167351245880127
Norm after each mp layer: 10.683523178100586
Epoch: 305, Loss: 0.1992, Energy: 954.8709, Train: 97.19%, Valid: 78.00%, Test: 77.50%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4083348214626312
Norm after each mp layer: 1.245080590248108
Norm after each mp layer: 3.167351245880127
Norm after each mp layer: 10.683523178100586
Norm before input: 0.2552422881126404
Norm after input: 0.40822580456733704
Norm after each mp layer: 1.2409512996673584
Norm after each mp layer: 3.1310577392578125
Norm after each mp layer: 10.335699081420898
Norm before input: 0.2552422881126404
Norm after input: 0.40822580456733704
Norm after each mp layer: 1.2409512996673584
Norm after each mp layer: 3.1310577392578125
Norm after each mp layer: 10.335699081420898
Norm before input: 0.2552422881126404
Norm after input: 0.40815967321395874
Norm after each mp layer: 1.2284984588623047
Norm after each mp layer: 3.0679543018341064
Norm after each mp layer: 9.744986534118652
Norm before input: 0.2552422881126404
Norm after input: 0.40815967321395874
Norm after each mp layer: 1.2284984588623047
Norm after each mp layer: 3.0679543018341064
Norm after each mp layer: 9.744986534118652
Norm before input: 0.2552422881126404
Norm after input: 0.40779605507850647
Norm after each mp layer: 1.2163262367248535
Norm after each mp layer: 3.0220184326171875
Norm after each mp layer: 9.507613182067871
Norm before input: 0.2552422881126404
Norm after input: 0.40779605507850647
Norm after each mp layer: 1.2163262367248535
Norm after each mp layer: 3.0220184326171875
Norm after each mp layer: 9.507613182067871
Norm before input: 0.2552422881126404
Norm after input: 0.4069553315639496
Norm after each mp layer: 1.2139830589294434
Norm after each mp layer: 3.0275421142578125
Norm after each mp layer: 10.03054141998291
Norm before input: 0.2552422881126404
Norm after input: 0.4069553315639496
Norm after each mp layer: 1.2139830589294434
Norm after each mp layer: 3.0275421142578125
Norm after each mp layer: 10.03054141998291
Norm before input: 0.2552422881126404
Norm after input: 0.4066069722175598
Norm after each mp layer: 1.2049368619918823
Norm after each mp layer: 2.989482879638672
Norm after each mp layer: 10.020220756530762
Epoch: 310, Loss: 0.1881, Energy: 806.5026, Train: 97.35%, Valid: 76.40%, Test: 76.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4066069722175598
Norm after each mp layer: 1.2049368619918823
Norm after each mp layer: 2.989482879638672
Norm after each mp layer: 10.020220756530762
Norm before input: 0.2552422881126404
Norm after input: 0.40675458312034607
Norm after each mp layer: 1.1898647546768188
Norm after each mp layer: 2.903433084487915
Norm after each mp layer: 9.349485397338867
Norm before input: 0.2552422881126404
Norm after input: 0.40675458312034607
Norm after each mp layer: 1.1898647546768188
Norm after each mp layer: 2.903433084487915
Norm after each mp layer: 9.349485397338867
Norm before input: 0.2552422881126404
Norm after input: 0.4070632755756378
Norm after each mp layer: 1.1787205934524536
Norm after each mp layer: 2.8399577140808105
Norm after each mp layer: 8.782905578613281
Norm before input: 0.2552422881126404
Norm after input: 0.4070632755756378
Norm after each mp layer: 1.1787205934524536
Norm after each mp layer: 2.8399577140808105
Norm after each mp layer: 8.782905578613281
Norm before input: 0.2552422881126404
Norm after input: 0.40675467252731323
Norm after each mp layer: 1.178536295890808
Norm after each mp layer: 2.8370590209960938
Norm after each mp layer: 8.684008598327637
Norm before input: 0.2552422881126404
Norm after input: 0.40675467252731323
Norm after each mp layer: 1.178536295890808
Norm after each mp layer: 2.8370590209960938
Norm after each mp layer: 8.684008598327637
Norm before input: 0.2552422881126404
Norm after input: 0.4059833884239197
Norm after each mp layer: 1.187644124031067
Norm after each mp layer: 2.8869223594665527
Norm after each mp layer: 9.044530868530273
Norm before input: 0.2552422881126404
Norm after input: 0.4059833884239197
Norm after each mp layer: 1.187644124031067
Norm after each mp layer: 2.8869223594665527
Norm after each mp layer: 9.044530868530273
Norm before input: 0.2552422881126404
Norm after input: 0.4054831266403198
Norm after each mp layer: 1.1965597867965698
Norm after each mp layer: 2.9390552043914795
Norm after each mp layer: 9.393110275268555
Epoch: 315, Loss: 0.1460, Energy: 542.1881, Train: 97.43%, Valid: 78.20%, Test: 76.20%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4054831266403198
Norm after each mp layer: 1.1965597867965698
Norm after each mp layer: 2.9390552043914795
Norm after each mp layer: 9.393110275268555
Norm before input: 0.2552422881126404
Norm after input: 0.4053437411785126
Norm after each mp layer: 1.1985952854156494
Norm after each mp layer: 2.9490175247192383
Norm after each mp layer: 9.322683334350586
Norm before input: 0.2552422881126404
Norm after input: 0.4053437411785126
Norm after each mp layer: 1.1985952854156494
Norm after each mp layer: 2.9490175247192383
Norm after each mp layer: 9.322683334350586
Norm before input: 0.2552422881126404
Norm after input: 0.4054708778858185
Norm after each mp layer: 1.194837212562561
Norm after each mp layer: 2.9260551929473877
Norm after each mp layer: 9.028039932250977
Norm before input: 0.2552422881126404
Norm after input: 0.4054708778858185
Norm after each mp layer: 1.194837212562561
Norm after each mp layer: 2.9260551929473877
Norm after each mp layer: 9.028039932250977
Norm before input: 0.2552422881126404
Norm after input: 0.40559065341949463
Norm after each mp layer: 1.1906025409698486
Norm after each mp layer: 2.9067792892456055
Norm after each mp layer: 8.896042823791504
Norm before input: 0.2552422881126404
Norm after input: 0.40559065341949463
Norm after each mp layer: 1.1906025409698486
Norm after each mp layer: 2.9067792892456055
Norm after each mp layer: 8.896042823791504
Norm before input: 0.2552422881126404
Norm after input: 0.4053371548652649
Norm after each mp layer: 1.1890836954116821
Norm after each mp layer: 2.9106993675231934
Norm after each mp layer: 9.060334205627441
Norm before input: 0.2552422881126404
Norm after input: 0.4053371548652649
Norm after each mp layer: 1.1890836954116821
Norm after each mp layer: 2.9106993675231934
Norm after each mp layer: 9.060334205627441
Norm before input: 0.2552422881126404
Norm after input: 0.404947966337204
Norm after each mp layer: 1.1885910034179688
Norm after each mp layer: 2.9258930683135986
Norm after each mp layer: 9.337462425231934
Epoch: 320, Loss: 0.1358, Energy: 509.8315, Train: 98.43%, Valid: 78.60%, Test: 75.70%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.404947966337204
Norm after each mp layer: 1.1885910034179688
Norm after each mp layer: 2.9258930683135986
Norm after each mp layer: 9.337462425231934
Norm before input: 0.2552422881126404
Norm after input: 0.4046817421913147
Norm after each mp layer: 1.1866755485534668
Norm after each mp layer: 2.932487964630127
Norm after each mp layer: 9.512897491455078
Norm before input: 0.2552422881126404
Norm after input: 0.4046817421913147
Norm after each mp layer: 1.1866755485534668
Norm after each mp layer: 2.932488203048706
Norm after each mp layer: 9.512897491455078
Norm before input: 0.2552422881126404
Norm after input: 0.40463268756866455
Norm after each mp layer: 1.182202935218811
Norm after each mp layer: 2.918046474456787
Norm after each mp layer: 9.440595626831055
Norm before input: 0.2552422881126404
Norm after input: 0.40463268756866455
Norm after each mp layer: 1.182202935218811
Norm after each mp layer: 2.918046474456787
Norm after each mp layer: 9.440595626831055
Norm before input: 0.2552422881126404
Norm after input: 0.40473827719688416
Norm after each mp layer: 1.1771023273468018
Norm after each mp layer: 2.8950722217559814
Norm after each mp layer: 9.237340927124023
Norm before input: 0.2552422881126404
Norm after input: 0.40473827719688416
Norm after each mp layer: 1.1771023273468018
Norm after each mp layer: 2.8950722217559814
Norm after each mp layer: 9.237340927124023
Norm before input: 0.2552422881126404
Norm after input: 0.4047738015651703
Norm after each mp layer: 1.174410104751587
Norm after each mp layer: 2.884014844894409
Norm after each mp layer: 9.098618507385254
Norm before input: 0.2552422881126404
Norm after input: 0.4047738015651703
Norm after each mp layer: 1.174410104751587
Norm after each mp layer: 2.884014844894409
Norm after each mp layer: 9.098618507385254
Norm before input: 0.2552422881126404
Norm after input: 0.4045257270336151
Norm after each mp layer: 1.1758767366409302
Norm after each mp layer: 2.8950421810150146
Norm after each mp layer: 9.131391525268555
Epoch: 325, Loss: 0.1286, Energy: 530.3559, Train: 98.59%, Valid: 78.20%, Test: 75.20%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4045257270336151
Norm after each mp layer: 1.1758767366409302
Norm after each mp layer: 2.8950421810150146
Norm after each mp layer: 9.131391525268555
Norm before input: 0.2552422881126404
Norm after input: 0.40413355827331543
Norm after each mp layer: 1.1803200244903564
Norm after each mp layer: 2.9219651222229004
Norm after each mp layer: 9.320958137512207
Norm before input: 0.2552422881126404
Norm after input: 0.40413355827331543
Norm after each mp layer: 1.1803200244903564
Norm after each mp layer: 2.9219651222229004
Norm after each mp layer: 9.320958137512207
Norm before input: 0.2552422881126404
Norm after input: 0.40386539697647095
Norm after each mp layer: 1.1843230724334717
Norm after each mp layer: 2.945444107055664
Norm after each mp layer: 9.510563850402832
Norm before input: 0.2552422881126404
Norm after input: 0.40386539697647095
Norm after each mp layer: 1.1843230724334717
Norm after each mp layer: 2.945444107055664
Norm after each mp layer: 9.510563850402832
Norm before input: 0.2552422881126404
Norm after input: 0.40382155776023865
Norm after each mp layer: 1.184969186782837
Norm after each mp layer: 2.9478962421417236
Norm after each mp layer: 9.530416488647461
Norm before input: 0.2552422881126404
Norm after input: 0.40382155776023865
Norm after each mp layer: 1.1849690675735474
Norm after each mp layer: 2.9478962421417236
Norm after each mp layer: 9.530416488647461
Norm before input: 0.2552422881126404
Norm after input: 0.40395355224609375
Norm after each mp layer: 1.1826411485671997
Norm after each mp layer: 2.933228015899658
Norm after each mp layer: 9.417210578918457
Norm before input: 0.2552422881126404
Norm after input: 0.40395355224609375
Norm after each mp layer: 1.1826411485671997
Norm after each mp layer: 2.933228015899658
Norm after each mp layer: 9.417210578918457
Norm before input: 0.2552422881126404
Norm after input: 0.4040835499763489
Norm after each mp layer: 1.1800540685653687
Norm after each mp layer: 2.919870138168335
Norm after each mp layer: 9.339095115661621
Epoch: 330, Loss: 0.1129, Energy: 458.7078, Train: 98.68%, Valid: 77.00%, Test: 74.80%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4040835499763489
Norm after each mp layer: 1.1800540685653687
Norm after each mp layer: 2.919870138168335
Norm after each mp layer: 9.339095115661621
Norm before input: 0.2552422881126404
Norm after input: 0.4040086269378662
Norm after each mp layer: 1.1792190074920654
Norm after each mp layer: 2.9201579093933105
Norm after each mp layer: 9.381781578063965
Norm before input: 0.2552422881126404
Norm after input: 0.4040086269378662
Norm after each mp layer: 1.1792190074920654
Norm after each mp layer: 2.9201579093933105
Norm after each mp layer: 9.381781578063965
Norm before input: 0.2552422881126404
Norm after input: 0.40375980734825134
Norm after each mp layer: 1.1798819303512573
Norm after each mp layer: 2.9324803352355957
Norm after each mp layer: 9.516578674316406
Norm before input: 0.2552422881126404
Norm after input: 0.40375980734825134
Norm after each mp layer: 1.1798819303512573
Norm after each mp layer: 2.9324803352355957
Norm after each mp layer: 9.516578674316406
Norm before input: 0.2552422881126404
Norm after input: 0.40353938937187195
Norm after each mp layer: 1.180118441581726
Norm after each mp layer: 2.9444000720977783
Norm after each mp layer: 9.635120391845703
Norm before input: 0.2552422881126404
Norm after input: 0.40353938937187195
Norm after each mp layer: 1.180118441581726
Norm after each mp layer: 2.9444000720977783
Norm after each mp layer: 9.635120391845703
Norm before input: 0.2552422881126404
Norm after input: 0.4034751355648041
Norm after each mp layer: 1.1783937215805054
Norm after each mp layer: 2.944129705429077
Norm after each mp layer: 9.625457763671875
Norm before input: 0.2552422881126404
Norm after input: 0.4034751355648041
Norm after each mp layer: 1.1783937215805054
Norm after each mp layer: 2.944129705429077
Norm after each mp layer: 9.625456809997559
Norm before input: 0.2552422881126404
Norm after input: 0.40355145931243896
Norm after each mp layer: 1.1753205060958862
Norm after each mp layer: 2.9337210655212402
Norm after each mp layer: 9.5146484375
Epoch: 335, Loss: 0.1090, Energy: 466.3831, Train: 99.01%, Valid: 78.00%, Test: 74.90%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40355145931243896
Norm after each mp layer: 1.1753205060958862
Norm after each mp layer: 2.9337210655212402
Norm after each mp layer: 9.5146484375
Norm before input: 0.2552422881126404
Norm after input: 0.40362828969955444
Norm after each mp layer: 1.1729871034622192
Norm after each mp layer: 2.925551414489746
Norm after each mp layer: 9.429412841796875
Norm before input: 0.2552422881126404
Norm after input: 0.40362828969955444
Norm after each mp layer: 1.1729871034622192
Norm after each mp layer: 2.925551414489746
Norm after each mp layer: 9.429412841796875
Norm before input: 0.2552422881126404
Norm after input: 0.4035519063472748
Norm after each mp layer: 1.1728224754333496
Norm after each mp layer: 2.9282004833221436
Norm after each mp layer: 9.44440746307373
Norm before input: 0.2552422881126404
Norm after input: 0.4035519063472748
Norm after each mp layer: 1.1728224754333496
Norm after each mp layer: 2.9282004833221436
Norm after each mp layer: 9.44440746307373
Norm before input: 0.2552422881126404
Norm after input: 0.4033421277999878
Norm after each mp layer: 1.174368143081665
Norm after each mp layer: 2.9396164417266846
Norm after each mp layer: 9.543106079101562
Norm before input: 0.2552422881126404
Norm after input: 0.4033421277999878
Norm after each mp layer: 1.174368143081665
Norm after each mp layer: 2.9396164417266846
Norm after each mp layer: 9.543106079101562
Norm before input: 0.2552422881126404
Norm after input: 0.40314769744873047
Norm after each mp layer: 1.1757781505584717
Norm after each mp layer: 2.949535608291626
Norm after each mp layer: 9.637092590332031
Norm before input: 0.2552422881126404
Norm after input: 0.40314769744873047
Norm after each mp layer: 1.1757781505584717
Norm after each mp layer: 2.949535608291626
Norm after each mp layer: 9.637092590332031
Norm before input: 0.2552422881126404
Norm after input: 0.403060644865036
Norm after each mp layer: 1.1754990816116333
Norm after each mp layer: 2.94903826713562
Norm after each mp layer: 9.638145446777344
Epoch: 340, Loss: 0.1021, Energy: 446.3564, Train: 99.01%, Valid: 77.20%, Test: 74.90%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.403060644865036
Norm after each mp layer: 1.1754990816116333
Norm after each mp layer: 2.94903826713562
Norm after each mp layer: 9.638145446777344
Norm before input: 0.2552422881126404
Norm after input: 0.40306246280670166
Norm after each mp layer: 1.1736395359039307
Norm after each mp layer: 2.9398865699768066
Norm after each mp layer: 9.558211326599121
Norm before input: 0.2552422881126404
Norm after input: 0.40306246280670166
Norm after each mp layer: 1.1736395359039307
Norm after each mp layer: 2.9398865699768066
Norm after each mp layer: 9.558211326599121
Norm before input: 0.2552422881126404
Norm after input: 0.4030498266220093
Norm after each mp layer: 1.1716097593307495
Norm after each mp layer: 2.931710958480835
Norm after each mp layer: 9.484787940979004
Norm before input: 0.2552422881126404
Norm after input: 0.4030498266220093
Norm after each mp layer: 1.1716097593307495
Norm after each mp layer: 2.931710958480835
Norm after each mp layer: 9.484787940979004
Norm before input: 0.2552422881126404
Norm after input: 0.4029240608215332
Norm after each mp layer: 1.1705771684646606
Norm after each mp layer: 2.931429147720337
Norm after each mp layer: 9.47789478302002
Norm before input: 0.2552422881126404
Norm after input: 0.4029240608215332
Norm after each mp layer: 1.1705771684646606
Norm after each mp layer: 2.931429147720337
Norm after each mp layer: 9.47789478302002
Norm before input: 0.2552422881126404
Norm after input: 0.4027141332626343
Norm after each mp layer: 1.170465350151062
Norm after each mp layer: 2.937758207321167
Norm after each mp layer: 9.529359817504883
Norm before input: 0.2552422881126404
Norm after input: 0.4027141332626343
Norm after each mp layer: 1.170465350151062
Norm after each mp layer: 2.937758207321167
Norm after each mp layer: 9.529359817504883
Norm before input: 0.2552422881126404
Norm after input: 0.4025428593158722
Norm after each mp layer: 1.1702016592025757
Norm after each mp layer: 2.943145275115967
Norm after each mp layer: 9.576239585876465
Epoch: 345, Loss: 0.0962, Energy: 421.6251, Train: 99.01%, Valid: 76.60%, Test: 74.70%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4025428593158722
Norm after each mp layer: 1.1702016592025757
Norm after each mp layer: 2.943145275115967
Norm after each mp layer: 9.576239585876465
Norm before input: 0.2552422881126404
Norm after input: 0.4024890661239624
Norm after each mp layer: 1.1689043045043945
Norm after each mp layer: 2.9412455558776855
Norm after each mp layer: 9.560932159423828
Norm before input: 0.2552422881126404
Norm after input: 0.4024890661239624
Norm after each mp layer: 1.1689043045043945
Norm after each mp layer: 2.9412455558776855
Norm after each mp layer: 9.560932159423828
Norm before input: 0.2552422881126404
Norm after input: 0.402530312538147
Norm after each mp layer: 1.1669049263000488
Norm after each mp layer: 2.9340097904205322
Norm after each mp layer: 9.501761436462402
Norm before input: 0.2552422881126404
Norm after input: 0.402530312538147
Norm after each mp layer: 1.1669049263000488
Norm after each mp layer: 2.9340097904205322
Norm after each mp layer: 9.501761436462402
Norm before input: 0.2552422881126404
Norm after input: 0.40256309509277344
Norm after each mp layer: 1.165361762046814
Norm after each mp layer: 2.929075002670288
Norm after each mp layer: 9.465779304504395
Norm before input: 0.2552422881126404
Norm after input: 0.40256309509277344
Norm after each mp layer: 1.165361762046814
Norm after each mp layer: 2.929075002670288
Norm after each mp layer: 9.465779304504395
Norm before input: 0.2552422881126404
Norm after input: 0.40249669551849365
Norm after each mp layer: 1.1650060415267944
Norm after each mp layer: 2.9314465522766113
Norm after each mp layer: 9.490690231323242
Norm before input: 0.2552422881126404
Norm after input: 0.40249669551849365
Norm after each mp layer: 1.1650060415267944
Norm after each mp layer: 2.9314465522766113
Norm after each mp layer: 9.490690231323242
Norm before input: 0.2552422881126404
Norm after input: 0.40236252546310425
Norm after each mp layer: 1.1653884649276733
Norm after each mp layer: 2.9387571811676025
Norm after each mp layer: 9.552645683288574
Epoch: 350, Loss: 0.0924, Energy: 411.3839, Train: 99.01%, Valid: 76.40%, Test: 74.40%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40236252546310425
Norm after each mp layer: 1.1653884649276733
Norm after each mp layer: 2.9387571811676025
Norm after each mp layer: 9.552645683288574
Norm before input: 0.2552422881126404
Norm after input: 0.4022565186023712
Norm after each mp layer: 1.165428876876831
Norm after each mp layer: 2.944138288497925
Norm after each mp layer: 9.590866088867188
Norm before input: 0.2552422881126404
Norm after input: 0.4022565186023712
Norm after each mp layer: 1.165428876876831
Norm after each mp layer: 2.944138288497925
Norm after each mp layer: 9.590866088867188
Norm before input: 0.2552422881126404
Norm after input: 0.40222346782684326
Norm after each mp layer: 1.1645269393920898
Norm after each mp layer: 2.943455934524536
Norm after each mp layer: 9.569724082946777
Norm before input: 0.2552422881126404
Norm after input: 0.40222346782684326
Norm after each mp layer: 1.1645269393920898
Norm after each mp layer: 2.943455934524536
Norm after each mp layer: 9.569724082946777
Norm before input: 0.2552422881126404
Norm after input: 0.40222448110580444
Norm after each mp layer: 1.1631189584732056
Norm after each mp layer: 2.93966007232666
Norm after each mp layer: 9.520651817321777
Norm before input: 0.2552422881126404
Norm after input: 0.40222448110580444
Norm after each mp layer: 1.1631189584732056
Norm after each mp layer: 2.93966007232666
Norm after each mp layer: 9.520651817321777
Norm before input: 0.2552422881126404
Norm after input: 0.40217939019203186
Norm after each mp layer: 1.1620689630508423
Norm after each mp layer: 2.938591480255127
Norm after each mp layer: 9.500578880310059
Norm before input: 0.2552422881126404
Norm after input: 0.40217939019203186
Norm after each mp layer: 1.1620689630508423
Norm after each mp layer: 2.938591480255127
Norm after each mp layer: 9.500578880310059
Norm before input: 0.2552422881126404
Norm after input: 0.40206101536750793
Norm after each mp layer: 1.1616836786270142
Norm after each mp layer: 2.9422035217285156
Norm after each mp layer: 9.5298433303833
Epoch: 355, Loss: 0.0888, Energy: 397.4619, Train: 99.17%, Valid: 76.20%, Test: 74.00%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40206101536750793
Norm after each mp layer: 1.1616836786270142
Norm after each mp layer: 2.9422035217285156
Norm after each mp layer: 9.5298433303833
Norm before input: 0.2552422881126404
Norm after input: 0.4019337594509125
Norm after each mp layer: 1.1614069938659668
Norm after each mp layer: 2.9466001987457275
Norm after each mp layer: 9.575272560119629
Norm before input: 0.2552422881126404
Norm after input: 0.4019337594509125
Norm after each mp layer: 1.1614069938659668
Norm after each mp layer: 2.9466001987457275
Norm after each mp layer: 9.575272560119629
Norm before input: 0.2552422881126404
Norm after input: 0.40186601877212524
Norm after each mp layer: 1.1605509519577026
Norm after each mp layer: 2.9467716217041016
Norm after each mp layer: 9.588726043701172
Norm before input: 0.2552422881126404
Norm after input: 0.40186601877212524
Norm after each mp layer: 1.1605509519577026
Norm after each mp layer: 2.9467716217041016
Norm after each mp layer: 9.588726043701172
Norm before input: 0.2552422881126404
Norm after input: 0.4018598794937134
Norm after each mp layer: 1.1591321229934692
Norm after each mp layer: 2.942631721496582
Norm after each mp layer: 9.565095901489258
Norm before input: 0.2552422881126404
Norm after input: 0.4018598794937134
Norm after each mp layer: 1.1591321229934692
Norm after each mp layer: 2.942631721496582
Norm after each mp layer: 9.565095901489258
Norm before input: 0.2552422881126404
Norm after input: 0.401854544878006
Norm after each mp layer: 1.1578336954116821
Norm after each mp layer: 2.938978910446167
Norm after each mp layer: 9.544607162475586
Norm before input: 0.2552422881126404
Norm after input: 0.401854544878006
Norm after each mp layer: 1.1578336954116821
Norm after each mp layer: 2.938978910446167
Norm after each mp layer: 9.544607162475586
Norm before input: 0.2552422881126404
Norm after input: 0.4017922580242157
Norm after each mp layer: 1.157184362411499
Norm after each mp layer: 2.9396069049835205
Norm after each mp layer: 9.558013916015625
Epoch: 360, Loss: 0.0852, Energy: 387.7568, Train: 99.34%, Valid: 76.00%, Test: 73.50%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4017922580242157
Norm after each mp layer: 1.157184362411499
Norm after each mp layer: 2.9396069049835205
Norm after each mp layer: 9.558013916015625
Norm before input: 0.2552422881126404
Norm after input: 0.4016886353492737
Norm after each mp layer: 1.156982183456421
Norm after each mp layer: 2.9432804584503174
Norm after each mp layer: 9.593441009521484
Norm before input: 0.2552422881126404
Norm after input: 0.4016886353492737
Norm after each mp layer: 1.156982183456421
Norm after each mp layer: 2.9432804584503174
Norm after each mp layer: 9.593441009521484
Norm before input: 0.2552422881126404
Norm after input: 0.4016047418117523
Norm after each mp layer: 1.15657639503479
Norm after each mp layer: 2.945507526397705
Norm after each mp layer: 9.611417770385742
Norm before input: 0.2552422881126404
Norm after input: 0.4016047418117523
Norm after each mp layer: 1.15657639503479
Norm after each mp layer: 2.945507526397705
Norm after each mp layer: 9.611417770385742
Norm before input: 0.2552422881126404
Norm after input: 0.4015676975250244
Norm after each mp layer: 1.15565824508667
Norm after each mp layer: 2.94407057762146
Norm after each mp layer: 9.593611717224121
Norm before input: 0.2552422881126404
Norm after input: 0.4015676975250244
Norm after each mp layer: 1.15565824508667
Norm after each mp layer: 2.94407057762146
Norm after each mp layer: 9.593611717224121
Norm before input: 0.2552422881126404
Norm after input: 0.40154460072517395
Norm after each mp layer: 1.154603362083435
Norm after each mp layer: 2.94162917137146
Norm after each mp layer: 9.567453384399414
Norm before input: 0.2552422881126404
Norm after input: 0.40154460072517395
Norm after each mp layer: 1.154603362083435
Norm after each mp layer: 2.94162917137146
Norm after each mp layer: 9.567453384399414
Norm before input: 0.2552422881126404
Norm after input: 0.40148550271987915
Norm after each mp layer: 1.1539418697357178
Norm after each mp layer: 2.9419057369232178
Norm after each mp layer: 9.568719863891602
Epoch: 365, Loss: 0.0819, Energy: 376.9986, Train: 99.34%, Valid: 75.80%, Test: 73.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40148550271987915
Norm after each mp layer: 1.1539418697357178
Norm after each mp layer: 2.9419057369232178
Norm after each mp layer: 9.568719863891602
Norm before input: 0.2552422881126404
Norm after input: 0.401393324136734
Norm after each mp layer: 1.153662919998169
Norm after each mp layer: 2.9448354244232178
Norm after each mp layer: 9.59727954864502
Norm before input: 0.2552422881126404
Norm after input: 0.401393324136734
Norm after each mp layer: 1.153662919998169
Norm after each mp layer: 2.9448351860046387
Norm after each mp layer: 9.59727954864502
Norm before input: 0.2552422881126404
Norm after input: 0.40131938457489014
Norm after each mp layer: 1.1532539129257202
Norm after each mp layer: 2.9468154907226562
Norm after each mp layer: 9.619203567504883
Norm before input: 0.2552422881126404
Norm after input: 0.40131938457489014
Norm after each mp layer: 1.1532539129257202
Norm after each mp layer: 2.9468154907226562
Norm after each mp layer: 9.619203567504883
Norm before input: 0.2552422881126404
Norm after input: 0.4012916386127472
Norm after each mp layer: 1.1524089574813843
Norm after each mp layer: 2.945615291595459
Norm after each mp layer: 9.611714363098145
Norm before input: 0.2552422881126404
Norm after input: 0.4012916386127472
Norm after each mp layer: 1.1524089574813843
Norm after each mp layer: 2.945615291595459
Norm after each mp layer: 9.611714363098145
Norm before input: 0.2552422881126404
Norm after input: 0.40128278732299805
Norm after each mp layer: 1.1514095067977905
Norm after each mp layer: 2.9432711601257324
Norm after each mp layer: 9.592379570007324
Norm before input: 0.2552422881126404
Norm after input: 0.40128278732299805
Norm after each mp layer: 1.1514095067977905
Norm after each mp layer: 2.9432711601257324
Norm after each mp layer: 9.592379570007324
Norm before input: 0.2552422881126404
Norm after input: 0.401245653629303
Norm after each mp layer: 1.1507132053375244
Norm after each mp layer: 2.9431347846984863
Norm after each mp layer: 9.59133529663086
Epoch: 370, Loss: 0.0790, Energy: 368.5275, Train: 99.42%, Valid: 75.60%, Test: 73.10%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.401245653629303
Norm after each mp layer: 1.1507132053375244
Norm after each mp layer: 2.9431347846984863
Norm after each mp layer: 9.591336250305176
Norm before input: 0.2552422881126404
Norm after input: 0.4011746942996979
Norm after each mp layer: 1.1503372192382812
Norm after each mp layer: 2.945390224456787
Norm after each mp layer: 9.610603332519531
Norm before input: 0.2552422881126404
Norm after input: 0.4011746942996979
Norm after each mp layer: 1.1503372192382812
Norm after each mp layer: 2.945390224456787
Norm after each mp layer: 9.610603332519531
Norm before input: 0.2552422881126404
Norm after input: 0.4011087119579315
Norm after each mp layer: 1.1498686075210571
Norm after each mp layer: 2.9470133781433105
Norm after each mp layer: 9.623778343200684
Norm before input: 0.2552422881126404
Norm after input: 0.4011087119579315
Norm after each mp layer: 1.1498686075210571
Norm after each mp layer: 2.9470133781433105
Norm after each mp layer: 9.623778343200684
Norm before input: 0.2552422881126404
Norm after input: 0.4010702669620514
Norm after each mp layer: 1.1490638256072998
Norm after each mp layer: 2.946096897125244
Norm after each mp layer: 9.614314079284668
Norm before input: 0.2552422881126404
Norm after input: 0.4010702669620514
Norm after each mp layer: 1.1490638256072998
Norm after each mp layer: 2.946096897125244
Norm after each mp layer: 9.614314079284668
Norm before input: 0.2552422881126404
Norm after input: 0.40103837847709656
Norm after each mp layer: 1.1481720209121704
Norm after each mp layer: 2.944394588470459
Norm after each mp layer: 9.599157333374023
Norm before input: 0.2552422881126404
Norm after input: 0.40103837847709656
Norm after each mp layer: 1.1481720209121704
Norm after each mp layer: 2.944394588470459
Norm after each mp layer: 9.599157333374023
Norm before input: 0.2552422881126404
Norm after input: 0.4009813070297241
Norm after each mp layer: 1.1475485563278198
Norm after each mp layer: 2.944524049758911
Norm after each mp layer: 9.602092742919922
Epoch: 375, Loss: 0.0764, Energy: 358.2488, Train: 99.42%, Valid: 75.80%, Test: 72.70%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4009813070297241
Norm after each mp layer: 1.1475485563278198
Norm after each mp layer: 2.944524049758911
Norm after each mp layer: 9.602092742919922
Norm before input: 0.2552422881126404
Norm after input: 0.40090611577033997
Norm after each mp layer: 1.1471447944641113
Norm after each mp layer: 2.9461863040924072
Norm after each mp layer: 9.619593620300293
Norm before input: 0.2552422881126404
Norm after input: 0.40090611577033997
Norm after each mp layer: 1.1471447944641113
Norm after each mp layer: 2.9461863040924072
Norm after each mp layer: 9.619593620300293
Norm before input: 0.2552422881126404
Norm after input: 0.40084758400917053
Norm after each mp layer: 1.1466169357299805
Norm after each mp layer: 2.946859359741211
Norm after each mp layer: 9.627228736877441
Norm before input: 0.2552422881126404
Norm after input: 0.40084758400917053
Norm after each mp layer: 1.1466169357299805
Norm after each mp layer: 2.946859359741211
Norm after each mp layer: 9.627228736877441
Norm before input: 0.2552422881126404
Norm after input: 0.40081700682640076
Norm after each mp layer: 1.1458550691604614
Norm after each mp layer: 2.9457170963287354
Norm after each mp layer: 9.61616325378418
Norm before input: 0.2552422881126404
Norm after input: 0.40081700682640076
Norm after each mp layer: 1.1458550691604614
Norm after each mp layer: 2.9457170963287354
Norm after each mp layer: 9.61616325378418
Norm before input: 0.2552422881126404
Norm after input: 0.40078920125961304
Norm after each mp layer: 1.1451350450515747
Norm after each mp layer: 2.9448494911193848
Norm after each mp layer: 9.60566520690918
Norm before input: 0.2552422881126404
Norm after input: 0.40078920125961304
Norm after each mp layer: 1.1451350450515747
Norm after each mp layer: 2.9448494911193848
Norm after each mp layer: 9.60566520690918
Norm before input: 0.2552422881126404
Norm after input: 0.40074095129966736
Norm after each mp layer: 1.144684910774231
Norm after each mp layer: 2.94598650932312
Norm after each mp layer: 9.612198829650879
Epoch: 380, Loss: 0.0739, Energy: 349.8951, Train: 99.42%, Valid: 75.60%, Test: 72.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40074095129966736
Norm after each mp layer: 1.144684910774231
Norm after each mp layer: 2.94598650932312
Norm after each mp layer: 9.612198829650879
Norm before input: 0.2552422881126404
Norm after input: 0.4006863832473755
Norm after each mp layer: 1.1443437337875366
Norm after each mp layer: 2.947885274887085
Norm after each mp layer: 9.625269889831543
Norm before input: 0.2552422881126404
Norm after input: 0.4006863832473755
Norm after each mp layer: 1.1443437337875366
Norm after each mp layer: 2.947885274887085
Norm after each mp layer: 9.625269889831543
Norm before input: 0.2552422881126404
Norm after input: 0.4006495475769043
Norm after each mp layer: 1.1438349485397339
Norm after each mp layer: 2.948371171951294
Norm after each mp layer: 9.625475883483887
Norm before input: 0.2552422881126404
Norm after input: 0.4006495475769043
Norm after each mp layer: 1.1438349485397339
Norm after each mp layer: 2.948371171951294
Norm after each mp layer: 9.625475883483887
Norm before input: 0.2552422881126404
Norm after input: 0.4006248414516449
Norm after each mp layer: 1.1431984901428223
Norm after each mp layer: 2.947737216949463
Norm after each mp layer: 9.616117477416992
Norm before input: 0.2552422881126404
Norm after input: 0.4006248414516449
Norm after each mp layer: 1.1431984901428223
Norm after each mp layer: 2.947737216949463
Norm after each mp layer: 9.616117477416992
Norm before input: 0.2552422881126404
Norm after input: 0.4005865156650543
Norm after each mp layer: 1.1426928043365479
Norm after each mp layer: 2.948032855987549
Norm after each mp layer: 9.616337776184082
Norm before input: 0.2552422881126404
Norm after input: 0.4005865156650543
Norm after each mp layer: 1.1426928043365479
Norm after each mp layer: 2.948032855987549
Norm after each mp layer: 9.616337776184082
Norm before input: 0.2552422881126404
Norm after input: 0.4005303978919983
Norm after each mp layer: 1.1423555612564087
Norm after each mp layer: 2.9495933055877686
Norm after each mp layer: 9.628743171691895
Epoch: 385, Loss: 0.0716, Energy: 342.4554, Train: 99.42%, Valid: 75.60%, Test: 72.40%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4005303978919983
Norm after each mp layer: 1.1423555612564087
Norm after each mp layer: 2.9495933055877686
Norm after each mp layer: 9.628743171691895
Norm before input: 0.2552422881126404
Norm after input: 0.400479793548584
Norm after each mp layer: 1.1419545412063599
Norm after each mp layer: 2.950589418411255
Norm after each mp layer: 9.635588645935059
Norm before input: 0.2552422881126404
Norm after input: 0.400479793548584
Norm after each mp layer: 1.1419545412063599
Norm after each mp layer: 2.950589418411255
Norm after each mp layer: 9.635588645935059
Norm before input: 0.2552422881126404
Norm after input: 0.4004460573196411
Norm after each mp layer: 1.1413817405700684
Norm after each mp layer: 2.950146436691284
Norm after each mp layer: 9.628085136413574
Norm before input: 0.2552422881126404
Norm after input: 0.4004460573196411
Norm after each mp layer: 1.1413817405700684
Norm after each mp layer: 2.950146436691284
Norm after each mp layer: 9.628085136413574
Norm before input: 0.2552422881126404
Norm after input: 0.4004133939743042
Norm after each mp layer: 1.1408185958862305
Norm after each mp layer: 2.9497311115264893
Norm after each mp layer: 9.6200590133667
Norm before input: 0.2552422881126404
Norm after input: 0.4004133939743042
Norm after each mp layer: 1.1408185958862305
Norm after each mp layer: 2.9497311115264893
Norm after each mp layer: 9.6200590133667
Norm before input: 0.2552422881126404
Norm after input: 0.40036770701408386
Norm after each mp layer: 1.1404180526733398
Norm after each mp layer: 2.9505980014801025
Norm after each mp layer: 9.62364673614502
Norm before input: 0.2552422881126404
Norm after input: 0.40036770701408386
Norm after each mp layer: 1.1404180526733398
Norm after each mp layer: 2.9505980014801025
Norm after each mp layer: 9.62364673614502
Norm before input: 0.2552422881126404
Norm after input: 0.40032145380973816
Norm after each mp layer: 1.1400566101074219
Norm after each mp layer: 2.9517526626586914
Norm after each mp layer: 9.629975318908691
Epoch: 390, Loss: 0.0694, Energy: 335.5551, Train: 99.59%, Valid: 75.60%, Test: 72.00%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.40032145380973816
Norm after each mp layer: 1.1400566101074219
Norm after each mp layer: 2.9517526626586914
Norm after each mp layer: 9.629975318908691
Norm before input: 0.2552422881126404
Norm after input: 0.4002896547317505
Norm after each mp layer: 1.1395719051361084
Norm after each mp layer: 2.9518423080444336
Norm after each mp layer: 9.62648868560791
Norm before input: 0.2552422881126404
Norm after input: 0.4002896547317505
Norm after each mp layer: 1.1395719051361084
Norm after each mp layer: 2.9518423080444336
Norm after each mp layer: 9.62648868560791
Norm before input: 0.2552422881126404
Norm after input: 0.40026381611824036
Norm after each mp layer: 1.1390496492385864
Norm after each mp layer: 2.9515483379364014
Norm after each mp layer: 9.61974048614502
Norm before input: 0.2552422881126404
Norm after input: 0.40026381611824036
Norm after each mp layer: 1.1390496492385864
Norm after each mp layer: 2.9515483379364014
Norm after each mp layer: 9.61974048614502
Norm before input: 0.2552422881126404
Norm after input: 0.4002271592617035
Norm after each mp layer: 1.1386555433273315
Norm after each mp layer: 2.952254295349121
Norm after each mp layer: 9.622636795043945
Norm before input: 0.2552422881126404
Norm after input: 0.4002271592617035
Norm after each mp layer: 1.1386555433273315
Norm after each mp layer: 2.952254295349121
Norm after each mp layer: 9.622636795043945
Norm before input: 0.2552422881126404
Norm after input: 0.4001844823360443
Norm after each mp layer: 1.1383345127105713
Norm after each mp layer: 2.9535186290740967
Norm after each mp layer: 9.630599975585938
Norm before input: 0.2552422881126404
Norm after input: 0.4001844823360443
Norm after each mp layer: 1.1383345127105713
Norm after each mp layer: 2.9535186290740967
Norm after each mp layer: 9.630599975585938
Norm before input: 0.2552422881126404
Norm after input: 0.4001508057117462
Norm after each mp layer: 1.1379225254058838
Norm after each mp layer: 2.9539763927459717
Norm after each mp layer: 9.630215644836426
Epoch: 395, Loss: 0.0674, Energy: 329.1604, Train: 99.59%, Valid: 75.00%, Test: 71.80%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.4001508057117462
Norm after each mp layer: 1.1379225254058838
Norm after each mp layer: 2.9539763927459717
Norm after each mp layer: 9.630215644836426
Norm before input: 0.2552422881126404
Norm after input: 0.4001234769821167
Norm after each mp layer: 1.1374475955963135
Norm after each mp layer: 2.953866720199585
Norm after each mp layer: 9.623703002929688
Norm before input: 0.2552422881126404
Norm after input: 0.4001234769821167
Norm after each mp layer: 1.1374475955963135
Norm after each mp layer: 2.953866720199585
Norm after each mp layer: 9.623703002929688
Norm before input: 0.2552422881126404
Norm after input: 0.4000886082649231
Norm after each mp layer: 1.1370607614517212
Norm after each mp layer: 2.954477548599243
Norm after each mp layer: 9.62368106842041
Norm before input: 0.2552422881126404
Norm after input: 0.4000886082649231
Norm after each mp layer: 1.1370607614517212
Norm after each mp layer: 2.954477548599243
Norm after each mp layer: 9.62368106842041
Norm before input: 0.2552422881126404
Norm after input: 0.4000481069087982
Norm after each mp layer: 1.1367477178573608
Norm after each mp layer: 2.9556803703308105
Norm after each mp layer: 9.629321098327637
Norm before input: 0.2552422881126404
Norm after input: 0.4000481069087982
Norm after each mp layer: 1.1367477178573608
Norm after each mp layer: 2.9556803703308105
Norm after each mp layer: 9.629321098327637
Norm before input: 0.2552422881126404
Norm after input: 0.40001583099365234
Norm after each mp layer: 1.1363695859909058
Norm after each mp layer: 2.9562618732452393
Norm after each mp layer: 9.629220962524414
Norm before input: 0.2552422881126404
Norm after input: 0.40001583099365234
Norm after each mp layer: 1.1363695859909058
Norm after each mp layer: 2.9562618732452393
Norm after each mp layer: 9.629220962524414
Norm before input: 0.2552422881126404
Norm after input: 0.3999909460544586
Norm after each mp layer: 1.1359367370605469
Norm after each mp layer: 2.956299066543579
Norm after each mp layer: 9.62420654296875
Epoch: 400, Loss: 0.0655, Energy: 322.6342, Train: 99.59%, Valid: 74.80%, Test: 71.40%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3999909460544586
Norm after each mp layer: 1.1359367370605469
Norm after each mp layer: 2.956298828125
Norm after each mp layer: 9.62420654296875
Norm before input: 0.2552422881126404
Norm after input: 0.39996102452278137
Norm after each mp layer: 1.1355764865875244
Norm after each mp layer: 2.956921100616455
Norm after each mp layer: 9.62509536743164
Norm before input: 0.2552422881126404
Norm after input: 0.39996102452278137
Norm after each mp layer: 1.1355764865875244
Norm after each mp layer: 2.956921100616455
Norm after each mp layer: 9.62509536743164
Norm before input: 0.2552422881126404
Norm after input: 0.39992648363113403
Norm after each mp layer: 1.1352760791778564
Norm after each mp layer: 2.958052396774292
Norm after each mp layer: 9.630850791931152
Norm before input: 0.2552422881126404
Norm after input: 0.39992648363113403
Norm after each mp layer: 1.1352760791778564
Norm after each mp layer: 2.958052396774292
Norm after each mp layer: 9.630850791931152
Norm before input: 0.2552422881126404
Norm after input: 0.39989790320396423
Norm after each mp layer: 1.1349165439605713
Norm after each mp layer: 2.958652973175049
Norm after each mp layer: 9.631082534790039
Norm before input: 0.2552422881126404
Norm after input: 0.39989790320396423
Norm after each mp layer: 1.1349165439605713
Norm after each mp layer: 2.958652973175049
Norm after each mp layer: 9.631082534790039
Norm before input: 0.2552422881126404
Norm after input: 0.3998737037181854
Norm after each mp layer: 1.134513020515442
Norm after each mp layer: 2.9588472843170166
Norm after each mp layer: 9.62687873840332
Norm before input: 0.2552422881126404
Norm after input: 0.3998737037181854
Norm after each mp layer: 1.134513020515442
Norm after each mp layer: 2.9588472843170166
Norm after each mp layer: 9.62687873840332
Norm before input: 0.2552422881126404
Norm after input: 0.3998439610004425
Norm after each mp layer: 1.1341712474822998
Norm after each mp layer: 2.9595704078674316
Norm after each mp layer: 9.627495765686035
Epoch: 405, Loss: 0.0637, Energy: 316.9686, Train: 99.59%, Valid: 74.40%, Test: 71.20%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3998439610004425
Norm after each mp layer: 1.1341712474822998
Norm after each mp layer: 2.9595704078674316
Norm after each mp layer: 9.627495765686035
Norm before input: 0.2552422881126404
Norm after input: 0.39981117844581604
Norm after each mp layer: 1.1338675022125244
Norm after each mp layer: 2.960592269897461
Norm after each mp layer: 9.630925178527832
Norm before input: 0.2552422881126404
Norm after input: 0.39981117844581604
Norm after each mp layer: 1.1338675022125244
Norm after each mp layer: 2.960592269897461
Norm after each mp layer: 9.630925178527832
Norm before input: 0.2552422881126404
Norm after input: 0.39978453516960144
Norm after each mp layer: 1.1335079669952393
Norm after each mp layer: 2.961050033569336
Norm after each mp layer: 9.628910064697266
Norm before input: 0.2552422881126404
Norm after input: 0.39978453516960144
Norm after each mp layer: 1.1335079669952393
Norm after each mp layer: 2.961050033569336
Norm after each mp layer: 9.628910064697266
Norm before input: 0.2552422881126404
Norm after input: 0.39976078271865845
Norm after each mp layer: 1.1331291198730469
Norm after each mp layer: 2.961275815963745
Norm after each mp layer: 9.624797821044922
Norm before input: 0.2552422881126404
Norm after input: 0.39976078271865845
Norm after each mp layer: 1.1331291198730469
Norm after each mp layer: 2.961275815963745
Norm after each mp layer: 9.624797821044922
Norm before input: 0.2552422881126404
Norm after input: 0.3997321128845215
Norm after each mp layer: 1.1328107118606567
Norm after each mp layer: 2.962022542953491
Norm after each mp layer: 9.625956535339355
Norm before input: 0.2552422881126404
Norm after input: 0.3997321128845215
Norm after each mp layer: 1.1328107118606567
Norm after each mp layer: 2.962022542953491
Norm after each mp layer: 9.625956535339355
Norm before input: 0.2552422881126404
Norm after input: 0.39970308542251587
Norm after each mp layer: 1.1325019598007202
Norm after each mp layer: 2.962843656539917
Norm after each mp layer: 9.627848625183105
Epoch: 410, Loss: 0.0620, Energy: 311.4942, Train: 99.59%, Valid: 74.00%, Test: 70.90%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.39970308542251587
Norm after each mp layer: 1.1325019598007202
Norm after each mp layer: 2.962843656539917
Norm after each mp layer: 9.627848625183105
Norm before input: 0.2552422881126404
Norm after input: 0.39967960119247437
Norm after each mp layer: 1.1321371793746948
Norm after each mp layer: 2.963129758834839
Norm after each mp layer: 9.624322891235352
Norm before input: 0.2552422881126404
Norm after input: 0.39967960119247437
Norm after each mp layer: 1.1321371793746948
Norm after each mp layer: 2.963129758834839
Norm after each mp layer: 9.624322891235352
Norm before input: 0.2552422881126404
Norm after input: 0.39965611696243286
Norm after each mp layer: 1.131773829460144
Norm after each mp layer: 2.9634177684783936
Norm after each mp layer: 9.620720863342285
Norm before input: 0.2552422881126404
Norm after input: 0.39965611696243286
Norm after each mp layer: 1.131773829460144
Norm after each mp layer: 2.9634177684783936
Norm after each mp layer: 9.620720863342285
Norm before input: 0.2552422881126404
Norm after input: 0.39962857961654663
Norm after each mp layer: 1.1314572095870972
Norm after each mp layer: 2.964120388031006
Norm after each mp layer: 9.621262550354004
Norm before input: 0.2552422881126404
Norm after input: 0.39962857961654663
Norm after each mp layer: 1.1314572095870972
Norm after each mp layer: 2.964120388031006
Norm after each mp layer: 9.621262550354004
Norm before input: 0.2552422881126404
Norm after input: 0.3996031880378723
Norm after each mp layer: 1.1311243772506714
Norm after each mp layer: 2.9646220207214355
Norm after each mp layer: 9.619948387145996
Norm before input: 0.2552422881126404
Norm after input: 0.3996031880378723
Norm after each mp layer: 1.1311243772506714
Norm after each mp layer: 2.9646220207214355
Norm after each mp layer: 9.619948387145996
Norm before input: 0.2552422881126404
Norm after input: 0.3995821177959442
Norm after each mp layer: 1.1307588815689087
Norm after each mp layer: 2.964736223220825
Norm after each mp layer: 9.614938735961914
Epoch: 415, Loss: 0.0604, Energy: 305.7263, Train: 99.59%, Valid: 73.80%, Test: 70.40%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3995821177959442
Norm after each mp layer: 1.1307588815689087
Norm after each mp layer: 2.964736223220825
Norm after each mp layer: 9.614938735961914
Norm before input: 0.2552422881126404
Norm after input: 0.3995589315891266
Norm after each mp layer: 1.1304306983947754
Norm after each mp layer: 2.965125322341919
Norm after each mp layer: 9.612760543823242
Norm before input: 0.2552422881126404
Norm after input: 0.3995589315891266
Norm after each mp layer: 1.1304306983947754
Norm after each mp layer: 2.965125322341919
Norm after each mp layer: 9.612760543823242
Norm before input: 0.2552422881126404
Norm after input: 0.39953404664993286
Norm after each mp layer: 1.1301385164260864
Norm after each mp layer: 2.9657809734344482
Norm after each mp layer: 9.613141059875488
Norm before input: 0.2552422881126404
Norm after input: 0.39953404664993286
Norm after each mp layer: 1.1301385164260864
Norm after each mp layer: 2.9657809734344482
Norm after each mp layer: 9.613141059875488
Norm before input: 0.2552422881126404
Norm after input: 0.39951300621032715
Norm after each mp layer: 1.1298218965530396
Norm after each mp layer: 2.966130495071411
Norm after each mp layer: 9.610135078430176
Norm before input: 0.2552422881126404
Norm after input: 0.39951300621032715
Norm after each mp layer: 1.1298218965530396
Norm after each mp layer: 2.966130495071411
Norm after each mp layer: 9.610135078430176
Norm before input: 0.2552422881126404
Norm after input: 0.3994932174682617
Norm after each mp layer: 1.1295045614242554
Norm after each mp layer: 2.966421365737915
Norm after each mp layer: 9.606172561645508
Norm before input: 0.2552422881126404
Norm after input: 0.3994932174682617
Norm after each mp layer: 1.1295045614242554
Norm after each mp layer: 2.966421365737915
Norm after each mp layer: 9.606172561645508
Norm before input: 0.2552422881126404
Norm after input: 0.39947038888931274
Norm after each mp layer: 1.1292277574539185
Norm after each mp layer: 2.96708345413208
Norm after each mp layer: 9.60568618774414
Epoch: 420, Loss: 0.0589, Energy: 300.0597, Train: 99.67%, Valid: 73.80%, Test: 70.10%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.39947038888931274
Norm after each mp layer: 1.1292277574539185
Norm after each mp layer: 2.96708345413208
Norm after each mp layer: 9.60568618774414
Norm before input: 0.2552422881126404
Norm after input: 0.39944857358932495
Norm after each mp layer: 1.1289466619491577
Norm after each mp layer: 2.967679977416992
Norm after each mp layer: 9.604475021362305
Norm before input: 0.2552422881126404
Norm after input: 0.39944857358932495
Norm after each mp layer: 1.1289466619491577
Norm after each mp layer: 2.967679977416992
Norm after each mp layer: 9.604475021362305
Norm before input: 0.2552422881126404
Norm after input: 0.39942991733551025
Norm after each mp layer: 1.128638744354248
Norm after each mp layer: 2.9679746627807617
Norm after each mp layer: 9.600342750549316
Norm before input: 0.2552422881126404
Norm after input: 0.39942991733551025
Norm after each mp layer: 1.128638744354248
Norm after each mp layer: 2.9679746627807617
Norm after each mp layer: 9.600342750549316
Norm before input: 0.2552422881126404
Norm after input: 0.39941003918647766
Norm after each mp layer: 1.1283535957336426
Norm after each mp layer: 2.968451976776123
Norm after each mp layer: 9.598282814025879
Norm before input: 0.2552422881126404
Norm after input: 0.39941003918647766
Norm after each mp layer: 1.1283535957336426
Norm after each mp layer: 2.968451976776123
Norm after each mp layer: 9.598282814025879
Norm before input: 0.2552422881126404
Norm after input: 0.39938926696777344
Norm after each mp layer: 1.1280889511108398
Norm after each mp layer: 2.9690816402435303
Norm after each mp layer: 9.597905158996582
Norm before input: 0.2552422881126404
Norm after input: 0.39938926696777344
Norm after each mp layer: 1.1280889511108398
Norm after each mp layer: 2.9690816402435303
Norm after each mp layer: 9.597905158996582
Norm before input: 0.2552422881126404
Norm after input: 0.39937159419059753
Norm after each mp layer: 1.1278046369552612
Norm after each mp layer: 2.9694485664367676
Norm after each mp layer: 9.59478759765625
Epoch: 425, Loss: 0.0575, Energy: 294.7390, Train: 99.67%, Valid: 73.60%, Test: 69.80%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.39937159419059753
Norm after each mp layer: 1.1278046369552612
Norm after each mp layer: 2.9694485664367676
Norm after each mp layer: 9.59478759765625
Norm before input: 0.2552422881126404
Norm after input: 0.3993542790412903
Norm after each mp layer: 1.1275283098220825
Norm after each mp layer: 2.9698398113250732
Norm after each mp layer: 9.59166431427002
Norm before input: 0.2552422881126404
Norm after input: 0.3993542790412903
Norm after each mp layer: 1.1275283098220825
Norm after each mp layer: 2.9698398113250732
Norm after each mp layer: 9.59166431427002
Norm before input: 0.2552422881126404
Norm after input: 0.3993349075317383
Norm after each mp layer: 1.127284049987793
Norm after each mp layer: 2.9705116748809814
Norm after each mp layer: 9.591068267822266
Norm before input: 0.2552422881126404
Norm after input: 0.3993349075317383
Norm after each mp layer: 1.127284049987793
Norm after each mp layer: 2.9705116748809814
Norm after each mp layer: 9.591068267822266
Norm before input: 0.2552422881126404
Norm after input: 0.3993169963359833
Norm after each mp layer: 1.12703275680542
Norm after each mp layer: 2.971062660217285
Norm after each mp layer: 9.588949203491211
Norm before input: 0.2552422881126404
Norm after input: 0.3993169963359833
Norm after each mp layer: 1.12703275680542
Norm after each mp layer: 2.971062660217285
Norm after each mp layer: 9.588949203491211
Norm before input: 0.2552422881126404
Norm after input: 0.39930063486099243
Norm after each mp layer: 1.1267744302749634
Norm after each mp layer: 2.971489191055298
Norm after each mp layer: 9.585403442382812
Norm before input: 0.2552422881126404
Norm after input: 0.39930063486099243
Norm after each mp layer: 1.1267744302749634
Norm after each mp layer: 2.971489191055298
Norm after each mp layer: 9.585403442382812
Norm before input: 0.2552422881126404
Norm after input: 0.3992826044559479
Norm after each mp layer: 1.1265414953231812
Norm after each mp layer: 2.9721405506134033
Norm after each mp layer: 9.58414363861084
Epoch: 430, Loss: 0.0561, Energy: 289.5249, Train: 99.67%, Valid: 73.60%, Test: 69.60%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3992826044559479
Norm after each mp layer: 1.1265414953231812
Norm after each mp layer: 2.9721405506134033
Norm after each mp layer: 9.58414363861084
Norm before input: 0.2552422881126404
Norm after input: 0.39926519989967346
Norm after each mp layer: 1.1263108253479004
Norm after each mp layer: 2.9727694988250732
Norm after each mp layer: 9.582682609558105
Norm before input: 0.2552422881126404
Norm after input: 0.39926519989967346
Norm after each mp layer: 1.1263108253479004
Norm after each mp layer: 2.9727697372436523
Norm after each mp layer: 9.582682609558105
Norm before input: 0.2552422881126404
Norm after input: 0.39924997091293335
Norm after each mp layer: 1.1260669231414795
Norm after each mp layer: 2.9732143878936768
Norm after each mp layer: 9.579292297363281
Norm before input: 0.2552422881126404
Norm after input: 0.39924997091293335
Norm after each mp layer: 1.1260669231414795
Norm after each mp layer: 2.9732143878936768
Norm after each mp layer: 9.579292297363281
Norm before input: 0.2552422881126404
Norm after input: 0.39923396706581116
Norm after each mp layer: 1.1258422136306763
Norm after each mp layer: 2.9738223552703857
Norm after each mp layer: 9.577475547790527
Norm before input: 0.2552422881126404
Norm after input: 0.39923396706581116
Norm after each mp layer: 1.1258422136306763
Norm after each mp layer: 2.9738223552703857
Norm after each mp layer: 9.577475547790527
Norm before input: 0.2552422881126404
Norm after input: 0.39921796321868896
Norm after each mp layer: 1.1256283521652222
Norm after each mp layer: 2.9745028018951416
Norm after each mp layer: 9.576240539550781
Norm before input: 0.2552422881126404
Norm after input: 0.39921796321868896
Norm after each mp layer: 1.1256283521652222
Norm after each mp layer: 2.9745028018951416
Norm after each mp layer: 9.576240539550781
Norm before input: 0.2552422881126404
Norm after input: 0.3992040753364563
Norm after each mp layer: 1.1254032850265503
Norm after each mp layer: 2.9750053882598877
Norm after each mp layer: 9.572999954223633
Epoch: 435, Loss: 0.0548, Energy: 284.6817, Train: 99.67%, Valid: 73.60%, Test: 69.80%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3992040753364563
Norm after each mp layer: 1.1254032850265503
Norm after each mp layer: 2.9750053882598877
Norm after each mp layer: 9.572999954223633
Norm before input: 0.2552422881126404
Norm after input: 0.399189829826355
Norm after each mp layer: 1.125191569328308
Norm after each mp layer: 2.9755971431732178
Norm after each mp layer: 9.570603370666504
Norm before input: 0.2552422881126404
Norm after input: 0.399189829826355
Norm after each mp layer: 1.125191569328308
Norm after each mp layer: 2.9755971431732178
Norm after each mp layer: 9.570603370666504
Norm before input: 0.2552422881126404
Norm after input: 0.39917513728141785
Norm after each mp layer: 1.1249935626983643
Norm after each mp layer: 2.976283311843872
Norm after each mp layer: 9.569185256958008
Norm before input: 0.2552422881126404
Norm after input: 0.39917513728141785
Norm after each mp layer: 1.1249935626983643
Norm after each mp layer: 2.976283311843872
Norm after each mp layer: 9.569185256958008
Norm before input: 0.2552422881126404
Norm after input: 0.3991621732711792
Norm after each mp layer: 1.1247854232788086
Norm after each mp layer: 2.9768011569976807
Norm after each mp layer: 9.566031455993652
Norm before input: 0.2552422881126404
Norm after input: 0.3991621732711792
Norm after each mp layer: 1.1247854232788086
Norm after each mp layer: 2.9768011569976807
Norm after each mp layer: 9.566031455993652
Norm before input: 0.2552422881126404
Norm after input: 0.39914900064468384
Norm after each mp layer: 1.124585747718811
Norm after each mp layer: 2.977363348007202
Norm after each mp layer: 9.563324928283691
Norm before input: 0.2552422881126404
Norm after input: 0.39914900064468384
Norm after each mp layer: 1.124585747718811
Norm after each mp layer: 2.977363348007202
Norm after each mp layer: 9.563324928283691
Norm before input: 0.2552422881126404
Norm after input: 0.3991352915763855
Norm after each mp layer: 1.1244001388549805
Norm after each mp layer: 2.9780378341674805
Norm after each mp layer: 9.561708450317383
Epoch: 440, Loss: 0.0536, Energy: 279.8663, Train: 99.67%, Valid: 73.40%, Test: 69.70%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3991352915763855
Norm after each mp layer: 1.1244001388549805
Norm after each mp layer: 2.9780378341674805
Norm after each mp layer: 9.5617094039917
Norm before input: 0.2552422881126404
Norm after input: 0.39912307262420654
Norm after each mp layer: 1.1242071390151978
Norm after each mp layer: 2.9785823822021484
Norm after each mp layer: 9.558579444885254
Norm before input: 0.2552422881126404
Norm after input: 0.39912307262420654
Norm after each mp layer: 1.1242071390151978
Norm after each mp layer: 2.9785823822021484
Norm after each mp layer: 9.558579444885254
Norm before input: 0.2552422881126404
Norm after input: 0.3991110026836395
Norm after each mp layer: 1.1240217685699463
Norm after each mp layer: 2.9791653156280518
Norm after each mp layer: 9.55571460723877
Norm before input: 0.2552422881126404
Norm after input: 0.3991110026836395
Norm after each mp layer: 1.1240217685699463
Norm after each mp layer: 2.9791653156280518
Norm after each mp layer: 9.55571460723877
Norm before input: 0.2552422881126404
Norm after input: 0.3990985155105591
Norm after each mp layer: 1.1238505840301514
Norm after each mp layer: 2.9798593521118164
Norm after each mp layer: 9.553903579711914
Norm before input: 0.2552422881126404
Norm after input: 0.3990985155105591
Norm after each mp layer: 1.1238505840301514
Norm after each mp layer: 2.9798593521118164
Norm after each mp layer: 9.553903579711914
Norm before input: 0.2552422881126404
Norm after input: 0.3990873694419861
Norm after each mp layer: 1.123673915863037
Norm after each mp layer: 2.9804368019104004
Norm after each mp layer: 9.550787925720215
Norm before input: 0.2552422881126404
Norm after input: 0.3990873694419861
Norm after each mp layer: 1.123673915863037
Norm after each mp layer: 2.9804365634918213
Norm after each mp layer: 9.550787925720215
Norm before input: 0.2552422881126404
Norm after input: 0.3990764319896698
Norm after each mp layer: 1.123504877090454
Norm after each mp layer: 2.981046438217163
Norm after each mp layer: 9.547955513000488
Epoch: 445, Loss: 0.0524, Energy: 275.2386, Train: 99.67%, Valid: 73.20%, Test: 69.70%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3990764319896698
Norm after each mp layer: 1.123504877090454
Norm after each mp layer: 2.981046438217163
Norm after each mp layer: 9.547955513000488
Norm before input: 0.2552422881126404
Norm after input: 0.3990651071071625
Norm after each mp layer: 1.1233478784561157
Norm after each mp layer: 2.9817423820495605
Norm after each mp layer: 9.54594898223877
Norm before input: 0.2552422881126404
Norm after input: 0.3990651071071625
Norm after each mp layer: 1.1233478784561157
Norm after each mp layer: 2.9817426204681396
Norm after each mp layer: 9.54594898223877
Norm before input: 0.2552422881126404
Norm after input: 0.399055153131485
Norm after each mp layer: 1.123186469078064
Norm after each mp layer: 2.9823317527770996
Norm after each mp layer: 9.54267406463623
Norm before input: 0.2552422881126404
Norm after input: 0.399055153131485
Norm after each mp layer: 1.123186469078064
Norm after each mp layer: 2.9823317527770996
Norm after each mp layer: 9.54267406463623
Norm before input: 0.2552422881126404
Norm after input: 0.3990452289581299
Norm after each mp layer: 1.123033881187439
Norm after each mp layer: 2.9829702377319336
Norm after each mp layer: 9.539782524108887
Norm before input: 0.2552422881126404
Norm after input: 0.3990452289581299
Norm after each mp layer: 1.123033881187439
Norm after each mp layer: 2.9829702377319336
Norm after each mp layer: 9.539782524108887
Norm before input: 0.2552422881126404
Norm after input: 0.39903512597084045
Norm after each mp layer: 1.1228907108306885
Norm after each mp layer: 2.9836721420288086
Norm after each mp layer: 9.537444114685059
Norm before input: 0.2552422881126404
Norm after input: 0.39903512597084045
Norm after each mp layer: 1.1228907108306885
Norm after each mp layer: 2.9836721420288086
Norm after each mp layer: 9.537444114685059
Norm before input: 0.2552422881126404
Norm after input: 0.39902615547180176
Norm after each mp layer: 1.1227439641952515
Norm after each mp layer: 2.98427677154541
Norm after each mp layer: 9.533994674682617
Epoch: 450, Loss: 0.0512, Energy: 270.7932, Train: 99.67%, Valid: 73.40%, Test: 69.40%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.39902615547180176
Norm after each mp layer: 1.1227439641952515
Norm after each mp layer: 2.98427677154541
Norm after each mp layer: 9.533994674682617
Norm before input: 0.2552422881126404
Norm after input: 0.39901700615882874
Norm after each mp layer: 1.1226067543029785
Norm after each mp layer: 2.984945774078369
Norm after each mp layer: 9.531197547912598
Norm before input: 0.2552422881126404
Norm after input: 0.39901700615882874
Norm after each mp layer: 1.1226067543029785
Norm after each mp layer: 2.984945774078369
Norm after each mp layer: 9.531197547912598
Norm before input: 0.2552422881126404
Norm after input: 0.3990079164505005
Norm after each mp layer: 1.122475028038025
Norm after each mp layer: 2.985635995864868
Norm after each mp layer: 9.528589248657227
Norm before input: 0.2552422881126404
Norm after input: 0.3990079164505005
Norm after each mp layer: 1.122475028038025
Norm after each mp layer: 2.985635995864868
Norm after each mp layer: 9.528589248657227
Norm before input: 0.2552422881126404
Norm after input: 0.3989996016025543
Norm after each mp layer: 1.1223409175872803
Norm after each mp layer: 2.9862473011016846
Norm after each mp layer: 9.525059700012207
Norm before input: 0.2552422881126404
Norm after input: 0.3989996016025543
Norm after each mp layer: 1.1223409175872803
Norm after each mp layer: 2.9862473011016846
Norm after each mp layer: 9.525059700012207
Norm before input: 0.2552422881126404
Norm after input: 0.3989909589290619
Norm after each mp layer: 1.1222172975540161
Norm after each mp layer: 2.9869422912597656
Norm after each mp layer: 9.522320747375488
Norm before input: 0.2552422881126404
Norm after input: 0.3989909589290619
Norm after each mp layer: 1.1222172975540161
Norm after each mp layer: 2.9869422912597656
Norm after each mp layer: 9.522320747375488
Norm before input: 0.2552422881126404
Norm after input: 0.3989827632904053
Norm after each mp layer: 1.1220959424972534
Norm after each mp layer: 2.9876198768615723
Norm after each mp layer: 9.519266128540039
Epoch: 455, Loss: 0.0501, Energy: 266.4586, Train: 99.67%, Valid: 72.80%, Test: 69.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3989827632904053
Norm after each mp layer: 1.1220959424972534
Norm after each mp layer: 2.9876198768615723
Norm after each mp layer: 9.519266128540039
Norm before input: 0.2552422881126404
Norm after input: 0.39897528290748596
Norm after each mp layer: 1.1219756603240967
Norm after each mp layer: 2.988258123397827
Norm after each mp layer: 9.515666961669922
Norm before input: 0.2552422881126404
Norm after input: 0.39897528290748596
Norm after each mp layer: 1.1219756603240967
Norm after each mp layer: 2.988258123397827
Norm after each mp layer: 9.515666961669922
Norm before input: 0.2552422881126404
Norm after input: 0.3989675045013428
Norm after each mp layer: 1.1218658685684204
Norm after each mp layer: 2.988974094390869
Norm after each mp layer: 9.512829780578613
Norm before input: 0.2552422881126404
Norm after input: 0.3989675045013428
Norm after each mp layer: 1.1218658685684204
Norm after each mp layer: 2.988974094390869
Norm after each mp layer: 9.512829780578613
Norm before input: 0.2552422881126404
Norm after input: 0.39896056056022644
Norm after each mp layer: 1.1217552423477173
Norm after each mp layer: 2.989631175994873
Norm after each mp layer: 9.509286880493164
Norm before input: 0.2552422881126404
Norm after input: 0.39896056056022644
Norm after each mp layer: 1.1217552423477173
Norm after each mp layer: 2.989631175994873
Norm after each mp layer: 9.509286880493164
Norm before input: 0.2552422881126404
Norm after input: 0.3989538550376892
Norm after each mp layer: 1.1216492652893066
Norm after each mp layer: 2.990298271179199
Norm after each mp layer: 9.505780220031738
Norm before input: 0.2552422881126404
Norm after input: 0.3989538550376892
Norm after each mp layer: 1.1216492652893066
Norm after each mp layer: 2.990298271179199
Norm after each mp layer: 9.505780220031738
Norm before input: 0.2552422881126404
Norm after input: 0.3989470601081848
Norm after each mp layer: 1.1215499639511108
Norm after each mp layer: 2.991011142730713
Norm after each mp layer: 9.502676963806152
Epoch: 460, Loss: 0.0491, Energy: 262.2183, Train: 99.67%, Valid: 72.60%, Test: 69.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3989470601081848
Norm after each mp layer: 1.1215499639511108
Norm after each mp layer: 2.991011142730713
Norm after each mp layer: 9.502676010131836
Norm before input: 0.2552422881126404
Norm after input: 0.3989410102367401
Norm after each mp layer: 1.1214492321014404
Norm after each mp layer: 2.9916625022888184
Norm after each mp layer: 9.498785018920898
Norm before input: 0.2552422881126404
Norm after input: 0.3989410102367401
Norm after each mp layer: 1.1214492321014404
Norm after each mp layer: 2.9916625022888184
Norm after each mp layer: 9.498785018920898
Norm before input: 0.2552422881126404
Norm after input: 0.39893484115600586
Norm after each mp layer: 1.121356725692749
Norm after each mp layer: 2.992372512817383
Norm after each mp layer: 9.495420455932617
Norm before input: 0.2552422881126404
Norm after input: 0.39893484115600586
Norm after each mp layer: 1.1213566064834595
Norm after each mp layer: 2.992372512817383
Norm after each mp layer: 9.495420455932617
Norm before input: 0.2552422881126404
Norm after input: 0.39892902970314026
Norm after each mp layer: 1.121267318725586
Norm after each mp layer: 2.9930710792541504
Norm after each mp layer: 9.49182415008545
Norm before input: 0.2552422881126404
Norm after input: 0.39892902970314026
Norm after each mp layer: 1.121267318725586
Norm after each mp layer: 2.9930710792541504
Norm after each mp layer: 9.49182415008545
Norm before input: 0.2552422881126404
Norm after input: 0.39892369508743286
Norm after each mp layer: 1.121180772781372
Norm after each mp layer: 2.993748903274536
Norm after each mp layer: 9.487907409667969
Norm before input: 0.2552422881126404
Norm after input: 0.39892369508743286
Norm after each mp layer: 1.121180772781372
Norm after each mp layer: 2.993748903274536
Norm after each mp layer: 9.487907409667969
Norm before input: 0.2552422881126404
Norm after input: 0.3989182114601135
Norm after each mp layer: 1.1211026906967163
Norm after each mp layer: 2.994485855102539
Norm after each mp layer: 9.48454475402832
Epoch: 465, Loss: 0.0481, Energy: 258.0739, Train: 99.67%, Valid: 72.60%, Test: 69.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3989182114601135
Norm after each mp layer: 1.1211026906967163
Norm after each mp layer: 2.994485855102539
Norm after each mp layer: 9.48454475402832
Norm before input: 0.2552422881126404
Norm after input: 0.3989133834838867
Norm after each mp layer: 1.1210254430770874
Norm after each mp layer: 2.995176076889038
Norm after each mp layer: 9.480547904968262
Norm before input: 0.2552422881126404
Norm after input: 0.3989133834838867
Norm after each mp layer: 1.1210254430770874
Norm after each mp layer: 2.995176076889038
Norm after each mp layer: 9.480547904968262
Norm before input: 0.2552422881126404
Norm after input: 0.3989085555076599
Norm after each mp layer: 1.1209551095962524
Norm after each mp layer: 2.9959118366241455
Norm after each mp layer: 9.476926803588867
Norm before input: 0.2552422881126404
Norm after input: 0.3989085555076599
Norm after each mp layer: 1.1209551095962524
Norm after each mp layer: 2.9959118366241455
Norm after each mp layer: 9.476926803588867
Norm before input: 0.2552422881126404
Norm after input: 0.39890405535697937
Norm after each mp layer: 1.1208889484405518
Norm after each mp layer: 2.996650457382202
Norm after each mp layer: 9.473207473754883
Norm before input: 0.2552422881126404
Norm after input: 0.39890405535697937
Norm after each mp layer: 1.1208889484405518
Norm after each mp layer: 2.996650457382202
Norm after each mp layer: 9.473207473754883
Norm before input: 0.2552422881126404
Norm after input: 0.39890000224113464
Norm after each mp layer: 1.1208254098892212
Norm after each mp layer: 2.997373580932617
Norm after each mp layer: 9.469197273254395
Norm before input: 0.2552422881126404
Norm after input: 0.39890000224113464
Norm after each mp layer: 1.1208254098892212
Norm after each mp layer: 2.997373580932617
Norm after each mp layer: 9.469197273254395
Norm before input: 0.2552422881126404
Norm after input: 0.39889591932296753
Norm after each mp layer: 1.120769739151001
Norm after each mp layer: 2.9981467723846436
Norm after each mp layer: 9.465655326843262
Epoch: 470, Loss: 0.0471, Energy: 254.0665, Train: 99.67%, Valid: 72.60%, Test: 69.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.39889591932296753
Norm after each mp layer: 1.120769739151001
Norm after each mp layer: 2.9981467723846436
Norm after each mp layer: 9.465655326843262
Norm before input: 0.2552422881126404
Norm after input: 0.3988925516605377
Norm after each mp layer: 1.1207152605056763
Norm after each mp layer: 2.9988794326782227
Norm after each mp layer: 9.461557388305664
Norm before input: 0.2552422881126404
Norm after input: 0.3988925516605377
Norm after each mp layer: 1.1207152605056763
Norm after each mp layer: 2.9988794326782227
Norm after each mp layer: 9.461557388305664
Norm before input: 0.2552422881126404
Norm after input: 0.3988891839981079
Norm after each mp layer: 1.1206681728363037
Norm after each mp layer: 2.9996635913848877
Norm after each mp layer: 9.457907676696777
Norm before input: 0.2552422881126404
Norm after input: 0.3988891839981079
Norm after each mp layer: 1.1206681728363037
Norm after each mp layer: 2.9996635913848877
Norm after each mp layer: 9.457907676696777
Norm before input: 0.2552422881126404
Norm after input: 0.398886114358902
Norm after each mp layer: 1.12062406539917
Norm after each mp layer: 3.000433921813965
Norm after each mp layer: 9.453961372375488
Norm before input: 0.2552422881126404
Norm after input: 0.398886114358902
Norm after each mp layer: 1.12062406539917
Norm after each mp layer: 3.000433921813965
Norm after each mp layer: 9.453961372375488
Norm before input: 0.2552422881126404
Norm after input: 0.39888322353363037
Norm after each mp layer: 1.120584487915039
Norm after each mp layer: 3.0012192726135254
Norm after each mp layer: 9.450035095214844
Norm before input: 0.2552422881126404
Norm after input: 0.39888322353363037
Norm after each mp layer: 1.120584487915039
Norm after each mp layer: 3.0012192726135254
Norm after each mp layer: 9.450035095214844
Norm before input: 0.2552422881126404
Norm after input: 0.3988805115222931
Norm after each mp layer: 1.120550274848938
Norm after each mp layer: 3.0020289421081543
Norm after each mp layer: 9.446250915527344
Epoch: 475, Loss: 0.0462, Energy: 250.1989, Train: 99.67%, Valid: 72.40%, Test: 69.40%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3988805115222931
Norm after each mp layer: 1.120550274848938
Norm after each mp layer: 3.0020289421081543
Norm after each mp layer: 9.446250915527344
Norm before input: 0.2552422881126404
Norm after input: 0.3988782465457916
Norm after each mp layer: 1.1205182075500488
Norm after each mp layer: 3.002814769744873
Norm after each mp layer: 9.442096710205078
Norm before input: 0.2552422881126404
Norm after input: 0.3988782465457916
Norm after each mp layer: 1.1205182075500488
Norm after each mp layer: 3.002814769744873
Norm after each mp layer: 9.442096710205078
Norm before input: 0.2552422881126404
Norm after input: 0.3988761007785797
Norm after each mp layer: 1.1204934120178223
Norm after each mp layer: 3.0036473274230957
Norm after each mp layer: 9.438373565673828
Norm before input: 0.2552422881126404
Norm after input: 0.3988761007785797
Norm after each mp layer: 1.1204934120178223
Norm after each mp layer: 3.0036473274230957
Norm after each mp layer: 9.438373565673828
Norm before input: 0.2552422881126404
Norm after input: 0.3988744616508484
Norm after each mp layer: 1.120469570159912
Norm after each mp layer: 3.0044424533843994
Norm after each mp layer: 9.434120178222656
Norm before input: 0.2552422881126404
Norm after input: 0.3988744616508484
Norm after each mp layer: 1.120469570159912
Norm after each mp layer: 3.0044424533843994
Norm after each mp layer: 9.434120178222656
Norm before input: 0.2552422881126404
Norm after input: 0.39887285232543945
Norm after each mp layer: 1.120453119277954
Norm after each mp layer: 3.005293846130371
Norm after each mp layer: 9.430366516113281
Norm before input: 0.2552422881126404
Norm after input: 0.39887285232543945
Norm after each mp layer: 1.120453119277954
Norm after each mp layer: 3.005293846130371
Norm after each mp layer: 9.430366516113281
Norm before input: 0.2552422881126404
Norm after input: 0.39887163043022156
Norm after each mp layer: 1.1204379796981812
Norm after each mp layer: 3.0061120986938477
Norm after each mp layer: 9.42609691619873
Epoch: 480, Loss: 0.0453, Energy: 246.4498, Train: 99.67%, Valid: 72.20%, Test: 69.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.39887163043022156
Norm after each mp layer: 1.1204379796981812
Norm after each mp layer: 3.0061120986938477
Norm after each mp layer: 9.42609691619873
Norm before input: 0.2552422881126404
Norm after input: 0.39887040853500366
Norm after each mp layer: 1.120429515838623
Norm after each mp layer: 3.0069825649261475
Norm after each mp layer: 9.42227840423584
Norm before input: 0.2552422881126404
Norm after input: 0.39887040853500366
Norm after each mp layer: 1.120429515838623
Norm after each mp layer: 3.0069825649261475
Norm after each mp layer: 9.42227840423584
Norm before input: 0.2552422881126404
Norm after input: 0.39886975288391113
Norm after each mp layer: 1.120422601699829
Norm after each mp layer: 3.0078227519989014
Norm after each mp layer: 9.417999267578125
Norm before input: 0.2552422881126404
Norm after input: 0.39886975288391113
Norm after each mp layer: 1.120422601699829
Norm after each mp layer: 3.0078227519989014
Norm after each mp layer: 9.417999267578125
Norm before input: 0.2552422881126404
Norm after input: 0.3988690674304962
Norm after each mp layer: 1.120422601699829
Norm after each mp layer: 3.008707046508789
Norm after each mp layer: 9.414124488830566
Norm before input: 0.2552422881126404
Norm after input: 0.3988690674304962
Norm after each mp layer: 1.120422601699829
Norm after each mp layer: 3.008707046508789
Norm after each mp layer: 9.414124488830566
Norm before input: 0.2552422881126404
Norm after input: 0.3988688886165619
Norm after each mp layer: 1.1204240322113037
Norm after each mp layer: 3.0095622539520264
Norm after each mp layer: 9.409796714782715
Norm before input: 0.2552422881126404
Norm after input: 0.3988688886165619
Norm after each mp layer: 1.1204240322113037
Norm after each mp layer: 3.0095622539520264
Norm after each mp layer: 9.409796714782715
Norm before input: 0.2552422881126404
Norm after input: 0.3988686501979828
Norm after each mp layer: 1.1204324960708618
Norm after each mp layer: 3.010467767715454
Norm after each mp layer: 9.405928611755371
Epoch: 485, Loss: 0.0444, Energy: 242.7825, Train: 99.67%, Valid: 72.00%, Test: 69.30%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3988686501979828
Norm after each mp layer: 1.1204324960708618
Norm after each mp layer: 3.010467767715454
Norm after each mp layer: 9.405928611755371
Norm before input: 0.2552422881126404
Norm after input: 0.39886897802352905
Norm after each mp layer: 1.1204415559768677
Norm after each mp layer: 3.011336326599121
Norm after each mp layer: 9.4014892578125
Norm before input: 0.2552422881126404
Norm after input: 0.39886897802352905
Norm after each mp layer: 1.1204415559768677
Norm after each mp layer: 3.011336326599121
Norm after each mp layer: 9.4014892578125
Norm before input: 0.2552422881126404
Norm after input: 0.3988690674304962
Norm after each mp layer: 1.1204588413238525
Norm after each mp layer: 3.012277126312256
Norm after each mp layer: 9.397751808166504
Norm before input: 0.2552422881126404
Norm after input: 0.3988690674304962
Norm after each mp layer: 1.1204588413238525
Norm after each mp layer: 3.012277126312256
Norm after each mp layer: 9.397751808166504
Norm before input: 0.2552422881126404
Norm after input: 0.39886996150016785
Norm after each mp layer: 1.1204743385314941
Norm after each mp layer: 3.0131444931030273
Norm after each mp layer: 9.393036842346191
Norm before input: 0.2552422881126404
Norm after input: 0.39886996150016785
Norm after each mp layer: 1.1204743385314941
Norm after each mp layer: 3.0131444931030273
Norm after each mp layer: 9.393036842346191
Norm before input: 0.2552422881126404
Norm after input: 0.3988703191280365
Norm after each mp layer: 1.12050199508667
Norm after each mp layer: 3.01413893699646
Norm after each mp layer: 9.389681816101074
Norm before input: 0.2552422881126404
Norm after input: 0.3988703191280365
Norm after each mp layer: 1.12050199508667
Norm after each mp layer: 3.01413893699646
Norm after each mp layer: 9.389681816101074
Norm before input: 0.2552422881126404
Norm after input: 0.398872047662735
Norm after each mp layer: 1.120521903038025
Norm after each mp layer: 3.014968156814575
Norm after each mp layer: 9.384268760681152
Epoch: 490, Loss: 0.0436, Energy: 239.2494, Train: 99.67%, Valid: 72.00%, Test: 69.20%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.398872047662735
Norm after each mp layer: 1.120521903038025
Norm after each mp layer: 3.014968156814575
Norm after each mp layer: 9.384268760681152
Norm before input: 0.2552422881126404
Norm after input: 0.3988723158836365
Norm after each mp layer: 1.1205633878707886
Norm after each mp layer: 3.01607608795166
Norm after each mp layer: 9.38198471069336
Norm before input: 0.2552422881126404
Norm after input: 0.3988723158836365
Norm after each mp layer: 1.1205633878707886
Norm after each mp layer: 3.01607608795166
Norm after each mp layer: 9.38198471069336
Norm before input: 0.2552422881126404
Norm after input: 0.3988754153251648
Norm after each mp layer: 1.1205811500549316
Norm after each mp layer: 3.016772747039795
Norm after each mp layer: 9.374725341796875
Norm before input: 0.2552422881126404
Norm after input: 0.3988754153251648
Norm after each mp layer: 1.1205811500549316
Norm after each mp layer: 3.016772747039795
Norm after each mp layer: 9.374725341796875
Norm before input: 0.2552422881126404
Norm after input: 0.398874431848526
Norm after each mp layer: 1.1206480264663696
Norm after each mp layer: 3.0181689262390137
Norm after each mp layer: 9.375572204589844
Norm before input: 0.2552422881126404
Norm after input: 0.398874431848526
Norm after each mp layer: 1.1206480264663696
Norm after each mp layer: 3.0181689262390137
Norm after each mp layer: 9.375572204589844
Norm before input: 0.2552422881126404
Norm after input: 0.3988806903362274
Norm after each mp layer: 1.1206434965133667
Norm after each mp layer: 3.018429756164551
Norm after each mp layer: 9.362902641296387
Norm before input: 0.2552422881126404
Norm after input: 0.3988806903362274
Norm after each mp layer: 1.1206434965133667
Norm after each mp layer: 3.018429756164551
Norm after each mp layer: 9.362902641296387
Norm before input: 0.2552422881126404
Norm after input: 0.3988751471042633
Norm after each mp layer: 1.120771884918213
Norm after each mp layer: 3.020674467086792
Norm after each mp layer: 9.373649597167969
Epoch: 495, Loss: 0.0428, Energy: 235.6963, Train: 99.67%, Valid: 72.00%, Test: 69.20%, Best Valid: 80.20%, Best Test: 81.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3988751471042633
Norm after each mp layer: 1.120771884918213
Norm after each mp layer: 3.020674467086792
Norm after each mp layer: 9.373649597167969
Norm before input: 0.2552422881126404
Norm after input: 0.39889097213745117
Norm after each mp layer: 1.120681881904602
Norm after each mp layer: 3.019493341445923
Norm after each mp layer: 9.34381103515625
Norm before input: 0.2552422881126404
Norm after input: 0.39889097213745117
Norm after each mp layer: 1.120681881904602
Norm after each mp layer: 3.019493341445923
Norm after each mp layer: 9.34381103515625
Norm before input: 0.2552422881126404
Norm after input: 0.3988693356513977
Norm after each mp layer: 1.1210191249847412
Norm after each mp layer: 3.024934768676758
Norm after each mp layer: 9.394356727600098
Norm before input: 0.2552422881126404
Norm after input: 0.3988693356513977
Norm after each mp layer: 1.1210191249847412
Norm after each mp layer: 3.0249345302581787
Norm after each mp layer: 9.394356727600098
Norm before input: 0.2552422881126404
Norm after input: 0.3989264965057373
Norm after each mp layer: 1.1205552816390991
Norm after each mp layer: 3.0176689624786377
Norm after each mp layer: 9.296859741210938
Norm before input: 0.2552422881126404
Norm after input: 0.3989264965057373
Norm after each mp layer: 1.1205552816390991
Norm after each mp layer: 3.0176689624786377
Norm after each mp layer: 9.296860694885254
Norm before input: 0.2552422881126404
Norm after input: 0.3993189334869385
Norm after each mp layer: 1.1409883499145508
Norm after each mp layer: 3.276834487915039
Norm after each mp layer: 12.579910278320312
train_accuracy_list: [0.28228476821192056, 0.28228476821192056, 0.16225165562913907, 0.16225165562913907, 0.16225165562913907, 0.28228476821192056, 0.28228476821192056, 0.28228476821192056, 0.28228476821192056, 0.20281456953642385, 0.16225165562913907, 0.2293046357615894, 0.28311258278145696, 0.28228476821192056, 0.28311258278145696, 0.30960264900662254, 0.29304635761589404, 0.16390728476821192, 0.1630794701986755, 0.28807947019867547, 0.3253311258278146, 0.3253311258278146, 0.2889072847682119, 0.28642384105960267, 0.28807947019867547, 0.34602649006622516, 0.1837748344370861, 0.2533112582781457, 0.28807947019867547, 0.2913907284768212, 0.2889072847682119, 0.2855960264900662, 0.3220198675496689, 0.4056291390728477, 0.2947019867549669, 0.38741721854304634, 0.4925496688741722, 0.5256622516556292, 0.4776490066225166, 0.4544701986754967, 0.581953642384106, 0.41721854304635764, 0.47102649006622516, 0.6473509933774835, 0.5240066225165563, 0.5496688741721855, 0.5894039735099338, 0.515728476821192, 0.5579470198675497, 0.6738410596026491, 0.6192052980132451, 0.6456953642384106, 0.7367549668874173, 0.6804635761589404, 0.6746688741721855, 0.6846026490066225, 0.7243377483443708, 0.7715231788079471, 0.7574503311258278, 0.7408940397350994, 0.7284768211920529, 0.7392384105960265, 0.7682119205298014, 0.7897350993377483, 0.7855960264900662, 0.777317880794702, 0.8062913907284768, 0.8269867549668874, 0.8269867549668874, 0.8220198675496688, 0.8195364238410596, 0.8178807947019867, 0.8352649006622517, 0.8377483443708609, 0.8485099337748344, 0.8427152317880795, 0.8551324503311258, 0.859271523178808, 0.8625827814569537, 0.8675496688741722, 0.8683774834437086, 0.8758278145695364, 0.8857615894039735, 0.8791390728476821, 0.8832781456953642, 0.8915562913907285, 0.8923841059602649, 0.8923841059602649, 0.8973509933774835, 0.9048013245033113, 0.9097682119205298, 0.9139072847682119, 0.9188741721854304, 0.9263245033112583, 0.9271523178807947, 0.9279801324503312, 0.9263245033112583, 0.9271523178807947, 0.9288079470198676, 0.9329470198675497, 0.9354304635761589, 0.9362582781456954, 0.9420529801324503, 0.9470198675496688, 0.9470198675496688, 0.9478476821192053, 0.9519867549668874, 0.9519867549668874, 0.9552980132450332, 0.9586092715231788, 0.956953642384106, 0.9602649006622517, 0.9627483443708609, 0.9652317880794702, 0.9652317880794702, 0.9660596026490066, 0.9668874172185431, 0.9619205298013245, 0.9652317880794702, 0.9031456953642384, 0.7690397350993378, 0.8675496688741722, 0.8236754966887417, 0.9221854304635762, 0.8634105960264901, 0.8286423841059603, 0.9139072847682119, 0.945364238410596, 0.9205298013245033, 0.9147350993377483, 0.9246688741721855, 0.9478476821192053, 0.9428807947019867, 0.9445364238410596, 0.9511589403973509, 0.9511589403973509, 0.945364238410596, 0.9420529801324503, 0.9486754966887417, 0.9610927152317881, 0.9627483443708609, 0.9577814569536424, 0.956953642384106, 0.9610927152317881, 0.9594370860927153, 0.9602649006622517, 0.9586092715231788, 0.9627483443708609, 0.9619205298013245, 0.9610927152317881, 0.9561258278145696, 0.9644039735099338, 0.9668874172185431, 0.9685430463576159, 0.9644039735099338, 0.9668874172185431, 0.9677152317880795, 0.9635761589403974, 0.9644039735099338, 0.9677152317880795, 0.9685430463576159, 0.9693708609271523, 0.9693708609271523, 0.9718543046357616, 0.972682119205298, 0.9743377483443708, 0.9751655629139073, 0.9784768211920529, 0.9776490066225165, 0.9768211920529801, 0.9793046357615894, 0.9776490066225165, 0.9784768211920529, 0.9768211920529801, 0.9776490066225165, 0.9776490066225165, 0.9784768211920529, 0.9784768211920529, 0.9776490066225165, 0.9768211920529801, 0.9776490066225165, 0.9776490066225165, 0.9776490066225165, 0.9793046357615894, 0.9784768211920529, 0.9793046357615894, 0.9809602649006622, 0.9809602649006622, 0.9817880794701986, 0.9817880794701986, 0.9817880794701986, 0.9817880794701986, 0.9817880794701986, 0.9826158940397351, 0.9826158940397351, 0.9826158940397351, 0.9817880794701986, 0.9817880794701986, 0.9826158940397351, 0.9826158940397351, 0.9826158940397351, 0.9826158940397351, 0.9834437086092715, 0.9834437086092715, 0.9834437086092715, 0.9834437086092715, 0.9834437086092715, 0.984271523178808, 0.9834437086092715, 0.9834437086092715, 0.9834437086092715, 0.9834437086092715, 0.984271523178808, 0.9859271523178808, 0.9867549668874173, 0.9867549668874173, 0.9867549668874173, 0.9867549668874173, 0.9867549668874173, 0.9859271523178808, 0.9859271523178808, 0.9859271523178808, 0.9867549668874173, 0.9867549668874173, 0.9875827814569537, 0.9892384105960265, 0.9892384105960265, 0.9892384105960265, 0.9900662251655629, 0.9900662251655629, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9917218543046358, 0.9917218543046358, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9925496688741722, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9933774834437086, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9950331125827815, 0.9950331125827815, 0.9950331125827815, 0.9950331125827815, 0.9950331125827815, 0.9950331125827815, 0.9884105960264901, 0.7284768211920529, 0.5165562913907285, 0.5165562913907285, 0.8269867549668874, 0.6614238410596026, 0.847682119205298, 0.8874172185430463, 0.9089403973509934, 0.9230132450331126, 0.9354304635761589, 0.9072847682119205, 0.8807947019867549, 0.9511589403973509, 0.9362582781456954, 0.9139072847682119, 0.9412251655629139, 0.9387417218543046, 0.9495033112582781, 0.9470198675496688, 0.9354304635761589, 0.9528145695364238, 0.9536423841059603, 0.9511589403973509, 0.9552980132450332, 0.9677152317880795, 0.9619205298013245, 0.9635761589403974, 0.9718543046357616, 0.9735099337748344, 0.9701986754966887, 0.9685430463576159, 0.9735099337748344, 0.9735099337748344, 0.9701986754966887, 0.9693708609271523, 0.9759933774834437, 0.9768211920529801, 0.9743377483443708, 0.9759933774834437, 0.9793046357615894, 0.9817880794701986, 0.9834437086092715, 0.984271523178808, 0.9809602649006622, 0.9826158940397351, 0.9867549668874173, 0.9850993377483444, 0.9859271523178808, 0.984271523178808, 0.9826158940397351, 0.984271523178808, 0.9875827814569537, 0.9867549668874173, 0.9875827814569537, 0.9900662251655629, 0.9892384105960265, 0.9892384105960265, 0.9900662251655629, 0.9892384105960265, 0.9900662251655629, 0.9900662251655629, 0.9892384105960265, 0.9900662251655629, 0.9908940397350994, 0.9917218543046358, 0.9908940397350994, 0.9908940397350994, 0.9900662251655629, 0.9900662251655629, 0.9900662251655629, 0.9917218543046358, 0.9908940397350994, 0.9900662251655629, 0.9900662251655629, 0.9900662251655629, 0.9917218543046358, 0.9925496688741722, 0.9917218543046358, 0.9908940397350994, 0.9908940397350994, 0.9925496688741722, 0.9933774834437086, 0.9933774834437086, 0.9917218543046358, 0.9917218543046358, 0.9917218543046358, 0.9933774834437086, 0.9933774834437086, 0.9908940397350994, 0.9908940397350994, 0.9908940397350994, 0.9942052980132451, 0.9942052980132451, 0.9933774834437086, 0.9925496688741722, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9933774834437086, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9942052980132451, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9958609271523179, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9966887417218543, 0.9975165562913907, 0.9966887417218543, 0.8451986754966887]
valid_accuracy_list: [0.316, 0.316, 0.156, 0.156, 0.156, 0.316, 0.316, 0.316, 0.316, 0.236, 0.162, 0.206, 0.3, 0.316, 0.316, 0.334, 0.322, 0.162, 0.162, 0.318, 0.348, 0.35, 0.318, 0.318, 0.318, 0.362, 0.176, 0.222, 0.316, 0.318, 0.318, 0.318, 0.354, 0.428, 0.328, 0.408, 0.45, 0.476, 0.498, 0.464, 0.56, 0.442, 0.476, 0.582, 0.488, 0.5, 0.588, 0.504, 0.534, 0.618, 0.618, 0.64, 0.694, 0.658, 0.656, 0.656, 0.696, 0.726, 0.71, 0.686, 0.662, 0.68, 0.718, 0.72, 0.708, 0.704, 0.72, 0.74, 0.736, 0.722, 0.726, 0.736, 0.736, 0.734, 0.748, 0.742, 0.75, 0.74, 0.754, 0.754, 0.756, 0.748, 0.754, 0.756, 0.748, 0.754, 0.758, 0.756, 0.754, 0.756, 0.76, 0.76, 0.76, 0.758, 0.758, 0.764, 0.758, 0.76, 0.756, 0.76, 0.756, 0.758, 0.756, 0.756, 0.76, 0.758, 0.758, 0.758, 0.76, 0.758, 0.758, 0.756, 0.748, 0.752, 0.744, 0.746, 0.748, 0.74, 0.752, 0.678, 0.638, 0.684, 0.634, 0.754, 0.712, 0.694, 0.764, 0.776, 0.756, 0.758, 0.764, 0.778, 0.762, 0.78, 0.798, 0.796, 0.798, 0.79, 0.79, 0.796, 0.8, 0.802, 0.794, 0.8, 0.794, 0.796, 0.792, 0.802, 0.798, 0.792, 0.792, 0.784, 0.782, 0.78, 0.776, 0.778, 0.776, 0.778, 0.782, 0.778, 0.776, 0.778, 0.778, 0.78, 0.782, 0.784, 0.784, 0.78, 0.784, 0.784, 0.782, 0.774, 0.774, 0.776, 0.776, 0.766, 0.77, 0.772, 0.772, 0.772, 0.768, 0.766, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.772, 0.77, 0.768, 0.768, 0.766, 0.766, 0.766, 0.768, 0.766, 0.768, 0.768, 0.766, 0.766, 0.768, 0.768, 0.768, 0.766, 0.766, 0.766, 0.762, 0.764, 0.764, 0.764, 0.762, 0.76, 0.76, 0.758, 0.76, 0.758, 0.758, 0.758, 0.758, 0.756, 0.756, 0.756, 0.754, 0.754, 0.754, 0.754, 0.752, 0.75, 0.752, 0.752, 0.752, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.75, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.746, 0.746, 0.744, 0.744, 0.746, 0.744, 0.744, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.744, 0.742, 0.744, 0.74, 0.744, 0.738, 0.736, 0.572, 0.386, 0.418, 0.654, 0.552, 0.726, 0.762, 0.756, 0.756, 0.762, 0.726, 0.736, 0.78, 0.768, 0.744, 0.76, 0.762, 0.768, 0.766, 0.758, 0.776, 0.778, 0.768, 0.784, 0.784, 0.77, 0.776, 0.78, 0.782, 0.778, 0.774, 0.772, 0.764, 0.774, 0.77, 0.776, 0.782, 0.782, 0.782, 0.774, 0.772, 0.782, 0.786, 0.78, 0.78, 0.78, 0.78, 0.782, 0.782, 0.784, 0.784, 0.78, 0.77, 0.768, 0.782, 0.782, 0.782, 0.78, 0.77, 0.768, 0.772, 0.772, 0.772, 0.77, 0.766, 0.766, 0.768, 0.766, 0.766, 0.766, 0.762, 0.762, 0.764, 0.766, 0.766, 0.762, 0.762, 0.762, 0.764, 0.766, 0.762, 0.76, 0.76, 0.76, 0.762, 0.76, 0.76, 0.758, 0.762, 0.764, 0.76, 0.76, 0.756, 0.754, 0.754, 0.756, 0.758, 0.758, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.754, 0.752, 0.752, 0.75, 0.75, 0.748, 0.748, 0.748, 0.75, 0.748, 0.748, 0.748, 0.748, 0.746, 0.744, 0.744, 0.744, 0.744, 0.744, 0.74, 0.738, 0.736, 0.736, 0.736, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734, 0.732, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734, 0.732, 0.73, 0.728, 0.728, 0.728, 0.728, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.724, 0.724, 0.724, 0.724, 0.724, 0.722, 0.722, 0.722, 0.722, 0.722, 0.722, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.72, 0.718, 0.718, 0.626]
test_accuracy_list: [0.319, 0.319, 0.144, 0.144, 0.144, 0.319, 0.319, 0.319, 0.319, 0.223, 0.15, 0.205, 0.317, 0.319, 0.321, 0.341, 0.328, 0.151, 0.151, 0.323, 0.365, 0.363, 0.326, 0.322, 0.328, 0.373, 0.169, 0.236, 0.325, 0.337, 0.334, 0.327, 0.351, 0.41, 0.337, 0.402, 0.438, 0.475, 0.483, 0.472, 0.571, 0.435, 0.463, 0.599, 0.486, 0.507, 0.577, 0.504, 0.525, 0.634, 0.612, 0.64, 0.699, 0.659, 0.644, 0.647, 0.686, 0.713, 0.716, 0.696, 0.683, 0.696, 0.717, 0.731, 0.718, 0.72, 0.735, 0.744, 0.736, 0.728, 0.738, 0.735, 0.733, 0.734, 0.737, 0.741, 0.743, 0.741, 0.746, 0.751, 0.751, 0.753, 0.757, 0.751, 0.743, 0.756, 0.762, 0.755, 0.761, 0.772, 0.765, 0.769, 0.77, 0.759, 0.772, 0.77, 0.766, 0.77, 0.774, 0.774, 0.776, 0.765, 0.772, 0.767, 0.768, 0.773, 0.764, 0.777, 0.771, 0.773, 0.764, 0.773, 0.772, 0.762, 0.771, 0.764, 0.762, 0.766, 0.762, 0.685, 0.659, 0.681, 0.653, 0.764, 0.729, 0.703, 0.765, 0.783, 0.783, 0.778, 0.79, 0.803, 0.795, 0.802, 0.811, 0.811, 0.803, 0.799, 0.799, 0.793, 0.797, 0.798, 0.796, 0.799, 0.796, 0.798, 0.797, 0.797, 0.794, 0.801, 0.799, 0.793, 0.786, 0.782, 0.773, 0.775, 0.775, 0.772, 0.782, 0.779, 0.78, 0.777, 0.779, 0.776, 0.775, 0.777, 0.776, 0.775, 0.776, 0.778, 0.775, 0.773, 0.775, 0.781, 0.776, 0.775, 0.776, 0.781, 0.777, 0.773, 0.776, 0.776, 0.774, 0.773, 0.775, 0.775, 0.77, 0.769, 0.771, 0.768, 0.765, 0.767, 0.768, 0.767, 0.765, 0.767, 0.766, 0.767, 0.765, 0.766, 0.766, 0.763, 0.76, 0.759, 0.758, 0.759, 0.759, 0.759, 0.76, 0.759, 0.758, 0.756, 0.756, 0.754, 0.755, 0.752, 0.751, 0.751, 0.75, 0.749, 0.748, 0.747, 0.746, 0.745, 0.744, 0.744, 0.742, 0.742, 0.741, 0.74, 0.74, 0.74, 0.739, 0.74, 0.74, 0.738, 0.737, 0.735, 0.731, 0.731, 0.731, 0.73, 0.726, 0.726, 0.726, 0.723, 0.722, 0.721, 0.721, 0.719, 0.719, 0.719, 0.719, 0.717, 0.717, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.715, 0.715, 0.715, 0.714, 0.716, 0.713, 0.714, 0.713, 0.713, 0.711, 0.712, 0.712, 0.709, 0.706, 0.708, 0.703, 0.55, 0.372, 0.424, 0.651, 0.56, 0.7, 0.722, 0.735, 0.732, 0.722, 0.706, 0.699, 0.752, 0.745, 0.714, 0.751, 0.744, 0.761, 0.772, 0.757, 0.776, 0.768, 0.767, 0.775, 0.771, 0.763, 0.775, 0.775, 0.778, 0.767, 0.767, 0.767, 0.763, 0.749, 0.74, 0.743, 0.762, 0.762, 0.758, 0.756, 0.752, 0.754, 0.757, 0.758, 0.757, 0.751, 0.752, 0.752, 0.762, 0.76, 0.759, 0.751, 0.748, 0.747, 0.748, 0.757, 0.752, 0.749, 0.744, 0.744, 0.748, 0.749, 0.749, 0.748, 0.744, 0.745, 0.746, 0.747, 0.747, 0.746, 0.745, 0.744, 0.744, 0.746, 0.744, 0.744, 0.74, 0.74, 0.737, 0.735, 0.736, 0.735, 0.735, 0.734, 0.734, 0.732, 0.735, 0.733, 0.732, 0.733, 0.731, 0.732, 0.731, 0.729, 0.729, 0.729, 0.729, 0.727, 0.728, 0.728, 0.726, 0.724, 0.723, 0.722, 0.722, 0.723, 0.724, 0.724, 0.723, 0.723, 0.722, 0.721, 0.72, 0.72, 0.721, 0.72, 0.718, 0.718, 0.718, 0.716, 0.714, 0.714, 0.714, 0.714, 0.712, 0.712, 0.713, 0.712, 0.711, 0.71, 0.71, 0.71, 0.709, 0.707, 0.706, 0.705, 0.704, 0.704, 0.704, 0.702, 0.701, 0.701, 0.701, 0.701, 0.7, 0.699, 0.699, 0.698, 0.697, 0.697, 0.697, 0.697, 0.696, 0.696, 0.696, 0.696, 0.697, 0.698, 0.698, 0.698, 0.698, 0.697, 0.697, 0.697, 0.697, 0.697, 0.697, 0.697, 0.697, 0.695, 0.695, 0.694, 0.694, 0.694, 0.694, 0.694, 0.693, 0.693, 0.693, 0.692, 0.693, 0.692, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.694, 0.694, 0.694, 0.694, 0.694, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.693, 0.691, 0.692, 0.692, 0.692, 0.692, 0.692, 0.692, 0.693, 0.694, 0.689, 0.596]
best validation: 0.802
best test: 0.811
Experiment run
dataset: cora
num_mp_layers: 3
mp_hidden_dim: 4000
optimizer_lr: 0.001
loss_func: CrossEntropyLoss
total_epoch: 500
energy_lambda: 0.001
Norm before input: 0.2552422881126404
Norm after input: 0.9947524070739746
Norm after each mp layer: 0.6566787362098694
Norm after each mp layer: 0.6222918033599854
Norm after each mp layer: 0.6184736490249634
Norm before input: 0.2552422881126404
Norm after input: 0.9995574951171875
Norm after each mp layer: 1.8107880353927612
Norm after each mp layer: 1.498584270477295
Norm after each mp layer: 1.1950159072875977
Norm before input: 0.2552422881126404
Norm after input: 0.9995574951171875
Norm after each mp layer: 1.8107880353927612
Norm after each mp layer: 1.498584270477295
Norm after each mp layer: 1.1950159072875977
Norm before input: 0.2552422881126404
Norm after input: 0.9955419898033142
Norm after each mp layer: 2.3642520904541016
Norm after each mp layer: 2.9739577770233154
Norm after each mp layer: 6.818618297576904
Norm before input: 0.2552422881126404
Norm after input: 0.9955419898033142
Norm after each mp layer: 2.3642520904541016
Norm after each mp layer: 2.9739577770233154
Norm after each mp layer: 6.818618297576904
Norm before input: 0.2552422881126404
Norm after input: 0.9302351474761963
Norm after each mp layer: 1.3748445510864258
Norm after each mp layer: 0.6900583505630493
Norm after each mp layer: 1.0208256244659424
Norm before input: 0.2552422881126404
Norm after input: 0.9302351474761963
Norm after each mp layer: 1.3748445510864258
Norm after each mp layer: 0.6900583505630493
Norm after each mp layer: 1.0208256244659424
Norm before input: 0.2552422881126404
Norm after input: 0.8781449794769287
Norm after each mp layer: 0.9941741228103638
Norm after each mp layer: 0.9504368901252747
Norm after each mp layer: 1.274449348449707
Norm before input: 0.2552422881126404
Norm after input: 0.8781449794769287
Norm after each mp layer: 0.9941741228103638
Norm after each mp layer: 0.9504368901252747
Norm after each mp layer: 1.274449348449707
Norm before input: 0.2552422881126404
Norm after input: 0.8355346918106079
Norm after each mp layer: 1.1126970052719116
Norm after each mp layer: 0.7696222066879272
Norm after each mp layer: 1.2976831197738647
Epoch: 05, Loss: 7.6006, Energy: 67.4756, Train: 28.06%, Valid: 31.60%, Test: 31.80%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.8355346918106079
Norm after each mp layer: 1.1126970052719116
Norm after each mp layer: 0.7696222066879272
Norm after each mp layer: 1.2976831197738647
Norm before input: 0.2552422881126404
Norm after input: 0.8000194430351257
Norm after each mp layer: 1.397100806236267
Norm after each mp layer: 1.5695288181304932
Norm after each mp layer: 2.2530136108398438
Norm before input: 0.2552422881126404
Norm after input: 0.8000194430351257
Norm after each mp layer: 1.397100806236267
Norm after each mp layer: 1.5695288181304932
Norm after each mp layer: 2.2530136108398438
Norm before input: 0.2552422881126404
Norm after input: 0.7649937272071838
Norm after each mp layer: 1.5410279035568237
Norm after each mp layer: 2.201847553253174
Norm after each mp layer: 3.7328126430511475
Norm before input: 0.2552422881126404
Norm after input: 0.7649937272071838
Norm after each mp layer: 1.5410279035568237
Norm after each mp layer: 2.201847553253174
Norm after each mp layer: 3.7328126430511475
Norm before input: 0.2552422881126404
Norm after input: 0.7240450978279114
Norm after each mp layer: 1.4276286363601685
Norm after each mp layer: 1.6822994947433472
Norm after each mp layer: 2.8356852531433105
Norm before input: 0.2552422881126404
Norm after input: 0.7240450978279114
Norm after each mp layer: 1.4276286363601685
Norm after each mp layer: 1.6822994947433472
Norm after each mp layer: 2.8356852531433105
Norm before input: 0.2552422881126404
Norm after input: 0.6898754835128784
Norm after each mp layer: 1.3348307609558105
Norm after each mp layer: 1.6466542482376099
Norm after each mp layer: 3.942622184753418
Norm before input: 0.2552422881126404
Norm after input: 0.6898754835128784
Norm after each mp layer: 1.3348307609558105
Norm after each mp layer: 1.6466542482376099
Norm after each mp layer: 3.942622184753418
Norm before input: 0.2552422881126404
Norm after input: 0.6608161330223083
Norm after each mp layer: 1.2962422370910645
Norm after each mp layer: 1.4849047660827637
Norm after each mp layer: 3.3455843925476074
Epoch: 10, Loss: 8.0729, Energy: 1644.9867, Train: 21.77%, Valid: 25.80%, Test: 23.80%, Best Valid: 31.60%, Best Test: 31.90%
Norm before input: 0.2552422881126404
Norm after input: 0.6608161330223083
Norm after each mp layer: 1.296242117881775
Norm after each mp layer: 1.4849047660827637
Norm after each mp layer: 3.3455843925476074
Norm before input: 0.2552422881126404
Norm after input: 0.6381186842918396
Norm after each mp layer: 1.3295913934707642
Norm after each mp layer: 1.7660119533538818
Norm after each mp layer: 3.78285551071167
Norm before input: 0.2552422881126404
Norm after input: 0.6381186842918396
Norm after each mp layer: 1.3295913934707642
Norm after each mp layer: 1.7660119533538818
Norm after each mp layer: 3.78285551071167
Norm before input: 0.2552422881126404
Norm after input: 0.6137592196464539
Norm after each mp layer: 1.3062653541564941
Norm after each mp layer: 1.9565191268920898
Norm after each mp layer: 4.583150863647461
Norm before input: 0.2552422881126404
Norm after input: 0.6137592196464539
Norm after each mp layer: 1.3062653541564941
Norm after each mp layer: 1.9565191268920898
Norm after each mp layer: 4.583150863647461
Norm before input: 0.2552422881126404
Norm after input: 0.5840212106704712
Norm after each mp layer: 1.1683179140090942
Norm after each mp layer: 1.450538992881775
Norm after each mp layer: 3.669520378112793
Norm before input: 0.2552422881126404
Norm after input: 0.5840212106704712
Norm after each mp layer: 1.1683179140090942
Norm after each mp layer: 1.450538992881775
Norm after each mp layer: 3.669520854949951
Norm before input: 0.2552422881126404
Norm after input: 0.558708667755127
Norm after each mp layer: 1.0590462684631348
Norm after each mp layer: 1.26055109500885
Norm after each mp layer: 3.5557303428649902
Norm before input: 0.2552422881126404
Norm after input: 0.558708667755127
Norm after each mp layer: 1.0590462684631348
Norm after each mp layer: 1.26055109500885
Norm after each mp layer: 3.5557303428649902
Norm before input: 0.2552422881126404
Norm after input: 0.5407586097717285
Norm after each mp layer: 1.0459846258163452
Norm after each mp layer: 1.4192967414855957
Norm after each mp layer: 4.256927967071533
Epoch: 15, Loss: 5.6580, Energy: 589.5901, Train: 27.98%, Valid: 30.80%, Test: 31.80%, Best Valid: 31.60%, Best Test: 32.20%
Norm before input: 0.2552422881126404
Norm after input: 0.5407586097717285
Norm after each mp layer: 1.0459846258163452
Norm after each mp layer: 1.4192967414855957
Norm after each mp layer: 4.256927967071533
Norm before input: 0.2552422881126404
Norm after input: 0.529214084148407
Norm after each mp layer: 1.1041401624679565
Norm after each mp layer: 1.7288970947265625
Norm after each mp layer: 5.672242164611816
Norm before input: 0.2552422881126404
Norm after input: 0.529214084148407
Norm after each mp layer: 1.1041401624679565
Norm after each mp layer: 1.7288970947265625
Norm after each mp layer: 5.672242164611816
Norm before input: 0.2552422881126404
Norm after input: 0.5198849439620972
Norm after each mp layer: 1.1378188133239746
Norm after each mp layer: 1.9046971797943115
Norm after each mp layer: 6.641251564025879
Norm before input: 0.2552422881126404
Norm after input: 0.5198849439620972
Norm after each mp layer: 1.1378188133239746
Norm after each mp layer: 1.9046971797943115
Norm after each mp layer: 6.641251564025879
Norm before input: 0.2552422881126404
Norm after input: 0.5078333020210266
Norm after each mp layer: 1.0756258964538574
Norm after each mp layer: 1.5055209398269653
Norm after each mp layer: 5.552215576171875
Norm before input: 0.2552422881126404
Norm after input: 0.5078333020210266
Norm after each mp layer: 1.0756258964538574
Norm after each mp layer: 1.5055209398269653
Norm after each mp layer: 5.552215576171875
Norm before input: 0.2552422881126404
Norm after input: 0.49790143966674805
Norm after each mp layer: 1.0158003568649292
Norm after each mp layer: 1.0617480278015137
Norm after each mp layer: 4.056334495544434
Norm before input: 0.2552422881126404
Norm after input: 0.49790143966674805
Norm after each mp layer: 1.0158003568649292
Norm after each mp layer: 1.0617480278015137
Norm after each mp layer: 4.056334495544434
Norm before input: 0.2552422881126404
Norm after input: 0.4893415570259094
Norm after each mp layer: 1.0480105876922607
Norm after each mp layer: 1.2429282665252686
Norm after each mp layer: 4.82421875
Epoch: 20, Loss: 3.4260, Energy: 87.9806, Train: 28.48%, Valid: 31.60%, Test: 31.90%, Best Valid: 31.60%, Best Test: 32.20%
Norm before input: 0.2552422881126404
Norm after input: 0.4893415570259094
Norm after each mp layer: 1.0480105876922607
Norm after each mp layer: 1.2429282665252686
Norm after each mp layer: 4.82421875
Norm before input: 0.2552422881126404
Norm after input: 0.4821659028530121
Norm after each mp layer: 1.1246278285980225
Norm after each mp layer: 1.807521104812622
Norm after each mp layer: 7.0319647789001465
Norm before input: 0.2552422881126404
Norm after input: 0.4821659028530121
Norm after each mp layer: 1.1246278285980225
Norm after each mp layer: 1.807521104812622
Norm after each mp layer: 7.0319647789001465
Norm before input: 0.2552422881126404
Norm after input: 0.47302624583244324
Norm after each mp layer: 1.1275064945220947
Norm after each mp layer: 1.868269681930542
Norm after each mp layer: 7.422795295715332
Norm before input: 0.2552422881126404
Norm after input: 0.47302624583244324
Norm after each mp layer: 1.1275064945220947
Norm after each mp layer: 1.868269681930542
Norm after each mp layer: 7.422795295715332
Norm before input: 0.2552422881126404
Norm after input: 0.4626314043998718
Norm after each mp layer: 1.0709643363952637
Norm after each mp layer: 1.4967318773269653
Norm after each mp layer: 6.0321173667907715
Norm before input: 0.2552422881126404
Norm after input: 0.4626314043998718
Norm after each mp layer: 1.0709643363952637
Norm after each mp layer: 1.4967318773269653
Norm after each mp layer: 6.0321173667907715
Norm before input: 0.2552422881126404
Norm after input: 0.4525957405567169
Norm after each mp layer: 1.0558466911315918
Norm after each mp layer: 1.4755804538726807
Norm after each mp layer: 6.095732688903809
Norm before input: 0.2552422881126404
Norm after input: 0.4525957405567169
Norm after each mp layer: 1.0558466911315918
Norm after each mp layer: 1.4755804538726807
Norm after each mp layer: 6.095732688903809
Norm before input: 0.2552422881126404
Norm after input: 0.44251227378845215
Norm after each mp layer: 1.079734206199646
Norm after each mp layer: 1.7885570526123047
Norm after each mp layer: 7.693637847900391
Epoch: 25, Loss: 2.5337, Energy: 295.4198, Train: 28.31%, Valid: 31.60%, Test: 32.00%, Best Valid: 31.80%, Best Test: 32.20%
Norm before input: 0.2552422881126404
Norm after input: 0.44251227378845215
Norm after each mp layer: 1.079734206199646
Norm after each mp layer: 1.7885570526123047
Norm after each mp layer: 7.693637847900391
Norm before input: 0.2552422881126404
Norm after input: 0.43445444107055664
Norm after each mp layer: 1.0790716409683228
Norm after each mp layer: 1.840074896812439
Norm after each mp layer: 8.052661895751953
Norm before input: 0.2552422881126404
Norm after input: 0.43445444107055664
Norm after each mp layer: 1.0790716409683228
Norm after each mp layer: 1.840074896812439
Norm after each mp layer: 8.052661895751953
Norm before input: 0.2552422881126404
Norm after input: 0.42881497740745544
Norm after each mp layer: 1.0473278760910034
Norm after each mp layer: 1.5502792596817017
Norm after each mp layer: 6.716042518615723
Norm before input: 0.2552422881126404
Norm after input: 0.42881497740745544
Norm after each mp layer: 1.0473278760910034
Norm after each mp layer: 1.5502792596817017
Norm after each mp layer: 6.716042518615723
Norm before input: 0.2552422881126404
Norm after input: 0.42402657866477966
Norm after each mp layer: 1.0248020887374878
Norm after each mp layer: 1.3437024354934692
Norm after each mp layer: 5.6858439445495605
Norm before input: 0.2552422881126404
Norm after input: 0.42402657866477966
Norm after each mp layer: 1.0248020887374878
Norm after each mp layer: 1.3437024354934692
Norm after each mp layer: 5.6858439445495605
Norm before input: 0.2552422881126404
Norm after input: 0.41492167115211487
Norm after each mp layer: 1.009969711303711
Norm after each mp layer: 1.3881462812423706
Norm after each mp layer: 6.085663318634033
Norm before input: 0.2552422881126404
Norm after input: 0.41492167115211487
Norm after each mp layer: 1.009969711303711
Norm after each mp layer: 1.388146162033081
Norm after each mp layer: 6.085663318634033
Norm before input: 0.2552422881126404
Norm after input: 0.40652406215667725
Norm after each mp layer: 0.9974415302276611
Norm after each mp layer: 1.4524271488189697
Norm after each mp layer: 6.5691657066345215
Epoch: 30, Loss: 2.0099, Energy: 97.3238, Train: 28.15%, Valid: 31.60%, Test: 32.00%, Best Valid: 31.80%, Best Test: 32.30%
Norm before input: 0.2552422881126404
Norm after input: 0.40652406215667725
Norm after each mp layer: 0.9974415302276611
Norm after each mp layer: 1.4524271488189697
Norm after each mp layer: 6.5691657066345215
Norm before input: 0.2552422881126404
Norm after input: 0.40297731757164
Norm after each mp layer: 0.9889013171195984
Norm after each mp layer: 1.3738723993301392
Norm after each mp layer: 6.1523284912109375
Norm before input: 0.2552422881126404
Norm after input: 0.40297731757164
Norm after each mp layer: 0.9889013171195984
Norm after each mp layer: 1.3738723993301392
Norm after each mp layer: 6.1523284912109375
Norm before input: 0.2552422881126404
Norm after input: 0.4032304286956787
Norm after each mp layer: 0.9872171878814697
Norm after each mp layer: 1.228684425354004
Norm after each mp layer: 5.190667152404785
Norm before input: 0.2552422881126404
Norm after input: 0.4032304286956787
Norm after each mp layer: 0.9872171878814697
Norm after each mp layer: 1.228684425354004
Norm after each mp layer: 5.190667152404785
Norm before input: 0.2552422881126404
Norm after input: 0.40142685174942017
Norm after each mp layer: 0.9816458225250244
Norm after each mp layer: 1.1609737873077393
Norm after each mp layer: 4.701015949249268
Norm before input: 0.2552422881126404
Norm after input: 0.40142685174942017
Norm after each mp layer: 0.9816458225250244
Norm after each mp layer: 1.1609737873077393
Norm after each mp layer: 4.701015472412109
Norm before input: 0.2552422881126404
Norm after input: 0.3923283815383911
Norm after each mp layer: 0.9564640522003174
Norm after each mp layer: 1.2158726453781128
Norm after each mp layer: 5.30732536315918
Norm before input: 0.2552422881126404
Norm after input: 0.3923283815383911
Norm after each mp layer: 0.9564640522003174
Norm after each mp layer: 1.2158726453781128
Norm after each mp layer: 5.30732536315918
Norm before input: 0.2552422881126404
Norm after input: 0.3851572573184967
Norm after each mp layer: 0.9358652830123901
Norm after each mp layer: 1.2573559284210205
Norm after each mp layer: 5.725979804992676
Epoch: 35, Loss: 1.9307, Energy: 73.7038, Train: 28.23%, Valid: 31.60%, Test: 32.00%, Best Valid: 31.80%, Best Test: 32.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3851572573184967
Norm after each mp layer: 0.9358652830123901
Norm after each mp layer: 1.2573559284210205
Norm after each mp layer: 5.725979804992676
Norm before input: 0.2552422881126404
Norm after input: 0.382995069026947
Norm after each mp layer: 0.9251383543014526
Norm after each mp layer: 1.2152926921844482
Norm after each mp layer: 5.496860504150391
Norm before input: 0.2552422881126404
Norm after input: 0.382995069026947
Norm after each mp layer: 0.9251383543014526
Norm after each mp layer: 1.2152926921844482
Norm after each mp layer: 5.496860504150391
Norm before input: 0.2552422881126404
Norm after input: 0.38387608528137207
Norm after each mp layer: 0.920195996761322
Norm after each mp layer: 1.1355831623077393
Norm after each mp layer: 4.929091453552246
Norm before input: 0.2552422881126404
Norm after input: 0.38387608528137207
Norm after each mp layer: 0.920195996761322
Norm after each mp layer: 1.1355831623077393
Norm after each mp layer: 4.929091453552246
Norm before input: 0.2552422881126404
Norm after input: 0.3826013505458832
Norm after each mp layer: 0.9110738039016724
Norm after each mp layer: 1.0999797582626343
Norm after each mp layer: 4.686294078826904
Norm before input: 0.2552422881126404
Norm after input: 0.3826013505458832
Norm after each mp layer: 0.9110738039016724
Norm after each mp layer: 1.0999797582626343
Norm after each mp layer: 4.686294078826904
Norm before input: 0.2552422881126404
Norm after input: 0.3751930296421051
Norm after each mp layer: 0.8904998302459717
Norm after each mp layer: 1.1492863893508911
Norm after each mp layer: 5.180084228515625
Norm before input: 0.2552422881126404
Norm after input: 0.3751930296421051
Norm after each mp layer: 0.8904998302459717
Norm after each mp layer: 1.1492863893508911
Norm after each mp layer: 5.180084228515625
Norm before input: 0.2552422881126404
Norm after input: 0.36906805634498596
Norm after each mp layer: 0.8734092116355896
Norm after each mp layer: 1.1971896886825562
Norm after each mp layer: 5.57646369934082
Epoch: 40, Loss: 1.8135, Energy: 32.8603, Train: 30.38%, Valid: 33.20%, Test: 33.50%, Best Valid: 33.20%, Best Test: 33.50%
Norm before input: 0.2552422881126404
Norm after input: 0.36906805634498596
Norm after each mp layer: 0.8734092116355896
Norm after each mp layer: 1.1971896886825562
Norm after each mp layer: 5.57646369934082
Norm before input: 0.2552422881126404
Norm after input: 0.3667343854904175
Norm after each mp layer: 0.8612393736839294
Norm after each mp layer: 1.191845178604126
Norm after each mp layer: 5.5600409507751465
Norm before input: 0.2552422881126404
Norm after input: 0.3667343854904175
Norm after each mp layer: 0.8612393736839294
Norm after each mp layer: 1.191845178604126
Norm after each mp layer: 5.5600409507751465
Norm before input: 0.2552422881126404
Norm after input: 0.3674578368663788
Norm after each mp layer: 0.85334312915802
Norm after each mp layer: 1.1490237712860107
Norm after each mp layer: 5.245368480682373
Norm before input: 0.2552422881126404
Norm after input: 0.3674578368663788
Norm after each mp layer: 0.85334312915802
Norm after each mp layer: 1.1490237712860107
Norm after each mp layer: 5.245368957519531
Norm before input: 0.2552422881126404
Norm after input: 0.36846107244491577
Norm after each mp layer: 0.8498742580413818
Norm after each mp layer: 1.1269025802612305
Norm after each mp layer: 4.99846076965332
Norm before input: 0.2552422881126404
Norm after input: 0.36846107244491577
Norm after each mp layer: 0.8498742580413818
Norm after each mp layer: 1.1269025802612305
Norm after each mp layer: 4.99846076965332
Norm before input: 0.2552422881126404
Norm after input: 0.36576807498931885
Norm after each mp layer: 0.848576009273529
Norm after each mp layer: 1.1806082725524902
Norm after each mp layer: 5.238337993621826
Norm before input: 0.2552422881126404
Norm after input: 0.36576807498931885
Norm after each mp layer: 0.848576009273529
Norm after each mp layer: 1.1806082725524902
Norm after each mp layer: 5.238337993621826
Norm before input: 0.2552422881126404
Norm after input: 0.36156561970710754
Norm after each mp layer: 0.8472847938537598
Norm after each mp layer: 1.26645028591156
Norm after each mp layer: 5.676539897918701
Epoch: 45, Loss: 1.7301, Energy: 44.2737, Train: 32.95%, Valid: 35.00%, Test: 35.30%, Best Valid: 35.40%, Best Test: 35.90%
Norm before input: 0.2552422881126404
Norm after input: 0.36156561970710754
Norm after each mp layer: 0.8472847938537598
Norm after each mp layer: 1.26645028591156
Norm after each mp layer: 5.676539897918701
Norm before input: 0.2552422881126404
Norm after input: 0.3597564995288849
Norm after each mp layer: 0.8409591317176819
Norm after each mp layer: 1.296399712562561
Norm after each mp layer: 5.785669803619385
Norm before input: 0.2552422881126404
Norm after input: 0.3597564995288849
Norm after each mp layer: 0.8409591317176819
Norm after each mp layer: 1.296399712562561
Norm after each mp layer: 5.785669803619385
Norm before input: 0.2552422881126404
Norm after input: 0.36096036434173584
Norm after each mp layer: 0.8323100209236145
Norm after each mp layer: 1.2725924253463745
Norm after each mp layer: 5.551661014556885
Norm before input: 0.2552422881126404
Norm after input: 0.36096036434173584
Norm after each mp layer: 0.8323100209236145
Norm after each mp layer: 1.2725924253463745
Norm after each mp layer: 5.551661014556885
Norm before input: 0.2552422881126404
Norm after input: 0.36410248279571533
Norm after each mp layer: 0.8328085541725159
Norm after each mp layer: 1.2566843032836914
Norm after each mp layer: 5.268177509307861
Norm before input: 0.2552422881126404
Norm after input: 0.36410248279571533
Norm after each mp layer: 0.8328085541725159
Norm after each mp layer: 1.2566843032836914
Norm after each mp layer: 5.268177509307861
Norm before input: 0.2552422881126404
Norm after input: 0.3648413419723511
Norm after each mp layer: 0.8446863293647766
Norm after each mp layer: 1.3106536865234375
Norm after each mp layer: 5.377311706542969
Norm before input: 0.2552422881126404
Norm after input: 0.3648413419723511
Norm after each mp layer: 0.8446863293647766
Norm after each mp layer: 1.3106536865234375
Norm after each mp layer: 5.377311706542969
Norm before input: 0.2552422881126404
Norm after input: 0.362631618976593
Norm after each mp layer: 0.8517264723777771
Norm after each mp layer: 1.3895432949066162
Norm after each mp layer: 5.714400768280029
Epoch: 50, Loss: 1.6211, Energy: 94.3502, Train: 36.75%, Valid: 38.60%, Test: 37.70%, Best Valid: 39.00%, Best Test: 38.60%
Norm before input: 0.2552422881126404
Norm after input: 0.362631618976593
Norm after each mp layer: 0.8517264723777771
Norm after each mp layer: 1.3895432949066162
Norm after each mp layer: 5.714400768280029
Norm before input: 0.2552422881126404
Norm after input: 0.36080479621887207
Norm after each mp layer: 0.8433778285980225
Norm after each mp layer: 1.4092519283294678
Norm after each mp layer: 5.80605411529541
Norm before input: 0.2552422881126404
Norm after input: 0.36080479621887207
Norm after each mp layer: 0.8433778285980225
Norm after each mp layer: 1.4092519283294678
Norm after each mp layer: 5.80605411529541
Norm before input: 0.2552422881126404
Norm after input: 0.36136817932128906
Norm after each mp layer: 0.8347590565681458
Norm after each mp layer: 1.3920804262161255
Norm after each mp layer: 5.669718265533447
Norm before input: 0.2552422881126404
Norm after input: 0.36136817932128906
Norm after each mp layer: 0.8347590565681458
Norm after each mp layer: 1.3920804262161255
Norm after each mp layer: 5.669718265533447
Norm before input: 0.2552422881126404
Norm after input: 0.36466994881629944
Norm after each mp layer: 0.8419643640518188
Norm after each mp layer: 1.3790267705917358
Norm after each mp layer: 5.466288089752197
Norm before input: 0.2552422881126404
Norm after input: 0.36466994881629944
Norm after each mp layer: 0.8419643640518188
Norm after each mp layer: 1.3790267705917358
Norm after each mp layer: 5.466288089752197
Norm before input: 0.2552422881126404
Norm after input: 0.3670455515384674
Norm after each mp layer: 0.8496984243392944
Norm after each mp layer: 1.3769288063049316
Norm after each mp layer: 5.3474907875061035
Norm before input: 0.2552422881126404
Norm after input: 0.3670455515384674
Norm after each mp layer: 0.8496984243392944
Norm after each mp layer: 1.3769288063049316
Norm after each mp layer: 5.3474907875061035
Norm before input: 0.2552422881126404
Norm after input: 0.36311814188957214
Norm after each mp layer: 0.8338302969932556
Norm after each mp layer: 1.3801156282424927
Norm after each mp layer: 5.5099897384643555
Epoch: 55, Loss: 1.5353, Energy: 116.5361, Train: 45.86%, Valid: 46.20%, Test: 46.80%, Best Valid: 47.20%, Best Test: 46.80%
Norm before input: 0.2552422881126404
Norm after input: 0.36311814188957214
Norm after each mp layer: 0.8338302969932556
Norm after each mp layer: 1.3801156282424927
Norm after each mp layer: 5.5099897384643555
Norm before input: 0.2552422881126404
Norm after input: 0.36047837138175964
Norm after each mp layer: 0.8303242325782776
Norm after each mp layer: 1.399883508682251
Norm after each mp layer: 5.687727451324463
Norm before input: 0.2552422881126404
Norm after input: 0.36047837138175964
Norm after each mp layer: 0.8303242325782776
Norm after each mp layer: 1.399883508682251
Norm after each mp layer: 5.687727451324463
Norm before input: 0.2552422881126404
Norm after input: 0.36101797223091125
Norm after each mp layer: 0.8421093225479126
Norm after each mp layer: 1.4222675561904907
Norm after each mp layer: 5.749029159545898
Norm before input: 0.2552422881126404
Norm after input: 0.36101797223091125
Norm after each mp layer: 0.8421093225479126
Norm after each mp layer: 1.4222674369812012
Norm after each mp layer: 5.749029159545898
Norm before input: 0.2552422881126404
Norm after input: 0.36120977997779846
Norm after each mp layer: 0.8328611850738525
Norm after each mp layer: 1.3918083906173706
Norm after each mp layer: 5.630526542663574
Norm before input: 0.2552422881126404
Norm after input: 0.36120977997779846
Norm after each mp layer: 0.8328611850738525
Norm after each mp layer: 1.3918083906173706
Norm after each mp layer: 5.630526542663574
Norm before input: 0.2552422881126404
Norm after input: 0.36279696226119995
Norm after each mp layer: 0.8302306532859802
Norm after each mp layer: 1.3680965900421143
Norm after each mp layer: 5.47880744934082
Norm before input: 0.2552422881126404
Norm after input: 0.36279696226119995
Norm after each mp layer: 0.8302306532859802
Norm after each mp layer: 1.3680965900421143
Norm after each mp layer: 5.47880744934082
Norm before input: 0.2552422881126404
Norm after input: 0.36507001519203186
Norm after each mp layer: 0.8438385128974915
Norm after each mp layer: 1.3842618465423584
Norm after each mp layer: 5.438774585723877
Epoch: 60, Loss: 1.4050, Energy: 69.8746, Train: 46.61%, Valid: 44.80%, Test: 45.90%, Best Valid: 47.20%, Best Test: 46.90%
Norm before input: 0.2552422881126404
Norm after input: 0.36507001519203186
Norm after each mp layer: 0.8438385128974915
Norm after each mp layer: 1.3842618465423584
Norm after each mp layer: 5.438774585723877
Norm before input: 0.2552422881126404
Norm after input: 0.36065909266471863
Norm after each mp layer: 0.8305349349975586
Norm after each mp layer: 1.4063836336135864
Norm after each mp layer: 5.700061798095703
Norm before input: 0.2552422881126404
Norm after input: 0.36065909266471863
Norm after each mp layer: 0.8305349349975586
Norm after each mp layer: 1.4063836336135864
Norm after each mp layer: 5.700061798095703
Norm before input: 0.2552422881126404
Norm after input: 0.3598991930484772
Norm after each mp layer: 0.8452456593513489
Norm after each mp layer: 1.4671730995178223
Norm after each mp layer: 5.937055587768555
Norm before input: 0.2552422881126404
Norm after input: 0.3598991930484772
Norm after each mp layer: 0.8452456593513489
Norm after each mp layer: 1.4671730995178223
Norm after each mp layer: 5.937055587768555
Norm before input: 0.2552422881126404
Norm after input: 0.35930153727531433
Norm after each mp layer: 0.8405853509902954
Norm after each mp layer: 1.4738383293151855
Norm after each mp layer: 5.982137680053711
Norm before input: 0.2552422881126404
Norm after input: 0.35930153727531433
Norm after each mp layer: 0.8405853509902954
Norm after each mp layer: 1.4738383293151855
Norm after each mp layer: 5.982137680053711
Norm before input: 0.2552422881126404
Norm after input: 0.36082279682159424
Norm after each mp layer: 0.8389338850975037
Norm after each mp layer: 1.4672003984451294
Norm after each mp layer: 5.895308494567871
Norm before input: 0.2552422881126404
Norm after input: 0.36082279682159424
Norm after each mp layer: 0.8389338850975037
Norm after each mp layer: 1.4672003984451294
Norm after each mp layer: 5.895308494567871
Norm before input: 0.2552422881126404
Norm after input: 0.36369362473487854
Norm after each mp layer: 0.8480254411697388
Norm after each mp layer: 1.477901816368103
Norm after each mp layer: 5.816549301147461
Epoch: 65, Loss: 1.3289, Energy: 67.8136, Train: 49.25%, Valid: 47.40%, Test: 46.20%, Best Valid: 47.40%, Best Test: 46.90%
Norm before input: 0.2552422881126404
Norm after input: 0.36369362473487854
Norm after each mp layer: 0.8480254411697388
Norm after each mp layer: 1.477901816368103
Norm after each mp layer: 5.816549301147461
Norm before input: 0.2552422881126404
Norm after input: 0.3608752191066742
Norm after each mp layer: 0.8309711813926697
Norm after each mp layer: 1.4854958057403564
Norm after each mp layer: 5.95358419418335
Norm before input: 0.2552422881126404
Norm after input: 0.3608752191066742
Norm after each mp layer: 0.8309711813926697
Norm after each mp layer: 1.4854958057403564
Norm after each mp layer: 5.95358419418335
Norm before input: 0.2552422881126404
Norm after input: 0.3625093400478363
Norm after each mp layer: 0.8650851249694824
Norm after each mp layer: 1.5790297985076904
Norm after each mp layer: 6.2173051834106445
Norm before input: 0.2552422881126404
Norm after input: 0.3625093400478363
Norm after each mp layer: 0.8650851249694824
Norm after each mp layer: 1.5790297985076904
Norm after each mp layer: 6.2173051834106445
Norm before input: 0.2552422881126404
Norm after input: 0.35644233226776123
Norm after each mp layer: 0.8179565668106079
Norm after each mp layer: 1.54473876953125
Norm after each mp layer: 6.308422088623047
Norm before input: 0.2552422881126404
Norm after input: 0.35644233226776123
Norm after each mp layer: 0.8179565668106079
Norm after each mp layer: 1.54473876953125
Norm after each mp layer: 6.308422088623047
Norm before input: 0.2552422881126404
Norm after input: 0.36279311776161194
Norm after each mp layer: 0.8829141855239868
Norm after each mp layer: 1.6619173288345337
Norm after each mp layer: 6.458490371704102
Norm before input: 0.2552422881126404
Norm after input: 0.36279311776161194
Norm after each mp layer: 0.8829141855239868
Norm after each mp layer: 1.6619173288345337
Norm after each mp layer: 6.458490371704102
Norm before input: 0.2552422881126404
Norm after input: 0.35757285356521606
Norm after each mp layer: 0.8308036923408508
Norm after each mp layer: 1.55934739112854
Norm after each mp layer: 6.234500408172607
Epoch: 70, Loss: 1.4736, Energy: 221.4425, Train: 50.33%, Valid: 44.60%, Test: 44.00%, Best Valid: 47.40%, Best Test: 46.90%
Norm before input: 0.2552422881126404
Norm after input: 0.35757285356521606
Norm after each mp layer: 0.8308036923408508
Norm after each mp layer: 1.5593472719192505
Norm after each mp layer: 6.234500408172607
Norm before input: 0.2552422881126404
Norm after input: 0.35934722423553467
Norm after each mp layer: 0.8432565331459045
Norm after each mp layer: 1.5650516748428345
Norm after each mp layer: 6.131768226623535
Norm before input: 0.2552422881126404
Norm after input: 0.35934722423553467
Norm after each mp layer: 0.8432565331459045
Norm after each mp layer: 1.5650516748428345
Norm after each mp layer: 6.131768226623535
Norm before input: 0.2552422881126404
Norm after input: 0.3632393777370453
Norm after each mp layer: 0.8789724111557007
Norm after each mp layer: 1.6310276985168457
Norm after each mp layer: 6.167025089263916
Norm before input: 0.2552422881126404
Norm after input: 0.3632393777370453
Norm after each mp layer: 0.8789724111557007
Norm after each mp layer: 1.6310276985168457
Norm after each mp layer: 6.167025089263916
Norm before input: 0.2552422881126404
Norm after input: 0.3581099212169647
Norm after each mp layer: 0.8371589183807373
Norm after each mp layer: 1.5608868598937988
Norm after each mp layer: 6.054670810699463
Norm before input: 0.2552422881126404
Norm after input: 0.3581099212169647
Norm after each mp layer: 0.8371589183807373
Norm after each mp layer: 1.5608868598937988
Norm after each mp layer: 6.054670810699463
Norm before input: 0.2552422881126404
Norm after input: 0.3577631413936615
Norm after each mp layer: 0.846387505531311
Norm after each mp layer: 1.5853323936462402
Norm after each mp layer: 6.108469009399414
Norm before input: 0.2552422881126404
Norm after input: 0.3577631413936615
Norm after each mp layer: 0.846387505531311
Norm after each mp layer: 1.5853323936462402
Norm after each mp layer: 6.108469009399414
Norm before input: 0.2552422881126404
Norm after input: 0.3602229952812195
Norm after each mp layer: 0.8805297613143921
Norm after each mp layer: 1.6646180152893066
Norm after each mp layer: 6.252732753753662
Epoch: 75, Loss: 1.2418, Energy: 81.6035, Train: 51.74%, Valid: 45.40%, Test: 46.00%, Best Valid: 47.60%, Best Test: 46.90%
Norm before input: 0.2552422881126404
Norm after input: 0.3602229952812195
Norm after each mp layer: 0.8805297613143921
Norm after each mp layer: 1.6646180152893066
Norm after each mp layer: 6.252732753753662
Norm before input: 0.2552422881126404
Norm after input: 0.35728973150253296
Norm after each mp layer: 0.854872465133667
Norm after each mp layer: 1.615931749343872
Norm after each mp layer: 6.141416072845459
Norm before input: 0.2552422881126404
Norm after input: 0.35728973150253296
Norm after each mp layer: 0.854872465133667
Norm after each mp layer: 1.615931749343872
Norm after each mp layer: 6.141416072845459
Norm before input: 0.2552422881126404
Norm after input: 0.3563518524169922
Norm after each mp layer: 0.8426976203918457
Norm after each mp layer: 1.600675344467163
Norm after each mp layer: 6.066046714782715
Norm before input: 0.2552422881126404
Norm after input: 0.3563518524169922
Norm after each mp layer: 0.8426976203918457
Norm after each mp layer: 1.600675344467163
Norm after each mp layer: 6.066046714782715
Norm before input: 0.2552422881126404
Norm after input: 0.35974401235580444
Norm after each mp layer: 0.8738245368003845
Norm after each mp layer: 1.6562429666519165
Norm after each mp layer: 6.097314357757568
Norm before input: 0.2552422881126404
Norm after input: 0.35974401235580444
Norm after each mp layer: 0.8738245368003845
Norm after each mp layer: 1.6562429666519165
Norm after each mp layer: 6.097314357757568
Norm before input: 0.2552422881126404
Norm after input: 0.35956552624702454
Norm after each mp layer: 0.8712670207023621
Norm after each mp layer: 1.6558879613876343
Norm after each mp layer: 6.058794975280762
Norm before input: 0.2552422881126404
Norm after input: 0.35956552624702454
Norm after each mp layer: 0.8712670207023621
Norm after each mp layer: 1.6558879613876343
Norm after each mp layer: 6.058794975280762
Norm before input: 0.2552422881126404
Norm after input: 0.35685932636260986
Norm after each mp layer: 0.8474870920181274
Norm after each mp layer: 1.6267985105514526
Norm after each mp layer: 6.006495952606201
Epoch: 80, Loss: 1.1988, Energy: 77.4629, Train: 55.88%, Valid: 49.20%, Test: 47.80%, Best Valid: 49.20%, Best Test: 48.10%
Norm before input: 0.2552422881126404
Norm after input: 0.35685932636260986
Norm after each mp layer: 0.8474870920181274
Norm after each mp layer: 1.6267985105514526
Norm after each mp layer: 6.006495952606201
Norm before input: 0.2552422881126404
Norm after input: 0.3577345609664917
Norm after each mp layer: 0.8653479814529419
Norm after each mp layer: 1.6701515913009644
Norm after each mp layer: 6.103819847106934
Norm before input: 0.2552422881126404
Norm after input: 0.3577345609664917
Norm after each mp layer: 0.8653479814529419
Norm after each mp layer: 1.6701515913009644
Norm after each mp layer: 6.103819847106934
Norm before input: 0.2552422881126404
Norm after input: 0.3587662875652313
Norm after each mp layer: 0.8825592994689941
Norm after each mp layer: 1.7200568914413452
Norm after each mp layer: 6.214956760406494
Norm before input: 0.2552422881126404
Norm after input: 0.3587662875652313
Norm after each mp layer: 0.8825592994689941
Norm after each mp layer: 1.7200568914413452
Norm after each mp layer: 6.214956760406494
Norm before input: 0.2552422881126404
Norm after input: 0.35630759596824646
Norm after each mp layer: 0.8586319088935852
Norm after each mp layer: 1.6768789291381836
Norm after each mp layer: 6.1092681884765625
Norm before input: 0.2552422881126404
Norm after input: 0.35630759596824646
Norm after each mp layer: 0.8586319088935852
Norm after each mp layer: 1.6768789291381836
Norm after each mp layer: 6.1092681884765625
Norm before input: 0.2552422881126404
Norm after input: 0.3561762273311615
Norm after each mp layer: 0.8562330603599548
Norm after each mp layer: 1.67582106590271
Norm after each mp layer: 6.074258327484131
Norm before input: 0.2552422881126404
Norm after input: 0.3561762273311615
Norm after each mp layer: 0.8562330603599548
Norm after each mp layer: 1.67582106590271
Norm after each mp layer: 6.074258327484131
Norm before input: 0.2552422881126404
Norm after input: 0.358452171087265
Norm after each mp layer: 0.8790505528450012
Norm after each mp layer: 1.7223836183547974
Norm after each mp layer: 6.133376598358154
Epoch: 85, Loss: 1.1644, Energy: 83.6649, Train: 57.28%, Valid: 49.80%, Test: 49.70%, Best Valid: 49.80%, Best Test: 49.70%
Norm before input: 0.2552422881126404
Norm after input: 0.358452171087265
Norm after each mp layer: 0.8790505528450012
Norm after each mp layer: 1.7223836183547974
Norm after each mp layer: 6.133376598358154
Norm before input: 0.2552422881126404
Norm after input: 0.35757333040237427
Norm after each mp layer: 0.8684173822402954
Norm after each mp layer: 1.7010270357131958
Norm after each mp layer: 6.055680751800537
Norm before input: 0.2552422881126404
Norm after input: 0.35757333040237427
Norm after each mp layer: 0.8684173822402954
Norm after each mp layer: 1.7010270357131958
Norm after each mp layer: 6.055680751800537
Norm before input: 0.2552422881126404
Norm after input: 0.35634148120880127
Norm after each mp layer: 0.8556248545646667
Norm after each mp layer: 1.6827608346939087
Norm after each mp layer: 6.0024919509887695
Norm before input: 0.2552422881126404
Norm after input: 0.35634148120880127
Norm after each mp layer: 0.8556248545646667
Norm after each mp layer: 1.6827608346939087
Norm after each mp layer: 6.0024919509887695
Norm before input: 0.2552422881126404
Norm after input: 0.35728928446769714
Norm after each mp layer: 0.8710236549377441
Norm after each mp layer: 1.7200157642364502
Norm after each mp layer: 6.085625648498535
Norm before input: 0.2552422881126404
Norm after input: 0.35728928446769714
Norm after each mp layer: 0.8710236549377441
Norm after each mp layer: 1.7200157642364502
Norm after each mp layer: 6.085625648498535
Norm before input: 0.2552422881126404
Norm after input: 0.35731241106987
Norm after each mp layer: 0.8753066658973694
Norm after each mp layer: 1.7388898134231567
Norm after each mp layer: 6.133462905883789
Norm before input: 0.2552422881126404
Norm after input: 0.35731241106987
Norm after each mp layer: 0.8753066658973694
Norm after each mp layer: 1.7388898134231567
Norm after each mp layer: 6.133462905883789
Norm before input: 0.2552422881126404
Norm after input: 0.3555111885070801
Norm after each mp layer: 0.8571528196334839
Norm after each mp layer: 1.713228464126587
Norm after each mp layer: 6.081480026245117
Epoch: 90, Loss: 1.1132, Energy: 77.5610, Train: 58.20%, Valid: 49.60%, Test: 49.70%, Best Valid: 50.20%, Best Test: 50.50%
Norm before input: 0.2552422881126404
Norm after input: 0.3555111885070801
Norm after each mp layer: 0.8571528196334839
Norm after each mp layer: 1.713228464126587
Norm after each mp layer: 6.081480026245117
Norm before input: 0.2552422881126404
Norm after input: 0.3565829396247864
Norm after each mp layer: 0.8698416352272034
Norm after each mp layer: 1.744654893875122
Norm after each mp layer: 6.139741897583008
Norm before input: 0.2552422881126404
Norm after input: 0.3565829396247864
Norm after each mp layer: 0.8698416352272034
Norm after each mp layer: 1.744654893875122
Norm after each mp layer: 6.139741897583008
Norm before input: 0.2552422881126404
Norm after input: 0.35781216621398926
Norm after each mp layer: 0.8811264634132385
Norm after each mp layer: 1.7757476568222046
Norm after each mp layer: 6.188467979431152
Norm before input: 0.2552422881126404
Norm after input: 0.35781216621398926
Norm after each mp layer: 0.8811264634132385
Norm after each mp layer: 1.7757476568222046
Norm after each mp layer: 6.188467979431152
Norm before input: 0.2552422881126404
Norm after input: 0.35626351833343506
Norm after each mp layer: 0.8616889119148254
Norm after each mp layer: 1.7456899881362915
Norm after each mp layer: 6.112480640411377
Norm before input: 0.2552422881126404
Norm after input: 0.35626351833343506
Norm after each mp layer: 0.8616889119148254
Norm after each mp layer: 1.7456899881362915
Norm after each mp layer: 6.112480640411377
Norm before input: 0.2552422881126404
Norm after input: 0.3571884036064148
Norm after each mp layer: 0.8743709921836853
Norm after each mp layer: 1.7800118923187256
Norm after each mp layer: 6.189665794372559
Norm before input: 0.2552422881126404
Norm after input: 0.3571884036064148
Norm after each mp layer: 0.8743709921836853
Norm after each mp layer: 1.7800118923187256
Norm after each mp layer: 6.189666271209717
Norm before input: 0.2552422881126404
Norm after input: 0.358077734708786
Norm after each mp layer: 0.8864107728004456
Norm after each mp layer: 1.8175233602523804
Norm after each mp layer: 6.279736518859863
Epoch: 95, Loss: 1.0508, Energy: 78.6059, Train: 60.35%, Valid: 51.00%, Test: 50.70%, Best Valid: 51.80%, Best Test: 51.80%
Norm before input: 0.2552422881126404
Norm after input: 0.358077734708786
Norm after each mp layer: 0.8864107728004456
Norm after each mp layer: 1.8175233602523804
Norm after each mp layer: 6.279736518859863
Norm before input: 0.2552422881126404
Norm after input: 0.3560934364795685
Norm after each mp layer: 0.8653082847595215
Norm after each mp layer: 1.7876087427139282
Norm after each mp layer: 6.22355318069458
Norm before input: 0.2552422881126404
Norm after input: 0.3560934364795685
Norm after each mp layer: 0.8653082847595215
Norm after each mp layer: 1.7876087427139282
Norm after each mp layer: 6.22355318069458
Norm before input: 0.2552422881126404
Norm after input: 0.35734546184539795
Norm after each mp layer: 0.8817026615142822
Norm after each mp layer: 1.8282692432403564
Norm after each mp layer: 6.313473701477051
Norm before input: 0.2552422881126404
Norm after input: 0.35734546184539795
Norm after each mp layer: 0.8817026615142822
Norm after each mp layer: 1.8282692432403564
Norm after each mp layer: 6.313473701477051
Norm before input: 0.2552422881126404
Norm after input: 0.35843396186828613
Norm after each mp layer: 0.8914895057678223
Norm after each mp layer: 1.8563958406448364
Norm after each mp layer: 6.357995986938477
Norm before input: 0.2552422881126404
Norm after input: 0.35843396186828613
Norm after each mp layer: 0.8914895057678223
Norm after each mp layer: 1.8563958406448364
Norm after each mp layer: 6.357995986938477
Norm before input: 0.2552422881126404
Norm after input: 0.3566415011882782
Norm after each mp layer: 0.867948591709137
Norm after each mp layer: 1.8191108703613281
Norm after each mp layer: 6.25910758972168
Norm before input: 0.2552422881126404
Norm after input: 0.3566415011882782
Norm after each mp layer: 0.867948591709137
Norm after each mp layer: 1.8191108703613281
Norm after each mp layer: 6.25910758972168
Norm before input: 0.2552422881126404
Norm after input: 0.3585195541381836
Norm after each mp layer: 0.8924509882926941
Norm after each mp layer: 1.87518310546875
Norm after each mp layer: 6.380714416503906
Epoch: 100, Loss: 1.0340, Energy: 113.0093, Train: 66.64%, Valid: 56.00%, Test: 56.50%, Best Valid: 56.60%, Best Test: 56.50%
Norm before input: 0.2552422881126404
Norm after input: 0.3585195541381836
Norm after each mp layer: 0.8924509882926941
Norm after each mp layer: 1.87518310546875
Norm after each mp layer: 6.380714416503906
Norm before input: 0.2552422881126404
Norm after input: 0.3585462272167206
Norm after each mp layer: 0.8943455219268799
Norm after each mp layer: 1.889391303062439
Norm after each mp layer: 6.409244537353516
Norm before input: 0.2552422881126404
Norm after input: 0.3585462272167206
Norm after each mp layer: 0.8943455219268799
Norm after each mp layer: 1.889391303062439
Norm after each mp layer: 6.409244537353516
Norm before input: 0.2552422881126404
Norm after input: 0.3574589788913727
Norm after each mp layer: 0.880273163318634
Norm after each mp layer: 1.871282696723938
Norm after each mp layer: 6.354243755340576
Norm before input: 0.2552422881126404
Norm after input: 0.3574589788913727
Norm after each mp layer: 0.880273163318634
Norm after each mp layer: 1.871282696723938
Norm after each mp layer: 6.354243755340576
Norm before input: 0.2552422881126404
Norm after input: 0.3591890335083008
Norm after each mp layer: 0.9008417725563049
Norm after each mp layer: 1.9206739664077759
Norm after each mp layer: 6.453362941741943
Norm before input: 0.2552422881126404
Norm after input: 0.3591890335083008
Norm after each mp layer: 0.9008417725563049
Norm after each mp layer: 1.9206739664077759
Norm after each mp layer: 6.453362941741943
Norm before input: 0.2552422881126404
Norm after input: 0.35925331711769104
Norm after each mp layer: 0.9004912376403809
Norm after each mp layer: 1.9287570714950562
Norm after each mp layer: 6.456917762756348
Norm before input: 0.2552422881126404
Norm after input: 0.35925331711769104
Norm after each mp layer: 0.9004912376403809
Norm after each mp layer: 1.9287570714950562
Norm after each mp layer: 6.456917762756348
Norm before input: 0.2552422881126404
Norm after input: 0.35891300439834595
Norm after each mp layer: 0.8941558003425598
Norm after each mp layer: 1.9266533851623535
Norm after each mp layer: 6.437185287475586
Epoch: 105, Loss: 0.9005, Energy: 97.1375, Train: 74.59%, Valid: 64.40%, Test: 62.90%, Best Valid: 64.40%, Best Test: 62.90%
Norm before input: 0.2552422881126404
Norm after input: 0.35891300439834595
Norm after each mp layer: 0.8941558003425598
Norm after each mp layer: 1.9266533851623535
Norm after each mp layer: 6.437185287475586
Norm before input: 0.2552422881126404
Norm after input: 0.3602404296398163
Norm after each mp layer: 0.9098352789878845
Norm after each mp layer: 1.9697151184082031
Norm after each mp layer: 6.531395435333252
Norm before input: 0.2552422881126404
Norm after input: 0.3602404296398163
Norm after each mp layer: 0.9098352789878845
Norm after each mp layer: 1.9697151184082031
Norm after each mp layer: 6.531395435333252
Norm before input: 0.2552422881126404
Norm after input: 0.36010515689849854
Norm after each mp layer: 0.9098238945007324
Norm after each mp layer: 1.9844226837158203
Norm after each mp layer: 6.5760297775268555
Norm before input: 0.2552422881126404
Norm after input: 0.36010515689849854
Norm after each mp layer: 0.9098238945007324
Norm after each mp layer: 1.9844226837158203
Norm after each mp layer: 6.5760297775268555
Norm before input: 0.2552422881126404
Norm after input: 0.359945148229599
Norm after each mp layer: 0.908057689666748
Norm after each mp layer: 1.996287226676941
Norm after each mp layer: 6.612972259521484
Norm before input: 0.2552422881126404
Norm after input: 0.359945148229599
Norm after each mp layer: 0.908057689666748
Norm after each mp layer: 1.996287226676941
Norm after each mp layer: 6.612972259521484
Norm before input: 0.2552422881126404
Norm after input: 0.36122798919677734
Norm after each mp layer: 0.9213714003562927
Norm after each mp layer: 2.0355772972106934
Norm after each mp layer: 6.700253009796143
Norm before input: 0.2552422881126404
Norm after input: 0.36122798919677734
Norm after each mp layer: 0.9213714003562927
Norm after each mp layer: 2.0355772972106934
Norm after each mp layer: 6.700253009796143
Norm before input: 0.2552422881126404
Norm after input: 0.36106422543525696
Norm after each mp layer: 0.9194221496582031
Norm after each mp layer: 2.046740770339966
Norm after each mp layer: 6.740955352783203
Epoch: 110, Loss: 0.8024, Energy: 118.2087, Train: 79.72%, Valid: 67.40%, Test: 66.90%, Best Valid: 67.40%, Best Test: 66.90%
Norm before input: 0.2552422881126404
Norm after input: 0.36106422543525696
Norm after each mp layer: 0.9194221496582031
Norm after each mp layer: 2.046740770339966
Norm after each mp layer: 6.740955352783203
Norm before input: 0.2552422881126404
Norm after input: 0.36132851243019104
Norm after each mp layer: 0.9235076904296875
Norm after each mp layer: 2.0697004795074463
Norm after each mp layer: 6.813259124755859
Norm before input: 0.2552422881126404
Norm after input: 0.36132851243019104
Norm after each mp layer: 0.9235076904296875
Norm after each mp layer: 2.0697004795074463
Norm after each mp layer: 6.813259124755859
Norm before input: 0.2552422881126404
Norm after input: 0.36198678612709045
Norm after each mp layer: 0.9327579140663147
Norm after each mp layer: 2.1014254093170166
Norm after each mp layer: 6.904146671295166
Norm before input: 0.2552422881126404
Norm after input: 0.36198678612709045
Norm after each mp layer: 0.9327579140663147
Norm after each mp layer: 2.1014254093170166
Norm after each mp layer: 6.904146671295166
Norm before input: 0.2552422881126404
Norm after input: 0.36146843433380127
Norm after each mp layer: 0.929029643535614
Norm after each mp layer: 2.1076464653015137
Norm after each mp layer: 6.951611518859863
Norm before input: 0.2552422881126404
Norm after input: 0.36146843433380127
Norm after each mp layer: 0.929029643535614
Norm after each mp layer: 2.1076464653015137
Norm after each mp layer: 6.951611518859863
Norm before input: 0.2552422881126404
Norm after input: 0.3619953393936157
Norm after each mp layer: 0.9358425736427307
Norm after each mp layer: 2.12941312789917
Norm after each mp layer: 7.011014461517334
Norm before input: 0.2552422881126404
Norm after input: 0.3619953393936157
Norm after each mp layer: 0.9358425736427307
Norm after each mp layer: 2.12941312789917
Norm after each mp layer: 7.011014461517334
Norm before input: 0.2552422881126404
Norm after input: 0.3621484339237213
Norm after each mp layer: 0.9388229846954346
Norm after each mp layer: 2.1425790786743164
Norm after each mp layer: 7.053891658782959
Epoch: 115, Loss: 0.7177, Energy: 145.3812, Train: 81.79%, Valid: 70.20%, Test: 69.00%, Best Valid: 70.20%, Best Test: 69.00%
Norm before input: 0.2552422881126404
Norm after input: 0.3621484339237213
Norm after each mp layer: 0.9388229846954346
Norm after each mp layer: 2.1425790786743164
Norm after each mp layer: 7.053891658782959
Norm before input: 0.2552422881126404
Norm after input: 0.3615073263645172
Norm after each mp layer: 0.9341498017311096
Norm after each mp layer: 2.1411170959472656
Norm after each mp layer: 7.080051422119141
Norm before input: 0.2552422881126404
Norm after input: 0.3615073263645172
Norm after each mp layer: 0.9341498017311096
Norm after each mp layer: 2.1411170959472656
Norm after each mp layer: 7.080051422119141
Norm before input: 0.2552422881126404
Norm after input: 0.3619479835033417
Norm after each mp layer: 0.9402921199798584
Norm after each mp layer: 2.155353546142578
Norm after each mp layer: 7.105381488800049
Norm before input: 0.2552422881126404
Norm after input: 0.3619479835033417
Norm after each mp layer: 0.9402921199798584
Norm after each mp layer: 2.155353546142578
Norm after each mp layer: 7.105381488800049
Norm before input: 0.2552422881126404
Norm after input: 0.3615219295024872
Norm after each mp layer: 0.9369915127754211
Norm after each mp layer: 2.1514110565185547
Norm after each mp layer: 7.1065993309021
Norm before input: 0.2552422881126404
Norm after input: 0.3615219295024872
Norm after each mp layer: 0.9369915127754211
Norm after each mp layer: 2.1514110565185547
Norm after each mp layer: 7.1065993309021
Norm before input: 0.2552422881126404
Norm after input: 0.3613958954811096
Norm after each mp layer: 0.9361864328384399
Norm after each mp layer: 2.150047540664673
Norm after each mp layer: 7.09844970703125
Norm before input: 0.2552422881126404
Norm after input: 0.3613958954811096
Norm after each mp layer: 0.9361864328384399
Norm after each mp layer: 2.150047540664673
Norm after each mp layer: 7.09844970703125
Norm before input: 0.2552422881126404
Norm after input: 0.3616284728050232
Norm after each mp layer: 0.9397158026695251
Norm after each mp layer: 2.1560275554656982
Norm after each mp layer: 7.090512752532959
Epoch: 120, Loss: 0.6550, Energy: 155.2483, Train: 82.95%, Valid: 71.40%, Test: 69.00%, Best Valid: 71.40%, Best Test: 69.00%
Norm before input: 0.2552422881126404
Norm after input: 0.3616284728050232
Norm after each mp layer: 0.9397158026695251
Norm after each mp layer: 2.1560275554656982
Norm after each mp layer: 7.090512752532959
Norm before input: 0.2552422881126404
Norm after input: 0.36081671714782715
Norm after each mp layer: 0.9336535930633545
Norm after each mp layer: 2.145616054534912
Norm after each mp layer: 7.0793561935424805
Norm before input: 0.2552422881126404
Norm after input: 0.36081671714782715
Norm after each mp layer: 0.9336535930633545
Norm after each mp layer: 2.145616054534912
Norm after each mp layer: 7.0793561935424805
Norm before input: 0.2552422881126404
Norm after input: 0.36116886138916016
Norm after each mp layer: 0.9365354776382446
Norm after each mp layer: 2.1473183631896973
Norm after each mp layer: 7.042970180511475
Norm before input: 0.2552422881126404
Norm after input: 0.36116886138916016
Norm after each mp layer: 0.9365354776382446
Norm after each mp layer: 2.1473183631896973
Norm after each mp layer: 7.042970180511475
Norm before input: 0.2552422881126404
Norm after input: 0.3608870804309845
Norm after each mp layer: 0.9338338375091553
Norm after each mp layer: 2.139589548110962
Norm after each mp layer: 7.006992340087891
Norm before input: 0.2552422881126404
Norm after input: 0.3608870804309845
Norm after each mp layer: 0.9338338375091553
Norm after each mp layer: 2.139589548110962
Norm after each mp layer: 7.006992340087891
Norm before input: 0.2552422881126404
Norm after input: 0.36036115884780884
Norm after each mp layer: 0.929806113243103
Norm after each mp layer: 2.1302919387817383
Norm after each mp layer: 6.97664737701416
Norm before input: 0.2552422881126404
Norm after input: 0.36036115884780884
Norm after each mp layer: 0.929806113243103
Norm after each mp layer: 2.1302919387817383
Norm after each mp layer: 6.97664737701416
Norm before input: 0.2552422881126404
Norm after input: 0.3605548143386841
Norm after each mp layer: 0.9318753480911255
Norm after each mp layer: 2.1305084228515625
Norm after each mp layer: 6.93572473526001
Epoch: 125, Loss: 0.5964, Energy: 145.4056, Train: 84.85%, Valid: 70.60%, Test: 68.90%, Best Valid: 71.40%, Best Test: 69.10%
Norm before input: 0.2552422881126404
Norm after input: 0.3605548143386841
Norm after each mp layer: 0.9318753480911255
Norm after each mp layer: 2.1305084228515625
Norm after each mp layer: 6.93572473526001
Norm before input: 0.2552422881126404
Norm after input: 0.3596869707107544
Norm after each mp layer: 0.9245996475219727
Norm after each mp layer: 2.114575147628784
Norm after each mp layer: 6.901931285858154
Norm before input: 0.2552422881126404
Norm after input: 0.3596869707107544
Norm after each mp layer: 0.9245996475219727
Norm after each mp layer: 2.114575147628784
Norm after each mp layer: 6.901931285858154
Norm before input: 0.2552422881126404
Norm after input: 0.3600360155105591
Norm after each mp layer: 0.9279915690422058
Norm after each mp layer: 2.1165833473205566
Norm after each mp layer: 6.856794834136963
Norm before input: 0.2552422881126404
Norm after input: 0.3600360155105591
Norm after each mp layer: 0.9279915690422058
Norm after each mp layer: 2.1165833473205566
Norm after each mp layer: 6.856794834136963
Norm before input: 0.2552422881126404
Norm after input: 0.3590750992298126
Norm after each mp layer: 0.9209121465682983
Norm after each mp layer: 2.1019701957702637
Norm after each mp layer: 6.8317975997924805
Norm before input: 0.2552422881126404
Norm after input: 0.3590750992298126
Norm after each mp layer: 0.9209121465682983
Norm after each mp layer: 2.1019701957702637
Norm after each mp layer: 6.8317975997924805
Norm before input: 0.2552422881126404
Norm after input: 0.35936444997787476
Norm after each mp layer: 0.9236122965812683
Norm after each mp layer: 2.103015422821045
Norm after each mp layer: 6.78864860534668
Norm before input: 0.2552422881126404
Norm after input: 0.35936444997787476
Norm after each mp layer: 0.9236122965812683
Norm after each mp layer: 2.103015422821045
Norm after each mp layer: 6.78864860534668
Norm before input: 0.2552422881126404
Norm after input: 0.3586260676383972
Norm after each mp layer: 0.9181119799613953
Norm after each mp layer: 2.090898275375366
Norm after each mp layer: 6.762656211853027
Epoch: 130, Loss: 0.5430, Energy: 130.9467, Train: 85.93%, Valid: 70.40%, Test: 69.40%, Best Valid: 71.40%, Best Test: 69.40%
Norm before input: 0.2552422881126404
Norm after input: 0.3586260676383972
Norm after each mp layer: 0.9181119799613953
Norm after each mp layer: 2.090898275375366
Norm after each mp layer: 6.762656211853027
Norm before input: 0.2552422881126404
Norm after input: 0.3586482107639313
Norm after each mp layer: 0.9186925888061523
Norm after each mp layer: 2.0892751216888428
Norm after each mp layer: 6.730203151702881
Norm before input: 0.2552422881126404
Norm after input: 0.3586482107639313
Norm after each mp layer: 0.9186924695968628
Norm after each mp layer: 2.0892751216888428
Norm after each mp layer: 6.730203151702881
Norm before input: 0.2552422881126404
Norm after input: 0.35823768377304077
Norm after each mp layer: 0.9160272479057312
Norm after each mp layer: 2.0830371379852295
Norm after each mp layer: 6.709437370300293
Norm before input: 0.2552422881126404
Norm after input: 0.35823768377304077
Norm after each mp layer: 0.9160272479057312
Norm after each mp layer: 2.0830371379852295
Norm after each mp layer: 6.709437370300293
Norm before input: 0.2552422881126404
Norm after input: 0.35797959566116333
Norm after each mp layer: 0.9141131639480591
Norm after each mp layer: 2.0778722763061523
Norm after each mp layer: 6.687199592590332
Norm before input: 0.2552422881126404
Norm after input: 0.35797959566116333
Norm after each mp layer: 0.9141131639480591
Norm after each mp layer: 2.0778722763061523
Norm after each mp layer: 6.687199592590332
Norm before input: 0.2552422881126404
Norm after input: 0.3579900860786438
Norm after each mp layer: 0.9140604734420776
Norm after each mp layer: 2.0761077404022217
Norm after each mp layer: 6.664252758026123
Norm before input: 0.2552422881126404
Norm after input: 0.3579900860786438
Norm after each mp layer: 0.9140604734420776
Norm after each mp layer: 2.0761077404022217
Norm after each mp layer: 6.664252758026123
Norm before input: 0.2552422881126404
Norm after input: 0.3574539124965668
Norm after each mp layer: 0.909804105758667
Norm after each mp layer: 2.0680880546569824
Norm after each mp layer: 6.652890205383301
Epoch: 135, Loss: 0.4942, Energy: 119.5920, Train: 88.49%, Valid: 70.80%, Test: 69.60%, Best Valid: 71.40%, Best Test: 69.60%
Norm before input: 0.2552422881126404
Norm after input: 0.3574539124965668
Norm after each mp layer: 0.909804105758667
Norm after each mp layer: 2.0680880546569824
Norm after each mp layer: 6.652890205383301
Norm before input: 0.2552422881126404
Norm after input: 0.35784801840782166
Norm after each mp layer: 0.9121800661087036
Norm after each mp layer: 2.070887565612793
Norm after each mp layer: 6.62952995300293
Norm before input: 0.2552422881126404
Norm after input: 0.35784801840782166
Norm after each mp layer: 0.9121800661087036
Norm after each mp layer: 2.070887565612793
Norm after each mp layer: 6.62952995300293
Norm before input: 0.2552422881126404
Norm after input: 0.3569871783256531
Norm after each mp layer: 0.9047340154647827
Norm after each mp layer: 2.0574865341186523
Norm after each mp layer: 6.621575355529785
Norm before input: 0.2552422881126404
Norm after input: 0.3569871783256531
Norm after each mp layer: 0.9047340154647827
Norm after each mp layer: 2.0574865341186523
Norm after each mp layer: 6.621575355529785
Norm before input: 0.2552422881126404
Norm after input: 0.358001172542572
Norm after each mp layer: 0.9118682742118835
Norm after each mp layer: 2.069425582885742
Norm after each mp layer: 6.5989203453063965
Norm before input: 0.2552422881126404
Norm after input: 0.358001172542572
Norm after each mp layer: 0.9118682742118835
Norm after each mp layer: 2.069425582885742
Norm after each mp layer: 6.5989203453063965
Norm before input: 0.2552422881126404
Norm after input: 0.35588592290878296
Norm after each mp layer: 0.8930227756500244
Norm after each mp layer: 2.0369179248809814
Norm after each mp layer: 6.596399784088135
Norm before input: 0.2552422881126404
Norm after input: 0.35588592290878296
Norm after each mp layer: 0.8930227756500244
Norm after each mp layer: 2.0369179248809814
Norm after each mp layer: 6.596399784088135
Norm before input: 0.2552422881126404
Norm after input: 0.36027225852012634
Norm after each mp layer: 0.9338497519493103
Norm after each mp layer: 2.137535810470581
Norm after each mp layer: 6.677191734313965
Epoch: 140, Loss: 0.4914, Energy: 138.3642, Train: 85.76%, Valid: 70.60%, Test: 70.20%, Best Valid: 71.40%, Best Test: 71.00%
Norm before input: 0.2552422881126404
Norm after input: 0.36027225852012634
Norm after each mp layer: 0.9338497519493103
Norm after each mp layer: 2.137535810470581
Norm after each mp layer: 6.677191734313965
Norm before input: 0.2552422881126404
Norm after input: 0.3518246114253998
Norm after each mp layer: 0.8545256853103638
Norm after each mp layer: 2.022196054458618
Norm after each mp layer: 6.66733980178833
Norm before input: 0.2552422881126404
Norm after input: 0.3518246114253998
Norm after each mp layer: 0.854525625705719
Norm after each mp layer: 2.022196054458618
Norm after each mp layer: 6.66733980178833
Norm before input: 0.2552422881126404
Norm after input: 0.36290326714515686
Norm after each mp layer: 0.9574611783027649
Norm after each mp layer: 2.187818765640259
Norm after each mp layer: 6.430387020111084
Norm before input: 0.2552422881126404
Norm after input: 0.36290326714515686
Norm after each mp layer: 0.9574611783027649
Norm after each mp layer: 2.187818765640259
Norm after each mp layer: 6.430387020111084
Norm before input: 0.2552422881126404
Norm after input: 0.3592030704021454
Norm after each mp layer: 0.9349125027656555
Norm after each mp layer: 2.093125104904175
Norm after each mp layer: 6.1346964836120605
Norm before input: 0.2552422881126404
Norm after input: 0.3592030704021454
Norm after each mp layer: 0.9349125027656555
Norm after each mp layer: 2.093125104904175
Norm after each mp layer: 6.1346964836120605
Norm before input: 0.2552422881126404
Norm after input: 0.35376885533332825
Norm after each mp layer: 0.8868461847305298
Norm after each mp layer: 1.9717251062393188
Norm after each mp layer: 5.906129360198975
Norm before input: 0.2552422881126404
Norm after input: 0.35376885533332825
Norm after each mp layer: 0.8868461847305298
Norm after each mp layer: 1.9717251062393188
Norm after each mp layer: 5.906129360198975
Norm before input: 0.2552422881126404
Norm after input: 0.3542279601097107
Norm after each mp layer: 0.877278208732605
Norm after each mp layer: 1.9454498291015625
Norm after each mp layer: 5.625572681427002
Epoch: 145, Loss: 0.6314, Energy: 144.0629, Train: 81.79%, Valid: 65.60%, Test: 64.10%, Best Valid: 72.60%, Best Test: 71.00%
Norm before input: 0.2552422881126404
Norm after input: 0.3542279601097107
Norm after each mp layer: 0.877278208732605
Norm after each mp layer: 1.9454498291015625
Norm after each mp layer: 5.625572681427002
Norm before input: 0.2552422881126404
Norm after input: 0.3559426963329315
Norm after each mp layer: 0.895198404788971
Norm after each mp layer: 1.9626028537750244
Norm after each mp layer: 5.528845310211182
Norm before input: 0.2552422881126404
Norm after input: 0.3559426963329315
Norm after each mp layer: 0.895198404788971
Norm after each mp layer: 1.9626028537750244
Norm after each mp layer: 5.528845310211182
Norm before input: 0.2552422881126404
Norm after input: 0.3589053750038147
Norm after each mp layer: 0.9319691061973572
Norm after each mp layer: 2.0626659393310547
Norm after each mp layer: 5.593026638031006
Norm before input: 0.2552422881126404
Norm after input: 0.3589053750038147
Norm after each mp layer: 0.9319691061973572
Norm after each mp layer: 2.0626659393310547
Norm after each mp layer: 5.593026638031006
Norm before input: 0.2552422881126404
Norm after input: 0.3619934618473053
Norm after each mp layer: 0.9690300822257996
Norm after each mp layer: 2.1863439083099365
Norm after each mp layer: 5.681509971618652
Norm before input: 0.2552422881126404
Norm after input: 0.3619934618473053
Norm after each mp layer: 0.9690300822257996
Norm after each mp layer: 2.1863439083099365
Norm after each mp layer: 5.681509971618652
Norm before input: 0.2552422881126404
Norm after input: 0.36225947737693787
Norm after each mp layer: 0.9749175310134888
Norm after each mp layer: 2.2118935585021973
Norm after each mp layer: 5.672084808349609
Norm before input: 0.2552422881126404
Norm after input: 0.36225947737693787
Norm after each mp layer: 0.9749175310134888
Norm after each mp layer: 2.2118935585021973
Norm after each mp layer: 5.672084808349609
Norm before input: 0.2552422881126404
Norm after input: 0.3581218421459198
Norm after each mp layer: 0.947172224521637
Norm after each mp layer: 2.178658962249756
Norm after each mp layer: 5.699820041656494
Epoch: 150, Loss: 0.5491, Energy: 131.6049, Train: 91.31%, Valid: 77.00%, Test: 76.90%, Best Valid: 77.00%, Best Test: 76.90%
Norm before input: 0.2552422881126404
Norm after input: 0.3581218421459198
Norm after each mp layer: 0.947172224521637
Norm after each mp layer: 2.178658962249756
Norm after each mp layer: 5.699820041656494
Norm before input: 0.2552422881126404
Norm after input: 0.3556371033191681
Norm after each mp layer: 0.9217334985733032
Norm after each mp layer: 2.1465063095092773
Norm after each mp layer: 5.656479835510254
Norm before input: 0.2552422881126404
Norm after input: 0.3556371033191681
Norm after each mp layer: 0.9217334985733032
Norm after each mp layer: 2.1465063095092773
Norm after each mp layer: 5.656479835510254
Norm before input: 0.2552422881126404
Norm after input: 0.35653871297836304
Norm after each mp layer: 0.9133406281471252
Norm after each mp layer: 2.121995210647583
Norm after each mp layer: 5.520467281341553
Norm before input: 0.2552422881126404
Norm after input: 0.35653871297836304
Norm after each mp layer: 0.9133406281471252
Norm after each mp layer: 2.121995210647583
Norm after each mp layer: 5.520467281341553
Norm before input: 0.2552422881126404
Norm after input: 0.35857248306274414
Norm after each mp layer: 0.9240538477897644
Norm after each mp layer: 2.1276602745056152
Norm after each mp layer: 5.44528341293335
Norm before input: 0.2552422881126404
Norm after input: 0.35857248306274414
Norm after each mp layer: 0.9240538477897644
Norm after each mp layer: 2.1276602745056152
Norm after each mp layer: 5.44528341293335
Norm before input: 0.2552422881126404
Norm after input: 0.3594033420085907
Norm after each mp layer: 0.947463870048523
Norm after each mp layer: 2.1842901706695557
Norm after each mp layer: 5.492020606994629
Norm before input: 0.2552422881126404
Norm after input: 0.3594033420085907
Norm after each mp layer: 0.947463870048523
Norm after each mp layer: 2.1842901706695557
Norm after each mp layer: 5.492020606994629
Norm before input: 0.2552422881126404
Norm after input: 0.3600531816482544
Norm after each mp layer: 0.9733902812004089
Norm after each mp layer: 2.255702018737793
Norm after each mp layer: 5.563371181488037
Epoch: 155, Loss: 0.4478, Energy: 133.1771, Train: 91.89%, Valid: 78.80%, Test: 76.40%, Best Valid: 79.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3600531816482544
Norm after each mp layer: 0.9733902812004089
Norm after each mp layer: 2.255702018737793
Norm after each mp layer: 5.563371181488037
Norm before input: 0.2552422881126404
Norm after input: 0.36048227548599243
Norm after each mp layer: 0.9816185832023621
Norm after each mp layer: 2.268061399459839
Norm after each mp layer: 5.526072025299072
Norm before input: 0.2552422881126404
Norm after input: 0.36048227548599243
Norm after each mp layer: 0.9816185832023621
Norm after each mp layer: 2.268061399459839
Norm after each mp layer: 5.526072025299072
Norm before input: 0.2552422881126404
Norm after input: 0.3589181900024414
Norm after each mp layer: 0.9657293558120728
Norm after each mp layer: 2.2335073947906494
Norm after each mp layer: 5.461634635925293
Norm before input: 0.2552422881126404
Norm after input: 0.3589181900024414
Norm after each mp layer: 0.9657293558120728
Norm after each mp layer: 2.2335073947906494
Norm after each mp layer: 5.461634635925293
Norm before input: 0.2552422881126404
Norm after input: 0.3579680919647217
Norm after each mp layer: 0.946300745010376
Norm after each mp layer: 2.1807241439819336
Norm after each mp layer: 5.368477821350098
Norm before input: 0.2552422881126404
Norm after input: 0.3579680919647217
Norm after each mp layer: 0.946300745010376
Norm after each mp layer: 2.1807241439819336
Norm after each mp layer: 5.368477821350098
Norm before input: 0.2552422881126404
Norm after input: 0.35678648948669434
Norm after each mp layer: 0.9356149435043335
Norm after each mp layer: 2.1591672897338867
Norm after each mp layer: 5.3535871505737305
Norm before input: 0.2552422881126404
Norm after input: 0.35678648948669434
Norm after each mp layer: 0.9356149435043335
Norm after each mp layer: 2.1591672897338867
Norm after each mp layer: 5.3535871505737305
Norm before input: 0.2552422881126404
Norm after input: 0.3557467460632324
Norm after each mp layer: 0.9358753561973572
Norm after each mp layer: 2.1692214012145996
Norm after each mp layer: 5.396378040313721
Epoch: 160, Loss: 0.4252, Energy: 115.4477, Train: 92.63%, Valid: 78.00%, Test: 75.60%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3557467460632324
Norm after each mp layer: 0.9358753561973572
Norm after each mp layer: 2.1692214012145996
Norm after each mp layer: 5.396378040313721
Norm before input: 0.2552422881126404
Norm after input: 0.35597777366638184
Norm after each mp layer: 0.945364236831665
Norm after each mp layer: 2.193902015686035
Norm after each mp layer: 5.451382160186768
Norm before input: 0.2552422881126404
Norm after input: 0.35597777366638184
Norm after each mp layer: 0.945364236831665
Norm after each mp layer: 2.193902015686035
Norm after each mp layer: 5.451382160186768
Norm before input: 0.2552422881126404
Norm after input: 0.35712742805480957
Norm after each mp layer: 0.9587987065315247
Norm after each mp layer: 2.2265844345092773
Norm after each mp layer: 5.509660720825195
Norm before input: 0.2552422881126404
Norm after input: 0.35712742805480957
Norm after each mp layer: 0.9587987065315247
Norm after each mp layer: 2.2265844345092773
Norm after each mp layer: 5.509660720825195
Norm before input: 0.2552422881126404
Norm after input: 0.35873088240623474
Norm after each mp layer: 0.9700475931167603
Norm after each mp layer: 2.2511541843414307
Norm after each mp layer: 5.532818794250488
Norm before input: 0.2552422881126404
Norm after input: 0.35873088240623474
Norm after each mp layer: 0.9700475931167603
Norm after each mp layer: 2.2511541843414307
Norm after each mp layer: 5.532818794250488
Norm before input: 0.2552422881126404
Norm after input: 0.35869210958480835
Norm after each mp layer: 0.9706822037696838
Norm after each mp layer: 2.257896900177002
Norm after each mp layer: 5.55137825012207
Norm before input: 0.2552422881126404
Norm after input: 0.35869210958480835
Norm after each mp layer: 0.9706822037696838
Norm after each mp layer: 2.257896900177002
Norm after each mp layer: 5.55137825012207
Norm before input: 0.2552422881126404
Norm after input: 0.35671746730804443
Norm after each mp layer: 0.9583040475845337
Norm after each mp layer: 2.2391369342803955
Norm after each mp layer: 5.560156345367432
Epoch: 165, Loss: 0.3871, Energy: 114.4167, Train: 92.72%, Valid: 75.60%, Test: 76.40%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35671746730804443
Norm after each mp layer: 0.9583040475845337
Norm after each mp layer: 2.2391369342803955
Norm after each mp layer: 5.560156345367432
Norm before input: 0.2552422881126404
Norm after input: 0.3552352488040924
Norm after each mp layer: 0.9440306425094604
Norm after each mp layer: 2.207392692565918
Norm after each mp layer: 5.5261969566345215
Norm before input: 0.2552422881126404
Norm after input: 0.3552352488040924
Norm after each mp layer: 0.9440306425094604
Norm after each mp layer: 2.207392692565918
Norm after each mp layer: 5.5261969566345215
Norm before input: 0.2552422881126404
Norm after input: 0.3544042408466339
Norm after each mp layer: 0.9333916306495667
Norm after each mp layer: 2.1850528717041016
Norm after each mp layer: 5.502314567565918
Norm before input: 0.2552422881126404
Norm after input: 0.3544042408466339
Norm after each mp layer: 0.9333916306495667
Norm after each mp layer: 2.1850528717041016
Norm after each mp layer: 5.502314567565918
Norm before input: 0.2552422881126404
Norm after input: 0.35399743914604187
Norm after each mp layer: 0.9300945997238159
Norm after each mp layer: 2.183532476425171
Norm after each mp layer: 5.527390003204346
Norm before input: 0.2552422881126404
Norm after input: 0.35399743914604187
Norm after each mp layer: 0.9300945997238159
Norm after each mp layer: 2.183532476425171
Norm after each mp layer: 5.527390003204346
Norm before input: 0.2552422881126404
Norm after input: 0.35486501455307007
Norm after each mp layer: 0.9353724718093872
Norm after each mp layer: 2.18951678276062
Norm after each mp layer: 5.524094104766846
Norm before input: 0.2552422881126404
Norm after input: 0.35486501455307007
Norm after each mp layer: 0.9353724718093872
Norm after each mp layer: 2.18951678276062
Norm after each mp layer: 5.524094104766846
Norm before input: 0.2552422881126404
Norm after input: 0.3550540804862976
Norm after each mp layer: 0.9433270692825317
Norm after each mp layer: 2.2114152908325195
Norm after each mp layer: 5.566941261291504
Epoch: 170, Loss: 0.3486, Energy: 107.7769, Train: 93.87%, Valid: 75.20%, Test: 76.00%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3550540804862976
Norm after each mp layer: 0.9433270692825317
Norm after each mp layer: 2.2114152908325195
Norm after each mp layer: 5.566941261291504
Norm before input: 0.2552422881126404
Norm after input: 0.35478153824806213
Norm after each mp layer: 0.9459396004676819
Norm after each mp layer: 2.220564126968384
Norm after each mp layer: 5.608635425567627
Norm before input: 0.2552422881126404
Norm after input: 0.35478153824806213
Norm after each mp layer: 0.9459396004676819
Norm after each mp layer: 2.220564126968384
Norm after each mp layer: 5.608635425567627
Norm before input: 0.2552422881126404
Norm after input: 0.35388004779815674
Norm after each mp layer: 0.9345223307609558
Norm after each mp layer: 2.191270112991333
Norm after each mp layer: 5.60816764831543
Norm before input: 0.2552422881126404
Norm after input: 0.35388004779815674
Norm after each mp layer: 0.9345223307609558
Norm after each mp layer: 2.191270351409912
Norm after each mp layer: 5.60816764831543
Norm before input: 0.2552422881126404
Norm after input: 0.3538145422935486
Norm after each mp layer: 0.9236388802528381
Norm after each mp layer: 2.1557462215423584
Norm after each mp layer: 5.582132339477539
Norm before input: 0.2552422881126404
Norm after input: 0.3538145422935486
Norm after each mp layer: 0.9236388802528381
Norm after each mp layer: 2.1557462215423584
Norm after each mp layer: 5.582132339477539
Norm before input: 0.2552422881126404
Norm after input: 0.3537469208240509
Norm after each mp layer: 0.9200624227523804
Norm after each mp layer: 2.1470255851745605
Norm after each mp layer: 5.6047797203063965
Norm before input: 0.2552422881126404
Norm after input: 0.3537469208240509
Norm after each mp layer: 0.9200624227523804
Norm after each mp layer: 2.1470255851745605
Norm after each mp layer: 5.6047797203063965
Norm before input: 0.2552422881126404
Norm after input: 0.35371729731559753
Norm after each mp layer: 0.9241891503334045
Norm after each mp layer: 2.1606080532073975
Norm after each mp layer: 5.644333362579346
Epoch: 175, Loss: 0.3266, Energy: 105.1731, Train: 94.21%, Valid: 74.20%, Test: 75.20%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35371729731559753
Norm after each mp layer: 0.9241891503334045
Norm after each mp layer: 2.1606080532073975
Norm after each mp layer: 5.644333362579346
Norm before input: 0.2552422881126404
Norm after input: 0.35378536581993103
Norm after each mp layer: 0.9299149513244629
Norm after each mp layer: 2.1752865314483643
Norm after each mp layer: 5.666754245758057
Norm before input: 0.2552422881126404
Norm after input: 0.35378536581993103
Norm after each mp layer: 0.9299149513244629
Norm after each mp layer: 2.1752865314483643
Norm after each mp layer: 5.666754245758057
Norm before input: 0.2552422881126404
Norm after input: 0.3533213436603546
Norm after each mp layer: 0.9282424449920654
Norm after each mp layer: 2.1745331287384033
Norm after each mp layer: 5.679746627807617
Norm before input: 0.2552422881126404
Norm after input: 0.3533213436603546
Norm after each mp layer: 0.9282424449920654
Norm after each mp layer: 2.1745331287384033
Norm after each mp layer: 5.679746627807617
Norm before input: 0.2552422881126404
Norm after input: 0.353042334318161
Norm after each mp layer: 0.9206602573394775
Norm after each mp layer: 2.1499452590942383
Norm after each mp layer: 5.646921157836914
Norm before input: 0.2552422881126404
Norm after input: 0.353042334318161
Norm after each mp layer: 0.9206602573394775
Norm after each mp layer: 2.1499452590942383
Norm after each mp layer: 5.646921157836914
Norm before input: 0.2552422881126404
Norm after input: 0.35306277871131897
Norm after each mp layer: 0.913958728313446
Norm after each mp layer: 2.122323513031006
Norm after each mp layer: 5.594021797180176
Norm before input: 0.2552422881126404
Norm after input: 0.35306277871131897
Norm after each mp layer: 0.913958728313446
Norm after each mp layer: 2.122323513031006
Norm after each mp layer: 5.594021797180176
Norm before input: 0.2552422881126404
Norm after input: 0.3523363769054413
Norm after each mp layer: 0.9106276631355286
Norm after each mp layer: 2.117323875427246
Norm after each mp layer: 5.593873023986816
Epoch: 180, Loss: 0.3014, Energy: 92.2231, Train: 94.45%, Valid: 74.00%, Test: 74.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3523363769054413
Norm after each mp layer: 0.9106276631355286
Norm after each mp layer: 2.117323875427246
Norm after each mp layer: 5.593873023986816
Norm before input: 0.2552422881126404
Norm after input: 0.35215604305267334
Norm after each mp layer: 0.9124606251716614
Norm after each mp layer: 2.1229050159454346
Norm after each mp layer: 5.601672649383545
Norm before input: 0.2552422881126404
Norm after input: 0.35215604305267334
Norm after each mp layer: 0.9124606251716614
Norm after each mp layer: 2.1229050159454346
Norm after each mp layer: 5.601673126220703
Norm before input: 0.2552422881126404
Norm after input: 0.3527413606643677
Norm after each mp layer: 0.916515588760376
Norm after each mp layer: 2.1274805068969727
Norm after each mp layer: 5.597955226898193
Norm before input: 0.2552422881126404
Norm after input: 0.3527413606643677
Norm after each mp layer: 0.916515588760376
Norm after each mp layer: 2.1274805068969727
Norm after each mp layer: 5.597955226898193
Norm before input: 0.2552422881126404
Norm after input: 0.3528395891189575
Norm after each mp layer: 0.9168431162834167
Norm after each mp layer: 2.130450963973999
Norm after each mp layer: 5.613165855407715
Norm before input: 0.2552422881126404
Norm after input: 0.3528395891189575
Norm after each mp layer: 0.9168431162834167
Norm after each mp layer: 2.130450963973999
Norm after each mp layer: 5.613165855407715
Norm before input: 0.2552422881126404
Norm after input: 0.3523833155632019
Norm after each mp layer: 0.9114689826965332
Norm after each mp layer: 2.1217355728149414
Norm after each mp layer: 5.623034477233887
Norm before input: 0.2552422881126404
Norm after input: 0.3523833155632019
Norm after each mp layer: 0.9114689826965332
Norm after each mp layer: 2.1217358112335205
Norm after each mp layer: 5.623034477233887
Norm before input: 0.2552422881126404
Norm after input: 0.352260559797287
Norm after each mp layer: 0.9057359099388123
Norm after each mp layer: 2.1047704219818115
Norm after each mp layer: 5.605930805206299
Epoch: 185, Loss: 0.2841, Energy: 87.2935, Train: 94.78%, Valid: 73.40%, Test: 73.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.352260559797287
Norm after each mp layer: 0.9057359099388123
Norm after each mp layer: 2.1047704219818115
Norm after each mp layer: 5.605930805206299
Norm before input: 0.2552422881126404
Norm after input: 0.35195544362068176
Norm after each mp layer: 0.9025123119354248
Norm after each mp layer: 2.100374460220337
Norm after each mp layer: 5.61769437789917
Norm before input: 0.2552422881126404
Norm after input: 0.35195544362068176
Norm after each mp layer: 0.9025123119354248
Norm after each mp layer: 2.100374460220337
Norm after each mp layer: 5.61769437789917
Norm before input: 0.2552422881126404
Norm after input: 0.3516598045825958
Norm after each mp layer: 0.9036149978637695
Norm after each mp layer: 2.112452745437622
Norm after each mp layer: 5.6685709953308105
Norm before input: 0.2552422881126404
Norm after input: 0.3516598045825958
Norm after each mp layer: 0.9036149978637695
Norm after each mp layer: 2.112452745437622
Norm after each mp layer: 5.6685709953308105
Norm before input: 0.2552422881126404
Norm after input: 0.35235247015953064
Norm after each mp layer: 0.9073697328567505
Norm after each mp layer: 2.1158981323242188
Norm after each mp layer: 5.674120903015137
Norm before input: 0.2552422881126404
Norm after input: 0.35235247015953064
Norm after each mp layer: 0.9073697328567505
Norm after each mp layer: 2.1158981323242188
Norm after each mp layer: 5.674120903015137
Norm before input: 0.2552422881126404
Norm after input: 0.35227325558662415
Norm after each mp layer: 0.907179057598114
Norm after each mp layer: 2.1161038875579834
Norm after each mp layer: 5.686653137207031
Norm before input: 0.2552422881126404
Norm after input: 0.35227325558662415
Norm after each mp layer: 0.907179057598114
Norm after each mp layer: 2.1161038875579834
Norm after each mp layer: 5.686653137207031
Norm before input: 0.2552422881126404
Norm after input: 0.3512558937072754
Norm after each mp layer: 0.9018085598945618
Norm after each mp layer: 2.1134161949157715
Norm after each mp layer: 5.714755058288574
Epoch: 190, Loss: 0.2711, Energy: 85.9539, Train: 95.03%, Valid: 74.00%, Test: 73.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3512558937072754
Norm after each mp layer: 0.9018085598945618
Norm after each mp layer: 2.1134161949157715
Norm after each mp layer: 5.714755058288574
Norm before input: 0.2552422881126404
Norm after input: 0.3513231873512268
Norm after each mp layer: 0.8978157639503479
Norm after each mp layer: 2.097777843475342
Norm after each mp layer: 5.692685127258301
Norm before input: 0.2552422881126404
Norm after input: 0.3513231873512268
Norm after each mp layer: 0.8978157639503479
Norm after each mp layer: 2.097777843475342
Norm after each mp layer: 5.692685127258301
Norm before input: 0.2552422881126404
Norm after input: 0.3515579402446747
Norm after each mp layer: 0.8968873620033264
Norm after each mp layer: 2.091564655303955
Norm after each mp layer: 5.688040256500244
Norm before input: 0.2552422881126404
Norm after input: 0.3515579402446747
Norm after each mp layer: 0.8968873620033264
Norm after each mp layer: 2.091564655303955
Norm after each mp layer: 5.688040256500244
Norm before input: 0.2552422881126404
Norm after input: 0.35099849104881287
Norm after each mp layer: 0.8975263833999634
Norm after each mp layer: 2.1040375232696533
Norm after each mp layer: 5.734111785888672
Norm before input: 0.2552422881126404
Norm after input: 0.35099849104881287
Norm after each mp layer: 0.8975263833999634
Norm after each mp layer: 2.1040375232696533
Norm after each mp layer: 5.734111785888672
Norm before input: 0.2552422881126404
Norm after input: 0.3512527644634247
Norm after each mp layer: 0.8993648290634155
Norm after each mp layer: 2.105588436126709
Norm after each mp layer: 5.732340335845947
Norm before input: 0.2552422881126404
Norm after input: 0.3512527644634247
Norm after each mp layer: 0.8993648290634155
Norm after each mp layer: 2.105588436126709
Norm after each mp layer: 5.732340335845947
Norm before input: 0.2552422881126404
Norm after input: 0.35121697187423706
Norm after each mp layer: 0.8976495265960693
Norm after each mp layer: 2.100656509399414
Norm after each mp layer: 5.72902250289917
Epoch: 195, Loss: 0.2572, Energy: 82.8082, Train: 95.36%, Valid: 72.80%, Test: 72.80%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35121697187423706
Norm after each mp layer: 0.8976495265960693
Norm after each mp layer: 2.100656509399414
Norm after each mp layer: 5.72902250289917
Norm before input: 0.2552422881126404
Norm after input: 0.3505953848361969
Norm after each mp layer: 0.8935992121696472
Norm after each mp layer: 2.1004905700683594
Norm after each mp layer: 5.758199214935303
Norm before input: 0.2552422881126404
Norm after input: 0.3505953848361969
Norm after each mp layer: 0.8935992121696472
Norm after each mp layer: 2.1004905700683594
Norm after each mp layer: 5.758199691772461
Norm before input: 0.2552422881126404
Norm after input: 0.35107722878456116
Norm after each mp layer: 0.8931488990783691
Norm after each mp layer: 2.093437910079956
Norm after each mp layer: 5.7428388595581055
Norm before input: 0.2552422881126404
Norm after input: 0.35107722878456116
Norm after each mp layer: 0.8931488990783691
Norm after each mp layer: 2.093437910079956
Norm after each mp layer: 5.7428388595581055
Norm before input: 0.2552422881126404
Norm after input: 0.3511824309825897
Norm after each mp layer: 0.8944928050041199
Norm after each mp layer: 2.0980796813964844
Norm after each mp layer: 5.752683162689209
Norm before input: 0.2552422881126404
Norm after input: 0.3511824309825897
Norm after each mp layer: 0.8944928050041199
Norm after each mp layer: 2.0980796813964844
Norm after each mp layer: 5.752683162689209
Norm before input: 0.2552422881126404
Norm after input: 0.3505500853061676
Norm after each mp layer: 0.8948745727539062
Norm after each mp layer: 2.1123173236846924
Norm after each mp layer: 5.7970075607299805
Norm before input: 0.2552422881126404
Norm after input: 0.3505500853061676
Norm after each mp layer: 0.8948745727539062
Norm after each mp layer: 2.1123173236846924
Norm after each mp layer: 5.7970075607299805
Norm before input: 0.2552422881126404
Norm after input: 0.35093626379966736
Norm after each mp layer: 0.8949567675590515
Norm after each mp layer: 2.107351064682007
Norm after each mp layer: 5.780197620391846
Epoch: 200, Loss: 0.2472, Energy: 84.5557, Train: 95.78%, Valid: 72.60%, Test: 72.60%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35093626379966736
Norm after each mp layer: 0.8949567675590515
Norm after each mp layer: 2.107351064682007
Norm after each mp layer: 5.780197620391846
Norm before input: 0.2552422881126404
Norm after input: 0.3509768545627594
Norm after each mp layer: 0.8923561573028564
Norm after each mp layer: 2.0999200344085693
Norm after each mp layer: 5.771945953369141
Norm before input: 0.2552422881126404
Norm after input: 0.3509768545627594
Norm after each mp layer: 0.8923561573028564
Norm after each mp layer: 2.0999200344085693
Norm after each mp layer: 5.771946430206299
Norm before input: 0.2552422881126404
Norm after input: 0.35042738914489746
Norm after each mp layer: 0.8892987966537476
Norm after each mp layer: 2.099928617477417
Norm after each mp layer: 5.788048267364502
Norm before input: 0.2552422881126404
Norm after input: 0.35042738914489746
Norm after each mp layer: 0.8892987966537476
Norm after each mp layer: 2.099928617477417
Norm after each mp layer: 5.788048267364502
Norm before input: 0.2552422881126404
Norm after input: 0.3506234884262085
Norm after each mp layer: 0.8896793723106384
Norm after each mp layer: 2.0965349674224854
Norm after each mp layer: 5.769745826721191
Norm before input: 0.2552422881126404
Norm after input: 0.3506234884262085
Norm after each mp layer: 0.8896793723106384
Norm after each mp layer: 2.0965349674224854
Norm after each mp layer: 5.769745826721191
Norm before input: 0.2552422881126404
Norm after input: 0.35048627853393555
Norm after each mp layer: 0.8906983733177185
Norm after each mp layer: 2.10225772857666
Norm after each mp layer: 5.782172203063965
Norm before input: 0.2552422881126404
Norm after input: 0.35048627853393555
Norm after each mp layer: 0.8906983733177185
Norm after each mp layer: 2.10225772857666
Norm after each mp layer: 5.782172203063965
Norm before input: 0.2552422881126404
Norm after input: 0.35030966997146606
Norm after each mp layer: 0.8900808095932007
Norm after each mp layer: 2.1036040782928467
Norm after each mp layer: 5.791783809661865
Epoch: 205, Loss: 0.2345, Energy: 80.2756, Train: 96.11%, Valid: 72.80%, Test: 73.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35030966997146606
Norm after each mp layer: 0.8900808095932007
Norm after each mp layer: 2.1036040782928467
Norm after each mp layer: 5.791783809661865
Norm before input: 0.2552422881126404
Norm after input: 0.35048338770866394
Norm after each mp layer: 0.8880807161331177
Norm after each mp layer: 2.0925753116607666
Norm after each mp layer: 5.76761531829834
Norm before input: 0.2552422881126404
Norm after input: 0.35048338770866394
Norm after each mp layer: 0.8880807161331177
Norm after each mp layer: 2.0925753116607666
Norm after each mp layer: 5.76761531829834
Norm before input: 0.2552422881126404
Norm after input: 0.3500341773033142
Norm after each mp layer: 0.8851011395454407
Norm after each mp layer: 2.089416742324829
Norm after each mp layer: 5.7731242179870605
Norm before input: 0.2552422881126404
Norm after input: 0.3500341773033142
Norm after each mp layer: 0.8851011395454407
Norm after each mp layer: 2.089416742324829
Norm after each mp layer: 5.7731242179870605
Norm before input: 0.2552422881126404
Norm after input: 0.3499821722507477
Norm after each mp layer: 0.8848844170570374
Norm after each mp layer: 2.089590072631836
Norm after each mp layer: 5.775723934173584
Norm before input: 0.2552422881126404
Norm after input: 0.3499821722507477
Norm after each mp layer: 0.8848844170570374
Norm after each mp layer: 2.089590072631836
Norm after each mp layer: 5.775723934173584
Norm before input: 0.2552422881126404
Norm after input: 0.35024017095565796
Norm after each mp layer: 0.886339545249939
Norm after each mp layer: 2.0909616947174072
Norm after each mp layer: 5.774455547332764
Norm before input: 0.2552422881126404
Norm after input: 0.35024017095565796
Norm after each mp layer: 0.886339545249939
Norm after each mp layer: 2.0909616947174072
Norm after each mp layer: 5.774455547332764
Norm before input: 0.2552422881126404
Norm after input: 0.3499285876750946
Norm after each mp layer: 0.8854067921638489
Norm after each mp layer: 2.094526529312134
Norm after each mp layer: 5.7916083335876465
Epoch: 210, Loss: 0.2247, Energy: 77.5295, Train: 96.36%, Valid: 71.80%, Test: 71.60%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3499285876750946
Norm after each mp layer: 0.8854067921638489
Norm after each mp layer: 2.094526529312134
Norm after each mp layer: 5.7916083335876465
Norm before input: 0.2552422881126404
Norm after input: 0.3499184548854828
Norm after each mp layer: 0.8834078907966614
Norm after each mp layer: 2.0876245498657227
Norm after each mp layer: 5.777888774871826
Norm before input: 0.2552422881126404
Norm after input: 0.3499184548854828
Norm after each mp layer: 0.8834078907966614
Norm after each mp layer: 2.0876245498657227
Norm after each mp layer: 5.777888774871826
Norm before input: 0.2552422881126404
Norm after input: 0.34976109862327576
Norm after each mp layer: 0.8813719749450684
Norm after each mp layer: 2.084141254425049
Norm after each mp layer: 5.776901721954346
Norm before input: 0.2552422881126404
Norm after input: 0.34976109862327576
Norm after each mp layer: 0.8813719749450684
Norm after each mp layer: 2.084141254425049
Norm after each mp layer: 5.776901721954346
Norm before input: 0.2552422881126404
Norm after input: 0.3496452867984772
Norm after each mp layer: 0.8810481429100037
Norm after each mp layer: 2.086469888687134
Norm after each mp layer: 5.786986827850342
Norm before input: 0.2552422881126404
Norm after input: 0.3496452867984772
Norm after each mp layer: 0.8810481429100037
Norm after each mp layer: 2.086469888687134
Norm after each mp layer: 5.786986827850342
Norm before input: 0.2552422881126404
Norm after input: 0.34983277320861816
Norm after each mp layer: 0.8819506168365479
Norm after each mp layer: 2.086547374725342
Norm after each mp layer: 5.780709266662598
Norm before input: 0.2552422881126404
Norm after input: 0.34983277320861816
Norm after each mp layer: 0.8819506168365479
Norm after each mp layer: 2.086547374725342
Norm after each mp layer: 5.780709266662598
Norm before input: 0.2552422881126404
Norm after input: 0.34947797656059265
Norm after each mp layer: 0.8807346820831299
Norm after each mp layer: 2.0893805027008057
Norm after each mp layer: 5.795501708984375
Epoch: 215, Loss: 0.2153, Energy: 75.5647, Train: 96.44%, Valid: 72.20%, Test: 71.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.34947797656059265
Norm after each mp layer: 0.8807346820831299
Norm after each mp layer: 2.0893805027008057
Norm after each mp layer: 5.795501708984375
Norm before input: 0.2552422881126404
Norm after input: 0.34953486919403076
Norm after each mp layer: 0.8793156147003174
Norm after each mp layer: 2.0837130546569824
Norm after each mp layer: 5.785120964050293
Norm before input: 0.2552422881126404
Norm after input: 0.34953486919403076
Norm after each mp layer: 0.8793156147003174
Norm after each mp layer: 2.0837130546569824
Norm after each mp layer: 5.785120964050293
Norm before input: 0.2552422881126404
Norm after input: 0.34946319460868835
Norm after each mp layer: 0.8780822157859802
Norm after each mp layer: 2.0818426609039307
Norm after each mp layer: 5.7879958152771
Norm before input: 0.2552422881126404
Norm after input: 0.34946319460868835
Norm after each mp layer: 0.8780822157859802
Norm after each mp layer: 2.0818426609039307
Norm after each mp layer: 5.7879958152771
Norm before input: 0.2552422881126404
Norm after input: 0.3493055999279022
Norm after each mp layer: 0.87788325548172
Norm after each mp layer: 2.085148811340332
Norm after each mp layer: 5.801029205322266
Norm before input: 0.2552422881126404
Norm after input: 0.3493055999279022
Norm after each mp layer: 0.87788325548172
Norm after each mp layer: 2.085148811340332
Norm after each mp layer: 5.801029205322266
Norm before input: 0.2552422881126404
Norm after input: 0.3494049310684204
Norm after each mp layer: 0.8784308433532715
Norm after each mp layer: 2.0860164165496826
Norm after each mp layer: 5.801552772521973
Norm before input: 0.2552422881126404
Norm after input: 0.3494049310684204
Norm after each mp layer: 0.8784308433532715
Norm after each mp layer: 2.0860164165496826
Norm after each mp layer: 5.801552772521973
Norm before input: 0.2552422881126404
Norm after input: 0.3491366505622864
Norm after each mp layer: 0.8772435784339905
Norm after each mp layer: 2.0887978076934814
Norm after each mp layer: 5.820990562438965
Epoch: 220, Loss: 0.2063, Energy: 74.6879, Train: 96.77%, Valid: 72.80%, Test: 71.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3491366505622864
Norm after each mp layer: 0.8772435784339905
Norm after each mp layer: 2.0887978076934814
Norm after each mp layer: 5.820990562438965
Norm before input: 0.2552422881126404
Norm after input: 0.34929049015045166
Norm after each mp layer: 0.8763104677200317
Norm after each mp layer: 2.0835061073303223
Norm after each mp layer: 5.811588764190674
Norm before input: 0.2552422881126404
Norm after input: 0.34929049015045166
Norm after each mp layer: 0.8763104677200317
Norm after each mp layer: 2.0835061073303223
Norm after each mp layer: 5.811588764190674
Norm before input: 0.2552422881126404
Norm after input: 0.34893861413002014
Norm after each mp layer: 0.8750278353691101
Norm after each mp layer: 2.0868468284606934
Norm after each mp layer: 5.83230447769165
Norm before input: 0.2552422881126404
Norm after input: 0.34893861413002014
Norm after each mp layer: 0.8750278353691101
Norm after each mp layer: 2.0868468284606934
Norm after each mp layer: 5.83230447769165
Norm before input: 0.2552422881126404
Norm after input: 0.3490976095199585
Norm after each mp layer: 0.875531792640686
Norm after each mp layer: 2.086068630218506
Norm after each mp layer: 5.828678607940674
Norm before input: 0.2552422881126404
Norm after input: 0.3490976095199585
Norm after each mp layer: 0.875531792640686
Norm after each mp layer: 2.086068630218506
Norm after each mp layer: 5.828678607940674
Norm before input: 0.2552422881126404
Norm after input: 0.348878413438797
Norm after each mp layer: 0.8750340342521667
Norm after each mp layer: 2.089961528778076
Norm after each mp layer: 5.84890079498291
Norm before input: 0.2552422881126404
Norm after input: 0.348878413438797
Norm after each mp layer: 0.8750340342521667
Norm after each mp layer: 2.089961528778076
Norm after each mp layer: 5.84890079498291
Norm before input: 0.2552422881126404
Norm after input: 0.348991721868515
Norm after each mp layer: 0.8743191361427307
Norm after each mp layer: 2.084817409515381
Norm after each mp layer: 5.836803436279297
Epoch: 225, Loss: 0.1980, Energy: 73.8960, Train: 97.27%, Valid: 73.20%, Test: 70.70%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.348991721868515
Norm after each mp layer: 0.8743191361427307
Norm after each mp layer: 2.084817409515381
Norm after each mp layer: 5.836803436279297
Norm before input: 0.2552422881126404
Norm after input: 0.3485536575317383
Norm after each mp layer: 0.8724596500396729
Norm after each mp layer: 2.087223529815674
Norm after each mp layer: 5.857173442840576
Norm before input: 0.2552422881126404
Norm after input: 0.3485536575317383
Norm after each mp layer: 0.8724596500396729
Norm after each mp layer: 2.087223529815674
Norm after each mp layer: 5.857173442840576
Norm before input: 0.2552422881126404
Norm after input: 0.3489179313182831
Norm after each mp layer: 0.8729535341262817
Norm after each mp layer: 2.081620454788208
Norm after each mp layer: 5.836432456970215
Norm before input: 0.2552422881126404
Norm after input: 0.3489179313182831
Norm after each mp layer: 0.8729535341262817
Norm after each mp layer: 2.081620454788208
Norm after each mp layer: 5.836432456970215
Norm before input: 0.2552422881126404
Norm after input: 0.34820303320884705
Norm after each mp layer: 0.8717929720878601
Norm after each mp layer: 2.0941579341888428
Norm after each mp layer: 5.889870643615723
Norm before input: 0.2552422881126404
Norm after input: 0.34820303320884705
Norm after each mp layer: 0.8717929720878601
Norm after each mp layer: 2.0941579341888428
Norm after each mp layer: 5.889870643615723
Norm before input: 0.2552422881126404
Norm after input: 0.34930363297462463
Norm after each mp layer: 0.8734961748123169
Norm after each mp layer: 2.0764732360839844
Norm after each mp layer: 5.820797920227051
Norm before input: 0.2552422881126404
Norm after input: 0.34930363297462463
Norm after each mp layer: 0.8734961748123169
Norm after each mp layer: 2.0764732360839844
Norm after each mp layer: 5.820797920227051
Norm before input: 0.2552422881126404
Norm after input: 0.3451833426952362
Norm after each mp layer: 0.8697353601455688
Norm after each mp layer: 2.1790173053741455
Norm after each mp layer: 6.2698974609375
Epoch: 230, Loss: 0.2247, Energy: 83.1582, Train: 92.88%, Valid: 70.60%, Test: 67.60%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3451833426952362
Norm after each mp layer: 0.8697353601455688
Norm after each mp layer: 2.1790173053741455
Norm after each mp layer: 6.2698974609375
Norm before input: 0.2552422881126404
Norm after input: 0.354990690946579
Norm after each mp layer: 0.8925463557243347
Norm after each mp layer: 2.082301616668701
Norm after each mp layer: 5.871987819671631
Norm before input: 0.2552422881126404
Norm after input: 0.354990690946579
Norm after each mp layer: 0.8925463557243347
Norm after each mp layer: 2.082301616668701
Norm after each mp layer: 5.871987819671631
Norm before input: 0.2552422881126404
Norm after input: 0.34228411316871643
Norm after each mp layer: 0.8687934875488281
Norm after each mp layer: 2.228320837020874
Norm after each mp layer: 6.224672794342041
Norm before input: 0.2552422881126404
Norm after input: 0.34228411316871643
Norm after each mp layer: 0.8687934875488281
Norm after each mp layer: 2.228320837020874
Norm after each mp layer: 6.224672794342041
Norm before input: 0.2552422881126404
Norm after input: 0.3433607816696167
Norm after each mp layer: 0.8546609282493591
Norm after each mp layer: 2.1399073600769043
Norm after each mp layer: 5.90942907333374
Norm before input: 0.2552422881126404
Norm after input: 0.3433607816696167
Norm after each mp layer: 0.8546609282493591
Norm after each mp layer: 2.1399073600769043
Norm after each mp layer: 5.90942907333374
Norm before input: 0.2552422881126404
Norm after input: 0.34868574142456055
Norm after each mp layer: 0.8504579067230225
Norm after each mp layer: 2.0143134593963623
Norm after each mp layer: 5.456432342529297
Norm before input: 0.2552422881126404
Norm after input: 0.34868574142456055
Norm after each mp layer: 0.8504579067230225
Norm after each mp layer: 2.0143134593963623
Norm after each mp layer: 5.456431865692139
Norm before input: 0.2552422881126404
Norm after input: 0.3528256118297577
Norm after each mp layer: 0.8706126809120178
Norm after each mp layer: 2.0423033237457275
Norm after each mp layer: 5.469690322875977
Epoch: 235, Loss: 0.3230, Energy: 99.5703, Train: 84.35%, Valid: 60.60%, Test: 60.60%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3528256118297577
Norm after each mp layer: 0.8706126809120178
Norm after each mp layer: 2.0423033237457275
Norm after each mp layer: 5.469690322875977
Norm before input: 0.2552422881126404
Norm after input: 0.3560628592967987
Norm after each mp layer: 0.8885596990585327
Norm after each mp layer: 2.0527942180633545
Norm after each mp layer: 5.445353984832764
Norm before input: 0.2552422881126404
Norm after input: 0.3560628592967987
Norm after each mp layer: 0.8885596990585327
Norm after each mp layer: 2.0527942180633545
Norm after each mp layer: 5.445353984832764
Norm before input: 0.2552422881126404
Norm after input: 0.350503534078598
Norm after each mp layer: 0.8439070582389832
Norm after each mp layer: 1.9767190217971802
Norm after each mp layer: 5.251047134399414
Norm before input: 0.2552422881126404
Norm after input: 0.350503534078598
Norm after each mp layer: 0.8439070582389832
Norm after each mp layer: 1.9767190217971802
Norm after each mp layer: 5.251047134399414
Norm before input: 0.2552422881126404
Norm after input: 0.34687018394470215
Norm after each mp layer: 0.8300490975379944
Norm after each mp layer: 1.9745545387268066
Norm after each mp layer: 5.214400768280029
Norm before input: 0.2552422881126404
Norm after input: 0.34687018394470215
Norm after each mp layer: 0.8300490975379944
Norm after each mp layer: 1.9745545387268066
Norm after each mp layer: 5.214400768280029
Norm before input: 0.2552422881126404
Norm after input: 0.34515005350112915
Norm after each mp layer: 0.8332601189613342
Norm after each mp layer: 2.0034639835357666
Norm after each mp layer: 5.182312488555908
Norm before input: 0.2552422881126404
Norm after input: 0.34515005350112915
Norm after each mp layer: 0.8332601189613342
Norm after each mp layer: 2.0034639835357666
Norm after each mp layer: 5.182312488555908
Norm before input: 0.2552422881126404
Norm after input: 0.344396710395813
Norm after each mp layer: 0.826579749584198
Norm after each mp layer: 1.9836680889129639
Norm after each mp layer: 5.058159351348877
Epoch: 240, Loss: 0.4440, Energy: 96.8633, Train: 91.14%, Valid: 69.20%, Test: 67.00%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.344396710395813
Norm after each mp layer: 0.826579749584198
Norm after each mp layer: 1.9836680889129639
Norm after each mp layer: 5.058159351348877
Norm before input: 0.2552422881126404
Norm after input: 0.34525108337402344
Norm after each mp layer: 0.8189727067947388
Norm after each mp layer: 1.9452770948410034
Norm after each mp layer: 4.932326793670654
Norm before input: 0.2552422881126404
Norm after input: 0.34525108337402344
Norm after each mp layer: 0.8189727067947388
Norm after each mp layer: 1.9452770948410034
Norm after each mp layer: 4.932326793670654
Norm before input: 0.2552422881126404
Norm after input: 0.347709059715271
Norm after each mp layer: 0.8241287469863892
Norm after each mp layer: 1.9388364553451538
Norm after each mp layer: 4.8758769035339355
Norm before input: 0.2552422881126404
Norm after input: 0.347709059715271
Norm after each mp layer: 0.8241287469863892
Norm after each mp layer: 1.9388364553451538
Norm after each mp layer: 4.8758769035339355
Norm before input: 0.2552422881126404
Norm after input: 0.3507775366306305
Norm after each mp layer: 0.8385311365127563
Norm after each mp layer: 1.9663751125335693
Norm after each mp layer: 4.906338691711426
Norm before input: 0.2552422881126404
Norm after input: 0.3507775366306305
Norm after each mp layer: 0.8385311365127563
Norm after each mp layer: 1.9663751125335693
Norm after each mp layer: 4.906338691711426
Norm before input: 0.2552422881126404
Norm after input: 0.353000670671463
Norm after each mp layer: 0.841591477394104
Norm after each mp layer: 1.9807790517807007
Norm after each mp layer: 4.964536190032959
Norm before input: 0.2552422881126404
Norm after input: 0.353000670671463
Norm after each mp layer: 0.841591477394104
Norm after each mp layer: 1.9807790517807007
Norm after each mp layer: 4.964536190032959
Norm before input: 0.2552422881126404
Norm after input: 0.35481777787208557
Norm after each mp layer: 0.8496642708778381
Norm after each mp layer: 2.0227277278900146
Norm after each mp layer: 5.095503807067871
Epoch: 245, Loss: 0.3084, Energy: 55.0895, Train: 92.80%, Valid: 70.00%, Test: 69.50%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35481777787208557
Norm after each mp layer: 0.8496642708778381
Norm after each mp layer: 2.0227277278900146
Norm after each mp layer: 5.095503807067871
Norm before input: 0.2552422881126404
Norm after input: 0.3564334809780121
Norm after each mp layer: 0.8669102787971497
Norm after each mp layer: 2.091120481491089
Norm after each mp layer: 5.295498847961426
Norm before input: 0.2552422881126404
Norm after input: 0.3564334809780121
Norm after each mp layer: 0.8669102787971497
Norm after each mp layer: 2.091120481491089
Norm after each mp layer: 5.295498847961426
Norm before input: 0.2552422881126404
Norm after input: 0.3568631410598755
Norm after each mp layer: 0.8762228488922119
Norm after each mp layer: 2.1496329307556152
Norm after each mp layer: 5.491703987121582
Norm before input: 0.2552422881126404
Norm after input: 0.3568631410598755
Norm after each mp layer: 0.8762228488922119
Norm after each mp layer: 2.1496329307556152
Norm after each mp layer: 5.491703987121582
Norm before input: 0.2552422881126404
Norm after input: 0.3560766279697418
Norm after each mp layer: 0.8701946139335632
Norm after each mp layer: 2.165224075317383
Norm after each mp layer: 5.602591514587402
Norm before input: 0.2552422881126404
Norm after input: 0.3560766279697418
Norm after each mp layer: 0.8701946139335632
Norm after each mp layer: 2.165224075317383
Norm after each mp layer: 5.602591514587402
Norm before input: 0.2552422881126404
Norm after input: 0.355925977230072
Norm after each mp layer: 0.8692741394042969
Norm after each mp layer: 2.1825687885284424
Norm after each mp layer: 5.705130100250244
Norm before input: 0.2552422881126404
Norm after input: 0.355925977230072
Norm after each mp layer: 0.8692741394042969
Norm after each mp layer: 2.1825687885284424
Norm after each mp layer: 5.705130100250244
Norm before input: 0.2552422881126404
Norm after input: 0.35645514726638794
Norm after each mp layer: 0.8751124739646912
Norm after each mp layer: 2.206935167312622
Norm after each mp layer: 5.802055835723877
Epoch: 250, Loss: 0.2745, Energy: 114.8454, Train: 95.94%, Valid: 74.40%, Test: 73.60%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35645514726638794
Norm after each mp layer: 0.8751124739646912
Norm after each mp layer: 2.206935167312622
Norm after each mp layer: 5.802055835723877
Norm before input: 0.2552422881126404
Norm after input: 0.356193482875824
Norm after each mp layer: 0.8767906427383423
Norm after each mp layer: 2.2188165187835693
Norm after each mp layer: 5.835353851318359
Norm before input: 0.2552422881126404
Norm after input: 0.356193482875824
Norm after each mp layer: 0.8767906427383423
Norm after each mp layer: 2.2188165187835693
Norm after each mp layer: 5.835353851318359
Norm before input: 0.2552422881126404
Norm after input: 0.35664185881614685
Norm after each mp layer: 0.8763595223426819
Norm after each mp layer: 2.212069511413574
Norm after each mp layer: 5.817837238311768
Norm before input: 0.2552422881126404
Norm after input: 0.35664185881614685
Norm after each mp layer: 0.8763595223426819
Norm after each mp layer: 2.212069511413574
Norm after each mp layer: 5.817837238311768
Norm before input: 0.2552422881126404
Norm after input: 0.35807156562805176
Norm after each mp layer: 0.8764994740486145
Norm after each mp layer: 2.1951162815093994
Norm after each mp layer: 5.772347927093506
Norm before input: 0.2552422881126404
Norm after input: 0.35807156562805176
Norm after each mp layer: 0.8764994740486145
Norm after each mp layer: 2.1951162815093994
Norm after each mp layer: 5.772347927093506
Norm before input: 0.2552422881126404
Norm after input: 0.35915935039520264
Norm after each mp layer: 0.8760905265808105
Norm after each mp layer: 2.179849624633789
Norm after each mp layer: 5.729719161987305
Norm before input: 0.2552422881126404
Norm after input: 0.35915935039520264
Norm after each mp layer: 0.8760905265808105
Norm after each mp layer: 2.179849624633789
Norm after each mp layer: 5.729719161987305
Norm before input: 0.2552422881126404
Norm after input: 0.35797402262687683
Norm after each mp layer: 0.8734942078590393
Norm after each mp layer: 2.178248643875122
Norm after each mp layer: 5.7076263427734375
Epoch: 255, Loss: 0.2466, Energy: 106.0684, Train: 96.19%, Valid: 72.80%, Test: 71.30%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35797402262687683
Norm after each mp layer: 0.8734942078590393
Norm after each mp layer: 2.178248643875122
Norm after each mp layer: 5.7076263427734375
Norm before input: 0.2552422881126404
Norm after input: 0.3579046130180359
Norm after each mp layer: 0.8707722425460815
Norm after each mp layer: 2.1680850982666016
Norm after each mp layer: 5.683985233306885
Norm before input: 0.2552422881126404
Norm after input: 0.3579046130180359
Norm after each mp layer: 0.8707722425460815
Norm after each mp layer: 2.1680850982666016
Norm after each mp layer: 5.683985233306885
Norm before input: 0.2552422881126404
Norm after input: 0.35724523663520813
Norm after each mp layer: 0.863616943359375
Norm after each mp layer: 2.1485369205474854
Norm after each mp layer: 5.641106605529785
Norm before input: 0.2552422881126404
Norm after input: 0.35724523663520813
Norm after each mp layer: 0.863616943359375
Norm after each mp layer: 2.1485369205474854
Norm after each mp layer: 5.641106605529785
Norm before input: 0.2552422881126404
Norm after input: 0.35594236850738525
Norm after each mp layer: 0.8556437492370605
Norm after each mp layer: 2.1313250064849854
Norm after each mp layer: 5.588144302368164
Norm before input: 0.2552422881126404
Norm after input: 0.35594236850738525
Norm after each mp layer: 0.8556437492370605
Norm after each mp layer: 2.1313250064849854
Norm after each mp layer: 5.588144302368164
Norm before input: 0.2552422881126404
Norm after input: 0.35558027029037476
Norm after each mp layer: 0.8553672432899475
Norm after each mp layer: 2.1319005489349365
Norm after each mp layer: 5.566803455352783
Norm before input: 0.2552422881126404
Norm after input: 0.35558027029037476
Norm after each mp layer: 0.8553672432899475
Norm after each mp layer: 2.1319005489349365
Norm after each mp layer: 5.566803455352783
Norm before input: 0.2552422881126404
Norm after input: 0.3564792573451996
Norm after each mp layer: 0.8579898476600647
Norm after each mp layer: 2.130293607711792
Norm after each mp layer: 5.537734031677246
Epoch: 260, Loss: 0.2258, Energy: 86.8306, Train: 96.44%, Valid: 72.80%, Test: 72.50%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3564792573451996
Norm after each mp layer: 0.8579898476600647
Norm after each mp layer: 2.130293607711792
Norm after each mp layer: 5.537734031677246
Norm before input: 0.2552422881126404
Norm after input: 0.3575741946697235
Norm after each mp layer: 0.8579851388931274
Norm after each mp layer: 2.1188971996307373
Norm after each mp layer: 5.489014148712158
Norm before input: 0.2552422881126404
Norm after input: 0.3575741946697235
Norm after each mp layer: 0.8579851388931274
Norm after each mp layer: 2.1188971996307373
Norm after each mp layer: 5.489014625549316
Norm before input: 0.2552422881126404
Norm after input: 0.35706448554992676
Norm after each mp layer: 0.8553745150566101
Norm after each mp layer: 2.116086959838867
Norm after each mp layer: 5.456726551055908
Norm before input: 0.2552422881126404
Norm after input: 0.35706448554992676
Norm after each mp layer: 0.8553745150566101
Norm after each mp layer: 2.116086959838867
Norm after each mp layer: 5.456726551055908
Norm before input: 0.2552422881126404
Norm after input: 0.35745471715927124
Norm after each mp layer: 0.8590279817581177
Norm after each mp layer: 2.1255810260772705
Norm after each mp layer: 5.4581618309021
Norm before input: 0.2552422881126404
Norm after input: 0.35745471715927124
Norm after each mp layer: 0.8590279817581177
Norm after each mp layer: 2.1255810260772705
Norm after each mp layer: 5.4581618309021
Norm before input: 0.2552422881126404
Norm after input: 0.3576298952102661
Norm after each mp layer: 0.851243793964386
Norm after each mp layer: 2.1000068187713623
Norm after each mp layer: 5.410654544830322
Norm before input: 0.2552422881126404
Norm after input: 0.3576298952102661
Norm after each mp layer: 0.851243793964386
Norm after each mp layer: 2.1000068187713623
Norm after each mp layer: 5.410654544830322
Norm before input: 0.2552422881126404
Norm after input: 0.3552877604961395
Norm after each mp layer: 0.8513777256011963
Norm after each mp layer: 2.1277294158935547
Norm after each mp layer: 5.459989547729492
Epoch: 265, Loss: 0.2246, Energy: 76.7386, Train: 96.61%, Valid: 72.60%, Test: 71.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3552877604961395
Norm after each mp layer: 0.8513777256011963
Norm after each mp layer: 2.1277294158935547
Norm after each mp layer: 5.459989547729492
Norm before input: 0.2552422881126404
Norm after input: 0.3547295331954956
Norm after each mp layer: 0.8494789004325867
Norm after each mp layer: 2.1304428577423096
Norm after each mp layer: 5.4665656089782715
Norm before input: 0.2552422881126404
Norm after input: 0.3547295331954956
Norm after each mp layer: 0.8494789004325867
Norm after each mp layer: 2.1304428577423096
Norm after each mp layer: 5.4665656089782715
Norm before input: 0.2552422881126404
Norm after input: 0.35573723912239075
Norm after each mp layer: 0.8474672436714172
Norm after each mp layer: 2.1147854328155518
Norm after each mp layer: 5.436952590942383
Norm before input: 0.2552422881126404
Norm after input: 0.35573723912239075
Norm after each mp layer: 0.8474672436714172
Norm after each mp layer: 2.1147854328155518
Norm after each mp layer: 5.436952590942383
Norm before input: 0.2552422881126404
Norm after input: 0.3561604619026184
Norm after each mp layer: 0.8495092988014221
Norm after each mp layer: 2.118215322494507
Norm after each mp layer: 5.439158916473389
Norm before input: 0.2552422881126404
Norm after input: 0.3561604619026184
Norm after each mp layer: 0.8495092988014221
Norm after each mp layer: 2.118215322494507
Norm after each mp layer: 5.439158916473389
Norm before input: 0.2552422881126404
Norm after input: 0.35536137223243713
Norm after each mp layer: 0.8536333441734314
Norm after each mp layer: 2.1397974491119385
Norm after each mp layer: 5.467859745025635
Norm before input: 0.2552422881126404
Norm after input: 0.35536137223243713
Norm after each mp layer: 0.8536333441734314
Norm after each mp layer: 2.1397974491119385
Norm after each mp layer: 5.467859745025635
Norm before input: 0.2552422881126404
Norm after input: 0.3556442856788635
Norm after each mp layer: 0.8560062646865845
Norm after each mp layer: 2.1439707279205322
Norm after each mp layer: 5.471992015838623
Epoch: 270, Loss: 0.1994, Energy: 77.7729, Train: 97.27%, Valid: 72.40%, Test: 72.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3556442856788635
Norm after each mp layer: 0.8560062646865845
Norm after each mp layer: 2.1439707279205322
Norm after each mp layer: 5.471992015838623
Norm before input: 0.2552422881126404
Norm after input: 0.35712549090385437
Norm after each mp layer: 0.8550583720207214
Norm after each mp layer: 2.1247944831848145
Norm after each mp layer: 5.436803817749023
Norm before input: 0.2552422881126404
Norm after input: 0.35712549090385437
Norm after each mp layer: 0.8550583720207214
Norm after each mp layer: 2.1247944831848145
Norm after each mp layer: 5.436803817749023
Norm before input: 0.2552422881126404
Norm after input: 0.35649573802948
Norm after each mp layer: 0.8543387055397034
Norm after each mp layer: 2.12863826751709
Norm after each mp layer: 5.448646068572998
Norm before input: 0.2552422881126404
Norm after input: 0.35649573802948
Norm after each mp layer: 0.8543387055397034
Norm after each mp layer: 2.12863826751709
Norm after each mp layer: 5.448646068572998
Norm before input: 0.2552422881126404
Norm after input: 0.35562390089035034
Norm after each mp layer: 0.855172872543335
Norm after each mp layer: 2.1412506103515625
Norm after each mp layer: 5.4811692237854
Norm before input: 0.2552422881126404
Norm after input: 0.35562390089035034
Norm after each mp layer: 0.855172872543335
Norm after each mp layer: 2.1412506103515625
Norm after each mp layer: 5.4811692237854
Norm before input: 0.2552422881126404
Norm after input: 0.3554941713809967
Norm after each mp layer: 0.8546209335327148
Norm after each mp layer: 2.141446828842163
Norm after each mp layer: 5.488210678100586
Norm before input: 0.2552422881126404
Norm after input: 0.3554941713809967
Norm after each mp layer: 0.8546209335327148
Norm after each mp layer: 2.141446828842163
Norm after each mp layer: 5.488210678100586
Norm before input: 0.2552422881126404
Norm after input: 0.3557940125465393
Norm after each mp layer: 0.8514290452003479
Norm after each mp layer: 2.1274478435516357
Norm after each mp layer: 5.468723297119141
Epoch: 275, Loss: 0.1864, Energy: 74.5930, Train: 97.60%, Valid: 73.80%, Test: 71.80%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3557940125465393
Norm after each mp layer: 0.8514290452003479
Norm after each mp layer: 2.1274478435516357
Norm after each mp layer: 5.468723297119141
Norm before input: 0.2552422881126404
Norm after input: 0.3556969463825226
Norm after each mp layer: 0.848507285118103
Norm after each mp layer: 2.1182541847229004
Norm after each mp layer: 5.454797744750977
Norm before input: 0.2552422881126404
Norm after input: 0.3556969463825226
Norm after each mp layer: 0.848507285118103
Norm after each mp layer: 2.1182541847229004
Norm after each mp layer: 5.454797744750977
Norm before input: 0.2552422881126404
Norm after input: 0.3551672101020813
Norm after each mp layer: 0.8507348299026489
Norm after each mp layer: 2.1320505142211914
Norm after each mp layer: 5.477344036102295
Norm before input: 0.2552422881126404
Norm after input: 0.3551672101020813
Norm after each mp layer: 0.8507348299026489
Norm after each mp layer: 2.1320505142211914
Norm after each mp layer: 5.477344036102295
Norm before input: 0.2552422881126404
Norm after input: 0.3550921082496643
Norm after each mp layer: 0.8516082763671875
Norm after each mp layer: 2.1365160942077637
Norm after each mp layer: 5.485445022583008
Norm before input: 0.2552422881126404
Norm after input: 0.3550921082496643
Norm after each mp layer: 0.8516082763671875
Norm after each mp layer: 2.1365160942077637
Norm after each mp layer: 5.485445022583008
Norm before input: 0.2552422881126404
Norm after input: 0.35551711916923523
Norm after each mp layer: 0.8485767245292664
Norm after each mp layer: 2.1216721534729004
Norm after each mp layer: 5.458643436431885
Norm before input: 0.2552422881126404
Norm after input: 0.35551711916923523
Norm after each mp layer: 0.8485767245292664
Norm after each mp layer: 2.1216721534729004
Norm after each mp layer: 5.458643436431885
Norm before input: 0.2552422881126404
Norm after input: 0.3555922210216522
Norm after each mp layer: 0.8459546566009521
Norm after each mp layer: 2.1124794483184814
Norm after each mp layer: 5.44710636138916
Epoch: 280, Loss: 0.1763, Energy: 69.3941, Train: 98.01%, Valid: 73.20%, Test: 71.30%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3555922210216522
Norm after each mp layer: 0.8459546566009521
Norm after each mp layer: 2.1124794483184814
Norm after each mp layer: 5.44710636138916
Norm before input: 0.2552422881126404
Norm after input: 0.3547147810459137
Norm after each mp layer: 0.8468689918518066
Norm after each mp layer: 2.1252195835113525
Norm after each mp layer: 5.475162506103516
Norm before input: 0.2552422881126404
Norm after input: 0.3547147810459137
Norm after each mp layer: 0.8468689918518066
Norm after each mp layer: 2.1252195835113525
Norm after each mp layer: 5.475162982940674
Norm before input: 0.2552422881126404
Norm after input: 0.3541826903820038
Norm after each mp layer: 0.8464728593826294
Norm after each mp layer: 2.1306636333465576
Norm after each mp layer: 5.493794918060303
Norm before input: 0.2552422881126404
Norm after input: 0.3541826903820038
Norm after each mp layer: 0.8464728593826294
Norm after each mp layer: 2.1306636333465576
Norm after each mp layer: 5.493794918060303
Norm before input: 0.2552422881126404
Norm after input: 0.3543309271335602
Norm after each mp layer: 0.8432424068450928
Norm after each mp layer: 2.118537664413452
Norm after each mp layer: 5.478468418121338
Norm before input: 0.2552422881126404
Norm after input: 0.3543309271335602
Norm after each mp layer: 0.843242347240448
Norm after each mp layer: 2.118537664413452
Norm after each mp layer: 5.478468894958496
Norm before input: 0.2552422881126404
Norm after input: 0.35444632172584534
Norm after each mp layer: 0.8406946063041687
Norm after each mp layer: 2.1095855236053467
Norm after each mp layer: 5.471108436584473
Norm before input: 0.2552422881126404
Norm after input: 0.35444632172584534
Norm after each mp layer: 0.8406946063041687
Norm after each mp layer: 2.1095855236053467
Norm after each mp layer: 5.471108436584473
Norm before input: 0.2552422881126404
Norm after input: 0.3534795939922333
Norm after each mp layer: 0.8409111499786377
Norm after each mp layer: 2.121741533279419
Norm after each mp layer: 5.50029182434082
Epoch: 285, Loss: 0.1730, Energy: 68.9548, Train: 97.68%, Valid: 73.00%, Test: 71.00%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3534795939922333
Norm after each mp layer: 0.8409111499786377
Norm after each mp layer: 2.121741533279419
Norm after each mp layer: 5.50029182434082
Norm before input: 0.2552422881126404
Norm after input: 0.3535173833370209
Norm after each mp layer: 0.8431618213653564
Norm after each mp layer: 2.1309099197387695
Norm after each mp layer: 5.521445274353027
Norm before input: 0.2552422881126404
Norm after input: 0.3535173833370209
Norm after each mp layer: 0.8431618213653564
Norm after each mp layer: 2.1309099197387695
Norm after each mp layer: 5.521445274353027
Norm before input: 0.2552422881126404
Norm after input: 0.35414814949035645
Norm after each mp layer: 0.8416370749473572
Norm after each mp layer: 2.119475841522217
Norm after each mp layer: 5.505036354064941
Norm before input: 0.2552422881126404
Norm after input: 0.35414814949035645
Norm after each mp layer: 0.8416370749473572
Norm after each mp layer: 2.119475841522217
Norm after each mp layer: 5.505036354064941
Norm before input: 0.2552422881126404
Norm after input: 0.3538014590740204
Norm after each mp layer: 0.8373149037361145
Norm after each mp layer: 2.10795259475708
Norm after each mp layer: 5.4914422035217285
Norm before input: 0.2552422881126404
Norm after input: 0.3538014590740204
Norm after each mp layer: 0.8373149037361145
Norm after each mp layer: 2.10795259475708
Norm after each mp layer: 5.4914422035217285
Norm before input: 0.2552422881126404
Norm after input: 0.35319870710372925
Norm after each mp layer: 0.8380734324455261
Norm after each mp layer: 2.117633581161499
Norm after each mp layer: 5.517263889312744
Norm before input: 0.2552422881126404
Norm after input: 0.35319870710372925
Norm after each mp layer: 0.8380734324455261
Norm after each mp layer: 2.117633581161499
Norm after each mp layer: 5.517264366149902
Norm before input: 0.2552422881126404
Norm after input: 0.35323020815849304
Norm after each mp layer: 0.8408412933349609
Norm after each mp layer: 2.1287219524383545
Norm after each mp layer: 5.545332431793213
Epoch: 290, Loss: 0.1647, Energy: 66.2229, Train: 98.10%, Valid: 72.60%, Test: 71.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35323020815849304
Norm after each mp layer: 0.8408412933349609
Norm after each mp layer: 2.1287219524383545
Norm after each mp layer: 5.545332431793213
Norm before input: 0.2552422881126404
Norm after input: 0.3528052270412445
Norm after each mp layer: 0.8306450843811035
Norm after each mp layer: 2.098353385925293
Norm after each mp layer: 5.51112174987793
Norm before input: 0.2552422881126404
Norm after input: 0.3528052270412445
Norm after each mp layer: 0.8306450843811035
Norm after each mp layer: 2.098353385925293
Norm after each mp layer: 5.51112174987793
Norm before input: 0.2552422881126404
Norm after input: 0.3528926372528076
Norm after each mp layer: 0.841198205947876
Norm after each mp layer: 2.1387245655059814
Norm after each mp layer: 5.586517810821533
Norm before input: 0.2552422881126404
Norm after input: 0.3528926372528076
Norm after each mp layer: 0.841198205947876
Norm after each mp layer: 2.1387245655059814
Norm after each mp layer: 5.586517810821533
Norm before input: 0.2552422881126404
Norm after input: 0.35252562165260315
Norm after each mp layer: 0.8425902128219604
Norm after each mp layer: 2.149976968765259
Norm after each mp layer: 5.611152648925781
Norm before input: 0.2552422881126404
Norm after input: 0.35252562165260315
Norm after each mp layer: 0.8425902128219604
Norm after each mp layer: 2.149976968765259
Norm after each mp layer: 5.611152648925781
Norm before input: 0.2552422881126404
Norm after input: 0.3523758351802826
Norm after each mp layer: 0.8349472284317017
Norm after each mp layer: 2.123304843902588
Norm after each mp layer: 5.565682888031006
Norm before input: 0.2552422881126404
Norm after input: 0.3523758351802826
Norm after each mp layer: 0.8349472284317017
Norm after each mp layer: 2.123304843902588
Norm after each mp layer: 5.565682888031006
Norm before input: 0.2552422881126404
Norm after input: 0.3531312644481659
Norm after each mp layer: 0.8372793793678284
Norm after each mp layer: 2.123079538345337
Norm after each mp layer: 5.560551643371582
Epoch: 295, Loss: 0.1643, Energy: 71.2827, Train: 98.18%, Valid: 72.20%, Test: 70.40%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3531312644481659
Norm after each mp layer: 0.8372793793678284
Norm after each mp layer: 2.123079538345337
Norm after each mp layer: 5.560551643371582
Norm before input: 0.2552422881126404
Norm after input: 0.35181063413619995
Norm after each mp layer: 0.838897705078125
Norm after each mp layer: 2.144989490509033
Norm after each mp layer: 5.601046562194824
Norm before input: 0.2552422881126404
Norm after input: 0.35181063413619995
Norm after each mp layer: 0.838897705078125
Norm after each mp layer: 2.144989490509033
Norm after each mp layer: 5.601046562194824
Norm before input: 0.2552422881126404
Norm after input: 0.35190287232398987
Norm after each mp layer: 0.837293267250061
Norm after each mp layer: 2.1378467082977295
Norm after each mp layer: 5.582635402679443
Norm before input: 0.2552422881126404
Norm after input: 0.35190287232398987
Norm after each mp layer: 0.837293267250061
Norm after each mp layer: 2.1378467082977295
Norm after each mp layer: 5.582635402679443
Norm before input: 0.2552422881126404
Norm after input: 0.35274890065193176
Norm after each mp layer: 0.8337001800537109
Norm after each mp layer: 2.1146633625030518
Norm after each mp layer: 5.541350841522217
Norm before input: 0.2552422881126404
Norm after input: 0.35274890065193176
Norm after each mp layer: 0.8337001800537109
Norm after each mp layer: 2.1146633625030518
Norm after each mp layer: 5.541350841522217
Norm before input: 0.2552422881126404
Norm after input: 0.3516552746295929
Norm after each mp layer: 0.8311278820037842
Norm after each mp layer: 2.1171114444732666
Norm after each mp layer: 5.544562816619873
Norm before input: 0.2552422881126404
Norm after input: 0.3516552746295929
Norm after each mp layer: 0.8311278820037842
Norm after each mp layer: 2.1171114444732666
Norm after each mp layer: 5.544562816619873
Norm before input: 0.2552422881126404
Norm after input: 0.3515186011791229
Norm after each mp layer: 0.8336479067802429
Norm after each mp layer: 2.1277096271514893
Norm after each mp layer: 5.56270694732666
Epoch: 300, Loss: 0.1575, Energy: 68.3515, Train: 98.10%, Valid: 72.40%, Test: 70.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3515186011791229
Norm after each mp layer: 0.8336479067802429
Norm after each mp layer: 2.1277096271514893
Norm after each mp layer: 5.56270694732666
Norm before input: 0.2552422881126404
Norm after input: 0.3523103892803192
Norm after each mp layer: 0.8339123129844666
Norm after each mp layer: 2.1193149089813232
Norm after each mp layer: 5.547946453094482
Norm before input: 0.2552422881126404
Norm after input: 0.3523103892803192
Norm after each mp layer: 0.8339123129844666
Norm after each mp layer: 2.1193149089813232
Norm after each mp layer: 5.547946453094482
Norm before input: 0.2552422881126404
Norm after input: 0.35175448656082153
Norm after each mp layer: 0.824871838092804
Norm after each mp layer: 2.092832565307617
Norm after each mp layer: 5.510993003845215
Norm before input: 0.2552422881126404
Norm after input: 0.35175448656082153
Norm after each mp layer: 0.824871838092804
Norm after each mp layer: 2.0928328037261963
Norm after each mp layer: 5.510993003845215
Norm before input: 0.2552422881126404
Norm after input: 0.3510151207447052
Norm after each mp layer: 0.8344570398330688
Norm after each mp layer: 2.1387414932250977
Norm after each mp layer: 5.589284896850586
Norm before input: 0.2552422881126404
Norm after input: 0.3510151207447052
Norm after each mp layer: 0.8344569802284241
Norm after each mp layer: 2.1387414932250977
Norm after each mp layer: 5.589284896850586
Norm before input: 0.2552422881126404
Norm after input: 0.35137930512428284
Norm after each mp layer: 0.8374773263931274
Norm after each mp layer: 2.1483089923858643
Norm after each mp layer: 5.601016044616699
Norm before input: 0.2552422881126404
Norm after input: 0.35137930512428284
Norm after each mp layer: 0.8374773263931274
Norm after each mp layer: 2.1483089923858643
Norm after each mp layer: 5.601016044616699
Norm before input: 0.2552422881126404
Norm after input: 0.3510059714317322
Norm after each mp layer: 0.8193971514701843
Norm after each mp layer: 2.0885069370269775
Norm after each mp layer: 5.503699779510498
Epoch: 305, Loss: 0.1632, Energy: 70.3664, Train: 97.85%, Valid: 71.20%, Test: 68.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3510059714317322
Norm after each mp layer: 0.8193971514701843
Norm after each mp layer: 2.0885069370269775
Norm after each mp layer: 5.503699779510498
Norm before input: 0.2552422881126404
Norm after input: 0.3515269160270691
Norm after each mp layer: 0.8357881903648376
Norm after each mp layer: 2.140503406524658
Norm after each mp layer: 5.564759254455566
Norm before input: 0.2552422881126404
Norm after input: 0.3515269160270691
Norm after each mp layer: 0.8357881903648376
Norm after each mp layer: 2.140503406524658
Norm after each mp layer: 5.564759254455566
Norm before input: 0.2552422881126404
Norm after input: 0.34935835003852844
Norm after each mp layer: 0.840535581111908
Norm after each mp layer: 2.1866259574890137
Norm after each mp layer: 5.631569862365723
Norm before input: 0.2552422881126404
Norm after input: 0.34935835003852844
Norm after each mp layer: 0.840535581111908
Norm after each mp layer: 2.1866259574890137
Norm after each mp layer: 5.631569862365723
Norm before input: 0.2552422881126404
Norm after input: 0.3502010107040405
Norm after each mp layer: 0.8238096833229065
Norm after each mp layer: 2.1052520275115967
Norm after each mp layer: 5.453610420227051
Norm before input: 0.2552422881126404
Norm after input: 0.3502010107040405
Norm after each mp layer: 0.8238096833229065
Norm after each mp layer: 2.1052520275115967
Norm after each mp layer: 5.453610420227051
Norm before input: 0.2552422881126404
Norm after input: 0.3516492545604706
Norm after each mp layer: 0.8215755820274353
Norm after each mp layer: 2.0818424224853516
Norm after each mp layer: 5.40813684463501
Norm before input: 0.2552422881126404
Norm after input: 0.3516492545604706
Norm after each mp layer: 0.8215755820274353
Norm after each mp layer: 2.0818424224853516
Norm after each mp layer: 5.40813684463501
Norm before input: 0.2552422881126404
Norm after input: 0.34914281964302063
Norm after each mp layer: 0.8336801528930664
Norm after each mp layer: 2.157548189163208
Norm after each mp layer: 5.514461040496826
Epoch: 310, Loss: 0.2442, Energy: 103.8721, Train: 97.19%, Valid: 71.80%, Test: 70.20%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.34914281964302063
Norm after each mp layer: 0.8336801528930664
Norm after each mp layer: 2.157548189163208
Norm after each mp layer: 5.514461040496826
Norm before input: 0.2552422881126404
Norm after input: 0.3506573438644409
Norm after each mp layer: 0.8472656607627869
Norm after each mp layer: 2.200953722000122
Norm after each mp layer: 5.555293083190918
Norm before input: 0.2552422881126404
Norm after input: 0.3506573438644409
Norm after each mp layer: 0.8472656607627869
Norm after each mp layer: 2.200953722000122
Norm after each mp layer: 5.555293083190918
Norm before input: 0.2552422881126404
Norm after input: 0.3508839011192322
Norm after each mp layer: 0.8245148062705994
Norm after each mp layer: 2.104273796081543
Norm after each mp layer: 5.389003753662109
Norm before input: 0.2552422881126404
Norm after input: 0.3508839011192322
Norm after each mp layer: 0.8245148062705994
Norm after each mp layer: 2.104273796081543
Norm after each mp layer: 5.389003753662109
Norm before input: 0.2552422881126404
Norm after input: 0.35098767280578613
Norm after each mp layer: 0.8147163391113281
Norm after each mp layer: 2.078000783920288
Norm after each mp layer: 5.331742763519287
Norm before input: 0.2552422881126404
Norm after input: 0.35098767280578613
Norm after each mp layer: 0.8147163391113281
Norm after each mp layer: 2.078000545501709
Norm after each mp layer: 5.331742763519287
Norm before input: 0.2552422881126404
Norm after input: 0.35126885771751404
Norm after each mp layer: 0.8398334383964539
Norm after each mp layer: 2.157552719116211
Norm after each mp layer: 5.3827314376831055
Norm before input: 0.2552422881126404
Norm after input: 0.35126885771751404
Norm after each mp layer: 0.8398334383964539
Norm after each mp layer: 2.157552719116211
Norm after each mp layer: 5.3827314376831055
Norm before input: 0.2552422881126404
Norm after input: 0.35006871819496155
Norm after each mp layer: 0.8461309671401978
Norm after each mp layer: 2.195491313934326
Norm after each mp layer: 5.388001918792725
Epoch: 315, Loss: 0.1729, Energy: 76.5137, Train: 97.10%, Valid: 73.60%, Test: 72.20%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35006871819496155
Norm after each mp layer: 0.8461309671401978
Norm after each mp layer: 2.195491313934326
Norm after each mp layer: 5.388001918792725
Norm before input: 0.2552422881126404
Norm after input: 0.3489847183227539
Norm after each mp layer: 0.8289613723754883
Norm after each mp layer: 2.13950514793396
Norm after each mp layer: 5.283193111419678
Norm before input: 0.2552422881126404
Norm after input: 0.3489847183227539
Norm after each mp layer: 0.8289613723754883
Norm after each mp layer: 2.13950514793396
Norm after each mp layer: 5.283193111419678
Norm before input: 0.2552422881126404
Norm after input: 0.35148072242736816
Norm after each mp layer: 0.83317631483078
Norm after each mp layer: 2.128612756729126
Norm after each mp layer: 5.224618434906006
Norm before input: 0.2552422881126404
Norm after input: 0.35148072242736816
Norm after each mp layer: 0.83317631483078
Norm after each mp layer: 2.128612995147705
Norm after each mp layer: 5.224618434906006
Norm before input: 0.2552422881126404
Norm after input: 0.3508409261703491
Norm after each mp layer: 0.8296777009963989
Norm after each mp layer: 2.12418270111084
Norm after each mp layer: 5.189614772796631
Norm before input: 0.2552422881126404
Norm after input: 0.3508409261703491
Norm after each mp layer: 0.8296777009963989
Norm after each mp layer: 2.12418270111084
Norm after each mp layer: 5.189614772796631
Norm before input: 0.2552422881126404
Norm after input: 0.3486064672470093
Norm after each mp layer: 0.8258373737335205
Norm after each mp layer: 2.1402604579925537
Norm after each mp layer: 5.204843044281006
Norm before input: 0.2552422881126404
Norm after input: 0.3486064672470093
Norm after each mp layer: 0.8258373737335205
Norm after each mp layer: 2.1402604579925537
Norm after each mp layer: 5.204843044281006
Norm before input: 0.2552422881126404
Norm after input: 0.3495517671108246
Norm after each mp layer: 0.8399540185928345
Norm after each mp layer: 2.1831836700439453
Norm after each mp layer: 5.22598123550415
Epoch: 320, Loss: 0.2147, Energy: 96.9888, Train: 98.10%, Valid: 73.80%, Test: 72.70%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3495517671108246
Norm after each mp layer: 0.8399540185928345
Norm after each mp layer: 2.183183431625366
Norm after each mp layer: 5.22598123550415
Norm before input: 0.2552422881126404
Norm after input: 0.35164332389831543
Norm after each mp layer: 0.849879264831543
Norm after each mp layer: 2.202575445175171
Norm after each mp layer: 5.227550506591797
Norm before input: 0.2552422881126404
Norm after input: 0.35164332389831543
Norm after each mp layer: 0.849879264831543
Norm after each mp layer: 2.202575445175171
Norm after each mp layer: 5.227550506591797
Norm before input: 0.2552422881126404
Norm after input: 0.35128793120384216
Norm after each mp layer: 0.8299078345298767
Norm after each mp layer: 2.122593641281128
Norm after each mp layer: 5.090394973754883
Norm before input: 0.2552422881126404
Norm after input: 0.35128793120384216
Norm after each mp layer: 0.8299078345298767
Norm after each mp layer: 2.122593641281128
Norm after each mp layer: 5.090394973754883
Norm before input: 0.2552422881126404
Norm after input: 0.35078877210617065
Norm after each mp layer: 0.819645881652832
Norm after each mp layer: 2.098358392715454
Norm after each mp layer: 5.043143272399902
Norm before input: 0.2552422881126404
Norm after input: 0.35078877210617065
Norm after each mp layer: 0.819645881652832
Norm after each mp layer: 2.098358392715454
Norm after each mp layer: 5.0431437492370605
Norm before input: 0.2552422881126404
Norm after input: 0.35054266452789307
Norm after each mp layer: 0.8303384780883789
Norm after each mp layer: 2.136958599090576
Norm after each mp layer: 5.063715934753418
Norm before input: 0.2552422881126404
Norm after input: 0.35054266452789307
Norm after each mp layer: 0.8303384780883789
Norm after each mp layer: 2.136958599090576
Norm after each mp layer: 5.063715934753418
Norm before input: 0.2552422881126404
Norm after input: 0.35004180669784546
Norm after each mp layer: 0.8408536314964294
Norm after each mp layer: 2.194852113723755
Norm after each mp layer: 5.122913837432861
Epoch: 325, Loss: 0.1630, Energy: 62.6610, Train: 97.60%, Valid: 74.00%, Test: 72.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35004180669784546
Norm after each mp layer: 0.8408536314964294
Norm after each mp layer: 2.194852113723755
Norm after each mp layer: 5.122913837432861
Norm before input: 0.2552422881126404
Norm after input: 0.34861433506011963
Norm after each mp layer: 0.8311058878898621
Norm after each mp layer: 2.169344186782837
Norm after each mp layer: 5.076597213745117
Norm before input: 0.2552422881126404
Norm after input: 0.34861433506011963
Norm after each mp layer: 0.8311058878898621
Norm after each mp layer: 2.169344186782837
Norm after each mp layer: 5.076596736907959
Norm before input: 0.2552422881126404
Norm after input: 0.3491378426551819
Norm after each mp layer: 0.8257527947425842
Norm after each mp layer: 2.146502733230591
Norm after each mp layer: 5.040126323699951
Norm before input: 0.2552422881126404
Norm after input: 0.3491378426551819
Norm after each mp layer: 0.8257527947425842
Norm after each mp layer: 2.146502733230591
Norm after each mp layer: 5.040126323699951
Norm before input: 0.2552422881126404
Norm after input: 0.35201773047447205
Norm after each mp layer: 0.8373318314552307
Norm after each mp layer: 2.163259267807007
Norm after each mp layer: 5.052887439727783
Norm before input: 0.2552422881126404
Norm after input: 0.35201773047447205
Norm after each mp layer: 0.8373318314552307
Norm after each mp layer: 2.163259267807007
Norm after each mp layer: 5.052887439727783
Norm before input: 0.2552422881126404
Norm after input: 0.3519916236400604
Norm after each mp layer: 0.8421924710273743
Norm after each mp layer: 2.1912784576416016
Norm after each mp layer: 5.099550724029541
Norm before input: 0.2552422881126404
Norm after input: 0.3519916236400604
Norm after each mp layer: 0.8421924710273743
Norm after each mp layer: 2.1912784576416016
Norm after each mp layer: 5.099550247192383
Norm before input: 0.2552422881126404
Norm after input: 0.34963762760162354
Norm after each mp layer: 0.837395191192627
Norm after each mp layer: 2.206609010696411
Norm after each mp layer: 5.143706321716309
Epoch: 330, Loss: 0.1749, Energy: 64.7301, Train: 97.85%, Valid: 73.40%, Test: 72.80%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.34963762760162354
Norm after each mp layer: 0.837395191192627
Norm after each mp layer: 2.206609010696411
Norm after each mp layer: 5.143706321716309
Norm before input: 0.2552422881126404
Norm after input: 0.3492358326911926
Norm after each mp layer: 0.8375939726829529
Norm after each mp layer: 2.21991229057312
Norm after each mp layer: 5.1797566413879395
Norm before input: 0.2552422881126404
Norm after input: 0.3492358326911926
Norm after each mp layer: 0.8375939726829529
Norm after each mp layer: 2.21991229057312
Norm after each mp layer: 5.1797566413879395
Norm before input: 0.2552422881126404
Norm after input: 0.3507806062698364
Norm after each mp layer: 0.8395456671714783
Norm after each mp layer: 2.2137632369995117
Norm after each mp layer: 5.171494483947754
Norm before input: 0.2552422881126404
Norm after input: 0.3507806062698364
Norm after each mp layer: 0.8395456671714783
Norm after each mp layer: 2.2137632369995117
Norm after each mp layer: 5.171494483947754
Norm before input: 0.2552422881126404
Norm after input: 0.35244494676589966
Norm after each mp layer: 0.8391145467758179
Norm after each mp layer: 2.194892406463623
Norm after each mp layer: 5.153372764587402
Norm before input: 0.2552422881126404
Norm after input: 0.35244494676589966
Norm after each mp layer: 0.8391145467758179
Norm after each mp layer: 2.194892406463623
Norm after each mp layer: 5.153372764587402
Norm before input: 0.2552422881126404
Norm after input: 0.35132840275764465
Norm after each mp layer: 0.8331435918807983
Norm after each mp layer: 2.1845269203186035
Norm after each mp layer: 5.132073879241943
Norm before input: 0.2552422881126404
Norm after input: 0.35132840275764465
Norm after each mp layer: 0.8331435918807983
Norm after each mp layer: 2.1845269203186035
Norm after each mp layer: 5.132073879241943
Norm before input: 0.2552422881126404
Norm after input: 0.3503477871417999
Norm after each mp layer: 0.8338907957077026
Norm after each mp layer: 2.200244903564453
Norm after each mp layer: 5.162291049957275
Epoch: 335, Loss: 0.1505, Energy: 57.6286, Train: 97.60%, Valid: 72.80%, Test: 71.60%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3503477871417999
Norm after each mp layer: 0.8338907957077026
Norm after each mp layer: 2.200244903564453
Norm after each mp layer: 5.162291049957275
Norm before input: 0.2552422881126404
Norm after input: 0.35065916180610657
Norm after each mp layer: 0.8428390622138977
Norm after each mp layer: 2.2328553199768066
Norm after each mp layer: 5.210763931274414
Norm before input: 0.2552422881126404
Norm after input: 0.35065916180610657
Norm after each mp layer: 0.8428390622138977
Norm after each mp layer: 2.2328553199768066
Norm after each mp layer: 5.210763931274414
Norm before input: 0.2552422881126404
Norm after input: 0.350888729095459
Norm after each mp layer: 0.8369112610816956
Norm after each mp layer: 2.2091588973999023
Norm after each mp layer: 5.18145751953125
Norm before input: 0.2552422881126404
Norm after input: 0.350888729095459
Norm after each mp layer: 0.8369112610816956
Norm after each mp layer: 2.2091588973999023
Norm after each mp layer: 5.18145751953125
Norm before input: 0.2552422881126404
Norm after input: 0.35134175419807434
Norm after each mp layer: 0.8313024044036865
Norm after each mp layer: 2.1850991249084473
Norm after each mp layer: 5.1578369140625
Norm before input: 0.2552422881126404
Norm after input: 0.35134175419807434
Norm after each mp layer: 0.8313024044036865
Norm after each mp layer: 2.1850991249084473
Norm after each mp layer: 5.1578369140625
Norm before input: 0.2552422881126404
Norm after input: 0.35133787989616394
Norm after each mp layer: 0.8354508876800537
Norm after each mp layer: 2.2034056186676025
Norm after each mp layer: 5.189077854156494
Norm before input: 0.2552422881126404
Norm after input: 0.35133787989616394
Norm after each mp layer: 0.8354508876800537
Norm after each mp layer: 2.2034056186676025
Norm after each mp layer: 5.189077854156494
Norm before input: 0.2552422881126404
Norm after input: 0.35125672817230225
Norm after each mp layer: 0.8419767022132874
Norm after each mp layer: 2.232267141342163
Norm after each mp layer: 5.244220733642578
Epoch: 340, Loss: 0.1420, Energy: 56.7189, Train: 98.43%, Valid: 74.40%, Test: 72.70%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35125672817230225
Norm after each mp layer: 0.8419767022132874
Norm after each mp layer: 2.232267141342163
Norm after each mp layer: 5.244220733642578
Norm before input: 0.2552422881126404
Norm after input: 0.35127508640289307
Norm after each mp layer: 0.8450119495391846
Norm after each mp layer: 2.2473230361938477
Norm after each mp layer: 5.282167434692383
Norm before input: 0.2552422881126404
Norm after input: 0.35127508640289307
Norm after each mp layer: 0.8450119495391846
Norm after each mp layer: 2.2473230361938477
Norm after each mp layer: 5.282167434692383
Norm before input: 0.2552422881126404
Norm after input: 0.35120078921318054
Norm after each mp layer: 0.8384495973587036
Norm after each mp layer: 2.227682590484619
Norm after each mp layer: 5.266386985778809
Norm before input: 0.2552422881126404
Norm after input: 0.35120078921318054
Norm after each mp layer: 0.8384495973587036
Norm after each mp layer: 2.227682590484619
Norm after each mp layer: 5.266386985778809
Norm before input: 0.2552422881126404
Norm after input: 0.3511149287223816
Norm after each mp layer: 0.8333130478858948
Norm after each mp layer: 2.212679624557495
Norm after each mp layer: 5.2619709968566895
Norm before input: 0.2552422881126404
Norm after input: 0.3511149287223816
Norm after each mp layer: 0.8333130478858948
Norm after each mp layer: 2.212679624557495
Norm after each mp layer: 5.2619709968566895
Norm before input: 0.2552422881126404
Norm after input: 0.3503481149673462
Norm after each mp layer: 0.832088828086853
Norm after each mp layer: 2.2187275886535645
Norm after each mp layer: 5.279836177825928
Norm before input: 0.2552422881126404
Norm after input: 0.3503481149673462
Norm after each mp layer: 0.832088828086853
Norm after each mp layer: 2.2187275886535645
Norm after each mp layer: 5.279836177825928
Norm before input: 0.2552422881126404
Norm after input: 0.35032859444618225
Norm after each mp layer: 0.8369560837745667
Norm after each mp layer: 2.236711025238037
Norm after each mp layer: 5.306201457977295
Epoch: 345, Loss: 0.1470, Energy: 66.0186, Train: 98.59%, Valid: 73.20%, Test: 71.60%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35032859444618225
Norm after each mp layer: 0.8369560837745667
Norm after each mp layer: 2.236711025238037
Norm after each mp layer: 5.306201457977295
Norm before input: 0.2552422881126404
Norm after input: 0.35108137130737305
Norm after each mp layer: 0.8426317572593689
Norm after each mp layer: 2.2472519874572754
Norm after each mp layer: 5.319632053375244
Norm before input: 0.2552422881126404
Norm after input: 0.35108137130737305
Norm after each mp layer: 0.8426317572593689
Norm after each mp layer: 2.2472519874572754
Norm after each mp layer: 5.319632053375244
Norm before input: 0.2552422881126404
Norm after input: 0.3513540029525757
Norm after each mp layer: 0.8373166918754578
Norm after each mp layer: 2.2216594219207764
Norm after each mp layer: 5.285226345062256
Norm before input: 0.2552422881126404
Norm after input: 0.3513540029525757
Norm after each mp layer: 0.8373166918754578
Norm after each mp layer: 2.2216594219207764
Norm after each mp layer: 5.285226345062256
Norm before input: 0.2552422881126404
Norm after input: 0.35168394446372986
Norm after each mp layer: 0.8378278613090515
Norm after each mp layer: 2.222531795501709
Norm after each mp layer: 5.296957969665527
Norm before input: 0.2552422881126404
Norm after input: 0.35168394446372986
Norm after each mp layer: 0.8378278613090515
Norm after each mp layer: 2.222531795501709
Norm after each mp layer: 5.296957969665527
Norm before input: 0.2552422881126404
Norm after input: 0.34978941082954407
Norm after each mp layer: 0.8322145342826843
Norm after each mp layer: 2.227898120880127
Norm after each mp layer: 5.324748992919922
Norm before input: 0.2552422881126404
Norm after input: 0.34978941082954407
Norm after each mp layer: 0.8322145342826843
Norm after each mp layer: 2.227898120880127
Norm after each mp layer: 5.324748992919922
Norm before input: 0.2552422881126404
Norm after input: 0.34917086362838745
Norm after each mp layer: 0.8293869495391846
Norm after each mp layer: 2.226072072982788
Norm after each mp layer: 5.33681583404541
Epoch: 350, Loss: 0.1425, Energy: 64.6046, Train: 98.26%, Valid: 72.40%, Test: 71.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.34917086362838745
Norm after each mp layer: 0.8293869495391846
Norm after each mp layer: 2.226072072982788
Norm after each mp layer: 5.33681583404541
Norm before input: 0.2552422881126404
Norm after input: 0.35041090846061707
Norm after each mp layer: 0.8314932584762573
Norm after each mp layer: 2.2155754566192627
Norm after each mp layer: 5.315649032592773
Norm before input: 0.2552422881126404
Norm after input: 0.35041090846061707
Norm after each mp layer: 0.8314932584762573
Norm after each mp layer: 2.2155754566192627
Norm after each mp layer: 5.315649032592773
Norm before input: 0.2552422881126404
Norm after input: 0.35200902819633484
Norm after each mp layer: 0.8357110023498535
Norm after each mp layer: 2.2098546028137207
Norm after each mp layer: 5.311563491821289
Norm before input: 0.2552422881126404
Norm after input: 0.35200902819633484
Norm after each mp layer: 0.8357110023498535
Norm after each mp layer: 2.2098546028137207
Norm after each mp layer: 5.311563491821289
Norm before input: 0.2552422881126404
Norm after input: 0.35057464241981506
Norm after each mp layer: 0.8341475129127502
Norm after each mp layer: 2.2234625816345215
Norm after each mp layer: 5.344417572021484
Norm before input: 0.2552422881126404
Norm after input: 0.35057464241981506
Norm after each mp layer: 0.8341475129127502
Norm after each mp layer: 2.2234625816345215
Norm after each mp layer: 5.344417572021484
Norm before input: 0.2552422881126404
Norm after input: 0.34966617822647095
Norm after each mp layer: 0.8286303877830505
Norm after each mp layer: 2.2168519496917725
Norm after each mp layer: 5.354194164276123
Norm before input: 0.2552422881126404
Norm after input: 0.34966617822647095
Norm after each mp layer: 0.8286303877830505
Norm after each mp layer: 2.2168519496917725
Norm after each mp layer: 5.354194164276123
Norm before input: 0.2552422881126404
Norm after input: 0.3499829173088074
Norm after each mp layer: 0.8273409008979797
Norm after each mp layer: 2.2080936431884766
Norm after each mp layer: 5.355400085449219
Epoch: 355, Loss: 0.1351, Energy: 60.5260, Train: 98.59%, Valid: 72.80%, Test: 70.30%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3499829173088074
Norm after each mp layer: 0.8273409008979797
Norm after each mp layer: 2.2080936431884766
Norm after each mp layer: 5.355400085449219
Norm before input: 0.2552422881126404
Norm after input: 0.35062113404273987
Norm after each mp layer: 0.8287959098815918
Norm after each mp layer: 2.2037363052368164
Norm after each mp layer: 5.350816249847412
Norm before input: 0.2552422881126404
Norm after input: 0.35062113404273987
Norm after each mp layer: 0.8287959098815918
Norm after each mp layer: 2.2037363052368164
Norm after each mp layer: 5.350816249847412
Norm before input: 0.2552422881126404
Norm after input: 0.3506556749343872
Norm after each mp layer: 0.8321693539619446
Norm after each mp layer: 2.2141003608703613
Norm after each mp layer: 5.3604817390441895
Norm before input: 0.2552422881126404
Norm after input: 0.3506556749343872
Norm after each mp layer: 0.8321693539619446
Norm after each mp layer: 2.2141003608703613
Norm after each mp layer: 5.360482215881348
Norm before input: 0.2552422881126404
Norm after input: 0.3506214916706085
Norm after each mp layer: 0.8343165516853333
Norm after each mp layer: 2.221283435821533
Norm after each mp layer: 5.378899097442627
Norm before input: 0.2552422881126404
Norm after input: 0.3506214916706085
Norm after each mp layer: 0.8343165516853333
Norm after each mp layer: 2.221283435821533
Norm after each mp layer: 5.378899097442627
Norm before input: 0.2552422881126404
Norm after input: 0.3506649136543274
Norm after each mp layer: 0.8302580714225769
Norm after each mp layer: 2.2052383422851562
Norm after each mp layer: 5.359771728515625
Norm before input: 0.2552422881126404
Norm after input: 0.3506649136543274
Norm after each mp layer: 0.8302580714225769
Norm after each mp layer: 2.2052383422851562
Norm after each mp layer: 5.359771728515625
Norm before input: 0.2552422881126404
Norm after input: 0.35048818588256836
Norm after each mp layer: 0.8282999992370605
Norm after each mp layer: 2.2012746334075928
Norm after each mp layer: 5.36924409866333
Epoch: 360, Loss: 0.1251, Energy: 52.3744, Train: 98.76%, Valid: 72.20%, Test: 70.30%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35048818588256836
Norm after each mp layer: 0.8282999992370605
Norm after each mp layer: 2.2012746334075928
Norm after each mp layer: 5.36924409866333
Norm before input: 0.2552422881126404
Norm after input: 0.3489997684955597
Norm after each mp layer: 0.8269057273864746
Norm after each mp layer: 2.218543767929077
Norm after each mp layer: 5.410752296447754
Norm before input: 0.2552422881126404
Norm after input: 0.3489997684955597
Norm after each mp layer: 0.8269057273864746
Norm after each mp layer: 2.218543767929077
Norm after each mp layer: 5.410752296447754
Norm before input: 0.2552422881126404
Norm after input: 0.34918859601020813
Norm after each mp layer: 0.8322712182998657
Norm after each mp layer: 2.238645076751709
Norm after each mp layer: 5.440359115600586
Norm before input: 0.2552422881126404
Norm after input: 0.34918859601020813
Norm after each mp layer: 0.8322712182998657
Norm after each mp layer: 2.238645076751709
Norm after each mp layer: 5.440359115600586
Norm before input: 0.2552422881126404
Norm after input: 0.3505763113498688
Norm after each mp layer: 0.8352910876274109
Norm after each mp layer: 2.2280311584472656
Norm after each mp layer: 5.4119157791137695
Norm before input: 0.2552422881126404
Norm after input: 0.3505763113498688
Norm after each mp layer: 0.8352910876274109
Norm after each mp layer: 2.2280311584472656
Norm after each mp layer: 5.4119157791137695
Norm before input: 0.2552422881126404
Norm after input: 0.35144534707069397
Norm after each mp layer: 0.8310576677322388
Norm after each mp layer: 2.197572708129883
Norm after each mp layer: 5.362967014312744
Norm before input: 0.2552422881126404
Norm after input: 0.35144534707069397
Norm after each mp layer: 0.8310576677322388
Norm after each mp layer: 2.197572708129883
Norm after each mp layer: 5.362967014312744
Norm before input: 0.2552422881126404
Norm after input: 0.3497507572174072
Norm after each mp layer: 0.8314997553825378
Norm after each mp layer: 2.2250969409942627
Norm after each mp layer: 5.402167797088623
Epoch: 365, Loss: 0.1435, Energy: 56.1283, Train: 98.84%, Valid: 72.40%, Test: 70.40%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3497507572174072
Norm after each mp layer: 0.8314997553825378
Norm after each mp layer: 2.2250969409942627
Norm after each mp layer: 5.402167797088623
Norm before input: 0.2552422881126404
Norm after input: 0.34887567162513733
Norm after each mp layer: 0.8311832547187805
Norm after each mp layer: 2.2398056983947754
Norm after each mp layer: 5.425412654876709
Norm before input: 0.2552422881126404
Norm after input: 0.34887567162513733
Norm after each mp layer: 0.8311832547187805
Norm after each mp layer: 2.2398056983947754
Norm after each mp layer: 5.425412654876709
Norm before input: 0.2552422881126404
Norm after input: 0.3493465781211853
Norm after each mp layer: 0.8276341557502747
Norm after each mp layer: 2.214860200881958
Norm after each mp layer: 5.374919891357422
Norm before input: 0.2552422881126404
Norm after input: 0.3493465781211853
Norm after each mp layer: 0.8276341557502747
Norm after each mp layer: 2.214860200881958
Norm after each mp layer: 5.374919891357422
Norm before input: 0.2552422881126404
Norm after input: 0.35045096278190613
Norm after each mp layer: 0.8252512216567993
Norm after each mp layer: 2.186400890350342
Norm after each mp layer: 5.320896625518799
Norm before input: 0.2552422881126404
Norm after input: 0.35045096278190613
Norm after each mp layer: 0.8252512216567993
Norm after each mp layer: 2.186400890350342
Norm after each mp layer: 5.320896625518799
Norm before input: 0.2552422881126404
Norm after input: 0.3506871163845062
Norm after each mp layer: 0.8266537189483643
Norm after each mp layer: 2.188220739364624
Norm after each mp layer: 5.321595191955566
Norm before input: 0.2552422881126404
Norm after input: 0.3506871163845062
Norm after each mp layer: 0.8266537189483643
Norm after each mp layer: 2.188220739364624
Norm after each mp layer: 5.321595191955566
Norm before input: 0.2552422881126404
Norm after input: 0.35036176443099976
Norm after each mp layer: 0.8339242935180664
Norm after each mp layer: 2.2238667011260986
Norm after each mp layer: 5.382442474365234
Epoch: 370, Loss: 0.1265, Energy: 51.1007, Train: 98.68%, Valid: 73.00%, Test: 69.80%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35036176443099976
Norm after each mp layer: 0.8339242935180664
Norm after each mp layer: 2.2238667011260986
Norm after each mp layer: 5.382442474365234
Norm before input: 0.2552422881126404
Norm after input: 0.34896695613861084
Norm after each mp layer: 0.822618305683136
Norm after each mp layer: 2.2009778022766113
Norm after each mp layer: 5.368790149688721
Norm before input: 0.2552422881126404
Norm after input: 0.34896695613861084
Norm after each mp layer: 0.822618305683136
Norm after each mp layer: 2.2009778022766113
Norm after each mp layer: 5.368790149688721
Norm before input: 0.2552422881126404
Norm after input: 0.34970638155937195
Norm after each mp layer: 0.8282047510147095
Norm after each mp layer: 2.2156624794006348
Norm after each mp layer: 5.399816989898682
Norm before input: 0.2552422881126404
Norm after input: 0.34970638155937195
Norm after each mp layer: 0.8282046914100647
Norm after each mp layer: 2.2156624794006348
Norm after each mp layer: 5.399816989898682
Norm before input: 0.2552422881126404
Norm after input: 0.34998032450675964
Norm after each mp layer: 0.8286547064781189
Norm after each mp layer: 2.2135400772094727
Norm after each mp layer: 5.400062561035156
Norm before input: 0.2552422881126404
Norm after input: 0.34998032450675964
Norm after each mp layer: 0.8286547064781189
Norm after each mp layer: 2.2135400772094727
Norm after each mp layer: 5.400062561035156
Norm before input: 0.2552422881126404
Norm after input: 0.34929224848747253
Norm after each mp layer: 0.8232933878898621
Norm after each mp layer: 2.199828624725342
Norm after each mp layer: 5.372961521148682
Norm before input: 0.2552422881126404
Norm after input: 0.34929224848747253
Norm after each mp layer: 0.8232933878898621
Norm after each mp layer: 2.199828624725342
Norm after each mp layer: 5.372961521148682
Norm before input: 0.2552422881126404
Norm after input: 0.3497842848300934
Norm after each mp layer: 0.828098475933075
Norm after each mp layer: 2.2099530696868896
Norm after each mp layer: 5.383528709411621
Epoch: 375, Loss: 0.1253, Energy: 57.0428, Train: 98.76%, Valid: 71.40%, Test: 69.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3497842848300934
Norm after each mp layer: 0.828098475933075
Norm after each mp layer: 2.2099530696868896
Norm after each mp layer: 5.383528709411621
Norm before input: 0.2552422881126404
Norm after input: 0.3510855734348297
Norm after each mp layer: 0.8395581245422363
Norm after each mp layer: 2.239934206008911
Norm after each mp layer: 5.424813747406006
Norm before input: 0.2552422881126404
Norm after input: 0.3510855734348297
Norm after each mp layer: 0.8395581245422363
Norm after each mp layer: 2.239934206008911
Norm after each mp layer: 5.424813747406006
Norm before input: 0.2552422881126404
Norm after input: 0.3496103584766388
Norm after each mp layer: 0.8205761909484863
Norm after each mp layer: 2.182051658630371
Norm after each mp layer: 5.332399845123291
Norm before input: 0.2552422881126404
Norm after input: 0.3496103584766388
Norm after each mp layer: 0.8205761909484863
Norm after each mp layer: 2.182051658630371
Norm after each mp layer: 5.332399845123291
Norm before input: 0.2552422881126404
Norm after input: 0.34866586327552795
Norm after each mp layer: 0.8202742338180542
Norm after each mp layer: 2.1979331970214844
Norm after each mp layer: 5.350402355194092
Norm before input: 0.2552422881126404
Norm after input: 0.34866586327552795
Norm after each mp layer: 0.8202742338180542
Norm after each mp layer: 2.1979331970214844
Norm after each mp layer: 5.350402355194092
Norm before input: 0.2552422881126404
Norm after input: 0.3499065935611725
Norm after each mp layer: 0.8376553058624268
Norm after each mp layer: 2.2557404041290283
Norm after each mp layer: 5.410250663757324
Norm before input: 0.2552422881126404
Norm after input: 0.3499065935611725
Norm after each mp layer: 0.8376553058624268
Norm after each mp layer: 2.2557404041290283
Norm after each mp layer: 5.410250663757324
Norm before input: 0.2552422881126404
Norm after input: 0.3502908945083618
Norm after each mp layer: 0.8338134288787842
Norm after each mp layer: 2.2330121994018555
Norm after each mp layer: 5.354071617126465
Epoch: 380, Loss: 0.1336, Energy: 62.2501, Train: 98.76%, Valid: 72.60%, Test: 70.80%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3502908945083618
Norm after each mp layer: 0.8338134288787842
Norm after each mp layer: 2.2330121994018555
Norm after each mp layer: 5.354071617126465
Norm before input: 0.2552422881126404
Norm after input: 0.35048288106918335
Norm after each mp layer: 0.8281579613685608
Norm after each mp layer: 2.2105143070220947
Norm after each mp layer: 5.304356575012207
Norm before input: 0.2552422881126404
Norm after input: 0.35048288106918335
Norm after each mp layer: 0.8281579613685608
Norm after each mp layer: 2.2105143070220947
Norm after each mp layer: 5.304356575012207
Norm before input: 0.2552422881126404
Norm after input: 0.3506345748901367
Norm after each mp layer: 0.835211992263794
Norm after each mp layer: 2.237346649169922
Norm after each mp layer: 5.317322254180908
Norm before input: 0.2552422881126404
Norm after input: 0.3506345748901367
Norm after each mp layer: 0.835211992263794
Norm after each mp layer: 2.237346649169922
Norm after each mp layer: 5.317322254180908
Norm before input: 0.2552422881126404
Norm after input: 0.3499286472797394
Norm after each mp layer: 0.8379037976264954
Norm after each mp layer: 2.2635691165924072
Norm after each mp layer: 5.337813854217529
Norm before input: 0.2552422881126404
Norm after input: 0.3499286472797394
Norm after each mp layer: 0.8379037976264954
Norm after each mp layer: 2.2635691165924072
Norm after each mp layer: 5.337813854217529
Norm before input: 0.2552422881126404
Norm after input: 0.34945809841156006
Norm after each mp layer: 0.8324460983276367
Norm after each mp layer: 2.246782064437866
Norm after each mp layer: 5.296909332275391
Norm before input: 0.2552422881126404
Norm after input: 0.34945809841156006
Norm after each mp layer: 0.8324460983276367
Norm after each mp layer: 2.246782064437866
Norm after each mp layer: 5.296909332275391
Norm before input: 0.2552422881126404
Norm after input: 0.34994247555732727
Norm after each mp layer: 0.8267273902893066
Norm after each mp layer: 2.2151055335998535
Norm after each mp layer: 5.236517429351807
Epoch: 385, Loss: 0.1226, Energy: 55.5751, Train: 98.76%, Valid: 72.00%, Test: 69.30%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.34994247555732727
Norm after each mp layer: 0.8267273902893066
Norm after each mp layer: 2.2151055335998535
Norm after each mp layer: 5.236517429351807
Norm before input: 0.2552422881126404
Norm after input: 0.3506755530834198
Norm after each mp layer: 0.8272346258163452
Norm after each mp layer: 2.2079200744628906
Norm after each mp layer: 5.224775314331055
Norm before input: 0.2552422881126404
Norm after input: 0.3506755530834198
Norm after each mp layer: 0.8272346258163452
Norm after each mp layer: 2.2079200744628906
Norm after each mp layer: 5.224775314331055
Norm before input: 0.2552422881126404
Norm after input: 0.34897831082344055
Norm after each mp layer: 0.8306372761726379
Norm after each mp layer: 2.255519151687622
Norm after each mp layer: 5.315009593963623
Norm before input: 0.2552422881126404
Norm after input: 0.34897831082344055
Norm after each mp layer: 0.8306372761726379
Norm after each mp layer: 2.255519151687622
Norm after each mp layer: 5.315009593963623
Norm before input: 0.2552422881126404
Norm after input: 0.34929007291793823
Norm after each mp layer: 0.8333986401557922
Norm after each mp layer: 2.2699575424194336
Norm after each mp layer: 5.346960067749023
Norm before input: 0.2552422881126404
Norm after input: 0.34929007291793823
Norm after each mp layer: 0.8333986401557922
Norm after each mp layer: 2.2699575424194336
Norm after each mp layer: 5.346960067749023
Norm before input: 0.2552422881126404
Norm after input: 0.34978926181793213
Norm after each mp layer: 0.8228539824485779
Norm after each mp layer: 2.2161927223205566
Norm after each mp layer: 5.279974460601807
Norm before input: 0.2552422881126404
Norm after input: 0.34978926181793213
Norm after each mp layer: 0.8228539824485779
Norm after each mp layer: 2.2161927223205566
Norm after each mp layer: 5.279974460601807
Norm before input: 0.2552422881126404
Norm after input: 0.3499711751937866
Norm after each mp layer: 0.8225602507591248
Norm after each mp layer: 2.212672710418701
Norm after each mp layer: 5.2736897468566895
Epoch: 390, Loss: 0.1306, Energy: 57.1759, Train: 99.01%, Valid: 71.40%, Test: 68.70%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3499711751937866
Norm after each mp layer: 0.8225602507591248
Norm after each mp layer: 2.212672710418701
Norm after each mp layer: 5.2736897468566895
Norm before input: 0.2552422881126404
Norm after input: 0.35001587867736816
Norm after each mp layer: 0.8375205993652344
Norm after each mp layer: 2.2749221324920654
Norm after each mp layer: 5.354008197784424
Norm before input: 0.2552422881126404
Norm after input: 0.35001587867736816
Norm after each mp layer: 0.8375205993652344
Norm after each mp layer: 2.2749221324920654
Norm after each mp layer: 5.354008197784424
Norm before input: 0.2552422881126404
Norm after input: 0.3495909571647644
Norm after each mp layer: 0.8353811502456665
Norm after each mp layer: 2.26997709274292
Norm after each mp layer: 5.345824718475342
Norm before input: 0.2552422881126404
Norm after input: 0.3495909571647644
Norm after each mp layer: 0.8353811502456665
Norm after each mp layer: 2.269977331161499
Norm after each mp layer: 5.345824718475342
Norm before input: 0.2552422881126404
Norm after input: 0.3500797152519226
Norm after each mp layer: 0.8291422128677368
Norm after each mp layer: 2.2334117889404297
Norm after each mp layer: 5.294929027557373
Norm before input: 0.2552422881126404
Norm after input: 0.3500797152519226
Norm after each mp layer: 0.8291422128677368
Norm after each mp layer: 2.2334117889404297
Norm after each mp layer: 5.294928550720215
Norm before input: 0.2552422881126404
Norm after input: 0.3498493731021881
Norm after each mp layer: 0.8274547457695007
Norm after each mp layer: 2.229849100112915
Norm after each mp layer: 5.297144412994385
Norm before input: 0.2552422881126404
Norm after input: 0.3498493731021881
Norm after each mp layer: 0.8274547457695007
Norm after each mp layer: 2.229849100112915
Norm after each mp layer: 5.297143936157227
Norm before input: 0.2552422881126404
Norm after input: 0.3489992022514343
Norm after each mp layer: 0.8322966694831848
Norm after each mp layer: 2.2663168907165527
Norm after each mp layer: 5.3451008796691895
Epoch: 395, Loss: 0.1373, Energy: 71.2665, Train: 98.92%, Valid: 72.20%, Test: 70.40%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3489992022514343
Norm after each mp layer: 0.8322966694831848
Norm after each mp layer: 2.2663168907165527
Norm after each mp layer: 5.3451008796691895
Norm before input: 0.2552422881126404
Norm after input: 0.3496240973472595
Norm after each mp layer: 0.8414285778999329
Norm after each mp layer: 2.29666805267334
Norm after each mp layer: 5.384025573730469
Norm before input: 0.2552422881126404
Norm after input: 0.3496240973472595
Norm after each mp layer: 0.8414285778999329
Norm after each mp layer: 2.29666805267334
Norm after each mp layer: 5.384025573730469
Norm before input: 0.2552422881126404
Norm after input: 0.35097846388816833
Norm after each mp layer: 0.8428951501846313
Norm after each mp layer: 2.2784576416015625
Norm after each mp layer: 5.353022575378418
Norm before input: 0.2552422881126404
Norm after input: 0.35097846388816833
Norm after each mp layer: 0.8428951501846313
Norm after each mp layer: 2.2784576416015625
Norm after each mp layer: 5.353022575378418
Norm before input: 0.2552422881126404
Norm after input: 0.35045236349105835
Norm after each mp layer: 0.8263112306594849
Norm after each mp layer: 2.223206043243408
Norm after each mp layer: 5.286491870880127
Norm before input: 0.2552422881126404
Norm after input: 0.35045236349105835
Norm after each mp layer: 0.8263112306594849
Norm after each mp layer: 2.223206043243408
Norm after each mp layer: 5.286491870880127
Norm before input: 0.2552422881126404
Norm after input: 0.3504464626312256
Norm after each mp layer: 0.833942711353302
Norm after each mp layer: 2.258917808532715
Norm after each mp layer: 5.332500457763672
Norm before input: 0.2552422881126404
Norm after input: 0.3504464626312256
Norm after each mp layer: 0.833942711353302
Norm after each mp layer: 2.258917808532715
Norm after each mp layer: 5.332500457763672
Norm before input: 0.2552422881126404
Norm after input: 0.3477168679237366
Norm after each mp layer: 0.8315491080284119
Norm after each mp layer: 2.301530361175537
Norm after each mp layer: 5.374956130981445
Epoch: 400, Loss: 0.1590, Energy: 74.9690, Train: 98.10%, Valid: 70.60%, Test: 69.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3477168679237366
Norm after each mp layer: 0.8315491080284119
Norm after each mp layer: 2.301530361175537
Norm after each mp layer: 5.374956130981445
Norm before input: 0.2552422881126404
Norm after input: 0.34821274876594543
Norm after each mp layer: 0.83144611120224
Norm after each mp layer: 2.2880964279174805
Norm after each mp layer: 5.310116767883301
Norm before input: 0.2552422881126404
Norm after input: 0.34821274876594543
Norm after each mp layer: 0.83144611120224
Norm after each mp layer: 2.2880964279174805
Norm after each mp layer: 5.310116767883301
Norm before input: 0.2552422881126404
Norm after input: 0.35125577449798584
Norm after each mp layer: 0.8355526328086853
Norm after each mp layer: 2.2468302249908447
Norm after each mp layer: 5.209125518798828
Norm before input: 0.2552422881126404
Norm after input: 0.35125577449798584
Norm after each mp layer: 0.8355526328086853
Norm after each mp layer: 2.2468302249908447
Norm after each mp layer: 5.209125518798828
Norm before input: 0.2552422881126404
Norm after input: 0.3516910672187805
Norm after each mp layer: 0.8304237723350525
Norm after each mp layer: 2.2204995155334473
Norm after each mp layer: 5.1663336753845215
Norm before input: 0.2552422881126404
Norm after input: 0.3516910672187805
Norm after each mp layer: 0.8304237723350525
Norm after each mp layer: 2.2204995155334473
Norm after each mp layer: 5.1663336753845215
Norm before input: 0.2552422881126404
Norm after input: 0.34929412603378296
Norm after each mp layer: 0.8316513895988464
Norm after each mp layer: 2.269562005996704
Norm after each mp layer: 5.213671684265137
Norm before input: 0.2552422881126404
Norm after input: 0.34929412603378296
Norm after each mp layer: 0.8316513895988464
Norm after each mp layer: 2.269562005996704
Norm after each mp layer: 5.213671684265137
Norm before input: 0.2552422881126404
Norm after input: 0.3484208881855011
Norm after each mp layer: 0.8350715637207031
Norm after each mp layer: 2.3118529319763184
Norm after each mp layer: 5.268239498138428
Epoch: 405, Loss: 0.1218, Energy: 57.0230, Train: 98.51%, Valid: 73.20%, Test: 70.20%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3484208881855011
Norm after each mp layer: 0.8350715637207031
Norm after each mp layer: 2.3118529319763184
Norm after each mp layer: 5.268239498138428
Norm before input: 0.2552422881126404
Norm after input: 0.34889736771583557
Norm after each mp layer: 0.8292961716651917
Norm after each mp layer: 2.2724812030792236
Norm after each mp layer: 5.193053722381592
Norm before input: 0.2552422881126404
Norm after input: 0.34889736771583557
Norm after each mp layer: 0.8292961716651917
Norm after each mp layer: 2.2724812030792236
Norm after each mp layer: 5.193053722381592
Norm before input: 0.2552422881126404
Norm after input: 0.34995344281196594
Norm after each mp layer: 0.8217041492462158
Norm after each mp layer: 2.2148733139038086
Norm after each mp layer: 5.08968448638916
Norm before input: 0.2552422881126404
Norm after input: 0.34995344281196594
Norm after each mp layer: 0.8217041492462158
Norm after each mp layer: 2.2148733139038086
Norm after each mp layer: 5.08968448638916
Norm before input: 0.2552422881126404
Norm after input: 0.35095250606536865
Norm after each mp layer: 0.8278123140335083
Norm after each mp layer: 2.2237470149993896
Norm after each mp layer: 5.075747489929199
Norm before input: 0.2552422881126404
Norm after input: 0.35095250606536865
Norm after each mp layer: 0.8278123140335083
Norm after each mp layer: 2.2237470149993896
Norm after each mp layer: 5.075747489929199
Norm before input: 0.2552422881126404
Norm after input: 0.35159364342689514
Norm after each mp layer: 0.8419200778007507
Norm after each mp layer: 2.276409387588501
Norm after each mp layer: 5.139063358306885
Norm before input: 0.2552422881126404
Norm after input: 0.35159364342689514
Norm after each mp layer: 0.8419200778007507
Norm after each mp layer: 2.276409387588501
Norm after each mp layer: 5.139063358306885
Norm before input: 0.2552422881126404
Norm after input: 0.34989115595817566
Norm after each mp layer: 0.829125165939331
Norm after each mp layer: 2.250978946685791
Norm after each mp layer: 5.10308837890625
Epoch: 410, Loss: 0.1633, Energy: 55.6874, Train: 98.84%, Valid: 71.40%, Test: 68.80%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.34989115595817566
Norm after each mp layer: 0.829125165939331
Norm after each mp layer: 2.250978946685791
Norm after each mp layer: 5.10308837890625
Norm before input: 0.2552422881126404
Norm after input: 0.35019469261169434
Norm after each mp layer: 0.8327398300170898
Norm after each mp layer: 2.268087148666382
Norm after each mp layer: 5.131308555603027
Norm before input: 0.2552422881126404
Norm after input: 0.35019469261169434
Norm after each mp layer: 0.8327398300170898
Norm after each mp layer: 2.268087148666382
Norm after each mp layer: 5.131308555603027
Norm before input: 0.2552422881126404
Norm after input: 0.35005635023117065
Norm after each mp layer: 0.8328384160995483
Norm after each mp layer: 2.276221513748169
Norm after each mp layer: 5.159555435180664
Norm before input: 0.2552422881126404
Norm after input: 0.35005635023117065
Norm after each mp layer: 0.8328384160995483
Norm after each mp layer: 2.276221513748169
Norm after each mp layer: 5.159555435180664
Norm before input: 0.2552422881126404
Norm after input: 0.3493399918079376
Norm after each mp layer: 0.8277856111526489
Norm after each mp layer: 2.2671468257904053
Norm after each mp layer: 5.143346309661865
Norm before input: 0.2552422881126404
Norm after input: 0.3493399918079376
Norm after each mp layer: 0.8277856111526489
Norm after each mp layer: 2.2671468257904053
Norm after each mp layer: 5.143346309661865
Norm before input: 0.2552422881126404
Norm after input: 0.35012468695640564
Norm after each mp layer: 0.8323419094085693
Norm after each mp layer: 2.274941921234131
Norm after each mp layer: 5.1513671875
Norm before input: 0.2552422881126404
Norm after input: 0.35012468695640564
Norm after each mp layer: 0.8323419094085693
Norm after each mp layer: 2.274941921234131
Norm after each mp layer: 5.151367664337158
Norm before input: 0.2552422881126404
Norm after input: 0.3522971570491791
Norm after each mp layer: 0.8456515669822693
Norm after each mp layer: 2.2959823608398438
Norm after each mp layer: 5.180548667907715
Epoch: 415, Loss: 0.1219, Energy: 51.5953, Train: 98.76%, Valid: 72.80%, Test: 70.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3522971570491791
Norm after each mp layer: 0.8456515669822693
Norm after each mp layer: 2.2959823608398438
Norm after each mp layer: 5.180548667907715
Norm before input: 0.2552422881126404
Norm after input: 0.35233795642852783
Norm after each mp layer: 0.838798463344574
Norm after each mp layer: 2.269087076187134
Norm after each mp layer: 5.153519153594971
Norm before input: 0.2552422881126404
Norm after input: 0.35233795642852783
Norm after each mp layer: 0.838798463344574
Norm after each mp layer: 2.269087076187134
Norm after each mp layer: 5.153519153594971
Norm before input: 0.2552422881126404
Norm after input: 0.3507339656352997
Norm after each mp layer: 0.8352218270301819
Norm after each mp layer: 2.28495454788208
Norm after each mp layer: 5.187958240509033
Norm before input: 0.2552422881126404
Norm after input: 0.3507339656352997
Norm after each mp layer: 0.8352218270301819
Norm after each mp layer: 2.28495454788208
Norm after each mp layer: 5.187958240509033
Norm before input: 0.2552422881126404
Norm after input: 0.35023996233940125
Norm after each mp layer: 0.8374910354614258
Norm after each mp layer: 2.308633804321289
Norm after each mp layer: 5.236573696136475
Norm before input: 0.2552422881126404
Norm after input: 0.35023996233940125
Norm after each mp layer: 0.8374910354614258
Norm after each mp layer: 2.308633804321289
Norm after each mp layer: 5.236573696136475
Norm before input: 0.2552422881126404
Norm after input: 0.35087883472442627
Norm after each mp layer: 0.8395460247993469
Norm after each mp layer: 2.308738946914673
Norm after each mp layer: 5.238784313201904
Norm before input: 0.2552422881126404
Norm after input: 0.35087883472442627
Norm after each mp layer: 0.8395460247993469
Norm after each mp layer: 2.308738946914673
Norm after each mp layer: 5.238784313201904
Norm before input: 0.2552422881126404
Norm after input: 0.35172000527381897
Norm after each mp layer: 0.8375188708305359
Norm after each mp layer: 2.2829697132110596
Norm after each mp layer: 5.191650867462158
Epoch: 420, Loss: 0.1272, Energy: 62.8368, Train: 99.25%, Valid: 71.60%, Test: 70.30%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35172000527381897
Norm after each mp layer: 0.8375188708305359
Norm after each mp layer: 2.2829697132110596
Norm after each mp layer: 5.191650867462158
Norm before input: 0.2552422881126404
Norm after input: 0.35234102606773376
Norm after each mp layer: 0.8370115756988525
Norm after each mp layer: 2.270925283432007
Norm after each mp layer: 5.172941207885742
Norm before input: 0.2552422881126404
Norm after input: 0.35234102606773376
Norm after each mp layer: 0.8370115756988525
Norm after each mp layer: 2.270925283432007
Norm after each mp layer: 5.172941207885742
Norm before input: 0.2552422881126404
Norm after input: 0.3525896370410919
Norm after each mp layer: 0.8459726572036743
Norm after each mp layer: 2.307112455368042
Norm after each mp layer: 5.231047630310059
Norm before input: 0.2552422881126404
Norm after input: 0.3525896370410919
Norm after each mp layer: 0.8459726572036743
Norm after each mp layer: 2.307112455368042
Norm after each mp layer: 5.231047630310059
Norm before input: 0.2552422881126404
Norm after input: 0.3515133261680603
Norm after each mp layer: 0.8370794057846069
Norm after each mp layer: 2.289520740509033
Norm after each mp layer: 5.220508098602295
Norm before input: 0.2552422881126404
Norm after input: 0.3515133261680603
Norm after each mp layer: 0.8370794057846069
Norm after each mp layer: 2.289520740509033
Norm after each mp layer: 5.220508098602295
Norm before input: 0.2552422881126404
Norm after input: 0.35160887241363525
Norm after each mp layer: 0.8365597128868103
Norm after each mp layer: 2.2891247272491455
Norm after each mp layer: 5.235692024230957
Norm before input: 0.2552422881126404
Norm after input: 0.35160887241363525
Norm after each mp layer: 0.8365597128868103
Norm after each mp layer: 2.2891247272491455
Norm after each mp layer: 5.235692024230957
Norm before input: 0.2552422881126404
Norm after input: 0.35194605588912964
Norm after each mp layer: 0.8411540389060974
Norm after each mp layer: 2.308225154876709
Norm after each mp layer: 5.282985687255859
Epoch: 425, Loss: 0.1068, Energy: 49.3511, Train: 99.01%, Valid: 71.60%, Test: 69.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35194605588912964
Norm after each mp layer: 0.8411540389060974
Norm after each mp layer: 2.308225154876709
Norm after each mp layer: 5.282985687255859
Norm before input: 0.2552422881126404
Norm after input: 0.3509865403175354
Norm after each mp layer: 0.8404186367988586
Norm after each mp layer: 2.3244075775146484
Norm after each mp layer: 5.3161163330078125
Norm before input: 0.2552422881126404
Norm after input: 0.3509865403175354
Norm after each mp layer: 0.8404186367988586
Norm after each mp layer: 2.3244075775146484
Norm after each mp layer: 5.3161163330078125
Norm before input: 0.2552422881126404
Norm after input: 0.3511885702610016
Norm after each mp layer: 0.8404894471168518
Norm after each mp layer: 2.321223497390747
Norm after each mp layer: 5.313085079193115
Norm before input: 0.2552422881126404
Norm after input: 0.3511885702610016
Norm after each mp layer: 0.8404894471168518
Norm after each mp layer: 2.321223497390747
Norm after each mp layer: 5.313085079193115
Norm before input: 0.2552422881126404
Norm after input: 0.3525750935077667
Norm after each mp layer: 0.8434209227561951
Norm after each mp layer: 2.30828595161438
Norm after each mp layer: 5.285628318786621
Norm before input: 0.2552422881126404
Norm after input: 0.3525750935077667
Norm after each mp layer: 0.8434209227561951
Norm after each mp layer: 2.30828595161438
Norm after each mp layer: 5.285628318786621
Norm before input: 0.2552422881126404
Norm after input: 0.3531605005264282
Norm after each mp layer: 0.8443044424057007
Norm after each mp layer: 2.30124831199646
Norm after each mp layer: 5.274035930633545
Norm before input: 0.2552422881126404
Norm after input: 0.3531605005264282
Norm after each mp layer: 0.8443044424057007
Norm after each mp layer: 2.30124831199646
Norm after each mp layer: 5.274035930633545
Norm before input: 0.2552422881126404
Norm after input: 0.3506543040275574
Norm after each mp layer: 0.8399462103843689
Norm after each mp layer: 2.3283305168151855
Norm after each mp layer: 5.3260416984558105
Epoch: 430, Loss: 0.1202, Energy: 48.8431, Train: 98.51%, Valid: 70.60%, Test: 69.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3506543040275574
Norm after each mp layer: 0.8399462103843689
Norm after each mp layer: 2.3283305168151855
Norm after each mp layer: 5.3260416984558105
Norm before input: 0.2552422881126404
Norm after input: 0.3513354957103729
Norm after each mp layer: 0.8433468341827393
Norm after each mp layer: 2.3347508907318115
Norm after each mp layer: 5.326063632965088
Norm before input: 0.2552422881126404
Norm after input: 0.3513354957103729
Norm after each mp layer: 0.8433468341827393
Norm after each mp layer: 2.3347508907318115
Norm after each mp layer: 5.326063632965088
Norm before input: 0.2552422881126404
Norm after input: 0.3519339859485626
Norm after each mp layer: 0.8402131199836731
Norm after each mp layer: 2.307457447052002
Norm after each mp layer: 5.286966323852539
Norm before input: 0.2552422881126404
Norm after input: 0.3519339859485626
Norm after each mp layer: 0.8402131199836731
Norm after each mp layer: 2.307457447052002
Norm after each mp layer: 5.286966323852539
Norm before input: 0.2552422881126404
Norm after input: 0.34949028491973877
Norm after each mp layer: 0.8356893658638
Norm after each mp layer: 2.33541202545166
Norm after each mp layer: 5.314562797546387
Norm before input: 0.2552422881126404
Norm after input: 0.34949028491973877
Norm after each mp layer: 0.8356893658638
Norm after each mp layer: 2.33541202545166
Norm after each mp layer: 5.314562797546387
Norm before input: 0.2552422881126404
Norm after input: 0.3511153757572174
Norm after each mp layer: 0.8444169759750366
Norm after each mp layer: 2.3427670001983643
Norm after each mp layer: 5.297774314880371
Norm before input: 0.2552422881126404
Norm after input: 0.3511153757572174
Norm after each mp layer: 0.8444169759750366
Norm after each mp layer: 2.3427670001983643
Norm after each mp layer: 5.297774314880371
Norm before input: 0.2552422881126404
Norm after input: 0.3542625904083252
Norm after each mp layer: 0.8510468602180481
Norm after each mp layer: 2.316582441329956
Norm after each mp layer: 5.256756782531738
Epoch: 435, Loss: 0.1332, Energy: 64.6236, Train: 97.52%, Valid: 72.20%, Test: 69.80%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3542625904083252
Norm after each mp layer: 0.8510468602180481
Norm after each mp layer: 2.316582441329956
Norm after each mp layer: 5.256756782531738
Norm before input: 0.2552422881126404
Norm after input: 0.3521292507648468
Norm after each mp layer: 0.8429639935493469
Norm after each mp layer: 2.318951368331909
Norm after each mp layer: 5.253239154815674
Norm before input: 0.2552422881126404
Norm after input: 0.3521292507648468
Norm after each mp layer: 0.8429639935493469
Norm after each mp layer: 2.318951368331909
Norm after each mp layer: 5.253239154815674
Norm before input: 0.2552422881126404
Norm after input: 0.35061049461364746
Norm after each mp layer: 0.8403795957565308
Norm after each mp layer: 2.3375113010406494
Norm after each mp layer: 5.289429664611816
Norm before input: 0.2552422881126404
Norm after input: 0.35061049461364746
Norm after each mp layer: 0.8403795957565308
Norm after each mp layer: 2.3375113010406494
Norm after each mp layer: 5.289429664611816
Norm before input: 0.2552422881126404
Norm after input: 0.3512428104877472
Norm after each mp layer: 0.847588062286377
Norm after each mp layer: 2.356166362762451
Norm after each mp layer: 5.3026838302612305
Norm before input: 0.2552422881126404
Norm after input: 0.3512428104877472
Norm after each mp layer: 0.847588062286377
Norm after each mp layer: 2.356166362762451
Norm after each mp layer: 5.3026838302612305
Norm before input: 0.2552422881126404
Norm after input: 0.35250720381736755
Norm after each mp layer: 0.8475504517555237
Norm after each mp layer: 2.326256513595581
Norm after each mp layer: 5.230833053588867
Norm before input: 0.2552422881126404
Norm after input: 0.35250720381736755
Norm after each mp layer: 0.8475504517555237
Norm after each mp layer: 2.326256513595581
Norm after each mp layer: 5.230833053588867
Norm before input: 0.2552422881126404
Norm after input: 0.3528464436531067
Norm after each mp layer: 0.8384912014007568
Norm after each mp layer: 2.274285316467285
Norm after each mp layer: 5.1158881187438965
Epoch: 440, Loss: 0.1225, Energy: 55.8048, Train: 99.25%, Valid: 71.00%, Test: 69.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3528464436531067
Norm after each mp layer: 0.8384912014007568
Norm after each mp layer: 2.274285316467285
Norm after each mp layer: 5.1158881187438965
Norm before input: 0.2552422881126404
Norm after input: 0.35154300928115845
Norm after each mp layer: 0.8355956077575684
Norm after each mp layer: 2.2851855754852295
Norm after each mp layer: 5.105246543884277
Norm before input: 0.2552422881126404
Norm after input: 0.35154300928115845
Norm after each mp layer: 0.8355956077575684
Norm after each mp layer: 2.2851855754852295
Norm after each mp layer: 5.105246543884277
Norm before input: 0.2552422881126404
Norm after input: 0.35189926624298096
Norm after each mp layer: 0.8551622033119202
Norm after each mp layer: 2.3737356662750244
Norm after each mp layer: 5.199483871459961
Norm before input: 0.2552422881126404
Norm after input: 0.35189926624298096
Norm after each mp layer: 0.8551622033119202
Norm after each mp layer: 2.3737356662750244
Norm after each mp layer: 5.199483871459961
Norm before input: 0.2552422881126404
Norm after input: 0.35144877433776855
Norm after each mp layer: 0.8443602323532104
Norm after each mp layer: 2.326873540878296
Norm after each mp layer: 5.084357738494873
Norm before input: 0.2552422881126404
Norm after input: 0.35144877433776855
Norm after each mp layer: 0.8443601727485657
Norm after each mp layer: 2.326873540878296
Norm after each mp layer: 5.084357738494873
Norm before input: 0.2552422881126404
Norm after input: 0.3516058921813965
Norm after each mp layer: 0.8341240882873535
Norm after each mp layer: 2.2775514125823975
Norm after each mp layer: 4.995473384857178
Norm before input: 0.2552422881126404
Norm after input: 0.3516058921813965
Norm after each mp layer: 0.8341240882873535
Norm after each mp layer: 2.2775514125823975
Norm after each mp layer: 4.995473384857178
Norm before input: 0.2552422881126404
Norm after input: 0.35258185863494873
Norm after each mp layer: 0.8432658314704895
Norm after each mp layer: 2.3068392276763916
Norm after each mp layer: 5.019649982452393
Epoch: 445, Loss: 0.1462, Energy: 63.0355, Train: 99.09%, Valid: 72.40%, Test: 70.30%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35258185863494873
Norm after each mp layer: 0.8432658314704895
Norm after each mp layer: 2.3068392276763916
Norm after each mp layer: 5.019649982452393
Norm before input: 0.2552422881126404
Norm after input: 0.3536042273044586
Norm after each mp layer: 0.8574836850166321
Norm after each mp layer: 2.3622448444366455
Norm after each mp layer: 5.111608028411865
Norm before input: 0.2552422881126404
Norm after input: 0.3536042273044586
Norm after each mp layer: 0.8574836850166321
Norm after each mp layer: 2.3622448444366455
Norm after each mp layer: 5.111608028411865
Norm before input: 0.2552422881126404
Norm after input: 0.3531332314014435
Norm after each mp layer: 0.8511961102485657
Norm after each mp layer: 2.3491439819335938
Norm after each mp layer: 5.115780830383301
Norm before input: 0.2552422881126404
Norm after input: 0.3531332314014435
Norm after each mp layer: 0.8511961102485657
Norm after each mp layer: 2.3491439819335938
Norm after each mp layer: 5.115780830383301
Norm before input: 0.2552422881126404
Norm after input: 0.3530878722667694
Norm after each mp layer: 0.8518059849739075
Norm after each mp layer: 2.3591806888580322
Norm after each mp layer: 5.135969161987305
Norm before input: 0.2552422881126404
Norm after input: 0.3530878722667694
Norm after each mp layer: 0.8518059849739075
Norm after each mp layer: 2.3591806888580322
Norm after each mp layer: 5.135969161987305
Norm before input: 0.2552422881126404
Norm after input: 0.3530903458595276
Norm after each mp layer: 0.8518911004066467
Norm after each mp layer: 2.364006519317627
Norm after each mp layer: 5.142767906188965
Norm before input: 0.2552422881126404
Norm after input: 0.3530903458595276
Norm after each mp layer: 0.8518911004066467
Norm after each mp layer: 2.364006519317627
Norm after each mp layer: 5.142767906188965
Norm before input: 0.2552422881126404
Norm after input: 0.35274186730384827
Norm after each mp layer: 0.8477185964584351
Norm after each mp layer: 2.35009503364563
Norm after each mp layer: 5.1079816818237305
Epoch: 450, Loss: 0.1141, Energy: 52.4937, Train: 99.09%, Valid: 71.60%, Test: 70.80%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35274186730384827
Norm after each mp layer: 0.8477185964584351
Norm after each mp layer: 2.35009503364563
Norm after each mp layer: 5.1079816818237305
Norm before input: 0.2552422881126404
Norm after input: 0.35244402289390564
Norm after each mp layer: 0.844088077545166
Norm after each mp layer: 2.336583375930786
Norm after each mp layer: 5.068838596343994
Norm before input: 0.2552422881126404
Norm after input: 0.35244402289390564
Norm after each mp layer: 0.844088077545166
Norm after each mp layer: 2.336583375930786
Norm after each mp layer: 5.068838596343994
Norm before input: 0.2552422881126404
Norm after input: 0.35262230038642883
Norm after each mp layer: 0.8449369668960571
Norm after each mp layer: 2.335322141647339
Norm after each mp layer: 5.054258346557617
Norm before input: 0.2552422881126404
Norm after input: 0.35262230038642883
Norm after each mp layer: 0.8449369668960571
Norm after each mp layer: 2.335322141647339
Norm after each mp layer: 5.054258346557617
Norm before input: 0.2552422881126404
Norm after input: 0.3532005846500397
Norm after each mp layer: 0.8487339615821838
Norm after each mp layer: 2.341609001159668
Norm after each mp layer: 5.0647969245910645
Norm before input: 0.2552422881126404
Norm after input: 0.3532005846500397
Norm after each mp layer: 0.8487339615821838
Norm after each mp layer: 2.341609001159668
Norm after each mp layer: 5.0647969245910645
Norm before input: 0.2552422881126404
Norm after input: 0.3529812693595886
Norm after each mp layer: 0.842727780342102
Norm after each mp layer: 2.3211793899536133
Norm after each mp layer: 5.060938358306885
Norm before input: 0.2552422881126404
Norm after input: 0.3529812693595886
Norm after each mp layer: 0.842727780342102
Norm after each mp layer: 2.3211793899536133
Norm after each mp layer: 5.060938358306885
Norm before input: 0.2552422881126404
Norm after input: 0.35295209288597107
Norm after each mp layer: 0.846459150314331
Norm after each mp layer: 2.345052480697632
Norm after each mp layer: 5.125568389892578
Epoch: 455, Loss: 0.1060, Energy: 42.1160, Train: 99.09%, Valid: 71.60%, Test: 69.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35295209288597107
Norm after each mp layer: 0.846459150314331
Norm after each mp layer: 2.345052480697632
Norm after each mp layer: 5.125568389892578
Norm before input: 0.2552422881126404
Norm after input: 0.3528349995613098
Norm after each mp layer: 0.8486237525939941
Norm after each mp layer: 2.364912986755371
Norm after each mp layer: 5.191566467285156
Norm before input: 0.2552422881126404
Norm after input: 0.3528349995613098
Norm after each mp layer: 0.8486237525939941
Norm after each mp layer: 2.364912986755371
Norm after each mp layer: 5.191566467285156
Norm before input: 0.2552422881126404
Norm after input: 0.35263046622276306
Norm after each mp layer: 0.8448359966278076
Norm after each mp layer: 2.3523645401000977
Norm after each mp layer: 5.196268081665039
Norm before input: 0.2552422881126404
Norm after input: 0.35263046622276306
Norm after each mp layer: 0.8448359966278076
Norm after each mp layer: 2.3523645401000977
Norm after each mp layer: 5.196268081665039
Norm before input: 0.2552422881126404
Norm after input: 0.3528735637664795
Norm after each mp layer: 0.8432471752166748
Norm after each mp layer: 2.342397928237915
Norm after each mp layer: 5.199112415313721
Norm before input: 0.2552422881126404
Norm after input: 0.3528735637664795
Norm after each mp layer: 0.8432471752166748
Norm after each mp layer: 2.342397928237915
Norm after each mp layer: 5.199112415313721
Norm before input: 0.2552422881126404
Norm after input: 0.353678435087204
Norm after each mp layer: 0.8525713086128235
Norm after each mp layer: 2.3719310760498047
Norm after each mp layer: 5.249565601348877
Norm before input: 0.2552422881126404
Norm after input: 0.353678435087204
Norm after each mp layer: 0.8525713086128235
Norm after each mp layer: 2.3719310760498047
Norm after each mp layer: 5.249565601348877
Norm before input: 0.2552422881126404
Norm after input: 0.3532884120941162
Norm after each mp layer: 0.8473666310310364
Norm after each mp layer: 2.356079339981079
Norm after each mp layer: 5.241678714752197
Epoch: 460, Loss: 0.1003, Energy: 45.0985, Train: 99.17%, Valid: 71.20%, Test: 69.30%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3532884120941162
Norm after each mp layer: 0.8473666310310364
Norm after each mp layer: 2.356079339981079
Norm after each mp layer: 5.241678714752197
Norm before input: 0.2552422881126404
Norm after input: 0.3530883491039276
Norm after each mp layer: 0.850000262260437
Norm after each mp layer: 2.3735854625701904
Norm after each mp layer: 5.275790691375732
Norm before input: 0.2552422881126404
Norm after input: 0.3530883491039276
Norm after each mp layer: 0.850000262260437
Norm after each mp layer: 2.3735854625701904
Norm after each mp layer: 5.275790691375732
Norm before input: 0.2552422881126404
Norm after input: 0.35305118560791016
Norm after each mp layer: 0.851804792881012
Norm after each mp layer: 2.3841707706451416
Norm after each mp layer: 5.298538684844971
Norm before input: 0.2552422881126404
Norm after input: 0.35305118560791016
Norm after each mp layer: 0.8518049120903015
Norm after each mp layer: 2.3841707706451416
Norm after each mp layer: 5.298538684844971
Norm before input: 0.2552422881126404
Norm after input: 0.3530174195766449
Norm after each mp layer: 0.8495743870735168
Norm after each mp layer: 2.3720269203186035
Norm after each mp layer: 5.283876419067383
Norm before input: 0.2552422881126404
Norm after input: 0.3530174195766449
Norm after each mp layer: 0.8495743870735168
Norm after each mp layer: 2.3720269203186035
Norm after each mp layer: 5.283875942230225
Norm before input: 0.2552422881126404
Norm after input: 0.3531019389629364
Norm after each mp layer: 0.8473098278045654
Norm after each mp layer: 2.3580777645111084
Norm after each mp layer: 5.267308235168457
Norm before input: 0.2552422881126404
Norm after input: 0.3531019389629364
Norm after each mp layer: 0.8473098278045654
Norm after each mp layer: 2.3580777645111084
Norm after each mp layer: 5.267308235168457
Norm before input: 0.2552422881126404
Norm after input: 0.3533416986465454
Norm after each mp layer: 0.8487012982368469
Norm after each mp layer: 2.3599226474761963
Norm after each mp layer: 5.2754058837890625
Epoch: 465, Loss: 0.0943, Energy: 45.6566, Train: 99.25%, Valid: 71.20%, Test: 70.10%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3533416986465454
Norm after each mp layer: 0.8487012982368469
Norm after each mp layer: 2.3599226474761963
Norm after each mp layer: 5.2754058837890625
Norm before input: 0.2552422881126404
Norm after input: 0.3534494936466217
Norm after each mp layer: 0.8520881533622742
Norm after each mp layer: 2.3760318756103516
Norm after each mp layer: 5.305624485015869
Norm before input: 0.2552422881126404
Norm after input: 0.3534494936466217
Norm after each mp layer: 0.8520881533622742
Norm after each mp layer: 2.3760318756103516
Norm after each mp layer: 5.305624485015869
Norm before input: 0.2552422881126404
Norm after input: 0.3524607717990875
Norm after each mp layer: 0.8407449126243591
Norm after each mp layer: 2.344533681869507
Norm after each mp layer: 5.277593612670898
Norm before input: 0.2552422881126404
Norm after input: 0.3524607717990875
Norm after each mp layer: 0.8407449126243591
Norm after each mp layer: 2.344533681869507
Norm after each mp layer: 5.277593612670898
Norm before input: 0.2552422881126404
Norm after input: 0.3530525863170624
Norm after each mp layer: 0.8525647521018982
Norm after each mp layer: 2.3949992656707764
Norm after each mp layer: 5.338131427764893
Norm before input: 0.2552422881126404
Norm after input: 0.3530525863170624
Norm after each mp layer: 0.8525647521018982
Norm after each mp layer: 2.3949992656707764
Norm after each mp layer: 5.338131427764893
Norm before input: 0.2552422881126404
Norm after input: 0.3525053858757019
Norm after each mp layer: 0.846526026725769
Norm after each mp layer: 2.3724019527435303
Norm after each mp layer: 5.279694080352783
Norm before input: 0.2552422881126404
Norm after input: 0.3525053858757019
Norm after each mp layer: 0.846526026725769
Norm after each mp layer: 2.3724019527435303
Norm after each mp layer: 5.279694080352783
Norm before input: 0.2552422881126404
Norm after input: 0.3522663712501526
Norm after each mp layer: 0.8402003645896912
Norm after each mp layer: 2.344431161880493
Norm after each mp layer: 5.216090202331543
Epoch: 470, Loss: 0.1026, Energy: 53.2803, Train: 99.34%, Valid: 71.20%, Test: 68.40%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3522663712501526
Norm after each mp layer: 0.8402003645896912
Norm after each mp layer: 2.344431161880493
Norm after each mp layer: 5.216090202331543
Norm before input: 0.2552422881126404
Norm after input: 0.3529312312602997
Norm after each mp layer: 0.8437565565109253
Norm after each mp layer: 2.3462698459625244
Norm after each mp layer: 5.196898460388184
Norm before input: 0.2552422881126404
Norm after input: 0.3529312312602997
Norm after each mp layer: 0.8437565565109253
Norm after each mp layer: 2.3462698459625244
Norm after each mp layer: 5.196898460388184
Norm before input: 0.2552422881126404
Norm after input: 0.3538374602794647
Norm after each mp layer: 0.8566553592681885
Norm after each mp layer: 2.392514705657959
Norm after each mp layer: 5.24879264831543
Norm before input: 0.2552422881126404
Norm after input: 0.3538374602794647
Norm after each mp layer: 0.8566553592681885
Norm after each mp layer: 2.392514705657959
Norm after each mp layer: 5.24879264831543
Norm before input: 0.2552422881126404
Norm after input: 0.35264265537261963
Norm after each mp layer: 0.8446402549743652
Norm after each mp layer: 2.3573811054229736
Norm after each mp layer: 5.194879055023193
Norm before input: 0.2552422881126404
Norm after input: 0.35264265537261963
Norm after each mp layer: 0.8446402549743652
Norm after each mp layer: 2.3573811054229736
Norm after each mp layer: 5.194879055023193
Norm before input: 0.2552422881126404
Norm after input: 0.3519977629184723
Norm after each mp layer: 0.8369026780128479
Norm after each mp layer: 2.3368468284606934
Norm after each mp layer: 5.174471378326416
Norm before input: 0.2552422881126404
Norm after input: 0.3519977629184723
Norm after each mp layer: 0.8369026780128479
Norm after each mp layer: 2.3368468284606934
Norm after each mp layer: 5.174471378326416
Norm before input: 0.2552422881126404
Norm after input: 0.3525463342666626
Norm after each mp layer: 0.8418159484863281
Norm after each mp layer: 2.3489654064178467
Norm after each mp layer: 5.185676097869873
Epoch: 475, Loss: 0.1107, Energy: 56.3618, Train: 99.42%, Valid: 71.40%, Test: 69.30%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3525463342666626
Norm after each mp layer: 0.8418159484863281
Norm after each mp layer: 2.3489654064178467
Norm after each mp layer: 5.185676097869873
Norm before input: 0.2552422881126404
Norm after input: 0.3533126413822174
Norm after each mp layer: 0.8506308197975159
Norm after each mp layer: 2.379002571105957
Norm after each mp layer: 5.225760459899902
Norm before input: 0.2552422881126404
Norm after input: 0.3533126413822174
Norm after each mp layer: 0.8506308197975159
Norm after each mp layer: 2.379002571105957
Norm after each mp layer: 5.225760459899902
Norm before input: 0.2552422881126404
Norm after input: 0.35289567708969116
Norm after each mp layer: 0.8488843441009521
Norm after each mp layer: 2.3789381980895996
Norm after each mp layer: 5.224673748016357
Norm before input: 0.2552422881126404
Norm after input: 0.35289567708969116
Norm after each mp layer: 0.8488843441009521
Norm after each mp layer: 2.3789381980895996
Norm after each mp layer: 5.224673748016357
Norm before input: 0.2552422881126404
Norm after input: 0.35270145535469055
Norm after each mp layer: 0.8424180150032043
Norm after each mp layer: 2.3510901927948
Norm after each mp layer: 5.184071063995361
Norm before input: 0.2552422881126404
Norm after input: 0.35270145535469055
Norm after each mp layer: 0.8424180150032043
Norm after each mp layer: 2.3510901927948
Norm after each mp layer: 5.184071063995361
Norm before input: 0.2552422881126404
Norm after input: 0.3534698188304901
Norm after each mp layer: 0.8497107028961182
Norm after each mp layer: 2.3747878074645996
Norm after each mp layer: 5.202860355377197
Norm before input: 0.2552422881126404
Norm after input: 0.3534698188304901
Norm after each mp layer: 0.8497107028961182
Norm after each mp layer: 2.3747878074645996
Norm after each mp layer: 5.202860355377197
Norm before input: 0.2552422881126404
Norm after input: 0.35199642181396484
Norm after each mp layer: 0.8414157032966614
Norm after each mp layer: 2.36496639251709
Norm after each mp layer: 5.186411380767822
Epoch: 480, Loss: 0.1085, Energy: 49.8623, Train: 99.09%, Valid: 70.00%, Test: 67.90%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35199642181396484
Norm after each mp layer: 0.8414157032966614
Norm after each mp layer: 2.36496639251709
Norm after each mp layer: 5.186411380767822
Norm before input: 0.2552422881126404
Norm after input: 0.35152918100357056
Norm after each mp layer: 0.8366268277168274
Norm after each mp layer: 2.3512887954711914
Norm after each mp layer: 5.167991638183594
Norm before input: 0.2552422881126404
Norm after input: 0.35152918100357056
Norm after each mp layer: 0.8366268277168274
Norm after each mp layer: 2.3512887954711914
Norm after each mp layer: 5.167991638183594
Norm before input: 0.2552422881126404
Norm after input: 0.3530929386615753
Norm after each mp layer: 0.8469023704528809
Norm after each mp layer: 2.369676113128662
Norm after each mp layer: 5.178571701049805
Norm before input: 0.2552422881126404
Norm after input: 0.3530929386615753
Norm after each mp layer: 0.8469023704528809
Norm after each mp layer: 2.369676113128662
Norm after each mp layer: 5.178571701049805
Norm before input: 0.2552422881126404
Norm after input: 0.3544110953807831
Norm after each mp layer: 0.856567919254303
Norm after each mp layer: 2.391679525375366
Norm after each mp layer: 5.220444679260254
Norm before input: 0.2552422881126404
Norm after input: 0.3544110953807831
Norm after each mp layer: 0.856567919254303
Norm after each mp layer: 2.391679525375366
Norm after each mp layer: 5.220444679260254
Norm before input: 0.2552422881126404
Norm after input: 0.35269638895988464
Norm after each mp layer: 0.8459367752075195
Norm after each mp layer: 2.379711866378784
Norm after each mp layer: 5.223013877868652
Norm before input: 0.2552422881126404
Norm after input: 0.35269638895988464
Norm after each mp layer: 0.8459367752075195
Norm after each mp layer: 2.379711866378784
Norm after each mp layer: 5.223013401031494
Norm before input: 0.2552422881126404
Norm after input: 0.3522157669067383
Norm after each mp layer: 0.8466132879257202
Norm after each mp layer: 2.3967769145965576
Norm after each mp layer: 5.252540111541748
Epoch: 485, Loss: 0.1108, Energy: 49.9574, Train: 99.25%, Valid: 71.40%, Test: 69.60%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.3522157669067383
Norm after each mp layer: 0.8466132879257202
Norm after each mp layer: 2.3967769145965576
Norm after each mp layer: 5.252540111541748
Norm before input: 0.2552422881126404
Norm after input: 0.3528136610984802
Norm after each mp layer: 0.8488419651985168
Norm after each mp layer: 2.3951048851013184
Norm after each mp layer: 5.247699737548828
Norm before input: 0.2552422881126404
Norm after input: 0.3528136610984802
Norm after each mp layer: 0.8488419651985168
Norm after each mp layer: 2.3951048851013184
Norm after each mp layer: 5.247699737548828
Norm before input: 0.2552422881126404
Norm after input: 0.3530203402042389
Norm after each mp layer: 0.8463659286499023
Norm after each mp layer: 2.37312912940979
Norm after each mp layer: 5.194827556610107
Norm before input: 0.2552422881126404
Norm after input: 0.3530203402042389
Norm after each mp layer: 0.8463659286499023
Norm after each mp layer: 2.37312912940979
Norm after each mp layer: 5.194827556610107
Norm before input: 0.2552422881126404
Norm after input: 0.35247287154197693
Norm after each mp layer: 0.8449209332466125
Norm after each mp layer: 2.375058174133301
Norm after each mp layer: 5.177927494049072
Norm before input: 0.2552422881126404
Norm after input: 0.35247287154197693
Norm after each mp layer: 0.8449209332466125
Norm after each mp layer: 2.375058174133301
Norm after each mp layer: 5.177927494049072
Norm before input: 0.2552422881126404
Norm after input: 0.35289737582206726
Norm after each mp layer: 0.8480233550071716
Norm after each mp layer: 2.377958059310913
Norm after each mp layer: 5.1703386306762695
Norm before input: 0.2552422881126404
Norm after input: 0.35289737582206726
Norm after each mp layer: 0.8480233550071716
Norm after each mp layer: 2.377958059310913
Norm after each mp layer: 5.1703386306762695
Norm before input: 0.2552422881126404
Norm after input: 0.35389795899391174
Norm after each mp layer: 0.8480406403541565
Norm after each mp layer: 2.3553669452667236
Norm after each mp layer: 5.127740383148193
Epoch: 490, Loss: 0.1099, Energy: 46.0187, Train: 99.17%, Valid: 70.40%, Test: 69.30%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35389795899391174
Norm after each mp layer: 0.8480406403541565
Norm after each mp layer: 2.3553669452667236
Norm after each mp layer: 5.127740383148193
Norm before input: 0.2552422881126404
Norm after input: 0.3521193563938141
Norm after each mp layer: 0.845769464969635
Norm after each mp layer: 2.38865327835083
Norm after each mp layer: 5.186728000640869
Norm before input: 0.2552422881126404
Norm after input: 0.3521193563938141
Norm after each mp layer: 0.845769464969635
Norm after each mp layer: 2.38865327835083
Norm after each mp layer: 5.186728000640869
Norm before input: 0.2552422881126404
Norm after input: 0.35162103176116943
Norm after each mp layer: 0.8424612879753113
Norm after each mp layer: 2.383927583694458
Norm after each mp layer: 5.18798303604126
Norm before input: 0.2552422881126404
Norm after input: 0.35162103176116943
Norm after each mp layer: 0.8424612879753113
Norm after each mp layer: 2.383927583694458
Norm after each mp layer: 5.18798303604126
Norm before input: 0.2552422881126404
Norm after input: 0.3532993793487549
Norm after each mp layer: 0.8452285528182983
Norm after each mp layer: 2.358614683151245
Norm after each mp layer: 5.1357903480529785
Norm before input: 0.2552422881126404
Norm after input: 0.3532993793487549
Norm after each mp layer: 0.8452285528182983
Norm after each mp layer: 2.358614683151245
Norm after each mp layer: 5.1357903480529785
Norm before input: 0.2552422881126404
Norm after input: 0.3546845316886902
Norm after each mp layer: 0.8502728939056396
Norm after each mp layer: 2.353867769241333
Norm after each mp layer: 5.131308078765869
Norm before input: 0.2552422881126404
Norm after input: 0.3546845316886902
Norm after each mp layer: 0.8502728939056396
Norm after each mp layer: 2.353867530822754
Norm after each mp layer: 5.131308078765869
Norm before input: 0.2552422881126404
Norm after input: 0.35227787494659424
Norm after each mp layer: 0.8540341854095459
Norm after each mp layer: 2.4363040924072266
Norm after each mp layer: 5.267899990081787
Epoch: 495, Loss: 0.1211, Energy: 44.9768, Train: 98.26%, Valid: 70.60%, Test: 69.70%, Best Valid: 80.00%, Best Test: 77.30%
Norm before input: 0.2552422881126404
Norm after input: 0.35227787494659424
Norm after each mp layer: 0.8540341854095459
Norm after each mp layer: 2.4363040924072266
Norm after each mp layer: 5.267899990081787
Norm before input: 0.2552422881126404
Norm after input: 0.3524741530418396
Norm after each mp layer: 0.8465136289596558
Norm after each mp layer: 2.395280599594116
Norm after each mp layer: 5.184394359588623
Norm before input: 0.2552422881126404
Norm after input: 0.3524741530418396
Norm after each mp layer: 0.8465136289596558
Norm after each mp layer: 2.395280599594116
Norm after each mp layer: 5.184394359588623
Norm before input: 0.2552422881126404
Norm after input: 0.35404840111732483
Norm after each mp layer: 0.8456205129623413
Norm after each mp layer: 2.35514497756958
Norm after each mp layer: 5.1268110275268555
Norm before input: 0.2552422881126404
Norm after input: 0.35404840111732483
Norm after each mp layer: 0.8456205129623413
Norm after each mp layer: 2.35514497756958
Norm after each mp layer: 5.1268110275268555
Norm before input: 0.2552422881126404
Norm after input: 0.3524641692638397
Norm after each mp layer: 0.8522292971611023
Norm after each mp layer: 2.4321584701538086
Norm after each mp layer: 5.200160503387451
Norm before input: 0.2552422881126404
Norm after input: 0.3524641692638397
Norm after each mp layer: 0.8522292971611023
Norm after each mp layer: 2.4321584701538086
Norm after each mp layer: 5.200160503387451
Norm before input: 0.2552422881126404
Norm after input: 0.35242292284965515
Norm after each mp layer: 0.8581889271736145
Norm after each mp layer: 2.4632577896118164
Norm after each mp layer: 5.212405681610107
train_accuracy_list: [0.28228476821192056, 0.0, 0.28228476821192056, 0.28228476821192056, 0.2806291390728477, 0.28228476821192056, 0.28228476821192056, 0.28228476821192056, 0.10678807947019868, 0.21771523178807947, 0.28228476821192056, 0.28228476821192056, 0.28228476821192056, 0.28228476821192056, 0.27980132450331124, 0.28228476821192056, 0.28228476821192056, 0.28228476821192056, 0.28228476821192056, 0.2847682119205298, 0.28228476821192056, 0.28228476821192056, 0.28394039735099336, 0.2855960264900662, 0.28311258278145696, 0.28228476821192056, 0.28394039735099336, 0.2814569536423841, 0.28394039735099336, 0.2814569536423841, 0.2814569536423841, 0.28394039735099336, 0.28394039735099336, 0.2814569536423841, 0.28228476821192056, 0.28228476821192056, 0.28394039735099336, 0.28807947019867547, 0.29387417218543044, 0.3038079470198676, 0.31788079470198677, 0.3336092715231788, 0.3360927152317881, 0.3344370860927152, 0.3294701986754967, 0.34271523178807944, 0.36258278145695366, 0.3716887417218543, 0.3658940397350993, 0.3675496688741722, 0.4197019867549669, 0.4519867549668874, 0.4519867549668874, 0.44867549668874174, 0.4586092715231788, 0.45695364238410596, 0.4644039735099338, 0.4685430463576159, 0.4718543046357616, 0.46605960264900664, 0.48344370860927155, 0.48013245033112584, 0.4908940397350993, 0.4991721854304636, 0.4925496688741722, 0.4859271523178808, 0.5082781456953642, 0.46274834437086093, 0.5049668874172185, 0.5033112582781457, 0.5264900662251656, 0.5182119205298014, 0.5322847682119205, 0.5413907284768212, 0.5173841059602649, 0.5480132450331126, 0.5463576158940397, 0.5364238410596026, 0.554635761589404, 0.5587748344370861, 0.5695364238410596, 0.5554635761589404, 0.5769867549668874, 0.5720198675496688, 0.5728476821192053, 0.5844370860927153, 0.5745033112582781, 0.5869205298013245, 0.5902317880794702, 0.581953642384106, 0.5927152317880795, 0.5852649006622517, 0.6026490066225165, 0.6084437086092715, 0.6034768211920529, 0.6324503311258278, 0.6324503311258278, 0.6390728476821192, 0.6672185430463576, 0.6663907284768212, 0.6829470198675497, 0.7086092715231788, 0.7160596026490066, 0.7326158940397351, 0.7458609271523179, 0.7632450331125827, 0.7723509933774835, 0.7806291390728477, 0.7922185430463576, 0.7971854304635762, 0.8004966887417219, 0.8087748344370861, 0.8087748344370861, 0.8120860927152318, 0.8178807947019867, 0.8195364238410596, 0.8220198675496688, 0.8245033112582781, 0.8294701986754967, 0.8294701986754967, 0.8311258278145696, 0.8352649006622517, 0.8394039735099338, 0.8451986754966887, 0.8485099337748344, 0.8543046357615894, 0.8559602649006622, 0.8584437086092715, 0.8567880794701986, 0.859271523178808, 0.8609271523178808, 0.8683774834437086, 0.8725165562913907, 0.8807947019867549, 0.8849337748344371, 0.9014900662251656, 0.8981788079470199, 0.9056291390728477, 0.8923841059602649, 0.8576158940397351, 0.8013245033112583, 0.8377483443708609, 0.8899006622516556, 0.8385761589403974, 0.8178807947019867, 0.8617549668874173, 0.8849337748344371, 0.8783112582781457, 0.8899006622516556, 0.9130794701986755, 0.9081125827814569, 0.9147350993377483, 0.9155629139072847, 0.9213576158940397, 0.9188741721854304, 0.9122516556291391, 0.9163907284768212, 0.9230132450331126, 0.9205298013245033, 0.9263245033112583, 0.9279801324503312, 0.9254966887417219, 0.9188741721854304, 0.9188741721854304, 0.9271523178807947, 0.9304635761589404, 0.9271523178807947, 0.9246688741721855, 0.9337748344370861, 0.9387417218543046, 0.9370860927152318, 0.9370860927152318, 0.9387417218543046, 0.9395695364238411, 0.9420529801324503, 0.945364238410596, 0.9428807947019867, 0.945364238410596, 0.9478476821192053, 0.9445364238410596, 0.9437086092715232, 0.9437086092715232, 0.9470198675496688, 0.9470198675496688, 0.9478476821192053, 0.9478476821192053, 0.9478476821192053, 0.9495033112582781, 0.9503311258278145, 0.9503311258278145, 0.9503311258278145, 0.9528145695364238, 0.9511589403973509, 0.9536423841059603, 0.9536423841059603, 0.9544701986754967, 0.9536423841059603, 0.9528145695364238, 0.9552980132450332, 0.9577814569536424, 0.9561258278145696, 0.9577814569536424, 0.9594370860927153, 0.9602649006622517, 0.9610927152317881, 0.9619205298013245, 0.9610927152317881, 0.9619205298013245, 0.9619205298013245, 0.9635761589403974, 0.9627483443708609, 0.9635761589403974, 0.9644039735099338, 0.9644039735099338, 0.9644039735099338, 0.9652317880794702, 0.9660596026490066, 0.9677152317880795, 0.9677152317880795, 0.9677152317880795, 0.9693708609271523, 0.9693708609271523, 0.9718543046357616, 0.972682119205298, 0.972682119205298, 0.972682119205298, 0.9751655629139073, 0.972682119205298, 0.9743377483443708, 0.9288079470198676, 0.8617549668874173, 0.9031456953642384, 0.9139072847682119, 0.9470198675496688, 0.8435430463576159, 0.8683774834437086, 0.9370860927152318, 0.9155629139072847, 0.8857615894039735, 0.9114238410596026, 0.9221854304635762, 0.9321192052980133, 0.9387417218543046, 0.9312913907284768, 0.9279801324503312, 0.9329470198675497, 0.9445364238410596, 0.9503311258278145, 0.9528145695364238, 0.9594370860927153, 0.9610927152317881, 0.9627483443708609, 0.9644039735099338, 0.9602649006622517, 0.9619205298013245, 0.9586092715231788, 0.9602649006622517, 0.9660596026490066, 0.9644039735099338, 0.9644039735099338, 0.9660596026490066, 0.9652317880794702, 0.9685430463576159, 0.9685430463576159, 0.9660596026490066, 0.9660596026490066, 0.9635761589403974, 0.9660596026490066, 0.9743377483443708, 0.972682119205298, 0.972682119205298, 0.9759933774834437, 0.9768211920529801, 0.9768211920529801, 0.9759933774834437, 0.9776490066225165, 0.9784768211920529, 0.9784768211920529, 0.9801324503311258, 0.9801324503311258, 0.9784768211920529, 0.9768211920529801, 0.9793046357615894, 0.9801324503311258, 0.9768211920529801, 0.9793046357615894, 0.9776490066225165, 0.9801324503311258, 0.9809602649006622, 0.9809602649006622, 0.9801324503311258, 0.9809602649006622, 0.9826158940397351, 0.9801324503311258, 0.9817880794701986, 0.9801324503311258, 0.9817880794701986, 0.9776490066225165, 0.9801324503311258, 0.9809602649006622, 0.9809602649006622, 0.9809602649006622, 0.9817880794701986, 0.9801324503311258, 0.9784768211920529, 0.9809602649006622, 0.9685430463576159, 0.9826158940397351, 0.9718543046357616, 0.9718543046357616, 0.9759933774834437, 0.9793046357615894, 0.9718543046357616, 0.9793046357615894, 0.9710264900662252, 0.9685430463576159, 0.9759933774834437, 0.9793046357615894, 0.9751655629139073, 0.9809602649006622, 0.9735099337748344, 0.9817880794701986, 0.9768211920529801, 0.9817880794701986, 0.9759933774834437, 0.9743377483443708, 0.9685430463576159, 0.9817880794701986, 0.9817880794701986, 0.9784768211920529, 0.9776490066225165, 0.9834437086092715, 0.9743377483443708, 0.9817880794701986, 0.9759933774834437, 0.9834437086092715, 0.9859271523178808, 0.9859271523178808, 0.9867549668874173, 0.984271523178808, 0.9793046357615894, 0.9850993377483444, 0.9834437086092715, 0.9834437086092715, 0.9859271523178808, 0.9834437086092715, 0.9850993377483444, 0.9817880794701986, 0.984271523178808, 0.9826158940397351, 0.9875827814569537, 0.9834437086092715, 0.9867549668874173, 0.9850993377483444, 0.9859271523178808, 0.9875827814569537, 0.9884105960264901, 0.9875827814569537, 0.9884105960264901, 0.9875827814569537, 0.9850993377483444, 0.984271523178808, 0.9867549668874173, 0.9859271523178808, 0.9884105960264901, 0.9834437086092715, 0.9884105960264901, 0.9892384105960265, 0.9884105960264901, 0.9867549668874173, 0.9875827814569537, 0.9859271523178808, 0.9859271523178808, 0.9900662251655629, 0.9875827814569537, 0.9859271523178808, 0.9875827814569537, 0.9867549668874173, 0.984271523178808, 0.9875827814569537, 0.9908940397350994, 0.9875827814569537, 0.9859271523178808, 0.9850993377483444, 0.9875827814569537, 0.9859271523178808, 0.984271523178808, 0.9850993377483444, 0.9884105960264901, 0.9900662251655629, 0.9859271523178808, 0.9850993377483444, 0.9917218543046358, 0.9908940397350994, 0.9892384105960265, 0.9850993377483444, 0.9884105960264901, 0.9859271523178808, 0.9867549668874173, 0.9809602649006622, 0.9850993377483444, 0.9900662251655629, 0.9826158940397351, 0.9900662251655629, 0.9850993377483444, 0.9875827814569537, 0.9917218543046358, 0.9908940397350994, 0.9867549668874173, 0.9884105960264901, 0.9892384105960265, 0.984271523178808, 0.9859271523178808, 0.9900662251655629, 0.9875827814569537, 0.9875827814569537, 0.9875827814569537, 0.9875827814569537, 0.9875827814569537, 0.9925496688741722, 0.9892384105960265, 0.9892384105960265, 0.9925496688741722, 0.9908940397350994, 0.9900662251655629, 0.9892384105960265, 0.9908940397350994, 0.9917218543046358, 0.9900662251655629, 0.9850993377483444, 0.9867549668874173, 0.984271523178808, 0.9776490066225165, 0.9867549668874173, 0.9751655629139073, 0.9925496688741722, 0.9850993377483444, 0.9867549668874173, 0.9884105960264901, 0.9925496688741722, 0.9850993377483444, 0.9850993377483444, 0.9900662251655629, 0.9867549668874173, 0.9908940397350994, 0.9892384105960265, 0.9892384105960265, 0.9908940397350994, 0.9892384105960265, 0.9908940397350994, 0.9917218543046358, 0.9908940397350994, 0.9892384105960265, 0.9892384105960265, 0.9908940397350994, 0.9875827814569537, 0.9892384105960265, 0.9900662251655629, 0.9933774834437086, 0.9917218543046358, 0.9917218543046358, 0.9917218543046358, 0.9933774834437086, 0.9933774834437086, 0.9925496688741722, 0.9933774834437086, 0.9908940397350994, 0.9884105960264901, 0.9917218543046358, 0.9933774834437086, 0.9917218543046358, 0.9892384105960265, 0.9933774834437086, 0.9925496688741722, 0.9942052980132451, 0.9908940397350994, 0.9900662251655629, 0.9917218543046358, 0.9908940397350994, 0.9908940397350994, 0.9892384105960265, 0.9933774834437086, 0.9908940397350994, 0.9925496688741722, 0.9925496688741722, 0.9900662251655629, 0.9933774834437086, 0.9942052980132451, 0.9908940397350994, 0.9917218543046358, 0.9925496688741722, 0.9892384105960265, 0.9950331125827815, 0.9917218543046358, 0.9826158940397351, 0.9917218543046358, 0.9892384105960265, 0.9908940397350994, 0.9884105960264901]
valid_accuracy_list: [0.316, 0.0, 0.316, 0.316, 0.316, 0.316, 0.316, 0.316, 0.082, 0.258, 0.316, 0.316, 0.316, 0.312, 0.308, 0.316, 0.316, 0.316, 0.316, 0.316, 0.316, 0.316, 0.316, 0.318, 0.316, 0.316, 0.318, 0.316, 0.316, 0.316, 0.316, 0.316, 0.318, 0.316, 0.316, 0.316, 0.318, 0.32, 0.322, 0.332, 0.342, 0.354, 0.354, 0.354, 0.35, 0.36, 0.384, 0.39, 0.384, 0.386, 0.428, 0.468, 0.472, 0.47, 0.462, 0.464, 0.46, 0.45, 0.45, 0.448, 0.446, 0.446, 0.458, 0.474, 0.474, 0.452, 0.474, 0.408, 0.468, 0.446, 0.476, 0.456, 0.466, 0.466, 0.454, 0.466, 0.464, 0.476, 0.488, 0.492, 0.494, 0.486, 0.498, 0.498, 0.498, 0.502, 0.5, 0.502, 0.496, 0.496, 0.498, 0.504, 0.512, 0.518, 0.51, 0.53, 0.536, 0.528, 0.566, 0.56, 0.586, 0.606, 0.598, 0.616, 0.644, 0.636, 0.654, 0.662, 0.668, 0.674, 0.682, 0.684, 0.692, 0.7, 0.702, 0.702, 0.704, 0.706, 0.706, 0.714, 0.704, 0.706, 0.706, 0.708, 0.706, 0.712, 0.706, 0.71, 0.708, 0.704, 0.704, 0.704, 0.704, 0.71, 0.708, 0.71, 0.712, 0.708, 0.714, 0.706, 0.628, 0.682, 0.726, 0.684, 0.656, 0.684, 0.732, 0.754, 0.768, 0.77, 0.78, 0.768, 0.772, 0.79, 0.788, 0.776, 0.8, 0.778, 0.77, 0.78, 0.772, 0.772, 0.754, 0.754, 0.756, 0.768, 0.76, 0.768, 0.752, 0.752, 0.75, 0.744, 0.744, 0.744, 0.742, 0.742, 0.744, 0.74, 0.74, 0.74, 0.74, 0.738, 0.738, 0.736, 0.734, 0.736, 0.74, 0.728, 0.726, 0.74, 0.732, 0.73, 0.734, 0.732, 0.728, 0.732, 0.728, 0.728, 0.732, 0.726, 0.724, 0.728, 0.724, 0.726, 0.728, 0.724, 0.72, 0.72, 0.724, 0.718, 0.72, 0.72, 0.722, 0.72, 0.722, 0.724, 0.724, 0.728, 0.722, 0.728, 0.728, 0.732, 0.73, 0.73, 0.732, 0.734, 0.73, 0.734, 0.734, 0.706, 0.624, 0.678, 0.69, 0.696, 0.606, 0.642, 0.68, 0.688, 0.676, 0.692, 0.7, 0.698, 0.7, 0.698, 0.7, 0.718, 0.724, 0.728, 0.734, 0.744, 0.744, 0.74, 0.736, 0.732, 0.728, 0.72, 0.728, 0.724, 0.726, 0.728, 0.728, 0.728, 0.734, 0.73, 0.726, 0.728, 0.732, 0.732, 0.726, 0.724, 0.722, 0.732, 0.732, 0.732, 0.738, 0.74, 0.728, 0.726, 0.732, 0.732, 0.728, 0.73, 0.73, 0.726, 0.73, 0.726, 0.726, 0.726, 0.732, 0.726, 0.718, 0.722, 0.726, 0.73, 0.722, 0.726, 0.724, 0.714, 0.724, 0.724, 0.726, 0.712, 0.724, 0.726, 0.712, 0.722, 0.72, 0.724, 0.706, 0.718, 0.732, 0.726, 0.714, 0.74, 0.736, 0.718, 0.73, 0.728, 0.728, 0.738, 0.744, 0.736, 0.72, 0.736, 0.74, 0.724, 0.722, 0.736, 0.738, 0.734, 0.738, 0.742, 0.734, 0.736, 0.728, 0.734, 0.74, 0.73, 0.74, 0.744, 0.742, 0.744, 0.738, 0.728, 0.732, 0.73, 0.732, 0.74, 0.722, 0.724, 0.726, 0.73, 0.732, 0.724, 0.728, 0.724, 0.734, 0.728, 0.726, 0.722, 0.716, 0.718, 0.728, 0.718, 0.724, 0.718, 0.72, 0.714, 0.716, 0.73, 0.714, 0.722, 0.722, 0.72, 0.714, 0.728, 0.712, 0.714, 0.724, 0.726, 0.722, 0.73, 0.72, 0.722, 0.72, 0.718, 0.72, 0.722, 0.714, 0.714, 0.73, 0.724, 0.722, 0.718, 0.722, 0.726, 0.732, 0.708, 0.72, 0.706, 0.714, 0.72, 0.702, 0.726, 0.732, 0.724, 0.716, 0.716, 0.73, 0.714, 0.718, 0.718, 0.716, 0.71, 0.728, 0.712, 0.714, 0.714, 0.72, 0.716, 0.714, 0.724, 0.72, 0.716, 0.716, 0.718, 0.712, 0.722, 0.718, 0.706, 0.72, 0.706, 0.706, 0.716, 0.722, 0.73, 0.72, 0.734, 0.732, 0.71, 0.716, 0.732, 0.716, 0.712, 0.724, 0.742, 0.73, 0.728, 0.72, 0.716, 0.724, 0.72, 0.724, 0.716, 0.716, 0.712, 0.708, 0.716, 0.72, 0.712, 0.718, 0.716, 0.714, 0.714, 0.712, 0.72, 0.712, 0.71, 0.71, 0.712, 0.718, 0.722, 0.722, 0.708, 0.714, 0.724, 0.722, 0.71, 0.716, 0.7, 0.706, 0.714, 0.722, 0.716, 0.714, 0.708, 0.704, 0.714, 0.712, 0.704, 0.704, 0.702, 0.712, 0.704, 0.706, 0.716, 0.712, 0.724, 0.726]
test_accuracy_list: [0.319, 0.0, 0.319, 0.319, 0.318, 0.319, 0.319, 0.319, 0.107, 0.238, 0.319, 0.319, 0.319, 0.322, 0.318, 0.319, 0.319, 0.319, 0.319, 0.319, 0.319, 0.319, 0.32, 0.32, 0.32, 0.319, 0.32, 0.323, 0.319, 0.32, 0.32, 0.318, 0.319, 0.32, 0.32, 0.321, 0.321, 0.324, 0.325, 0.335, 0.343, 0.356, 0.359, 0.357, 0.353, 0.364, 0.38, 0.386, 0.378, 0.377, 0.425, 0.451, 0.458, 0.452, 0.468, 0.466, 0.466, 0.469, 0.452, 0.459, 0.448, 0.456, 0.46, 0.466, 0.462, 0.446, 0.464, 0.423, 0.457, 0.44, 0.458, 0.461, 0.455, 0.46, 0.46, 0.468, 0.467, 0.472, 0.481, 0.478, 0.491, 0.488, 0.495, 0.495, 0.497, 0.504, 0.494, 0.505, 0.496, 0.497, 0.507, 0.498, 0.506, 0.518, 0.507, 0.532, 0.536, 0.534, 0.559, 0.565, 0.577, 0.6, 0.6, 0.621, 0.629, 0.636, 0.644, 0.657, 0.661, 0.669, 0.679, 0.685, 0.681, 0.684, 0.69, 0.687, 0.689, 0.689, 0.689, 0.69, 0.686, 0.689, 0.691, 0.69, 0.689, 0.689, 0.689, 0.688, 0.694, 0.694, 0.695, 0.689, 0.691, 0.695, 0.696, 0.7, 0.695, 0.71, 0.694, 0.702, 0.623, 0.692, 0.709, 0.658, 0.641, 0.683, 0.715, 0.744, 0.757, 0.769, 0.755, 0.751, 0.767, 0.773, 0.764, 0.761, 0.773, 0.762, 0.76, 0.756, 0.758, 0.773, 0.762, 0.759, 0.764, 0.765, 0.759, 0.753, 0.763, 0.76, 0.761, 0.76, 0.754, 0.751, 0.752, 0.76, 0.751, 0.746, 0.74, 0.741, 0.743, 0.748, 0.749, 0.743, 0.739, 0.737, 0.743, 0.74, 0.74, 0.739, 0.73, 0.726, 0.73, 0.731, 0.728, 0.73, 0.725, 0.725, 0.732, 0.726, 0.725, 0.728, 0.726, 0.728, 0.731, 0.726, 0.724, 0.722, 0.717, 0.716, 0.713, 0.712, 0.709, 0.711, 0.711, 0.711, 0.712, 0.711, 0.711, 0.711, 0.711, 0.708, 0.71, 0.709, 0.707, 0.707, 0.705, 0.703, 0.704, 0.676, 0.624, 0.666, 0.674, 0.684, 0.606, 0.629, 0.679, 0.673, 0.661, 0.67, 0.673, 0.68, 0.686, 0.69, 0.695, 0.71, 0.717, 0.727, 0.733, 0.736, 0.741, 0.734, 0.726, 0.72, 0.713, 0.715, 0.718, 0.72, 0.721, 0.725, 0.721, 0.723, 0.729, 0.714, 0.719, 0.72, 0.714, 0.713, 0.722, 0.721, 0.722, 0.721, 0.723, 0.723, 0.718, 0.715, 0.721, 0.718, 0.714, 0.713, 0.718, 0.721, 0.714, 0.717, 0.71, 0.716, 0.716, 0.712, 0.706, 0.711, 0.703, 0.711, 0.715, 0.702, 0.704, 0.704, 0.703, 0.705, 0.693, 0.701, 0.703, 0.694, 0.708, 0.712, 0.689, 0.709, 0.706, 0.706, 0.695, 0.702, 0.72, 0.7, 0.687, 0.715, 0.722, 0.701, 0.708, 0.71, 0.703, 0.727, 0.728, 0.71, 0.698, 0.718, 0.729, 0.721, 0.708, 0.726, 0.725, 0.728, 0.726, 0.726, 0.72, 0.717, 0.716, 0.73, 0.718, 0.714, 0.718, 0.727, 0.727, 0.721, 0.717, 0.71, 0.716, 0.722, 0.716, 0.711, 0.711, 0.711, 0.711, 0.706, 0.709, 0.708, 0.703, 0.701, 0.709, 0.702, 0.701, 0.703, 0.699, 0.704, 0.707, 0.692, 0.704, 0.705, 0.705, 0.694, 0.692, 0.698, 0.692, 0.698, 0.699, 0.691, 0.691, 0.707, 0.687, 0.69, 0.71, 0.708, 0.694, 0.707, 0.711, 0.702, 0.693, 0.692, 0.692, 0.702, 0.687, 0.687, 0.702, 0.702, 0.698, 0.698, 0.704, 0.706, 0.706, 0.69, 0.697, 0.699, 0.698, 0.701, 0.683, 0.695, 0.702, 0.704, 0.693, 0.693, 0.704, 0.688, 0.704, 0.702, 0.702, 0.695, 0.709, 0.693, 0.701, 0.707, 0.707, 0.703, 0.687, 0.704, 0.694, 0.698, 0.699, 0.7, 0.696, 0.703, 0.699, 0.691, 0.695, 0.686, 0.699, 0.692, 0.698, 0.704, 0.699, 0.705, 0.707, 0.699, 0.693, 0.713, 0.699, 0.693, 0.703, 0.717, 0.706, 0.709, 0.715, 0.708, 0.699, 0.7, 0.701, 0.69, 0.699, 0.701, 0.698, 0.694, 0.707, 0.693, 0.7, 0.701, 0.698, 0.7, 0.701, 0.703, 0.686, 0.697, 0.692, 0.684, 0.69, 0.701, 0.693, 0.679, 0.693, 0.699, 0.695, 0.686, 0.691, 0.679, 0.674, 0.692, 0.7, 0.689, 0.696, 0.697, 0.695, 0.689, 0.688, 0.693, 0.69, 0.69, 0.696, 0.687, 0.697, 0.699, 0.678, 0.704, 0.708]
best validation: 0.8
best test: 0.773
